<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "JmI6QmP3fMkLet6B",ck: "JmI6QmP3fMkLet6B"})</script>

  <link rel="apple-touch-icon" sizes="180x180" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/imgs/favicon.ico">
  <link rel="mask-icon" href="/imgs/favicon.ico" color="#222">
  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wulc.me","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文转载自 http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jasonfreak&#x2F;p&#x2F;5448385.html ,在某些部分会拓展补充一些内容，全文主要讲述有关特征工程中通常使用的方法以及在sklearn中的相关实现。">
<meta property="og:type" content="article">
<meta property="og:title" content="使用sklearn做单机特征工程">
<meta property="og:url" content="https://wulc.me/2017/01/03/%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="本文转载自 http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jasonfreak&#x2F;p&#x2F;5448385.html ,在某些部分会拓展补充一些内容，全文主要讲述有关特征工程中通常使用的方法以及在sklearn中的相关实现。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wulc.me/imgs/image_1b99tlsf81c6n5iicv3otetoa9.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1b99vqj1v1cffks1m321n8k1lalm.png">
<meta property="article:published_time" content="2017-01-03T12:15:44.000Z">
<meta property="article:modified_time" content="2023-07-16T15:28:28.152Z">
<meta property="article:author" content="良超">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wulc.me/imgs/image_1b99tlsf81c6n5iicv3otetoa9.png">


<link rel="canonical" href="https://wulc.me/2017/01/03/%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://wulc.me/2017/01/03/%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","path":"2017/01/03/使用sklearn做单机特征工程/","title":"使用sklearn做单机特征工程"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>使用sklearn做单机特征工程 | 吴良超的学习笔记</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="吴良超的学习笔记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">吴良超的学习笔记</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.</span> <span class="nav-text">特征工程是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E9%87%8F%E7%BA%B2%E5%8C%96"><span class="nav-number">2.1.</span> <span class="nav-text">无量纲化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-number">2.1.1.</span> <span class="nav-text">标准化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">2.1.2.</span> <span class="nav-text">归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%AE%9A%E9%87%8F%E7%89%B9%E5%BE%81%E4%BA%8C%E5%80%BC%E5%8C%96"><span class="nav-number">2.2.</span> <span class="nav-text">对定量特征二值化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%AE%9A%E6%80%A7%E7%89%B9%E5%BE%81%E5%93%91%E7%BC%96%E7%A0%81"><span class="nav-number">2.3.</span> <span class="nav-text">对定性特征哑编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E8%AE%A1%E7%AE%97"><span class="nav-number">2.4.</span> <span class="nav-text">缺失值计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%98%E6%8D%A2"><span class="nav-number">2.5.</span> <span class="nav-text">数据变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">2.6.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">3.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#filter"><span class="nav-number">3.1.</span> <span class="nav-text">Filter</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE%E9%80%89%E6%8B%A9%E6%B3%95"><span class="nav-number">3.1.1.</span> <span class="nav-text">方差选择法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E6%B3%95"><span class="nav-number">3.1.2.</span> <span class="nav-text">相关系数法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C"><span class="nav-number">3.1.3.</span> <span class="nav-text">卡方检验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AF%E6%B3%95"><span class="nav-number">3.1.4.</span> <span class="nav-text">互信息法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wrapper"><span class="nav-number">3.2.</span> <span class="nav-text">Wrapper</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E5%BD%92%E7%89%B9%E5%BE%81%E6%B6%88%E9%99%A4%E6%B3%95"><span class="nav-number">3.2.1.</span> <span class="nav-text">递归特征消除法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedded"><span class="nav-number">3.3.</span> <span class="nav-text">Embedded</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%83%A9%E7%BD%9A%E9%A1%B9%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%B3%95"><span class="nav-number">3.3.1.</span> <span class="nav-text">基于惩罚项的特征选择法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%B3%95"><span class="nav-number">3.3.2.</span> <span class="nav-text">基于树模型的特征选择法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-number">3.3.3.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%8D%E7%BB%B4"><span class="nav-number">4.</span> <span class="nav-text">降维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95pca"><span class="nav-number">4.1.</span> <span class="nav-text">主成分分析法（PCA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90%E6%B3%95lda"><span class="nav-number">4.2.</span> <span class="nav-text">线性判别分析法（LDA）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="nav-number">4.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="良超"
      src="/files/profile.jpg">
  <p class="site-author-name" itemprop="name">良超</p>
  <div class="site-description" itemprop="description">盈亏同源，享受生活的随机性</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">253</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/WuLC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuLC" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/wuliangchao/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;wuliangchao&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangchaowu5@gmail.com" title="E-Mail → mailto:liangchaowu5@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wulc.me/2017/01/03/%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/files/profile.jpg">
      <meta itemprop="name" content="良超">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吴良超的学习笔记">
      <meta itemprop="description" content="盈亏同源，享受生活的随机性">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="使用sklearn做单机特征工程 | 吴良超的学习笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用sklearn做单机特征工程
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-01-03 20:15:44" itemprop="dateCreated datePublished" datetime="2017-01-03T20:15:44+08:00">2017-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-tags"></i>
      </span>
      <span class="post-meta-item-text">标签</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文转载自 http://www.cnblogs.com/jasonfreak/p/5448385.html
,在某些部分会拓展补充一些内容，全文主要讲述有关特征工程中通常使用的方法以及在sklearn中的相关实现。</p>
<span id="more"></span>
<h2 id="特征工程是什么"><strong>特征工程是什么</strong></h2>
<p>有这么一句话在业界广泛流传：<strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</strong>那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面：</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1b99tlsf81c6n5iicv3otetoa9.png"
alt="特征工程思维导图" />
<figcaption aria-hidden="true">特征工程思维导图</figcaption>
</figure>
<p>特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！</p>
<p>本文中使用sklearn中的<a
target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris">IRIS（鸢尾花）数据集</a>来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris
Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris
Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入IRIS数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征矩阵</span></span><br><span class="line">iris.data</span><br><span class="line"></span><br><span class="line"><span class="comment">#目标向量</span></span><br><span class="line">iris.target</span><br></pre></td></tr></table></figure></p>
<h2 id="数据预处理"><strong>数据预处理</strong></h2>
<p>通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：</p>
<ul>
<li>不属于同一量纲：即特征的规格不一样，不能够放在一起比较。<strong>无量纲化</strong>可以解决这一问题。</li>
<li>信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。<strong>二值化</strong>可以解决这一问题。</li>
<li>定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用<strong>哑编码</strong>的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。<strong>哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。</strong></li>
<li>存在缺失值：缺失值需要补充。</li>
<li>信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，<strong>对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。</strong></li>
</ul>
<p>我们使用sklearn中的 <code>preproccessing</code>
库来进行数据预处理，可以覆盖以上问题的解决方案。</p>
<h3 id="无量纲化"><strong>无量纲化</strong></h3>
<p>　
无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有<strong>标准化和归一化。</strong></p>
<p><strong>标准化一般指的是0均值标准化(zero-mean
normalization)也叫z-score标准化。z-score标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布，即均值为0，标准差为1。</strong></p>
<p><strong>归一化一般指的是区间缩放法，利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0,
1]等。</strong></p>
<h4 id="标准化"><strong>标准化</strong></h4>
<p>0均值标准化需要计算特征的均值和标准差，公式表达为：<span
class="math display">\[x&#39; = \frac{x - \mu}{\sigma}\]</span> 公式中的
<span class="math inline">\(\mu\)</span> 为原始数据的期望， <span
class="math inline">\(\sigma\)</span>为原始数据的标准差。</p>
<p>使用 <code>preproccessing</code> 库的 <code>StandardScaler</code>
类对数据进行标准化的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#标准化，返回值为标准化后的数据</span></span><br><span class="line">StandardScaler().fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<p>标准化的原理比较复杂，它表示的是原始值与均值之间差多少个标准差，是一个相对值，所以也有去除量纲的功效。同时，它还带来两个附加的好处：均值为0，标准差为1。关于这两个属性的具体好处参考
http://www.zhaokv.com/2016/01/normalization-and-standardization.html
如下</p>
<blockquote>
<p>均值为0有什么好处呢？它可以使数据以0为中心左右分布（这不是废话嘛），而数据以0为中心左右分布会带来很多便利。比如在去中心化的数据上做SVD分解等价于在原始数据上做PCA；机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布（不一定对称）。</p>
<p>标准差为1有什么好处呢？这个更复杂一些。对于<span
class="math inline">\(x_i, x_{i&#39;}\)</span>两点间距离，往往表示为
<span
class="math display">\[D(x_i,x_{i&#39;})=\sum\limits_{j=1}^pw_j\cdot
d_j(x_{ij},x_{i&#39;j})(\sum\limits_{j=1}^pw_j=1)\]</span> 其中 <span
class="math inline">\(d_j(x_{ij},x_{i&#39;j})\)</span>是属性 <span
class="math inline">\(j\)</span> 两个点之间的距离, <span
class="math inline">\(w_j\)</span> 是该属性间距离在总距离中的权重 注意设
<span class="math inline">\(w_j=1,\forall
j\)</span>并不能实现每个属性对最后的结果贡献度相同。对于给定的数据集，所有点对间距离的平均值是个定值，即
<span
class="math display">\[\bar{D}=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i&#39;=1}^ND(x_i,x_{i&#39;})=\sum\limits_{j=1}^pw_j\cdot
\bar{d}_j\]</span> 是个常数，其中 <span
class="math inline">\(\bar{d}_j=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i&#39;=1}^Nd_j(x_{ij},
x_{x&#39;j})\)</span>, 可见第 <span class="math inline">\(j\)</span>
个变量对最终整体平均距离的影响是 <span class="math inline">\(w_j\cdot
\bar{d}_j\)</span>,所以设 <span class="math inline">\(w_j\sim
1/\bar{d}_j\)</span>
可以使所有属性对全体数据集平均距离的贡献相同。现在设 <span
class="math inline">\(d_j\)</span>
为欧氏距离的平方，它是最常用的距离衡量方法之一，则有<span
class="math display">\[\bar{d_j}=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i&#39;=1}^N(x_{ij}-x_{i&#39;j})^2=2\cdot
var_j\]</span>其中 <span class="math inline">\(var_j\)</span> 是 <span
class="math inline">\(Var(X_j)\)</span>
样本估计，也就是说每个变量的重要程度正比于这个变量在这个数据集上的方差<strong>。如果我们让每一维变量的标准差都为1（即方差都为1），每维变量在计算距离的时候重要程度相同。</strong></p>
</blockquote>
<p>因此在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，往往采用标准化方法。</p>
<h4 id="归一化"><strong>归一化</strong></h4>
<p>区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：<span
class="math display">\[x&#39; = \frac{x - Min}{Max - Min}\]</span> 使用
<code>preproccessing</code> 库的 <code>MinMaxScaler</code>
类对数据进行区间缩放的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#区间缩放，返回值为缩放到[0, 1]区间的数据</span></span><br><span class="line">MinMaxScaler().fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<p>归一化的依据非常简单，不同变量往往量纲不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。比如两个人体重差10KG，身高差0.02M，在衡量两个人的差别时体重的差距会把身高的差距完全掩盖，归一化之后就不会有这样的问题。</p>
<p>除此之外，<strong>归一化后能够加快梯度下降求最优解的速度</strong>，如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征
<span class="math inline">\(X1\)</span> 和 <span
class="math inline">\(X2\)</span> 的区间相差非常大，<span
class="math inline">\(X1\)</span> 区间是[0,2000]，<span
class="math inline">\(X2\)</span>区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。<strong>因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</strong></p>
<figure>
<img data-src="https://wulc.me/imgs/image_1b99vqj1v1cffks1m321n8k1lalm.png"
alt="归一化对收敛的影响" />
<figcaption aria-hidden="true">归一化对收敛的影响</figcaption>
</figure>
<p>因此。在涉及到计算点与点之间的距离时，使用归一化或标准化都会对最后的结果有所提升，甚至会有质的区别。那在归一化与标准化之间应该如何选择呢？根据上面论述我们看到，<strong>如果把所有维度的变量一视同仁，在最后计算距离中发挥相同的作用应该选择标准化，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化。另外，标准化更适合现代嘈杂大数据场景。</strong></p>
<h3 id="对定量特征二值化"><strong>对定量特征二值化</strong></h3>
<p>定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。使用
<code>preproccessing</code> 库的 <code>Binarizer</code>
类对数据进行二值化的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line"></span><br><span class="line"><span class="comment">#二值化，阈值设置为3，返回值为二值化后的数据</span></span><br><span class="line">Binarizer(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<h3 id="对定性特征哑编码"><strong>对定性特征哑编码</strong></h3>
<p>由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用
<code>preproccessing</code> 库的 <code>OneHotEncoder</code>
类对数据进行哑编码的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment">#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据</span></span><br><span class="line">OneHotEncoder().fit_transform(iris.target.reshape((-<span class="number">1</span>,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="缺失值计算"><strong>缺失值计算</strong></h3>
<p>由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用
<code>preproccessing</code> 库的 <code>Imputer</code>
类对数据进行缺失值计算的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> vstack, array, nan</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line"><span class="comment">#缺失值计算，返回值为计算缺失值后的数据</span></span><br><span class="line"><span class="comment">#参数missing_value为缺失值的表示形式，默认为NaN</span></span><br><span class="line"><span class="comment">#参数strategy为缺失值填充方式，默认为mean（均值）</span></span><br><span class="line">Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))</span><br></pre></td></tr></table></figure>
<h3 id="数据变换"><strong>数据变换</strong></h3>
<p>数据变换就是通过组合或变换方式将原来的特征转换为新的特征，常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。</p>
<p>例如，输入的一个二维特征为 <span class="math inline">\([a,
b]\)</span>,则度为2的多项式特征为 $ [1, a, b, a^2, ab, b^2]$</p>
<p>使用 <code>preproccessing</code> 库的 <code>PolynomialFeatures</code>
类对数据进行多项式转换的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line"><span class="comment">#多项式转换</span></span><br><span class="line"><span class="comment">#参数degree为度，默认值为2</span></span><br><span class="line">PolynomialFeatures().fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<p>基于单变元函数的数据变换可以使用一个统一的方式完成，使用
<code>preproccessing</code> 库的 <code>FunctionTransformer</code>
对数据进行对数函数转换的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> log1p</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> FunctionTransformer</span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义转换函数为对数函数的数据变换</span></span><br><span class="line"><span class="comment">#第一个参数是单变元函数</span></span><br><span class="line">FunctionTransformer(log1p).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>
<h3 id="小结"><strong>小结</strong></h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>类</th>
<th>功能</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>StandardScaler</td>
<td>无量纲化</td>
<td>标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布</td>
</tr>
<tr class="even">
<td>MinMaxScaler</td>
<td>无量纲化</td>
<td>区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上</td>
</tr>
<tr class="odd">
<td>Normalizer</td>
<td>归一化</td>
<td>基于特征矩阵的行，将样本向量转换为“单位向量”</td>
</tr>
<tr class="even">
<td>Binarizer</td>
<td>二值化</td>
<td>基于给定阈值，将定量特征按阈值划分</td>
</tr>
<tr class="odd">
<td>OneHotEncoder</td>
<td>哑编码</td>
<td>将定性数据编码为定量数据</td>
</tr>
<tr class="even">
<td>Imputer</td>
<td>缺失值计算</td>
<td>计算缺失值，缺失值可填充为均值等</td>
</tr>
<tr class="odd">
<td>PolynomialFeatures</td>
<td>多项式数据转换</td>
<td>多项式数据转换</td>
</tr>
<tr class="even">
<td>FunctionTransformer</td>
<td>自定义单元数据转换</td>
<td>使用单变元的函数来转换数据</td>
</tr>
</tbody>
</table>
<h2 id="特征选择"><strong>特征选择</strong></h2>
<p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li><strong>特征是否发散</strong>：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li>
<li><strong>特征与目标的相关性</strong>：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为3种：</p>
<ul>
<li><strong>Filter：过滤法</strong>，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li>
<li><strong>Wrapper：包装法</strong>，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li>
<li><strong>Embedded：嵌入法</strong>，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li>
</ul>
<p>我们使用sklearn中的feature_selection库来进行特征选择。</p>
<h3 id="filter"><strong>Filter</strong></h3>
<h4 id="方差选择法"><strong>方差选择法</strong></h4>
<p>使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用
<code>feature_selection</code> 库的 <code>VarianceThreshold</code>
类来选择特征的代码如下： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"></span><br><span class="line"><span class="comment">#方差选择法，返回值为特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure></p>
<h4 id="相关系数法"><strong>相关系数法</strong></h4>
<p>使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用
<code>feature_selection</code> 库的 <code>SelectKBest</code>
类结合相关系数来选择特征的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line"><span class="comment">#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数</span></span><br><span class="line"><span class="comment">#参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4 id="卡方检验"><strong>卡方检验</strong></h4>
<p>经典的卡方检验是<strong>检验定性自变量对定性因变量的相关性</strong>。具体的意义可参考这篇文章<a
target="_blank" rel="noopener" href="http://guoze.me/2015/09/07/chi-square/">卡方检验原理及应用</a>。用
<code>feature_selection</code> 库的 <code>SelectKBest</code>
类结合卡方检验来选择特征的代码如下</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择K个最好的特征，返回选择特征后的数据</span></span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4 id="互信息法"><strong>互信息法</strong></h4>
<p>经典的互信息也是变量间相互依赖性的量度，互信息计算公式如下： <span
class="math display">\[I(X,Y) = \sum_{x \in X}\sum_{y \in
Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}\]</span>
为了处理定量数据，最大信息系数法被提出，使用
<code>feature_selection</code> 库的 <code>SelectKBest</code>
类结合最大信息系数法来选择特征的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"></span><br><span class="line"><span class="comment">#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mic</span>(<span class="params">x, y</span>):</span><br><span class="line">    m = MINE()</span><br><span class="line">    m.compute_score(x, y)</span><br><span class="line">    <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#选择K个最好的特征，返回特征选择后的数据</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h3 id="wrapper"><strong>Wrapper</strong></h3>
<h4 id="递归特征消除法"><strong>递归特征消除法</strong></h4>
<p>递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment">#递归特征消除法，返回特征选择后的数据</span></span><br><span class="line"><span class="comment">#参数estimator为基模型</span></span><br><span class="line"><span class="comment">#参数n_features_to_select为选择的特征个数</span></span><br><span class="line">RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure> ----</p>
<h3 id="embedded"><strong>Embedded</strong></h3>
<h4
id="基于惩罚项的特征选择法"><strong>基于惩罚项的特征选择法</strong></h4>
<p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用<code>feature_selection</code>库的<code>SelectFromModel</code>类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment">#带L1惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<p>L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LR</span>(<span class="title class_ inherited__">LogisticRegression</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, threshold=<span class="number">0.01</span>, dual=<span class="literal">False</span>, tol=<span class="number">1e-4</span>, C=<span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">                 fit_intercept=<span class="literal">True</span>, intercept_scaling=<span class="number">1</span>, class_weight=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 random_state=<span class="literal">None</span>, solver=<span class="string">&#x27;liblinear&#x27;</span>, max_iter=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 multi_class=<span class="string">&#x27;ovr&#x27;</span>, verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>, n_jobs=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#权值相近的阈值</span></span><br><span class="line">        self.threshold = threshold</span><br><span class="line">        LogisticRegression.__init__(self, penalty=<span class="string">&#x27;l1&#x27;</span>, dual=dual, tol=tol, C=C,</span><br><span class="line">                 fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,</span><br><span class="line">                 random_state=random_state, solver=solver, max_iter=max_iter,</span><br><span class="line">                 multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)</span><br><span class="line">        <span class="comment">#使用同样的参数创建L2逻辑回归</span></span><br><span class="line">        self.l2 = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y, sample_weight=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#训练L1逻辑回归</span></span><br><span class="line">        <span class="built_in">super</span>(LR, self).fit(X, y, sample_weight=sample_weight)</span><br><span class="line">        self.coef_old_ = self.coef_.copy()</span><br><span class="line">        <span class="comment">#训练L2逻辑回归</span></span><br><span class="line">        self.l2.fit(X, y, sample_weight=sample_weight)</span><br><span class="line"></span><br><span class="line">        cntOfRow, cntOfCol = self.coef_.shape</span><br><span class="line">        <span class="comment">#权值系数矩阵的行数对应目标值的种类数目</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(cntOfRow):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(cntOfCol):</span><br><span class="line">                coef = self.coef_[i][j]</span><br><span class="line">                <span class="comment">#L1逻辑回归的权值系数不为0</span></span><br><span class="line">                <span class="keyword">if</span> coef != <span class="number">0</span>:</span><br><span class="line">                    idx = [j]</span><br><span class="line">                    <span class="comment">#对应在L2逻辑回归中的权值系数</span></span><br><span class="line">                    coef1 = self.l2.coef_[i][j]</span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(cntOfCol):</span><br><span class="line">                        coef2 = self.l2.coef_[i][k]</span><br><span class="line">                        <span class="comment">#在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0</span></span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">abs</span>(coef1-coef2) &lt; self.threshold <span class="keyword">and</span> j != k <span class="keyword">and</span> self.coef_[i][k] == <span class="number">0</span>:</span><br><span class="line">                            idx.append(k)</span><br><span class="line">                    <span class="comment">#计算这一类特征的权值系数均值</span></span><br><span class="line">                    mean = coef / <span class="built_in">len</span>(idx)</span><br><span class="line">                    self.coef_[i][idx] = mean</span><br><span class="line">        <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>
<p>使用<code>feature_selection</code>库的<code>SelectFromModel</code>类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"></span><br><span class="line"><span class="comment">#带L1和L2惩罚项的逻辑回归作为基模型的特征选择</span></span><br><span class="line"><span class="comment">#参数threshold为权值系数之差的阈值</span></span><br><span class="line">SelectFromModel(LR(threshold=<span class="number">0.5</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4
id="基于树模型的特征选择法"><strong>基于树模型的特征选择法</strong></h4>
<p>树模型中GBDT也可用来作为基模型进行特征选择，使用
<code>feature_selection</code> 库的 <code>SelectFromModel</code> 类结合
GBDT 模型，来选择特征的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment">#GBDT作为基模型的特征选择</span></span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<h4 id="小结-1"><strong>小结</strong></h4>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>类</th>
<th>所属方式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VarianceThreshold</td>
<td>Filter</td>
<td>方差选择法</td>
</tr>
<tr class="even">
<td>SelectKBest</td>
<td>Filter</td>
<td>可选关联系数、卡方校验、最大信息系数作为得分计算的方法</td>
</tr>
<tr class="odd">
<td>RFE</td>
<td>Wrapper</td>
<td>递归地训练基模型，将权值系数较小的特征从特征集合中消除</td>
</tr>
<tr class="even">
<td>SelectFromModel</td>
<td>Embedded</td>
<td>训练基模型，选择权值系数较高的特征</td>
</tr>
</tbody>
</table>
<h2 id="降维"><strong>降维</strong></h2>
<p>当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。<strong>PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</strong></p>
<h3 id="主成分分析法pca"><strong>主成分分析法（PCA）</strong></h3>
<p>使用 <code>decomposition</code> 库的PCA类选择特征的代码如下：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment">#主成分分析法，返回降维后的数据</span></span><br><span class="line"><span class="comment">#参数n_components为主成分数目</span></span><br><span class="line">PCA(n_components=<span class="number">2</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure></p>
<h3 id="线性判别分析法lda"><strong>线性判别分析法（LDA）</strong></h3>
<p>使用lda库的LDA类选择特征的代码如下： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.lda <span class="keyword">import</span> LDA</span><br><span class="line"></span><br><span class="line"><span class="comment">#线性判别分析法，返回降维后的数据</span></span><br><span class="line"><span class="comment">#参数n_components为降维后的维数</span></span><br><span class="line">LDA(n_components=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure></p>
<h3 id="小结-2">小结</h3>
<table>
<thead>
<tr class="header">
<th>库</th>
<th>类</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>decomposition</td>
<td>PCA</td>
<td>主成分分析法</td>
</tr>
<tr class="even">
<td>lda</td>
<td>LDA</td>
<td>线性判别分析法</td>
</tr>
</tbody>
</table>
<h2 id="总结">总结</h2>
<p>再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法<code>fit_transform</code>完成的，<code>fit_transform</code>要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法<code>fit_transform</code>中有fit这一单词，它和训练模型的fit方法有关联吗？接下来，在《<a
target="_blank" rel="noopener" href="http://www.cnblogs.com/jasonfreak/p/5448462.html">使用sklearn优雅地进行数据挖掘</a>》中将会阐述其中的奥妙！</p>
<hr />
<p>参考： <a
target="_blank" rel="noopener" href="http://www.zhaokv.com/2016/01/normalization-and-standardization.html">归一化与标准化</a>
<a
target="_blank" rel="noopener" href="https://uqer.io/community/share/56c3e9c6228e5b0fe6b17d95">研究｜数据预处理｜归一化
（标准化）</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2016/12/27/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--RIP%E4%B8%8EOSPF/" rel="prev" title="计算机网络课程总结--RIP与OSPF">
                  <i class="fa fa-chevron-left"></i> 计算机网络课程总结--RIP与OSPF
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2017/01/08/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/" rel="next" title="先验概率，后验概率，共轭分布与共轭先验">
                  先验概率，后验概率，共轭分布与共轭先验 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-pen"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">良超</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
