<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "JmI6QmP3fMkLet6B",ck: "JmI6QmP3fMkLet6B"})</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/imgs/favicon.ico">
  <link rel="mask-icon" href="/imgs/favicon.ico" color="#222">
  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wulc.me","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="ImageNet Classification with Deep Convolutional Neural Networks 这篇论文可以说是多层CNN用在图像领域的首次尝试（此前的LeNet也将CNN用在手写数字的识别上，但是没有用到连续多层CNN）。文中提出的网络模型 AlexNet(设计者的名字叫 Alex) 在 ImageNet 2010、2012 年的比赛上取得的效果远远地优于传统方法">
<meta property="og:type" content="article">
<meta property="og:title" content="《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记">
<meta property="og:url" content="https://wulc.me/2017/05/15/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="ImageNet Classification with Deep Convolutional Neural Networks 这篇论文可以说是多层CNN用在图像领域的首次尝试（此前的LeNet也将CNN用在手写数字的识别上，但是没有用到连续多层CNN）。文中提出的网络模型 AlexNet(设计者的名字叫 Alex) 在 ImageNet 2010、2012 年的比赛上取得的效果远远地优于传统方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wulc.me/imgs/single%20channel.gif">
<meta property="og:image" content="https://wulc.me/imgs/multiple_kernel.jpg">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglhhknu1e7jni414k7g2v1ip11d.png">
<meta property="og:image" content="https://wulc.me/imgs/multiple%20channel.gif">
<meta property="og:image" content="https://wulc.me/imgs/max_pooling.gif">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglieks0s4d1r5kklrdf7qg426.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglikg561ica1iq811mf1aduhd2j.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglj2kqt1mluolk8vjhfv7f430.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1bgljihpe1q911dbdib71phbop3d.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglkiiou130110g4ola16hkni3q.png">
<meta property="og:image" content="https://wulc.me/imgs/dropout.gif">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglq20231k02ek11q8lesb7i850.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1bglsl8l91sjp1k7bide1f5bbhc9.png">
<meta property="article:published_time" content="2017-05-15T04:00:37.000Z">
<meta property="article:modified_time" content="2023-04-30T05:12:49.060Z">
<meta property="article:author" content="良超">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wulc.me/imgs/single%20channel.gif">

<link rel="canonical" href="https://wulc.me/2017/05/15/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记 | 吴良超的学习笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="吴良超的学习笔记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">吴良超的学习笔记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wulc.me/2017/05/15/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/files/profile.jpg">
      <meta itemprop="name" content="良超">
      <meta itemprop="description" content="盈亏同源，享受生活的随机性">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吴良超的学习笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-05-15 12:00:37" itemprop="dateCreated datePublished" datetime="2017-05-15T12:00:37+08:00">2017-05-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a
target="_blank" rel="noopener" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">ImageNet
Classification with Deep Convolutional Neural Networks</a>
这篇论文可以说是多层CNN用在图像领域的首次尝试（此前的LeNet也将CNN用在手写数字的识别上，但是没有用到连续多层CNN）。文中提出的网络模型
AlexNet(设计者的名字叫 Alex) 在 ImageNet 2010、2012
年的比赛上取得的效果远远地优于传统方法，这篇文献最重要的工作是设计并验证了这样一个有多层卷积层的网络的有效性，对学术界和工业界的影响都很大。</p>
<p>本文主要介绍这篇文章中提出的网络模型
AlexNet，以及其他涉及到的一些知识，主要的介绍的内容有：CNN的基础知识，AlexNet
的设计，训练和效果，以及对网络泛化（generalization）性能的一些探讨。</p>
<span id="more"></span>
<h2 id="cnn-简介">CNN 简介</h2>
<p><a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>
全称是 Convolutional Neural
Network，翻译做卷积神经网络，类似于我们常见的神经网络，CNN
也是一层一层连接起来的。顺便一提，这种 layer by layer 的网络叫做
feed-forward network，特点是无环，以区别于贝叶斯网络这种有环的网络</p>
<p>但是与传统的多层神经网络不同点在于,组成 CNN
的各个网络层不是常见的神经网络的里的网络层，而是由以下四种特殊的网络层组成</p>
<ol type="1">
<li>卷积层(convolutional layer)</li>
<li>池化层(pooling layer)</li>
<li>ReLU层(ReLU layer)</li>
<li>全连接层(fully connected layer)</li>
</ol>
<p>下面分别介绍各个层的具体结构与作用</p>
<h3 id="卷积层convolutional-layer">卷积层(Convolutional layer)</h3>
<p>卷积层是 CNN 中最重要的层，也是 CNN
名称的来源，卷积层里面有几个重要概念：核（kernel/filter)、卷积（convolution）、输出（activation
map）。对照下图可以比较清晰的理解</p>
<figure>
<img data-src="https://wulc.me/imgs/single%20channel.gif"
alt="convolutional layer" />
<figcaption aria-hidden="true">convolutional layer</figcaption>
</figure>
<p>核（kernel/filter): 移动的橙色正方形 卷积（convolution）：kernel 和
image 相应区域的点积 输出（activation
map）：卷积的输出，图中的粉色区域（convolved feature）</p>
<p>卷积层的作用可以理解为特征抽取，这种设计来源于人脑里面的机制，但是我们也能够直观地理解这类操作的意义，假设说我们现在要观察一幅图像，往往是从左到右，从上到下来观察的，而相邻的区域往往具有相似性，对于小面积区域（也就是
kernel 覆盖的区域）可以用更少的数据来概括这部分的特征。</p>
<p>上面的图像只用了一个
kernel，我们将其看做是一个人观察这张图片得到的信息，那假如有更多的人观察这张图片，并将所有人观察到的信息综合起来，得到的信息是否会更加完备呢?答案是肯定的，这就涉及到了多个kernel的情况，具体如下图所示</p>
<figure>
<img data-src="https://wulc.me/imgs/multiple_kernel.jpg"
alt="multiple_kernel.jpg-16.2kB" />
<figcaption aria-hidden="true">multiple_kernel.jpg-16.2kB</figcaption>
</figure>
<p>除了多个 kernel，图片也会有多个channel，上图显示的 input image
只有一个channel，但是实际中用于分类的图片往往是 RGB
图片，有3个channel，分别是red， blue，green，如下图所示</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglhhknu1e7jni414k7g2v1ip11d.png"
alt="RGB channels" />
<figcaption aria-hidden="true">RGB channels</figcaption>
</figure>
<p>因此当输入的图片有多个channel，并且用多个 kernel
去进行卷积的时候，过程会如下图所示（<a
target="_blank" rel="noopener" href="http://cs231n.github.io/convolutional-networks/#conv">来源</a>）</p>
<figure>
<img data-src="https://wulc.me/imgs/multiple%20channel.gif"
alt="convolutional network" />
<figcaption aria-hidden="true">convolutional network</figcaption>
</figure>
<p>上图的左边是 input image 的三个 channel，中间是两个
kernel，右边是输出的两个 activation map。对于有多个 channel 的情况，每个
kernel 就不再是二维的，而是三维，除去表示 kernel
大小的两个维度，剩下的一个维度也叫深度，大小就是输入的 channel
的大小。将从各个 channel 得到的值加起来，就得到了对应的 activation map
相应位置的输出。这里需要注意的的是，无论输入有多少个 channel，每个
kernel 最终只产生一个 activation map。</p>
<h3 id="池化层pooling-layer">池化层(pooling layer)</h3>
<p>池化类似于一种下采样（down sampling),
目的是要较少参数数量和计算量，如下所示是一个 max polling
的例子，其步长（stride）为2，kernel 为 2 X 2。</p>
<figure>
<img data-src="https://wulc.me/imgs/max_pooling.gif" alt="max pooling" />
<figcaption aria-hidden="true">max pooling</figcaption>
</figure>
<p>max pooling 指的是每次取 kernel 中的最大值最为输出，除了 max
pooling，还有 average pooling
等其他方式。除了上面提到的减少参数数量和计算量，池化还可以避免过拟合。</p>
<p>当上面的步长改为1后，kernel 移动过的区域会有重叠，我们称之为
overlapping pooling，如下图所示</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglieks0s4d1r5kklrdf7qg426.png"
alt="overlapping pooling" />
<figcaption aria-hidden="true">overlapping pooling</figcaption>
</figure>
<h3 id="relu-层relu-layer">ReLU 层(ReLU layer)</h3>
<p>ReLU 的全称是 Rectified Linear Units，从严格意义上来讲，ReLU
只是一个激活函数，而不能称之为一个层。其函数表达式为 <span
class="math inline">\(f(x) = max(0, x)\)</span>，其图像如下所示</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglikg561ica1iq811mf1aduhd2j.png"
alt="ReLU" />
<figcaption aria-hidden="true">ReLU</figcaption>
</figure>
<p>ReLU的主要作用是提升整个网络的非线性判别能力。关于选择 ReLU
作为激活函数而不是 sigmoid 或 tanh，后面会有详细说明。</p>
<h3 id="全连接层fully-connected-layer">全连接层(fully connected
layer)</h3>
<p>全连接层就是我们常见的神经网络中的网络层，每个神经元都与前面或后面的各个神经元有连接，如下图所示</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglj2kqt1mluolk8vjhfv7f430.png"
alt="fully connected layer" />
<figcaption aria-hidden="true">fully connected layer</figcaption>
</figure>
<p>由于全连接层的参数过多，在 CNN
中全连接层往往是作为最后几层用于输出。</p>
<h2 id="网络的设计">网络的设计</h2>
<p>上面介绍的四类 layer 是构成这篇论文中的 CNN 网络四种
layer，下面介绍论文中的 CNN 网络的结构及其特点。</p>
<h3 id="网络结构总览">网络结构总览</h3>
<p>文中提出的 CNN 网络结构如下</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bgljihpe1q911dbdib71phbop3d.png"
alt="Alexnet" />
<figcaption aria-hidden="true">Alexnet</figcaption>
</figure>
<p>上图有以下几个特点</p>
<ol type="1">
<li>由5层卷积层+3层全连接层构成，并且整个网络在两个GPU上训练</li>
<li>在第 1、2、5 层卷积层后添加了最大池化的操作</li>
<li>在每层卷积层和全连接层后都有 ReLU 激活函数</li>
</ol>
<p>上图中网络就是
AlexNet，网络结构可以这样理解，首先上下两部分表示网络在两个GPU上训练，前五层表示5层卷积层，后三层表示3层全连接层；而立方体（最左边的是输入的图像，这里不算入）表示每层卷积层的输出，立方体里面的小立方体表示kernel的大小。</p>
<p>第一层卷积层采用了 48+48 共96个 kernel，输入的图像有三个
channel，但是前面提到无论有多少个channel，一个 kernel 只会产生一个
activation map，所以图中的第一个立方体 48 表示输出的 48 个 activation
map，而这48个 activation map 作为第二层卷积层的输入又成为了 48 个 input
channels，依次类推，第二层卷积层采用了 128+128 共 256 个kernel。</p>
<h3 id="网络的特点">网络的特点</h3>
<p>这个网络有三个特点并没有在上图中并不是非常显式地展示出来：分别是 ReLU
Nonlinearity、Local Response Normalization 和 Overlapping Pooling。</p>
<p><strong>ReLU Nonlinearity</strong></p>
<p>ReLU 在前面已经简单地进行了介绍，这里要讨论的是为什么采用了 ReLu
作为激活函数而不是 其他的如 sigmoid 或 tanh。主要原因是 ReLU
能够更快地收敛，因为其能够在一定程度上避免梯度消失（vanishing gradient
）的现象。</p>
<p>要解释这个原因首先需要看看这三个函数的图像（ReLU
的图像上面已经给出）</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglkiiou130110g4ola16hkni3q.png"
alt="tanh 和 sigmoid" />
<figcaption aria-hidden="true">tanh 和 sigmoid</figcaption>
</figure>
<p>这两个激活函数的图像非常相似，均是两边平，中间陡。当通过反向传播（backpropgation）训练时，需要通过链式法则求出总的梯度，而当激活值很大或很小的时候，也就是对应到上面图像两边平缓的地方是，对这两个激活函数的求导结果几乎为0，
从而导致相乘得到的总的梯度也几乎为0，错误不能有效地传播到前面的层，修正前面层的参数。这种现象就称为梯度消失。</p>
<p>而对于 ReLU
函数，当激活值小于0的时候，也存在着相同问题，而且这时候导数完全是0；但是大于0的时候
ReLU
的导数总是1，因此大于0的时候不存在梯度消失的现象。也有人说当激活值小于0的时候会带来稀疏性的好处。</p>
<p><strong>Local Response Normalization</strong></p>
<p>Local Response Normalization 指的是对网络中经过 ReLU
层输出的结果进行正规化， 其正规化的公式如下所示：</p>
<p><span class="math display">\[b\_{x,y}^i = a\_{x,y}^i/(k + \alpha
\sum\_{j=\max(0, i-n/2)}^{\min(N-1,
i+n/2)}(a\_{x,y}^j)^2)^{\beta}\]</span></p>
<p>上式中的 <span class="math inline">\(a\_{x,y}^i\)</span> 表示第 <span
class="math inline">\(i\)</span> 个kernel 在位置 <span
class="math inline">\((x,y)\)</span> 的原始输出，而 <span
class="math inline">\(b\_{x,y}^i\)</span>表示正规化后的输出，<span
class="math inline">\(N\)</span> 表示所有 kernel 的数目。上式表明对某个
kernel 在某个位置的输出的正规化利用了与这个 kernel 相邻的 <span
class="math inline">\(n\)</span> 个 kernel 在相同位置的值进行。</p>
<p>而其他参数 <span class="math inline">\(k, n, \alpha, \beta\)</span>
则是通过 cross-validataion 获得的参数，Local Response Normalization
分别被应用到第一层和第二层卷积层，文章里说这种方法分别将 top-1 error 和
top-5 error 降低了 1.4% 和1.2%。</p>
<p><strong>Overlapping Pooling</strong></p>
<p>这个机制我们在前面谈到池化层的时候已经提到，文章里说这种方法分别将
top-1 error 和 top-5 error 降低了 0.4% 和 0.3%。</p>
<h2 id="网络的训练">网络的训练</h2>
<h3 id="目标函数">目标函数</h3>
<p>文章中的问题是一个图像多分类的问题，多分类问题有若干种方法，在神经网络中最常用的就是
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a></p>
<p>单纯从数学的角度来讲，softmax
只是一种向量变换方式，假设现在有一个长度为 <span
class="math inline">\(k\)</span> 的向量 <span class="math inline">\(z =
(z\_1,...z\_k)\)</span>,对其进行 softmax 变换后得到向量 <span
class="math inline">\(\sigma(z)\)</span>, 其变换公式如下</p>
<p><span class="math display">\[\sigma(z)\_j =
\frac{e^{z\_j}}{\sum\_{l=1}^k e^{z\_l}}~~~(j=1,...k)\]</span></p>
<p>变换后的向量 <span class="math inline">\(\sigma(z)\)</span>
有一个重要特征，就是所有元素之和加起来为
1；从概率论的角度来考虑，很自然地可以将这个向量作为属于各个分类的一个概率分布，选择值最大的那个项对应的分类作为其分类。</p>
<p>这种“自然”也是有数学支撑的，实际上，softmax 的这个特性可以从 <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized
Linear Model</a> 中推导出来。这里就不详细展开论述了。</p>
<p>有了概率分布，很自然地一个想法就是做极大似然估计，如下是一个 <span
class="math inline">\(k\)</span> 分类问题中，最大化 <span
class="math inline">\(m\)</span> 个sample 的联合概率分布，其中 <span
class="math inline">\(1 \lbrace . \rbrace\)</span> 的含义为 <span
class="math inline">\(1 \lbrace True \rbrace = 1, 1 \lbrace False
\rbrace = 0\)</span>，如<span class="math inline">\(1 \lbrace 2=2
\rbrace = 1, 1 \lbrace 2=3 \rbrace = 0\)</span></p>
<p><span class="math display">\[\max \sum\_{i=1}^{m} \sum\_{j=1}^{k} 1
\lbrace y^{(i)} = j \rbrace \log \frac{e^{z\_j}}{\sum\_{l=1}^k
e^{z\_l}}\]</span></p>
<p>在其前面添加一个负号和一个常数 <span
class="math inline">\(\frac{1}{m}\)</span>
可以将其转为如下的极小化问题</p>
<p><span class="math display">\[\min -\frac{1}{m} \sum\_{i=1}^{m}
\sum\_{j=1}^{k} 1 \lbrace y^{(i)} = j \rbrace \log
\frac{e^{z\_j}}{\sum\_{l=1}^k e^{z\_l}}\]</span></p>
<p>实际上，上面要最小化的目标函数是交叉熵损失（cross-entropy
error），这个目标函数也可以通过交叉熵的定义推导出来。</p>
<h3 id="训练算法">训练算法</h3>
<p>上面得到的最后是一个无约束的最优化问题，对于这类最优化问题有多种方法可用，其中最常用的是随机梯度下降（Stochastic
Gradient Descent），但是这里没有用原始的SGD， 而是采用了带有 momentum,
weight decay 和 mini-batch 的SGD。</p>
<p>momentum, weight decay 和 mini-batch
是三个非常重要的概念，这里简单说明他们的作用</p>
<p><a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum">momentum</a>
指的是每次更新参数的梯度除了用当前迭代得到的梯度，还要加上前一次迭代得到的梯度，起作用是为了加快收敛，避免局部最优（如果问题非凸的话）</p>
<p><a
target="_blank" rel="noopener" href="https://metacademy.org/graphs/concepts/weight_decay_neural_networks">weight
decay</a> 实际上是 L2 regularization
微分后得到的项，其目的是为了防止过拟合。</p>
<p><a
target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">mini-batch</a>
则是指每个更新时不仅采用一个样本，而是采用多个样本，这种方法介于 BGD 和
SGD 之间。</p>
<p>其更新的规则如下所示,参数 ，<span class="math inline">\(v\_i\)</span>
被称作 momentum variable， $- 0.0005 w_i $ 是 weight decay 项，<span
class="math inline">\(&lt;\frac{\partial L}{\partial
w}|\_{w\_i}&gt;\_{D\_i}\)</span> 则是从 mini-batch 为 <span
class="math inline">\(D\_i\)</span> 中得到的gradient， <span
class="math inline">\(\epsilon\)</span> 为步长。</p>
<p><span class="math display">\[\begin{align\*}
&amp;v\_{i+1} = 0.9v\_i - 0.0005 \epsilon w\_i - \epsilon
&lt;\frac{\partial L}{\partial w}|\_{w\_i}&gt;\_{D\_i}\\\
&amp;w\_{i+1} = w\_i + v\_{i+1}
\end{align\*}\]</span></p>
<h3 id="多gpu训练">多GPU训练</h3>
<p>前面已经提到了整个网络在两个 GPU 上训练， 原因是 GPU
能够并行的处理数据，训练速度较快，而同时一个 GPU
限制了模型的大小，因此用到了两个，两个 GPU
是并行的训练网络的，除了在第三层的卷积层两个 GPU 交换了数据用于cross
validation。与一个GPU训练的模型相比，两个GPU训练的模型分别将 top-1 error
和 top-5 error 降低了 1.7% 和 1.2%。</p>
<h3 id="防止过拟合">防止过拟合</h3>
<p>为了防止过拟合，文章采用了两种方法，data augmentation 和
dropout。</p>
<p><strong>data augmentation</strong></p>
<p>data augmentation
指的是如何从提供的数据集中得到更多的数据，文中也采用了两种途径，其中一种是从原始图像（大小为
256 × 256）中抽出多个大小为 224 × 224
的块作为图像，因此一幅原始图像能够生成多个图像；另外一种途径就是在原始图像的像素上加上通过PCA从图像中抽取出来的信息，从而生成新的图像。这两种方法将
top-1 error 降低了1%。</p>
<p><strong>dropout</strong></p>
<p>droupout 指的是每个神经元每次传递值时只有 50%
的概率工作，如下图所示，灰色的神经元指的是该神经元并没有工作。</p>
<figure>
<img data-src="https://wulc.me/imgs/dropout.gif" alt="dropout" />
<figcaption aria-hidden="true">dropout</figcaption>
</figure>
<p>这种方法的好处是降低了神经元间的依赖性，使得每个神经元更加
robust。droupout 被添加在第一和第二层全连接层中。</p>
<h2 id="网络的效果">网络的效果</h2>
<p>文中采用的数据集是 <a
target="_blank" rel="noopener" href="http://www.image-net.org/">ImageNet</a>，这是一个有着约 14 million
张 labeled image 的图片集，每一年通过这个数据集会举办一次名为 ImageNet
Large-Scale Visual Recognition Challenge(ILSVRC)
的比赛，就是一个图片多分类比赛，文中展示了上面提到的 cnn 网络在 2010
年和2012年比赛中的表现结果，结果如下所示</p>
<p>2010年</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglq20231k02ek11q8lesb7i850.png"
alt="2010" />
<figcaption aria-hidden="true">2010</figcaption>
</figure>
<p>2012年 <img
src="https://wulc.me/imgs/image_1bglq28egel61eidsd6r4k11aa5d.png"
alt="2012" /></p>
<p>其中 2012 年的表格中 5 CNNs 表示用了 5 个CNN做投票 ensemble
后的效果，CNN*
表示在原来的5层卷积层的基础上再增加一层卷积层。可以看到CNN所得到的结果要远远优于第二名的，而这也是当年这篇文章震惊了学术界和工业界的原因。</p>
<h2 id="网络泛化能力的探讨">网络泛化能力的探讨</h2>
<p>从上面的论述中可知，我们将多个卷积层和全连接层连在一起，然后加上pooling，dropout等操作，就构建了一个取得非常好效果的网络，很自然我们会问，这个网络为什么能够取得这么好的效果？或者说这个网络的泛化能力为什么会这么好，是不是有什么保证了其泛化误差不会过大？</p>
<p>在统计机器学习中，有一个叫 <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/VC_dimension">VC dimension</a>
的概念，用于描述模型的复杂度，这个概念中的 VC bound
为泛化误差约束了一个bound，但是这个概念需要很复杂的数学推导，这里我们略去这些推导。只说一个
VC dimension
给我们揭示的一个很直观的概念：<strong>要取得较好的泛化能力，用于训练模型的样本数目应该至少是参数数目的10倍。</strong></p>
<p>这个理论在统计机器学习的svm等模型中都得到了较好的验证，但是文中提出的网络有
60 million的参数和1.25 million
的样本，因此这个条件远远没得到满足。但是网路却取得了很好的效果，这样看来，VC
dimension这个理论并适用于这个网络，实际上，不仅仅是这个网络，VC
dimension 在深度学习中多个网络中也不适用。</p>
<p>而这一点，也被 <a
target="_blank" rel="noopener" href="http://www.iclr.cc/doku.php?id=ICLR2017:main&amp;redirect=1">2017
ICLR</a> 的最佳论文 <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.03530">Understanding deep learning
requires rethinking generalization</a>指出，下图是从这篇论文的
presentation 中抽取的一张图片。</p>
<figure>
<img data-src="https://wulc.me/imgs/image_1bglsl8l91sjp1k7bide1f5bbhc9.png"
alt="4 networks" />
<figcaption aria-hidden="true">4 networks</figcaption>
</figure>
<p>图中四个宠物小精灵代表了四个著名的网络，随着 p/n 值越来越大，也就是
“样本/参数” 的比值越来越小，越不满足 VC dimension
提出的条件，但是泛化的误差却越来越小。</p>
<p>这篇最佳论文还做了很多其他实验，这里就不详细展开，但是从这篇文章并没有从理论上说明了这个网络泛化误差小的理论依据，也就是没有提出在深度学习领域适用的
“VC
dimension”，而这一工作将会是未来深度学习发展中非常重要和有意义的工作。</p>

    </div>

    
    
    
        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/05/14/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(6)--%E5%B9%BF%E5%91%8A%E4%BA%A4%E6%98%93%E5%B8%82%E5%9C%BA(Ad%20Exchange)/" rel="prev" title="计算广告(6)--广告交易市场(Ad Exchange)">
      <i class="fa fa-chevron-left"></i> 计算广告(6)--广告交易市场(Ad Exchange)
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/05/20/%E5%87%B8%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/" rel="next" title="凸优化总结">
      凸优化总结 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">CNN 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82convolutional-layer"><span class="nav-number">1.1.</span> <span class="nav-text">卷积层(Convolutional layer)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82pooling-layer"><span class="nav-number">1.2.</span> <span class="nav-text">池化层(pooling layer)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#relu-%E5%B1%82relu-layer"><span class="nav-number">1.3.</span> <span class="nav-text">ReLU 层(ReLU layer)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82fully-connected-layer"><span class="nav-number">1.4.</span> <span class="nav-text">全连接层(fully connected
layer)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.</span> <span class="nav-text">网络的设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%80%BB%E8%A7%88"><span class="nav-number">2.1.</span> <span class="nav-text">网络结构总览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">2.2.</span> <span class="nav-text">网络的特点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">网络的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">训练算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9Agpu%E8%AE%AD%E7%BB%83"><span class="nav-number">3.3.</span> <span class="nav-text">多GPU训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">3.4.</span> <span class="nav-text">防止过拟合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%88%E6%9E%9C"><span class="nav-number">4.</span> <span class="nav-text">网络的效果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E7%9A%84%E6%8E%A2%E8%AE%A8"><span class="nav-number">5.</span> <span class="nav-text">网络泛化能力的探讨</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="良超"
      src="/files/profile.jpg">
  <p class="site-author-name" itemprop="name">良超</p>
  <div class="site-description" itemprop="description">盈亏同源，享受生活的随机性</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">248</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/WuLC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuLC" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/wuliangchao/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;wuliangchao&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangchaowu5@gmail.com" title="E-Mail → mailto:liangchaowu5@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2023</span>
 
  <span class="author" itemprop="copyrightHolder">良超</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
