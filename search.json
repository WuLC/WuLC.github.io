[{"title":"2017 小结","url":"/2018/01/02/2017%20%E5%B0%8F%E7%BB%93/","content":"一个多月没写文章了，这个月主要是被各种焦头烂额的事情所烦扰：比赛、数据的采集与筛选、各种无聊的报告等等。一眨眼就踏入了 2018，本来也不打算写年度总结，但是后来想想还是做一下简单的记录，一是因为自己本来就有总结的习惯，要不也不会一直在写这个博客；二是因为不总结下，都不知道自己这一年过得有多烂（捂脸）。言归正传，下面主要写一下在这一年里干了啥。\n\n关于课程\n研究生的第一年还是以上课为主，当初从电信转到计算机一个原因就是对计算机更有兴趣，所以这一年的课程也是学得挺顺利的。计算机的基本素养课程：操作系统、计算机网络和数据库都有，虽然本科上过，但是研究生的课是对某些知识点进行了更深入的讲解，操作系统和计算机网络都在本站点上做了相应的总结，虽说当时总结起来一顿操作猛如虎，考试也考了 90+，但是现在的内容却忘得七七八八了，似乎是水过鸭背。但我还是觉得这个东西虽然不能被我清晰记起，但是当要再次捡起来的时候，还是会比较快的。现在回想如果我本科时候没那么认真学习操作系统和计算机网络，研究生这两门课也不会学得这么顺利，知乎有个高票答案就说到知识或者技能这种东西，学到了就跟你一辈子，也许说的就是这种情况吧。\n除了这几门常规课，其他让我觉得最有用的两门课就是 最优化基础\n和 凸优化了，两门都是关于优化的数学课，以往基本没接触过，搞数学建模的时候接触过一点线性规划，这里则是更详细地介绍了各种优化问题和解决方法。非常有用的两门课，尤其是对于我这种接触过机器学习和计算广告的人，因为从本质上来讲，这种优化思想在生活中无处不在：资源往往是有限的，我们总是想借助着有限的资源来最大化我们所希望的获取的利益。\n将这类问题量化成一个最优化的问题，就有了一个目标，然后通过优化算法，就有了一个方向，这样或早或晚都能走到局部最优或全局最优。\n还有一门就是模式分类，这门课是跟本科生一起上的，因此讲的内容并不是很深入，都是传统的机器学习算法，但是让我收获最大的是课程论文的阅读，读的论文是 12 年提出 AlexNet\n的经典论文 ImageNet Classification with Deep Convolutional Neural\nNetworks，当时为了讲好 PPT，做了较多的调研，还写了阅读笔记，这篇论文涵盖了很多深度学习的重要概念，毕竟是开山之作。详细了解了这篇论文后使得我后面上手深度学习的项目时比较快，还是印证了前面的那句话：知识或者技能这种东西，学到了就跟你一辈子。\n学校安排的课程中，能被我记住的基本就是上面提到的，其他的那些如上得不痛不痒的数据库，让人备受煎熬的中特，都没有多大印象了。这一年的课程成绩也还可以，最后的评优中虽然没有专利、活动之类的加分，但是因为成绩优势，最终也拿了一等奖学金，比刚入学时候的二等奖学金要好了，虽然我觉得如果我不跨院保研的话入学的时候也能拿一等（捂脸逃）。\n除了学校的课程，主要还学习了与机器学习和计算广告相关的课程。\n机器学习的几门课程包括吴恩达在斯坦福的公开课，台大林轩田的机器学习基石和机器学习技法；这几门课都是理论为主，数学偏多，硬着头皮也算啃了下来，其中让我印象最为深刻的不是各种各样的模型算法，而是\nNFL (No Free Lunch) 定理和 VC 维理论，NFL\n定理指出了模型之间并的差距必须要到某一个具体的数据集上才能体现出来，也就是说要解决一个机器学习问题时，必须要先对数据的分布等信息有较好的理解，才能选出适合这个数据的模型；VC\n维理论需给我们揭示了一个很直观的概念：要取得较好的泛化能力，用于训练模型的样本数目应该至少是模型参数数目的 10 倍。这个能够很好的解释复杂模型容易过拟合的问题。但是对于目前如火如荼的深度学习，VC\n维理论并不适用，因此 2017 ICLR 的最佳论文 Understanding deep learning\nrequires rethinking generalization\n通过实验指出了这个问题，同时深度学习目前也还没有公认的理论基础，因此这个问题也是亟需解决。\n计算广告算是今年看到的一门比较新的课程，之前对广告的认知只限于弹窗、强制推送等，后来看了刘鹏的计算广告学，才发现这门学科集理论知识（主要是优化方面）和工程技法于一体，而且广告可以说是大数据为数不多的正真落地的一个产业。看了视频后又买了跟视频配套的书计算广告又看了一遍，对于书中众多概念及需要解决问题才有了初步了解。\n这两方面的课程的内容虽然都看完了一遍，也做了一些笔记，但是还是感觉理解得还不够深入，还要重温一遍。\n关于项目\n研一基本在上课，直到研一的暑假才被大老板叫去做项目，之前一直是跟小老板做\nNLP\n方向的研究，但是大老板的项目是图像相关的，具体的就是做人脸的表情识别。由于很久没接触图像相关知识了，刚开始还有点害怕做不来。但是后来才发现有了一些机器学习的知识，上手也是挺快的。\n在这个过程中，对比了人工特征 + 传统机器学习方法和深度学习方法，传统的人工特征基本就是人脸的 68 个特征点以及特征点衍生出来的特征，在几个数据集上深度学习的效果基本上都要优于传统的机器学习方法，也许是我们提特征不够好，但是这也是深度学习的强大之处，将特征抽取和分类器的训练融合到一个模型中。\n暑假做了大概一个月的算法研究，开学后被派去搭建系统了，主要就是实现从监控获取图像，对图像中的人脸进行表情识别并可实时观察具体的识别效果。首先要解决的是数据传输问题，就是图像从摄像头传到服务器，服务器处理后送到展示端，展示端为了维护的便利性，采用的网页展示方式。因为之前已经听说过\nkafka\n这个工具，知道这个工具的大概作用，因此这里就做了一下调研，没想到这个工具还是挺好用的，吞吐量高且拓展性强，部署起来也不麻烦，因此系统中有数据传输的部分都用了\nkafka，需要存储的部分用了 redis，在数据传输存储中将图像按照 base64\n编码后，能够避免很多问题。而网页端的显示则是用了\nmultipart/x-mixed-replace 这个 content type，简单来说这个\ncontent type\n能够替换掉原位置上的数据，如果将图像一帧一帧地传过来，便可达到动态视频的效果。\n下面是具体效果\n\n\nFER demo\n\n在这个过程中需要用到人脸检测、图像处理的工具，因此也接触了 opencv，\ndlib 这两个功能强大的库， 在人脸检测上 dlib 的效果要优于 opencv，opencv\n则主要用在图像处理，如标注、裁剪等。\n后面由于模型在已有的数据集 (如 CK+,\nKDEF 等) 上的效果很好，但是人肉测试时效果并不好，因此就考虑数据扩充，除了常规的在已有的数据集上进行裁剪、翻转、颜色抖动等操作。还通过爬虫在网上采集人脸数据库，主要就是通过关键词在谷歌中搜索对应的图片，然后获取其下载链接并下载图片，这个小工具已开源，具体地址见\nhttps://github.com/WuLC/GoogleImagesDownloader\n通过这个工具搜集了一定数量的图片，通过预处理和人工筛选后得到最终的图片，其中人工筛选过这个步骤由于每个人的标准都不一样，因此最后出现某些类别很少的情况。但是在一定程度上也算是扩充了原有的数据集（7 种类别仅有 2000 多张图片）\n\n\n图片数量\n\n因为之前的测试都是内部的测试，没有一个对比的标准，因为我们做的这个项目是以实用性为主，而目前提供表情识别服务且比较有名公司有\n微软、谷歌、Face++、竹间智能等，因此就想到到了构建一个公有的数据集，来对比一下我们的模型和商用的差距。经过讨论后，决定去采集真实的人脸表情，这样一来所有的模型都不可能接触过这些数据集，因而能够比较公平地验证各个模型的泛化能力。因此就用\npython 写了一个采集程序，去采集一个人的 7\n种表情，并以视频形式存储。下图是采集的页面\n\n\n采集程序\n\n采集完了需要将视频转为图像帧序列，然后人工选出若干帧作为表情变化序列进行后续操作。不得不说人工筛选就是累。\n采集完验证集后，测试了上述的四个提供表情识别服务的公司的 API\n在验证集上的准确率，结果显示准确率大概在 52%～58%（七分类）\n，我们的模型最好的效果能达到 61%，且通过 confusion matrix\n可以看到所有模型基本上都有这个问题，就是 angry、fear、disgust、sad\n这几类表情被误分为\nneutral，原因就是正常人在做这些表情的时候幅度并不会太大，而训练集中的数据却都是动作幅度较大，表情比较明显。\n最后，还需要将从监控中获取到的不同人脸分来，就是一个人脸聚类问题。最开始采用的是人脸识别经典做法，就是每个人采用已知的人脸图片，然后通过预训练的\nFaceNet 抽取图像的 128 维特征，对于未知的人脸图片，也用 FaceNet 抽取出\n128 维特征，并和已知人脸的 128\n维特征计算相似度，这种做法对于人脸的角度鲁棒性不好，就是只能识别出与已知的人脸图片中人脸角度差别不大的人脸。后来改用了\nChinese\nwhispers 聚类算法，同样也是要通过预训练的 FaceNet 抽取图像的 128\n维特征，但是不需要提供已知的人脸图片，且算法对于人脸角度的鲁棒性较好。\n去年就只做了这个项目，虽然做的内容比较杂，但是也算是学到了不少东西。\n关于比赛\n上课的时候参加了一些比赛，但是由于课程作业、考试等原因，基本都半途而废；暑假以后主要参加了两个比赛：AI\nChallenger 的图像中文描述 和 CCF\n的计算智能大赛。\n参加第一个比赛主要是这个方向可能是我的毕设方向，通过这个比赛，也算是基本入门了这个方向，代码主要是参考了\ntensorflow 中提供的 im2txt 例子，基本看懂后做了一些改动，代码见这里，参数没有细调，因为到后面去搞\nCCF 的比赛了，最后 B 榜大概 30 名左右 。\nCCF\n的比赛中主要参加了法海风控的比赛，做的是命名实体识别 + 文本分类，我主要负责的是文本分类，就是判断抽取出来的实体到底是正向、负向还是中性的，尝试了一些开源的工具，也实现了一些模型如\nTextCNN 等，综合效率和准确率，fastText\n是最好的，了解这个工具也是我觉得是比赛中一个较大的收获。比赛过程很繁琐，最终以\nTop5 的成绩去了江苏答辩，最后第四名，也算是收获了一个奖项。\n总的来说，2017\n年里主要完成的就是上面这三个方面的事了，其他琐碎的也基本记不起。2018\n就要找工了，时间真的过得好快，希望在 2018 里能够继续保持\nstay hungry，stay foolish 的状态吧。\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"2020 小结","url":"/2021/01/02/2020%E5%B0%8F%E7%BB%93/","content":"很久没更新技术文章了，草稿箱里还有几篇半成品一直被我以工作日事情太多、周六日需要休息为由\ndelay 了好几周；而现在站在 2021 年的起点，望着 2020\n年的尾巴，不禁感慨一年就这么呲溜一下就过去了，总想写点东西来复盘一下\n2020 这一年，还记得上次写的这种年度总结的文章是 2017\n小结，那会还在上研一，现在回看这篇文章还是略有感慨，还是比较佩服当年那个充满激情与精力、对各种知识都充满好奇的自己；趁着元旦放假有空这几天，还是决定简单地对\n2020 年做个总结，几年后再回头看看，或许会有不同的感悟。\n\n关于工作\n工作后到目前为止学习到的关于 presentation\n的最重要的一点是：要有总结，甚至可以先把结论放在开头，所以这里简单概括工作后目前为止做的两个事情\n\n多目标出价的推导与实验\n\n联邦学习的探索与落地\n\n多目标出价的推导与实验是刚进来的时候被派去做的一个调研工作，现在想起来感觉还是挺绝望的：对业务不熟悉，身边没有人能给予相关的指导，硬生生靠着自己一个人去推公式、读代码，还得随时提防开实验的时候线上搞出个事故。且因为业务的特殊性，这个实验只能开一天且需要在凌晨开启，所以经常出现的现象是我在凌晨掐表开实验，观察个几十分钟，然后才敢去睡觉，但是半夜冷不丁还会因为数据有波动报警电话直接把你吵醒，那会真的是相当的绝望。不过最终一遍遍尝试与\ndebug 代码，也总算是通过实验验证了推导出来的结果，因为 infra\n等原因，反转实验一直被 delay，后面架构调整，这块业务也就不再跟进了。\n这里的技术细节因为涉及保密就不详细展开讲了，不过大概几个月后意外发现了这篇\npaper，Bid Optimization by Multivariable Control in Display\nAdvertising，然后我套用里面的方法来推导我之前的问题，发现结果跟我之前推出来的是一样的！！！，所以我也更加肯定了上面那个没开反转的实验是能带来收益的，这篇\npaper 我也特意写了一篇阅读笔记《Bid\nOptimization by Multivariable Control in Display\nAdvertising》阅读笔记\n如果说前面的事情是一个纯粹的探索，没\nkick-off，没人力支持，那联邦学习算是我参与的第一个正式的项目，在这个项目里跟\ninfra、产品、运营、法务跟多团队都有合作。这个事情简单来说，就是要去探索如果在保护数据隐私前提下利用多方的数据进行建模与落地，中间的方案迭代了好几版，也踩过非常多的坑，但是最后也算是把这个业务在几个行业落地了，关于这一块的细节可以参考这篇文章\n字节跳动联邦学习平台 Fedlearner：4 个月落地开源，投放增效 200%+\n这一年算是工作的第二年了，在毕业后也算顺利进入了自己规划的广告技术行业，工作这一年多的时间里，也对总体的广告系统有了初步的认识，也做出来了一些成果，总体来讲，So\nfar so good 吧。\n关于学习\n工作后的学习时间肯定不如读书的时间那么充裕，因为得对自己的业务负责，而且当前的业务涉及到的事情巨多。。。但是本着\nstay hungry，stay foolish\n的心态，这一年利用节假日和下班时间还是学习了一下所负责的业务以外的知识\n\n基本上看完了 程序的表示、转换与链接这门课，还有\n链接、装载与库\n这本书\n\n读了一些关于出价、召回、精排的 paper\n\n了解了一个广告系统内涉及到的模块、算法等原理\n\n学习第一部分内容一个原因是兴趣驱动，因为一直对编译和运行这些原理不太懂，这两部分内容跟《深度了解计算机系统》这本书的内容很相似且更通俗，相关的总结我也放到的站点上；另一个原因就是我一直觉得在业界的算法工程师首先得是个工程师，而当前的我认为一个工程师是需要能大概了解代码运行的原理、能看懂别人的代码并在别人的代码基础上添加新的功能、甚至能重新造一个轮子，而不是仅仅局限于改一下模型结构、调一下超参但是不考虑实际落地的开销。当然，这只是现在的我的看法，几年后再回头看看，也许会有变化。\n第二部分则可以算作是广告和推荐领域的一些比较前沿或经典的工作了，总体感觉收获比较大的工作也总结写在了博客里，可以概括为一下几个部分\n\n出价: 这部分收获较大的 paper 是 Bid Optimization by\nMultivariable Control in Display Advertising ，这是阿里发表在 kdd 的\npaper，给出了一种方法来推导出价公式和构建出价控制器，且普适性较高，详细内容可以参考本博客的\n这篇文章\n\n召回: 这部分收获较大的 paper 是 Embedding-based\nRetrieval in Facebook Search，这是 FB 发表的一篇基于 embedding 召回的\npaper，描述了他们如何从 0 到 1 构建一个召回系统，当中有不少经验值得借鉴\n，详细内容可以参考本博客的 这篇文章\n\n精排：这部分收获较大的 paper 是 Real-time\nPersonalization using Embeddings for Search Ranking at Airbnb，这是\nAirbnb 发表的一篇构造 embedding 且把 embedding 应用至 search ranking\n的系统，里面有很多从业务出发来设计算法的细节，这种思想我认为是比较值得借鉴的，而不是生搬硬套一个方法 / 模型过来，详细内容可以参考本博客的\n这篇文章\n\nDelay FeedBack：这部分主要是广告领域的 cvr\n模型面临的问题，那就是转化是有延迟的，点击后的样本不能马上当做负样本，而等待足够长的时间让\nlabel\n回流的时间又太长，因此有了这一领域的研究，关于这部分做了一个调研，可参考这篇文章\nDelay\nFeedBack In Computational Advertising\n\n第三部分则主要是得益于团队内部良好的分享传统，经常有分享介绍广告系统涉及到的各个模块的功能及其原理，同时也能了解到要上线的各个策略 / 算法，也算是对广告系统有一个初步了解，因为涉及到内部的机密信息，这部分也不便于公开了\n学习是一个有阵痛感的过程，因为要接触的可能是一个全新的领域，你要从浩瀚的知识库中筛选出觉得有用的部分并消化掉，而且这些东西并不像游戏或其他东西能获得即时的回报，甚至在很长一段时间内看不到任何变化。\n每每想到这，都会感觉有点松懈 (而最近的确也有点。。。)；但就在我写这篇总结的时候，我也反问了自己这些问题，而且在自我思考和自我剖析后，还是觉得要坚持，主要有两个原因，原因之一是这能让自己保持一种快速学习的能力，原因之二是说不定哪一天这些知识就排上用场了呢？如同乔布斯所说的\nconnectting the dots\n一样，当然还是得设定一定范围来进行学习的，有目的性其实能让学习速度更快，比如现在的我会问自己 “如果让我去构建一个广告系统，需要怎样的工程能力？算法能力？业务能力？”，当然不太可能做到面面俱到，而是要考虑如何在深度和广度之间做\ntrade-off，而这个 trade-off 的度，得在实践中慢慢摸索。\n总之，还是希望自己能够继续保持持续学习的状态，做能做一个 lifelong\nlearner 吧。\n关于生活与心态\n这一年基本都是在宿舍和公司过着两点一线的生活，独自一人在北京，处于一种\n“一人吃饱，全家不饿” 的状态，每天上班都激情满满，直到最近的 11\n月，洗头的时候发现自己居然有脱发的痕迹，不禁感慨 “自己都还没变强，怎么就秃了”，同时脸上疯狂长痘，几乎要毁容。\n于是去看医生，还没等我描述完病情，医生就打断我说这个问题很常见，就只是压力太大，然后有熬夜之类的不良习惯之类导致的；后来细细回想，那段时间的确是过于担心项目的进展，同时每天都被各方催，晚上经常有熬夜，也导致了那一段时间相当焦虑，焦虑时饮食又不节制，吃了不少辛辣油腻的东西（但公司的食堂除了健身餐其实也基本都是辛辣和油腻的。。。。）\n于是告诉自己健康重要，强行让自己不要那么焦虑，同时加强锻炼，尽量不熬夜、保持饮食清淡，前面提到的脱发的迹象也慢慢有了改善；后来在网上看到了下面这句话，感觉写到了心里，就摘录下来 (出处忘了，侵删)\n\n在历史的发展面前，个人的力量微不足道；作为我们这些普通老百姓呢，要调整好心理预期，尽量顺势而为，不要逆天而行。你当然可以继续努力工作，但千万不要抱有那种「趁年轻拼一把」的心态，事业发展是一场马拉松，健康可持续才是最重要的\n\n希望自己后续无论工作多忙，项目多赶，还是能够保持这种心态吧\n小结\n写这篇总结文章的过程，也是一个不断的自我剖析和自我反思的过程，剖析自我到底想成为一个什么样的人，反思自我还能不能做得更好，里面的很多观点也是当下年少气盛的我的一些看法，也许有偏颇，等到而立之年再回头看看这篇文章，看一下是否会嘲笑当年的幼稚想法，问一下自己 “当初的愿望实现了吗，事到如今只好祭奠吗”。\n最后，希望自己在新的一年里能继续保持 stay hungry，stay foolish\n的状态吧，也祝愿各位新年快乐，心想事成。\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"2021 小结","url":"/2022/01/03/2021%E5%B0%8F%E7%BB%93/","content":"2022\n年如期而至，如果说上一年还是在犹豫是否要写年度总结，今年则是早有规划要写一下这一年的总结。因为笔者逐渐意识到，记录过去的自己是一件很有意义的事情，这个话题说大了可以上升到各种哲学领域，但对于笔者来说，最重要的意义是能看到过去的自己是一个什么样的人，如今有了什么样的变化，发生了什么事情让自己有了这样的变化，这种跨度一年的自我觉察还是很有意思的。\n而比起 vlog\n等形式，笔者更倾向于用文字这种形式来记录，因为在写作过程中，会启动大脑的 “慢系统”，能更细致地去回顾和组织过去发生的事情，正好趁着放假的时间去梳理这一年的各种事情。\n\n关于工作\n今年在工作上主要推进的事情还是之前一直跟进的联邦学习的业务，这部分在去年的\n2020\n小结 中也有提过，如果说去年还是在打造 showcase\n的阶段，今年则是将这部分业务规模化推广\n规模化做的事情一定程度上是在减少边际成本，所以设计方案时会更多地考虑到方案的易用性和拓展性，最终也在两个比较重要的业务场景验证并上线了，其中一个业务场景针对的是中腰部的客户，另一个业务场景则是针对头部的客户，两者的技术能力、可利用的数据等差异较大，因此也设计了不同的方案来适配不同类型的客户。虽然现在描述起来似乎是轻描淡写，方案本质上也不复杂，但是因为项目的性质，合作涉及到两个公司，而且联系还比较紧密（比如说模型需要双方联合跨公网训练），所以在协作、沟通、debug\n等成本上是比较高的\n在这个过程也踩了不少的坑，比如说前期设计出价公式时，只是基于业务表象进行设计，没有深入挖掘业务需求，加了一些不必要的约束，导致设计出来的公式并没有取得业务上最优效果；建模上由于转化时间非常长，业务上决定了数据分布有一定周期性，在周期转换的时候如果模型不及时更新，效果会变得很差；另外，也克服了工程上的一些挑战，比如说由于\nranking 的过程涉及到和外部的机房的交互 (类似 rta\n模式)，需要保证延迟上可用等\n在这两个 launch\n后，联邦学习提供的基本方案基本能涵盖当前不同类型客户的需求了。而历时大概\n1 年半，笔者也算是经历了这个项目从 “脑暴 -&gt; 出方案 -&gt;\n拉客户 -&gt; 出 showcase -&gt; 平台化与规模化”\n的过程，在这个过程除了跟产品有较多讨论，与销售、法务等部门也有不少接触，也从更大的视角上窥探了商业系统运作的一隅。在联邦学习基本稳定后，为了寻找一些新的挑战，在内部进行了一次转岗，后续会探索一些与生态相关的事情，\n也算是一个新的旅途了\n(打个小广告，目前所在的大部门是巨量千川，字节闭环电商的变现部门，整个部门目前的业务增速还算比较快，也欢迎感兴趣的同学联系我了解)\n另一个让我印象比较深刻的事情，则是在上一年总结里提到的多目标出价公式的推导，虽然离线和实验验证了有收益，但是由于种种原因，最终没上线；没想到今年在\nlr\n会上听到了非中国区的类似业务使用了不同的方法，但是推导出来的结果是一样的，最终也上线了，也算是进一步验证了这个方法的有效性～\n另外，在这一年的工作里，无时无刻不觉得过去的自己非常的愚蠢：考虑问题不够周全、汇报没有重点、容易被情绪而不是结果驱动等，想起来都觉得当时的老板对自己是多么的包容；此外，由于业务发展非常快，所以不可避免地有很多调整，在这个过程中特别感谢遇到过的老板们，从他们身上学习到了很多东西，如技术判断力、团队管理能力、沟通能力等。\n关于学习\n这部分要单独来讲述，是因为笔者慢慢在生活中体会到那句耳熟能详的 “学习是一生的事情” 是朴素而正确的道理，这里的学习不是狭义上的上课、背书、考试，而是更广义上的对不了解的东西逐渐熟悉的过程。这个过程起因往往是工作需要或者好奇心驱使，而且往往后者比前者更加容易且可持久，因为这个过程是滋养你的，而学习如果由这两个原因同时驱动，那无疑是非常难得的。如同乔帮主的\n05 年的演讲上说到的下面这段话\n\nYou’ve got to find what you love. And that is as true for your work\nas it is for your lovers. Your work is going to fill a large part of\nyour life, and the only way to be truly satisfied is to do what you\nbelieve is great work. And the only way to do great work is to love what\nyou do. If you haven’t found it yet, keep looking. Don’t settle. As with\nall matters of the heart, you’ll know when you find it. And, like any\ngreat relationship, it just gets better and better as the years roll on.\nSo keep looking until you find it. Don’t settle.\n\n幸运的是，目前的工作所在的领域还是我所感兴趣的，所以今年基于对当前工作的的一些粗浅了解写了这篇文章\nAn\nOverview Of An Ad\nSystem，从几个视角 (技术、业务、产品) 去介绍了对广告系统的一点理解，在这个过程中也算是将自己学习过的一些零碎知识串联起来了。后面没想到的是还有几位字节的同学看到并在内部\nIM 软件\n上联系到我，得知为自己写的文章同时对其他人起到一定帮助作用，也是比较欣慰了；除了技术，还发现了一门由产品经理讲授的课，总体听下来，也是让我从更多视角去了解去了解商业化这个事情，详见《商业产品经理的实战修炼》学习笔记\n除了与本职工作所需技能相关的学习，也特意去看了其他一些通用能力相关的教程和书籍，因为笔者渐渐意识到，思维方式和习惯对一个人的影响是巨大的，这些内容在知乎上也有不少相关的回答，而在这个过程中，笔者认为影响比较深刻且成体系的内容主要有以下\n2 部分，同时也写了相应的文章来记录\n\n《如何成为快速阅读高手》：这个是在找资料过程中找到的一个干货，从心态到方法，都给出了非常详细的指导，其中的摆正阅读心态那部分让笔者印象深刻\n\n《认知红利》：这是知乎上一个高赞作者写的一本书，单纯讨论书的内容，对笔者的启发还是非常大的，全书有\n2\n部分内容，第一部分提供了更多视角去看待财富、自己和世界，第二部分则提供了具体的一些方法论，笔者针对这部分也分别写了阅读笔记，即《认知红利》阅读笔记 (1)- 概念重塑和\n《认知红利》阅读笔记 (2)- 大脑升级\n\n在学完这些理论知识后，更深刻感受到了 “为什么懂得那么多道理，依然过不好这一生”，因为知行合一实在是非常的难，也许在看书的时候会非常认可书里的道理，但是转头还是被习惯驱使着做事情；那怎么破？笔者当前觉得也许有效的方法是不断重温这些内容，加上刻意练习，最终形成习惯\n最后学习的一大部分内容则是投资了，虽然在上学那会就开始定投了指数基金，但是金额不大，都是小打小闹；而工作了一段时间，兜里有了少许可供支配的钱，对于我这种生活比较简单的人，一个比较好的去处便是投资了。\n于是便开始较为系统性的学习投资相关的事宜，虽说是系统性，但是并没有啃那些大部头，因为笔者很清楚自己不是那种专职投资者，所以基本都是看一些通俗的文章，学习一些基本概念，后来发现了一门个人投资课，里面讲到的一些投资理念和方法特别适合我这种上班族，针对这部分也系统性地写了如下三篇笔记，细节这里不赘述了，其中影响比较深刻的观点是：投资是一场无限游戏\n\n张潇雨的个人投资课 (1)- 市场规律\n\n张潇雨的个人投资课 (2)- 投资工具与自我局限\n\n张潇雨的个人投资课 (3)- 投资组合构建\n\n上面的课程的主讲人张潇雨，其实是笔者通过 得意忘形\n这个播客认识的，因为下半年开始，住的地方离公司远了一些，通勤时间也变长了，于是便开始听播客，其中一个印象比较深刻的播客便是得意忘形，里面的一些观点对比着还是挺有启发的，值得一听，这个播客的简介如下\n&gt;\n《得意忘形》是一个主张追求个体自由与探寻真理的媒体计划。我们见证了第一次工业革命以来科技对人类社会的极大推动与助益，但也意识到资本主义与市场经济不可避免地催生了消费文化、剥夺了个人价值、并窃取了大众时间。带着对生命的有限性与无目的性的敬畏，我们试图为读者与听众提供更全面的觉察自我与认知世界的工具，以不断重建当下的方式穿越时间、抵达生活的本质。\n后来顺藤摸瓜，找到了孟岩的播客无人知晓，同时了解他所打造到且慢这个产品的过程，以及后续离开且慢后独立创业所做的产品有知有形，这也是笔者近期使用较为频繁的一个\napp\n最后，在学些这些看起来跨度有点广的内容过程中，笔者觉得有一点是在各个领域都适用的，那就是不要盲信各种观点和知识，而是要保持独立思考的能力，要时刻思考这些内容是否合理，放到今天是否还适用，放到自己身上是否适用，毕竟尽信书不如无书\n关于生活\n这一年基本也还是在公司和住的地方之间穿梭，过着两点一线生活；虽然来北京\n2\n年多了，但那些著名的景点基本都没去过，以至于我妈每次跟我打电话都跟说我白去北京了～\n由于周六日经常回公司健身、看书 /paper、刷知乎 /b 站、写文章 (这一年写的文章基本都是周六日 + 各种节假日写的，因为上班真的很忙。。。)，以至于一些不明真相的人都觉得我 “卷”（hhh，这个词属实是被滥用了），但是他们可能不知道的是，我周六日会公司都会把\nlark\n给关了，不会用工作上的事情去打扰别人，也不希望别人打扰自己，专心做那些自己很想做但是由于在工作日太忙而无法去做的事情\n我也常常问自己：你是有在体验生活吗？你是在做苦行僧吗？但后来我慢慢发现，撸铁跑步带给我的满足感，并不亚于去探店吃到了心仪已久的美食；写完一篇文章、看到博客又记录了自己的变化所带来的的成就感，并不亚于去攀登了一座险峻的高峰；了解到一些表面现象背后的本质原因所带来的震撼，并不亚于看到了光怪陆离的自然风光；AC\n一道题目所带来的快感，也不亚于通关一款制作精良的游戏。\n说了这么多，并不是说前者就比后者好，恰恰相反，我觉得上面提到的很多 “不亚于” 的事情，都是很有意义的。只是相同的一件事情，对于不同的人来说意义是不同的，也许在外向的人那里是补充精力的，但在内向的人那里则是消耗精力的，笔者觉得关键还是要学会让自己每天过得充实而快乐。\n同时，需要将快乐和快感区分开，也体会过刷了一下午的手机后的那种空虚感，后来看到了\nze ran\n写的一段话，才意识到那是下面说的快感，而不是快乐\n\n快乐和快感不同，快乐是要去争取的，是要付出努力的，是可以回味的；而快感是廉价的，是短暂的，是空虚的。不是说快感不好，而是在快感过去后，不用叹息快乐短暂，因为逝去的并不是快乐。努力拼搏，学习锻炼，也不用向别人诉说自己的辛苦，因为收获的是快乐\n\n关于健康\n今年体检各项指标也都还算正常，父母检查出了一些小毛病，但总体并无大碍\n在运动上，刚刚看了下在内部健身房累积签到的次数，大概是 216 次，而\n2021 年的大概是 80 次，大概 4~5 天一次\n\n至于运动量，之前用的小米手环运动模式太少，比较难记录，所以双 11\n换了荣耀的 GS PRO\n手表，是真的好用，运动模式支持也比较多，所以也基本记录了 12\n月份基本数据\n\n今年下半年开始有了减脂和增肌的计划，所以也学些了不少这方面的知识，经过饮食和训练，现在的总体的体脂也下降到\n14%\n左右，六块腹肌的轮廓已经隐隐欲现了，希望明年的总结能够放上八块腹肌的照片 hhh\n\n这里也附上减脂过程中做的一些笔记，基本上知乎一搜一大堆，\n这个回答比较全面，可以看一下 如何把体脂降到\n15%?\n\n另外就是饮食和睡眠了，说实话这两点做得不够好，主要的问题是情绪性进食和报复性熬夜，前者往往出现在工作时比较繁忙时，总是忍不住会吃很多零食和正餐，后者则是因为工作、生活等压力等原因，工作日晚上容易熬夜无意义的刷手机等，当前已经意识到这个问题了，也有尝试小方法，希望明年总结的时候不会再被这两个问题困扰吧\n小结\n如同去年的小结的短语一样，“写这篇总结文章的过程，是一个不断的自我剖析和自我反思的过程，剖析自我到底想成为一个什么样的人，反思自我还能不能做得更好”；而今年我觉得还能再加一句，这也是一个对自己诚实的过程，做的好与不好都需要暴露给自己，多问自己几个为什么；惟其如此，才能知道自己真正想要的是什么，下一步应该怎么走。\n总体来说，去年的工作还算顺利、学习热情没有减退、生活能带来快乐、家人与自己身体还健康，感觉已经比较幸运了；至于技术 / 沟通 / 管理 / 投资等各种能力，在今年仍需慢慢培养，同时需要学会爱惜自己的身体。最后，也祝愿看到这里的你身体健康，心想事成。\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"2022 小结","url":"/2023/01/01/2022%20%E5%B0%8F%E7%BB%93/","content":"2022\n的最后一个月，在全国 “喜阳阳” 的氛围中度过了，写这篇文章时，恢复了差不多一周多，基本也没什么症状了。但回想下今年发生的各种事情，真的是可以用魔幻来形容，这个魔幻不仅仅指防疫政策的\n180\n度大转弯，更是发生在身上的各种事情，这些事情基本都可以总计为计划赶不上变化，或者说未来无人知晓\n一直都有写年度总结的习惯，而这么魔幻的一年，更值得写篇文章纪念下，正好也是元旦放假，趁着这几天把过去一年发生的事情梳理了一下～\n\n关于工作\n现在回头看 2021\n时的总结，发现当时写下的未来工作规划，跟后来实际的工作差异非常大；究其原因，还是总体业务和组织变化过于频繁\n上半年主要在做一些广告创意上的工作，从平台和技术视角来看，笔者将这个方向主要分为如下三个子方向：生成、优选和投放；其中生成是指利用素材 (标题、图片、视频、落地页等) 生成候选创意 (用户看到的广告)，优选是从计划的候选创意 (一个计划下的候选往往有多个) 中选择\ntopk 个用于投放，投放则是将优选出来的创意投放至线上\n学界研究得比较多的往往是优选和投放，优选是个很典型的\nE&amp;E (Exploration &amp; Exploitation) 问题，\n研究的方法也很多，当时看到一篇比较有参考价值的是淘宝的一篇\npaper，也针对这篇 paper 做了一些记录《A\nHybrid Bandit Model with Visual Priors for Creative Ranking in Display\nAdvertising》,paper 里提出的方法主要由两大模块组成：VAM 和 HBM, VAM\n是基于 list-wise 优选做 exploitation 模块，HBM 则是基于 badnit model 做\nexploration 模块\n投放则是一个更大的话题，往往涉及到成本、跑量、生态的问题，当时主要是针对生态问题做了一些研究和实践；这里的生态问题，往往是指复制问题和长尾问题，复制问题指的是投手利用系统的\nvariance\n复制很多相同的创意，以达到跑量的目的，长尾问题指的是很多创意压根投不出去，或者投出去的量非常少\n而从广告主的视角来看，素材制作是有成本的（如拍视频、修图等），且这个成本是在投放前就要花出去的，如果基于这些素材投放不起量，那这些成本就相当于打水漂了；而长尾也是导致复制问题的原因之一，毕竟就是投不出去，广告主才会去不断复制素材以达到跑量的目标；因此，通过技术手段缓解素材的长尾问题，对于整个系统的长期生态是有必要的\n回到长尾问题，也是推荐 / 广告中很常见的一个问题，一般从策略或模型角度都有一些解决思路\n\n策略层面，根据系统和业务特性设计规则，比如说对长尾的 item\n有特定的扶持，强行让这些 item 能触达到更多的用户\n\n模型层面，核心思想就是让模型能更好地学习到 long tail item 的\nrepresentation，因为这个问题的根本原因就是 long tail item\n的样本过少，进而导致模型学习的不好\n\n策略层面除了上面提到的一些强依赖业务的规则设计，也有一些通用的思路，比如说建模二部图，强行让创意的曝光量有\nlowerbound 的保证，这其实也是冷启动比较常用的一个套路，这部分可参考这篇\npaper 里提到的思路 《Dynamic\nCreative Optimization in Online Display Advertising》\n模型层面，当时研究下来，感觉比较靠谱的是 transfer learning 和\nself-supervised learning 两个思路，当时也写了这部分的一个总结 《Long\nTail Problem In Item Recommendation》\n在广告系统中，优选和投放还算是比较常规的部分；素材生成则是近年业界在研究的一个方向，比如说巨量千川推出的产品：高光快投，也是笔者深度参的一个项目，这个产品的目标就是为直播广告主截取直播间的高光片段，然后加工成素材投放，节省广告主的素材制作成本（尤其是素材制作能力弱的中小商家）\n从技术层面来说，这个难点在于 “高光” 在这里没有一个绝对的 ground\ntruth，而没有 ground truth 或者说 label\n是无法通过模型进行截取，而基于人工打标的方式势必是不可长期持续的；当时针对这部分也做了一些调研，绝大部分的\npaper 也都是基于人工打标的数据进行训练的，最后是 2019 年一篇 CVPR 的\npaper 给了笔者不少启示，即实际业务中有很多投后的指标数据（ctr、cvr、roi\n等），可以基于这些投后的数据来构建训练样本，即基于实际投放而不是人为的主观判断来决定一个视频是否属于高光，而在实际业务中，这也被证明是比较有效的；这部分的一些探索可参考《Highlight\nDetection In Video》\n在高光快投这个项目中，基本上把创意的生成、优选和投放环节都涉猎了一遍，整个产品的消耗也在接手后涨了差不多\n2 倍，但 7、8\n月左右，因为整体组织架构有调整等原因，这个方向的优先级总体有变化；虽然上半年在做创意时已经有一些调整，但都不如这次的大，笔者总体还是能理解这些调整的，毕竟不同的老大判断的项目的优先级不同，而新老大一般上来都要重新\nreview 当前的项目，停一些认为不合理的项目，立一些新的项目\n所以后面也去做了算力的相关工作，这部分工作挺有意思的，简单来说就是在机器资源有限的情况下，怎么提高机器的变现效率，从 “算力\n= 请求量 × 平均请求消耗资源”，可以从 2\n方面进行优化，这部分业界也已经有一些尝试了，针对这部分也写了一些粗浅的理解\n《浅谈算力优化》\n上面提到的基本都是技术方面的工作，而在今年还有一些谈不上正式的管理经历，自己也在这个问题上做了一些调研，于是便对这部分做个总结\n《聊聊管理》。在这个过程中，也发现了身边那些优秀的管理者，使用管理的手段是多种多样的，共性更多是在人上，比如说他们的能力是出众的，态度是谦和的；围绕着事而不是人开展业务等。同时也意识到当上了管理者就能一劳永逸是个妄念；而随着业务的发展不断调整、进化自己，以更年轻和开放的心态去迎接当前的挑战，有管理工作委任时能够扛得起，没有也能做个能打的\nIC，可能才是 “一劳永逸” 的答案；毕竟真正当上管理者的人是少的，而当上管理者可能也需要在合适的时机踩中合适的机会，有运气与努力成分；好好生活，快乐工作，才是每个普通人应该去努力追求的\n关于投资\n今年上半年算是断断续续地学些了有知有行上的相关内容，并且按照笔者的理解划分为：认知与心态、概念与常识、买与卖三大模块。认知与形态主要是投资前的心理建设部分，概念与常识主要是介绍一些基本概念（如周期、投资标的、宏观常识等），买与卖则是一个比较成体系的投资系统（涉及到资产、仓位配置等）\n\n投资这件事 (1)- 认知与心态\n\n投资这件事 (2)- 概念与常识\n\n投资这件事 (3)- 买与卖\n\n写完上面的文章后，给笔者留下最深刻印象的是 “盈亏同源” 这个概念，顾名思义，就是盈利与亏损都是出自同一个源头，或者说都是同一个原因造成的；比如说，导致当下亏损的操作，如果换了一个平行时空，可能就是盈利的了。因为市场是不确定的，无法预测的，要放弃对所谓确定性的追求，同时摸索出适合自己的选择判断体系，基于当下的信息作出最好的判断，不要一直对过去的操作后悔或悔恨。仔细想想，生活不也是这样么？\n而今年的整个投资市场非常魔幻，也是给笔者上了很好的一课，尤其是回想起比较疯狂的\n2021\n年初，大家疯狂买基金，连我妈啥都不懂都糊里糊涂地投了点钱进去，再看看今年惨淡，才更深刻地体会到教科书说的 “闲钱投资、不加杠杆、不接飞刀、仓位配置” 等是朴素却又正确的道理；也在各种媒体博眼球的报道中，体会到了 “三根大阳线改变信仰” 是怎么一回事\n今年整个市场的回顾可以参考知行小酒馆的播客《魔幻的 2022\n年过去了，2023 年的市场会好吗？》，给笔者印象最深刻的是 4\n月份那次大下跌，上海封城，当时整个市场可以说是一遍哀嚎，有知有行的温度计也到了\n0\n度，现在回头看是个买入的好时机，但也只局限于现在回头看，因为在当时，大家剩下的都只有 “恐惧” 了\n刚看了下账户，所有权益类资产今年总体回撤大概 5%\n左右，而且是在边跌边买的，如果一开始就重仓，回撤会更大；主要是亏在了主动型基金和中概上，主动型基金是源于对基金经理过往历史业绩的信任，所以总体仓位不低，这里并不是说主动基金不好，而是要知道在主动基金获利，业绩较好时，需要清楚这个是\nalpha 收益还是 beta\n收益，是周期带来的收益还是基金经理择股带来的收益，同时主动型基金的仓位配比信息是有延迟的，你不清楚自己在各行业的配置如何，所以笔者也在逐渐减少主动型基金的配比，更多放到了宽基指数和行业指数上\n中概则是过快地接了飞刀，在刚下跌的时候就开始加仓，导致后续越加仓越跌，当然，这里的 “过快” 其实也是站在当下来看而得到的结论，当时谁也不会觉得会跌到现在这样，是无法预测的；但交了中概这个学费后，笔者对安全边际的要求会更加严格，也更加敏感了。现在分析，当时这么快接飞刀，一是来源于不太懂，二是当时总体仓位还很轻，着急加仓\n今年盈利的都是大 A 以外的指数，主要是恒生和德国 DAX\n指数，两者都是在比较低点时候进场的，其实也进一步验证了有知有行常说的 “好资产 + 好价格” 的理念，同时也说明了分散投资以及海外投资的必要性\n另外，在今年做的比较多的一件事情，是经常看账户，看当前的盈亏；这听起来就是一个反面教材，因为根据一些研究，看账户越频繁，往往回撤会越大，因为恐惧割肉会比较多。但笔者在看账户的时候，更多在做的是观察自己面对上涨或下跌的情绪变化，算是自我情绪觉察的一个练习，然后根据当前情绪再来调整仓位。目标做到 “涨跌都舒服”，因为这样才能拿得住\n关于生活\n今年的生活也基本是两点一线，日常的状态也跟去年的差不多\n\n撸铁跑步带给我的满足感，并不亚于去探店吃到了心仪已久的美食；写完一篇文章、看到博客又记录了自己的变化所带来的的成就感，并不亚于去攀登了一座险峻的高峰；了解到一些表面现象背后的本质原因所带来的震撼，并不亚于看到了光怪陆离的自然风光；AC\n一道题目所带来的快感，也不亚于通关一款制作精良的游戏。\n\n这种状态一直持续到 6~7\n月，那会各大互联网公司在进行或开启裁员潮，内部业务在进行大调整，可以说是 “内忧外患”，于是开始认真审视当前的这种生活的可持续性，同时心中也萦绕这很多没有答案的问题，那段时间可以用\n2 个字来形容：焦虑\n于是笔者开始去调研，尝试回答那些没有答案的问题，也是在那个时间段，将所有调研到的内容作了梳理，写成了这篇文章《从焦虑谈起，聊聊生活的可能性与随机性》，也算是给自己做的心理按摩。简单来说，就是生活和工作是存在很多可能性的，不止大厂这一种叙事方式，只要能对外提供社会需要的服务或者说\nProductize\nYourself，也是一种安身立命的途径，也是一种可能的生活方式\n另外，焦虑在人生的每个阶段都是必然存在的，接受焦虑、接受生活的随机性，比起去对抗焦虑、精细规划未来、小心翼翼地迈出每一步，可能是一种更好的方式；毕竟随机性不可避免，而过分追求确定性可能会让我们瞻前顾后、步步惊心，还不如学着接受并享受生活的随机性；既然预定要飞向意大利的航班，最终有可能让你降落在荷兰，还不如好好享受荷兰的风光，对自己说一句：欢迎来到荷兰\n给自己做完了这个心理按摩后，心里的焦虑的确少了很多，或者说虽然日常也会焦虑，但是不再是之前的无头苍蝇的那种恐惧，更多的是对任务没有及时完成的那种着急，而不是无端生出的惶恐；因为认识到且认可了生活不止当前这一种叙事的方式，只要心态能够打开、积极拥抱变化，就不用过于恐惧所谓的\n35\n岁危机，毕竟未来的事情都是无人知晓的，而生活本身也是小马过河，其他人总结出来的经验未必适用你\n另一个比较有意思的事情，是认识了 P；跟 P 认识的过程也很有趣， 我跟 P\n不是一个序列的，但是都参与同一个业务，日常工作会有一些合作；最开始以为 P\n是被我纯正的 “广普” 吸引的（毕竟多次调侃我的普通话），后来跟她聊天才发现，原来她在为她的闺蜜找对象，而我成了她 “盲狙” 的目标之一\n对象当然没有谈成，因为还是比较享受一个人的状态～总体感觉就还是处于《我（们）的孤老生活》中的孤老第一阶段，至于是否会迈入第二阶段或者何时迈入，跟前面说的很多事情一样，无人知晓也无法预测\n\n孤老的两个阶段：（1）一个人的阶段，对个人成长非常重要（2）结束一个人的阶段，渴望有家庭甚至有陪伴\n当你去探索生命的本源问题的时候，你会感到一种很极致的孤独。你会觉得人是没有办法被理解的。但这我觉得这是一个人心灵成熟的一个必要条件\n而当孤独走到极致，或者说自立走到极致，会发现别人给不了我什么，或者不需要其他人给我什么，才真正明白了怎么去爱别人（付出的爱是溢出的，是允许）\n\n倒是在跟 P 的几次沟通下来，慢慢成为了兄弟（她是大哥 hhh）；P\n是那种很早就知道自己想要什么的人，也一直有规划怎么去达成这些目标，跟她一块聊工作、聊生活、聊规划，探讨了很多困惑笔者比较久的问题，P\n也给了我不少受用建议，同时也了解到了 P\n比较神奇的经历，包括但不限于平静却又凡尔赛地更跟我说如何考上清华、工作不爽然后出国读了个书、面对外人看来的不幸经历却付之一笑...\n后来在某天晚上心血来潮跟 P\n去夜游了亮马河，算是我在北京为数不多出去看风景的经历；然后去吃火锅，压马路到凌晨\n2\n点，也算是一段难忘的经历了～（船在动 + 手机渣像素导致了这种图片，绝对不是个人没怎么拍过照、摄影技术差 [狗头]）\n\n可惜的是 P 马上就要到南方城市了，原本约定好的潮牛火锅，也在最后\n11、12\n月的居家中落空了；有人说职场中，我们在都在坐一趟时光的公交车，有些人中途上车了，和我们成了朋友，但也有些人下车后转到另一辆车了，希望跟\nP 能在以后的公交中有再次重逢的机会吧～\n关于健康\n记得去年测体脂的时候立了个 flag：希望今年体脂能降得更低，能放上 8\n块腹肌的照片；可是这个计划随着 11、12 月的居家去不了健身房 +\n居家时管不住嘴 +\n康复后不敢做剧烈运动等等 (为自己找的借口)，最终未能实现；现在的体脂得有\n16% 了吧\n今年 10\n月的体检，总体无太大问题，但一些小毛病还是少不了，毕竟机器越用越坏；也买了一些保险，也当做是为未知的风险做一些对冲准备～\n整年的运动基本都用运动手表记录下来了，回顾才发现居然耗时最长的是每天上下班的骑车部分，其次才是为了减脂做的力量训练，11、12\n月基本是躺平状态了，因为都是居家办公 + 康复的状态\n\n然后看了下几年去健身房的次数，最近一次就停留在 11\n月初居家前，跟去年的数对比了下，大概今年去了 70 次左右，\n\n而去年提到的情绪性进食和报复性熬夜两个问题，今年也都有一些缓解，因为慢慢意识到，这两个行为本质上都是一样的，就是 “把自己当做自己的泄欲工具”，而不在这么做的前提是 “把自己当做一个真正的人”，这部分可以去听一下上面提到的播客《我（们）的孤老生活》。我们除了把自己当做自己的泄欲工具，也常常会把他人当做自己的泄欲工具，套用播客里的话是这么说的\n\n在一段关系里边，不管就是情侣关系，还是朋友之间的关系，还是家人等等。就是有的时候我们会简化对方或者理想化对方或者对扭曲化对方，都是因为对方我们没有把对方当成一个完整的人，是我们的欲望的投射，永远在衡量你有没有给到我我要那个东西，不允许对方成为他们的样子；但这里还有个前提：能允许别人是别人，首先要允许他自己是一个人，真正他把自己当成一个完整的人；只有先把自己当做一个人，才能把其他人当做一个人，才能不把自己当做一个泄欲的工具\n允许自己是一个人：不再去做自我评判，接纳自己就是这样一个人（尽人事，听天命）\n\n小结\n去年真的是魔幻的一年，大到防疫政策的 180\n度大转变、二级市场的此起彼伏、席卷而来的海内外裁员潮，小到个人工作的频繁变动；而这些事都在说明同一个事实：市场是无法预测的，未来是无人知晓的\n去年虽然曲折，但家人与自己身体仍然健康，工作也还算顺利，也开始思考了生活更多的可能性；去年提到要培养的技术 / 沟通 / 管理 / 投资等各种能力，今年也都进行相应的学习和总结，也算是有所得\n而今年最大的体会，就是生活存在非常多的随机性，正是这些随机性导致了前面提到的无法预测和无人知晓的未来，相比于面对随机性焦虑不安，小心翼翼走好每一步，更好的选择是享受这些随机性，基于当下境遇作出当下最好的选择，无须后悔或懊恼，毕竟 “盈亏同源”，与君共勉\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"A Survey of Multi-Domain","url":"/2023/03/19/A%20Survey%20of%20Multi-Domain/","content":"在实际的业务中，数据往往由多个 domain\n组成，以广告为例，往往会存在多个转化目标，在 ctr、cvr\n的预估时也要考虑不同转化目标的影响，因为在不同转化目标下，ctr、cvr\n的分布 (如均值、方差) 往往是不一致的\n解决这个问题最直观的思路是加 domain 相关特征或根据 domain\n拆模型，前者属于隐式的方法，需要特征的区分性足够强、能被模型学到，但这个足够强没有一个量化的标准，基本只能看实验效果；后者则存在维护成本过高的问题，比如说有\nn 个 domain 就要拆成 n 个模型\n本文着重讲如何通过一个模型 serve 多个 domain\n的方法，主要是在业界验证有收益且公开发表的工作，基本上可以分为 3 类\n\nmulti-head 结构\n\nLHUC 机制\n\nGRL 机制\n\n\nMMOE\n在一个模型中根据多个 domain 拆成多个 head（每个 head 代表一个\ndomain），通过每个 head 的参数学习特定 domain\n的分布，是一种比较直观和常见的做法\n这类方法的代表是 MMOE: Modeling Task\nRelationships in Multi-task Learning with Multi-gate\nMixture-of-Experts\n下图直观展示了拆 head 的集中常见做法\n\nMMOE 中的两个 M，第一个代表 multi-gate，第二个代表\nmulti-expert；multi-expert 比较好理解，从 ensemble 的角度来看，就是在做\nbagging，而 gate 就是在控制每个 expert 的 weight，multi-gate 则是为每个\nexpert 分配一个 gate，本质上就是做到 domain-wise 的优化\n而 gate 的具体实现，也是一个 mlp, 最终通过 softmax 得到每个 expert 的\nweight，对于第 \\(k\\) 个\ntask，计算过程如下图所示\n\nSTAR\n这是阿里的一篇 paper，应用场景就是比较典型的 CTR 业务，One Model to Serve All: Star\nTopology Adaptive Recommender for Multi-Domain CTR Prediction\n在模型结构上，也是为每个 domain\n分配一部分自己的参数，从而达到在一个模型中 serve 多个 domain\n的目的，这一点跟 MMOE 原理上是一样的，文章是这么说的\n\nEssentially, the network of each domain consists of two factorized\nnetworks: one centered network shared by all domains and the\ndomain-specific network tailored for each domain\n\nSTAR 基本结构如下图 (b) 所示，直观来看，有一个公共的 head，同时为每个\ndomain 分配了一个 head，最终每个 head 的参数是公共 head 参数与 domain\nhead 参数的 element-wise 结果\n\n\nPartitioned Normalization(PN)\n\n在上面的结构中，有一个 Partitioned Normalization (PN)\n的部分，目标是解决统一 batch normalization 在这 multi-domain\n中不适用的问题\n常规的 batch normalization 会计算 batch 内所有的样本的 mean 和\nvariance，然后做归一化，如下图所示；这里有个假设就是这批样本是独立同分布 (i.i.d.) 的，但\nmulti-domain 本身的要解决的问题就是不同 domain\n的分布不一样，因此不能直接用原始的 batch normalization；关于 BN\n为何有效，可参考这篇文章：Rethinking “Batch” in\nBatchNorm\n\n而 PN 的做法是为原始 BN 中的参数 \\(\\gamma\\) 和 \\(\\beta\\) 生成额外的 domain-specify\n参数，如下图所示\n\n\nAuxiliary Network\n\n这是个小网络，输入是 domain indicator (表示这个样本来自哪个\ndomain)，输出是一个预估值，最终输出的预估值会与上面的 STAR\n的模型加和做最终预估；作用等价于为每个 domain 增加了一个 bias 项\n\n实验结果这里不展开，paper 效果显示比一些已有的 multi-domain\n任务要好（参数量是否打平没提到）；也对 PN 的效果做了消融，结果显示 PN\n的效果比直接用 BN 要好\nLHUC\n前面的两篇文章基本思路都是为不同的 domain 分配多一个 head\n的参数，然后通过不同 head 来描述不同 domain 的差异\n提出 LHUC 这篇文章则没有显式地分 head ，而是通过在 hidden layer\n上乘上一个 domain-aware embedding，来达到这样的效果：Learning Hidden Unit\nContributions for Unsupervised Acoustic Model Adaptation\n这篇 paper 最早是在 speech 领域提出的一个方法，基本的思路是为每个\nspeaker 单独调整 nn 中全连接层里部分的参数，从而达到每个 speaker\n的个性化预估；总体思路比较直观，也很容易把方法迁移至推荐上\n快手的提出的 PEPNet (Parameter and Embedding\nPersonalized Network for Infusing with Personalized Prior\nInformation) 也是借鉴了 LHUC 的这个思想\n整个模型结构如下图所示，LHUC 部分是最右边的 PPNet 部分，每个 GateNU\n相当于为每个 hidden layer 生成一个 domain-aware 的 embedding，左边的\nEPNet 则是相当于为 embedding 不用分生成类似的 embedding\n\nGate NU 可以理解为一个简单两层的 nn\n网络，输入是依靠先验知识挑选的能够区分不同 domain\n的特征，输出则是一个 tensor（维度大小与 hidden layer 一样）\n\n\n除了结构上的改进，这篇 paper\n还做了较多的工程上的有优化，这里不详细展开\nGRL\n这里主要想介绍 GRL（Gradient Reversal\nLayer）这个机制，这个机制出自论文 Unsupervised Domain\nAdaptation by Backpropagation\n论文主要想解决的问题是 domain adaption，即在 source domain\n有较多数据，target domain 较少数据，怎么能够较好地同时解决两个 domain\n的问题，paper 里提到的方案是从特征层面去解决这个问题\n\nthe approach promotes the emergence of “deep” features that are (i)\ndiscriminative for the main learning task on the source domain and (ii)\ninvariant with respect to the shift between the domains.\n\n为了达到这个目标，论文提出的 GRL 机制如下图所示\n\n从结构上来看，这也是个 multi-head 的结构，但蓝色的 head 是 source\ndomain 的原始 task，红色的 head（后面简称为 discriminator）\n则是一个 domain classifier，即是用来区分样本是属于哪一个 domain\n的\n两个 task 在做 bp 时，蓝色的 head 正常回传梯度，discriminator\n则在梯度回传到 feature extractor 即图中色绿色部分时，乘上一个 negative\nconstant，这就是 GRL 的机制\n那为什么要这么做呢？paper 给出的解释是这样的，\n\nwe want to make the features \\(f\\)\ndomain-invariant. That is, we want to make the distributions\n\\(S(f) = \\lbrace G_f(x;\\theta_f)|x∼S(x)\n\\rbrace\\) and \\(T(f) = \\lbrace\nG_f(x;\\theta_f)|x∼T(x) \\rbrace\\) to be similar. Under\nthe covariate shift assumption, this would make the label prediction\naccuracy on the target domain to be the same as on the source domain\n(Shimodaira, 2000).\n\n即希望 feature extractor 抽取出来的特征是 domain-invariant、对 domain\n不敏感的，或者说基于抽取出来的特征，discriminator\n不能很好地区分样本来自哪个 domain\n而如果不加 GRL，正常的 bp 是会 discriminator 能够区分样本来自哪个\ndomain 的，加了 GRL 后，则能够达到上面提到的 “discriminator\n不能很好地区分样本来自哪个 domain”\n那另一个问题来了，即为什么要在 feature extractor 这一层做，而不是在\n\\(L_d\\) 上就加一个负号？\n因为如果直接在 \\(L_d\\)\n上加负号，相当于让 discriminator 把它分到错误的那个 domain，但一个好的\nfeature 应该是让 discriminator 分辨不出它来自哪个\ndomain，而不是把它分到错误的那个 domain\n因此，GRL 机制某种程度上也是一类 feature\nengineering，用于提取出适用于多个 domain 的 feature\n小结\n综上，本文主要介绍了三种解决 multi-domain 的思路，分别是 multi-head\n结构，LHUC 机制和 GRL\n机制；据笔者的了解，目前这几种方案在业界都有落地的案例，值得在相应业务中进行尝试～\nmulti-head 机制比较直观，为每个 domain 分配一个 head，分别学习不同\ndomain 的分布，MMOE 和 STAR 这一类模型属于这种；LHUC\n机制则是根据先验选择一批有区分度的特征，通过一个小的 nn\n学习一个隐变量作用到 hidden-layer 上（其实也能作用到 embedding 上，PEPNet\n中没有介绍的 EPNet 就是这部分，原理基本一致）；GRL\n则是通过训练方式使得模型抽取出来的 feature 是 domain-invariant\n的，训练的思想跟 GAN 的对抗训练比较类似\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"A Hybrid Bandit Model with Visual Priors for Creative Ranking in Display Advertising","url":"/2022/03/05/A%20Hybrid%20Bandit%20Model%20with%20Visual%20Priors%20for%20Creative%20Ranking%20in%20Display%20Advertising/","content":"之前的文章 Dynamic\nCreative Optimization in Online Display\nAdvertising 中提到，广告创意往往可分为创意生成、创意优选和创意投放三大块，本文主要讲创意优选这部分的一些做法，这个过程一般会涉及到\nE&amp;E 的过程。\n本文的主要内容是选自阿里发表的一篇 paper：A Hybrid Bandit Model with\nVisual Priors for Creative Ranking in Display Advertising，paper\n通过 list-wise\n的训练方式达到对同一计划下的候选创意进行排序 (即优选) 的目标；list-wise\n可以算作 Exploitation 部分，paper 还通过了一个 bandit model 达到\nExploration\n的目的，总体的做法比较合理，也在业界实际场景验证了有效性，值得一看。\n\npaper 里提出的方法主要分为两大模块：VAM (visual-aware ranking model)\n和 HBM (hybrid bandit model), 总体的模块图如下所示，VAM 即上面提到的基于\nlist-wise 优选做 exploitation 模块，HBM 则是基于 badnit model 做\nexploration 模块，下面也主要从这两个模块进行介绍。\n\n\narchitecture\n\nVAM\nlist-wise loss\nlist-wise 是 learning to rank 里一种建模方式，另外两种分别是\npoint-wise 和 pair-wise，常见的 ctr/cvr 预估都是采用 point-wise\n的方式；\n关于这几种建模方式可参考下面两篇 paper，两篇都是 Microsoft 发表的\npaper，第一篇讲了 point-wise -&gt; pair-wise 的过程，第二篇讲了\npair-wise 到 list-wise 的过程\n\nFrom\nRankNet to LambdaRank to LambdaMART: An Overview\n\nLearning\nto Rank: From Pairwise Approach to Listwise Approach\n\nVAM 采用的 list-wise loss 即是第二篇 paper 中的提出的\nloss，其流程如下图所示，更详细的推导可参考上面第二篇 paper\n\n\nVAM loss\n回到 VAM，上图的 query 相当于商品 (product)，而 list 则是每个 product\n对应的所有创意 (一个 product 往往会有多个候选的创意)\n因此，list 中的每个 item 的 prediction  和 ground truth  表示如下，\n每个符号含义可参考最上面的总体框架图右上角\n\n\n 中的  的作用是\nadjust the scale of the value so that make the probability of top1 sample close to 1，则对于第\n 个 product, 其 list-wise loss\n如下所示\n\n除了常规的 list-wise loss, paper 里还添加了一项 point-wise 的\nauxiliary regression loss，其含义也比较直观，就是让 VAM 的 prediction\n尽可能接近其真实的 CTR 值，其表示如下\n\n根据原文的描述making the outputs close to the real CTRs will significantly stabilize the bandit learning procedure，其作用是让后续的\nHBM 训练更加稳定，则第  个 list\n的总体 loss 如下所示（实验中 \n= 0.5）\n\nnoise mitigation\n这里的 noise mitigation 指的是部分创意的 impression\n会比较少，统计的后验 CTR 波动性较大 (极端的比如只曝光一次)，\n一个粗暴的方法是对 impression 卡个阈值，小于阈值的 item\n就不作为训练数据，但这样可能会导致训练的数据量过少，paper 里采用了如下 2\n种方法\n第一种方法是 label smoothing, 也是这篇 paper 提出的方法 Click-Through Rate\nEstimation for\nRare Events in Online Advertising，其思想是基于贝叶斯学派给点击数\nclicks 和 CTR\n值整个先验分布，这样遇到极端值时也有分布约束，导致最终的值不会太离谱，\n\n\nlabel smoothing\n\n第二种方法是 weighted sampling，就是给点击数少的样本更小的权重，paper\n的做法是对点击数取了个 log 变换作为这个样本的 weight。\nHBM\nHBM 本质上是一个 Bayesian\nLinear\nRegression，从名字大概就能猜测，这个是贝叶斯学派的方法，即认为模型参数是服从一个分布，通过从分布里采样达到\nexploration 的目的，其推导过程如下\n假设线上的数据按照如下方式产生， 表示是否点击， 表示通过 VAM 抽取出来的 visual\nrepresentation,  和\n 则是模型的参数\n\npaper 里将 \n先验分布假设为一个正态分布即 , 同样将 \n假设为一个正态分布，两者互为共轭\n\n\n参考上面 wiki 的推导过程，总体模型的 training 和 serving\n过程如下所示\n\n\nhbm_train_serving\n\n公式 18 可以认为是 training 过程 (对于贝叶斯方法，有个特定的名字 Bayesian\ninference)，在贝叶斯的方法中，更新模型就是更新假定的 distribution\n中的各个参数，本例中就是上面两个分布中的 、、、,\n这里使用的是解析法，但是很多问题解析法是无法解决的，因此常常会利用 Monte\nCarlo sampling 一类方法，\n而 serving 则是从 training 得到的分布中抽样得到 , 计算最终的 score，\n上面的计算 score 的方法是第  个\nproduct 下所有 creative 共用一套参数 , paper 还提出了针对每个 creative\n也应该有一套参数，即针对第  个\nproduct 下的第  个\ncreative，计算的 score 应该是\n\npaper 这样做的原因是\n\nThis simple linear assumption works well for small datasets, but\nbecomes inferior when dealing with industrial data. For example, bright\nand vivid colors will be more attractive for women’s top while concise\ncolors are more proper for 3C digital accessories. In addition to this\nproduct-wise characteristic, a creative may contain a unique designed\nattribute that is not expressed by the shared weights. Hence, it is\nhelpful to have weights that have both shared and non-shared\ncomponents.\n\n所以 paper 根据曝光量算对 score 算了一个权重 , 其计算方法如下\n\n则最终的 socre 如下\n\n这里的思想是在某个 product 的曝光量充足时，更加相信其 product-wise 的\nscore，反之则更相信 creative-wise 的 socre\n但是笔者对这里的做法存疑，笔者认为这个  参数应该做在 creative 粒度，当\ncreative 粒度的数据充足时，应该更相信 creative-wise 的 socre，反之更相信\nproduct-wise 的 score。\n因为如果每个 creative 都有足够的后验数据来进行训练，那做到\ncreative 粒度的个性化参数效果上应该会是最好的，但是问题是现实是很多\ncreative 的后验数据非常系数甚至是没有后验数据的，这个时候采用\nproduct-wise 的 score 相当于是做了一个 clustering，笔者认为这样更加 make\nsense\n因此，HBM 的算法流程如下图所示\n\n\n实验\npaper 里采用了 2 个评估指标：Simulated CTR (sCTR) 和 Cumulative\nregret, 前者模拟 online learning 过程，后者则是评估 bandit model\n，两者计算方法如下，但是好像这两个指标不是非常通用？\n\n\n\n\n采用的评估数据集有 2\n个，一个是自建的，另一个是公开数据集，效果上自然也是 paper\n提出的效果最好，但是 paper 没有做在线的 ab\n实验，逼近离线指标跟线上的效果还是有 diff 的\n小结\n总的来说，这篇 paper 提出一种 creative selection 的方法，由 VAM + HBM\n组成，笔者认为有以下几点值得学习\n\nVAM 利用投后数据 (ctr), 通过 list-wise 方法学习出 creative 的 visual\nrepresentation\n\nHBM 利用 VAM 的 visual representation 通过 bandit model，来实现\nexploration 部分，同时考虑了 product-wise 和 creative-wise 建模和预估\nscore 的融合\n\n但是也有以下几点笔者是存疑的\n\nproduct-wise 和 creative-wise 的分数时， 参数只考虑到 product-wise\n的信息，没能很好体现 creative-wise 的权重，具体原因上面说了\n\n广告系统通常是召回 + 精排的环节，精排往往是 creative\n粒度的，上面提出的系统未必能完整地融入现有的广告系统，倒是 VAM\n训练得到的 visual representation 作为精排模型的一个 feature\n是一种可能的方法\n\nVAM 已经可以对候选创意做优选了，为什么还需要 HBM 来做\nexploration？或者说 exploration 是为了拿什么收益？\n根据笔者的经验，在广告系统中 exploration 意味着破坏原来系统 feedback\nloop\n所形成的分布，这往往会破坏系统由于马太效应形成的稳态，往往会造成收入的下降，相对应兑换的是一些生态指标或者信仰指标的提升。\n\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"A Tour of Go 摘记","url":"/2018/11/28/A%20Tour%20of%20Go%20%20%E6%91%98%E8%AE%B0/","content":"最近在学习 golang，本文主要是 A Tour of Go 的一些摘记，涵盖了 go\n的一些基本语法与数据类型、通过 struct 和 method 实现类的特性、以及 go\n中重要的 concurrency 特性。\n\nBasics\npackages, variables and\nfunctions\n\n程序的入口在 main 这个 package 中，\n需要在文件头声明\nimport 通过 () 引入多个 package\nimport (    \"fmt\"    \"math/rand\")\n定义变量的几种形式（定义常量只能用第一种方法，且 var 改成 const)\n// 方式 1var i, j intvar i, j int = 1, 2var i, j int = 1, 2// 方式 2i, j := 1, 2\ngolang 中的变量类型 boolstringint  int8  int16  int32  int64uint uint8 uint16 uint32 uint64 uintptrbyte // alias for uint8rune // alias for int32     // represents a Unicode code pointfloat32 float64complex64 complex128\n函数声明时类型放在最后：参数类型放在参数后，返回类型放在函数名后；\n且连续多个参数类型一样时可只为最后一个写类型，如下\nfunc add(x int, y int) int {    return x + y}// same as abovefunc add(x , y int) int {    return x + y}\n函数可返回多个值，\n且返回值可以被命名（此时的返回值相当函数里两个命名的变量）\n// 多个返回值func swap(x, y string) (string, string) {    return y, x}// 具名返回值, 返回 x 和 yfunc split(sum int) (x, y int) {    x = sum * 4 / 9    y = sum - x    return}\n函数可作为参数传入其他函数，也可作为返回值，如下\nimport (    \"fmt\"    \"math\")func compute(fn func(float64, float64) float64) float64 {    return fn(3, 4)}func main() {    hypot := func(x, y float64) float64 {        return math.Sqrt(x*x + y*y)    }    fmt.Println(hypot(5, 12))    fmt.Println(compute(hypot))    fmt.Println(compute(math.Pow))}\n闭包（一个函数访问并能更新在其函数域外的变量）一般会将函数作为返回值\nfunc fibonacci() func() int {    a, b := -1, 1    return func() int {        a, b = b, a + b        return b    }}func main() {    f := fibonacci()    for i := 0; i &lt; 10; i++ {        fmt.Println(f())    }}\n\nfor, if else, switch and\ndefer\n\ngo 里面没有 while，for 相当于 while\n 包含 for 的三个元素的圆括号 () 是没有的 (不仅是 for\n语句，if else 等其他语句也没有)，且花括号 {}\n总是必须的 (哪怕只有一条语句) sum := 0for i := 0; i &lt; 10; i++ {    sum += i}// 三个元素的头尾元素可缺省sum := 1for ; sum &lt; 1000; {    sum += sum}\nfor 跟 range 结合可以遍历 slice 和 map, 每次返回两个值，对于 map\n是 key:value，对于 slice 则是 index:value var pow = []int{1, 2, 4, 8, 16, 32, 64, 128}func main() {    for i, v := range pow {        fmt.Printf(\"2**%d = %d\\n\", i, v)    }    // 不要第一个元素    for _, v := range pow {    }    // 不要第二个元素    for i := range pow {    }}\nif 语句的判断条件中可带有一个短的声明语句 if v := math.Pow(3, 5); v &lt; 100 {    return v}\nswitch 语句不需要 break（实际上是 go 自动添加了）\nimport (    \"fmt\"    \"runtime\")func main() {    fmt.Print(\"Go runs on \")    switch os := runtime.GOOS; os {    case \"darwin\":        fmt.Println(\"OS X.\")    case \"linux\":        fmt.Println(\"Linux.\")    default:        // freebsd, openbsd,        // plan9, windows...        fmt.Printf(\"%s.\", os)    }}\ndefer 后面的语句直到其所在的函数返回才执行，实际上 defer\n后面的语句被 push 到了 stack 中，返回时就出栈 // 输出 hello worldfunc main() {    defer fmt.Println(\"world\")    fmt.Println(\"hello\")}// 从 9 到 0 输出for i := 0; i &lt; 10; i++ {    defer fmt.Println(i)}\n\npointer, struct, slice and\nmap\n\ngo 中的指针跟 C/C++ 中的类似 import \"fmt\"func main() {    i, j := 42, 2701    var p1 *int             p1 = &amp;i    fmt.Println(*p1) // read i through the pointer    *p1 = 21         // set i through the pointer    fmt.Println(i)  // see the new value of i    p2 := &amp;j         // point to j    *p2 = *p2 / 37   // divide j through the pointer    fmt.Println(j) // see the new value of j}\ngo 中的结构体也跟 C/C++ 的类似，只是声明方式不一样，多了 type\n这个关键字，且类型 struct 放在最后 type Vertex struct {    X int    Y int}func main() {    v := Vertex{1, 2}    fmt.Println(v.X)}\n指向结构体的指针访问结构体的元素的方式跟 C/C++ 不一样，C/C++ 要用\n-&gt;, go 中可直接使用 .\ntype Vertex struct {    X int    Y int}func main() {    v := Vertex{1, 2}    p := &amp;v    p.X = 1e9    fmt.Println(v)}\narray 与 slice 是两个不同的类型，区别在于 array\n长度固定，slice 长度可变，声明时一个指定长度，一个不指定长度，语法均是\n[] type func main() {    // array 声明方式一    var a [2]string    a[0] = \"Hello\"    a[1] = \"World\"    fmt.Println(a[0], a[1])    fmt.Println(a)    // array 声明方式二    primes := [6]int{2, 3, 5, 7, 11, 13}    fmt.Println(primes)    // slice 声明方式一    s := prime[:4]    // slice 声明方式二    var s []int    // slice 声明方式三    s := []int{2, 3, 5, 7, 11, 13}    // slice 声明方式四    s := make([]int, 5)\n需要注意的是，每个 slice 都有一个 underlying array，\nslice 就是这个 array 的 reference ，当 slice 被其他 array\n的部分元素初始化时，修改 slice 就是在修改这个 array；slice\n有两个方法：len() 和 cap(), len()\n是 slice 的长度，cap() 则是 slice 对应的 underlying array\n从 slice 的第一个元素开始计算的长度 func printSlice(s []string) {    fmt.Printf(\"len=%d cap=%d %v\\n\", len(s), cap(s), s)}func main() {    names := [4]string{        \"John\",        \"Paul\",        \"George\",        \"Ringo\",    }    fmt.Println(names)    a := names[0:2]    b := names[1:3]    printSlice(a)    printSlice(b)    b[0] = \"XXX\"    fmt.Println(a, b)    fmt.Println(names)}// 输出如下[John Paul George Ringo]len=2 cap=4 [John Paul]len=2 cap=3 [Paul George][John XXX] [XXX George][John XXX George Ringo]\nslice 是可变长的，通过 append 函数往 slice 末尾添加元素，如下\nfunc main() {    var s []int    printSlice(s)    // The slice grows as needed.    s = append(s, 1)    printSlice(s)    // We can add more than one element at a time.    s = append(s, 2, 3, 4)    printSlice(s)}\nmap 初始化方式有以下几种，map 声明的中括号里面是 key\n的类型，外面是 value 的类型，即 map[key]value\n// method 1m := make(map[int]int)// method 2var m = map[int]int{1:1, 2:2}\n删除 map 中某个元素可直接使用\ndelete (map, key)\n测试某个 key 是否在 map 中可通过 map[key]\n返回一个两元组实现，即 elem, ok := m[key], 如果 ok 为\ntrue，则表示 key 里面\n\nMethods and interfaces\n\ngo 没有提供类，但是可以通过为 struct 定义 method\n来提供类相近的特性；method 就是在普通函数基础上定义一个 receiver\n参数（定义在 func 和函数名之间），表明这个方法是属于某个\nstruct 的 type Vertex struct {    X, Y float64}func (v1 Vertex) Abs() float64 {    return math.Sqrt(v1.X*v1.X + v1.Y*v1.Y)}func main() {    v := Vertex{3, 4}    fmt.Println(v.Abs())}\nreveiver 参数也可以是指针类型，这意味着通过 receiver\n参数能够直接修改 struct\n的值 (函数的普通参数也是需要指针才能修改实参的值)， 同时也不用在调用\nmethod 时创建新的内存来存储临时对象，因此指针类型的 receiver 也更加常用\ntype Vertex struct {    X, Y float64}func (v Vertex) Abs() float64 {    return math.Sqrt(v.X*v.X + v.Y*v.Y)}func (v *Vertex) Scale(f float64) {    v.X = v.X * f    v.Y = v.Y * f}func main() {    v := Vertex{3, 4}    v.Scale(10) // equal to (&amp;v).Scale(10)    fmt.Println(v.Abs())}\ngo 中的接口（interface) 概念与\nJava 中的类似，声明一系列的方法，实现了这些方法就是实现了这个接口 (无需显式声明）\ntype I interface {    M()}type T struct {    S string}// This method means type T implements the interface I,// but we don't need to explicitly declare that it does so.func (t T) M() {    fmt.Println(t.S)}func main() {    var i I = T{\"hello\"}    i.M()}\n一个非常普遍的 interface 是 Stringer, 这是由\nfmt 这个 package 中定义的，一旦实现过了这个接口里面的\nString() string 方法，fmt.Println()\n时就会调用这个方法 type Stringer interface {    String() string}type Person struct {    Name string    Age  int}func (p Person) String() string {    return fmt.Sprintf(\"%v (%v years)\", p.Name, p.Age)}func main() {    a := Person{\"Arthur Dent\", 42}    z := Person{\"Zaphod Beeblebrox\", 9001}    fmt.Println(a, z)}// 输出Arthur Dent (42 years) Zaphod Beeblebrox (9001 years)\n类似于上面的接口 fmt.Stringer, error\n也是一个常用的接口，实现该接口需要实现 Error 方法\nimport (    \"fmt\"    \"time\")type MyError struct {    When time.Time    What string}func (e *MyError) Error() string {    return fmt.Sprintf(\"at %v, %s\",        e.When, e.What)}func run() error {    return &amp;MyError{        time.Now(),        \"it didn't work\",    }}func main() {    if err := run(); err != nil {        fmt.Println(err)    }}\ngo 的很多标准库都实现了 io.Reader\n这个接口，接口主要定义了这个方法\nfunc (T) Read(b []byte) (n int, err error),\n如下是一个简单地用法，每次从 string 中读取 8 个 byte\nimport (    \"fmt\"    \"io\"    \"strings\")func main() {    r := strings.NewReader(\"Hello, Reader!\")    b := make([]byte, 8)    for {        n, err := r.Read(b)        fmt.Printf(\"n = %v err = %v b = %v\\n\", n, err, b)        fmt.Printf(\"b[:n] = %q\\n\", b[:n])        if err == io.EOF {            break        }    }}\n\nConcurrency\n\ngoroutine 相当于是轻量级的线程，使用方法很简单\ngo f() 就启动了一个 goroutine 来执行函数\nf()\nchannel\n类似于一个队列，但是出列时要等到所有入列的操作已完成则可进行，反之亦然，这就为\ngoroutine 提供了 synchronize 的功能 ch := make(chan int) // create a channel of type intch &lt;- v    // Send v to channel ch.v := &lt;-ch  // Receive from ch, and assign value to v.// simple examplefunc sum(s []int, c chan int) {    sum := 0    for _, v := range s {        sum += v    }    c &lt;- sum // send sum to c}func main() {    s := []int{7, 2, 8, -9, 4, 0}    c := make(chan int)    go sum(s[:len(s)/2], c)    go sum(s[len(s)/2:], c)    x, y := &lt;-c, &lt;-c // receive from c    fmt.Println(x, y, x+y)}// output -5 17 12\nBuffered channel：可以为 channel\n指定长度 (如 chan := make(chan int, 100))，这样当 channel\n满了之后不能再往其中写数据（再写会报错），这种 channel 也被称为 buffered\nchannel；也可以 close 一个 channel，\n这样也不能继续入列\n通过 range 来遍历 channel 会自动判断 channel\n是否已经为空，即 for v := range channel,\n需要注意的是，用 for 来遍历一个 channel 时，该 channel 必须要先\nclose，否则会出现错误 fatal error: all goroutines are\nasleep - deadlock!\nselect 包含的代码块中有多个 case\n语句，当其中的任一条件被满足时才会执行，否则会阻塞（可以添加\ndefault 选项使得 select\n不会被阻塞），当有多个被满足时则随机选一个；通常 select\n也被用来协调多个 goroutine 的通信，如下 func fibonacci(c, quit chan int) {    x, y := 0, 1    for {        select {        case c &lt;- x:            x, y = y, x+y        case &lt;-quit:            fmt.Println(\"quit\")            return        }    }}func main() {    c := make(chan int)    quit := make(chan int)    go func() {        for i := 0; i &lt; 10; i++ {            fmt.Println(&lt;-c)        }        quit &lt;- 0    }()    fibonacci(c, quit)}\nchannel 是 goroutine 通信的一个有效工具，但是除了通信，多个\ngoroutine 往往还会存在着同时读写一个变量的情况，这时候就要加锁，os\n中也将这样的锁机制成为互斥量（mutex），go 的 sync.Mutex\n就是一个能够实现加锁和解锁的互斥量；如下是多个 goroutine 共享一个 Counter\n的例子 import (    \"fmt\"    \"sync\"    \"time\")// SafeCounter is safe to use concurrently.type SafeCounter struct {    v   map[string]int    mux sync.Mutex}// Inc increments the counter for the given key.func (c *SafeCounter) Inc(key string) {    c.mux.Lock()    // Lock so only one goroutine at a time can access the map c.v.    c.v[key]++    c.mux.Unlock()}// Value returns the current value of the counter for the given key.func (c *SafeCounter) Value(key string) int {    c.mux.Lock()    // Lock so only one goroutine at a time can access the map c.v.    defer c.mux.Unlock()    return c.v[key]}func main() {    c := SafeCounter{v: make(map[string]int)}    for i := 0; i &lt; 1000; i++ {        go c.Inc(\"somekey\")    }    time.Sleep(time.Second)    fmt.Println(c.Value(\"somekey\"))}\n\n","categories":["go"],"tags":["go"]},{"title":"Adam 那么棒，为什么还对 SGD 念念不忘","url":"/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/","content":"好久没更新了，最近在忙着写毕业论文，刚好写到与优化相关部分，想起了之前在知乎上收藏过的一篇很好的文章，重新看一遍还是觉得获益良多，特意转载。原文链接见这里，侵删。\n\n机器学习界有一群炼丹师，他们每天的日常是：\n拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。\n不过，当过厨子的都知道，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。\n机器学习也是一样，模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。(注：笔者有过亲身经历，曾经使用\nAdam 一开局就陷入局部最优，检查了好多遍代码，最后近乎绝望地换了 SGD\n，然后效果蹭蹭蹭上升)\n说到优化算法，入门级必从 SGD\n学起，老司机则会告诉你更好的还有 AdaGrad/AdaDelta，或者直接无脑用\nAdam。可是看看学术界的最新 paper，却发现一众大神还在用着入门级的\nSGD，最多加个 Momentum 或者 Nesterov ，还经常会黑一下 Adam。比如 UC\nBerkeley 的一篇论文就在 Conclusion 中写道 (注：这篇论文是 The\nMarginal Value of Adaptive Gradient Methods in Machine Learning)\n\nDespite the fact that our experimental evidence demonstrates that\nadaptive methods are not advantageous for machine learning, the Adam\nalgorithm remains incredibly popular. We are not sure exactly as to why\n……\n\n无奈与酸楚之情溢于言表。\n这是为什么呢？难道平平淡淡才是真？\n一个框架回顾优化算法\n首先我们来回顾一下各类优化算法。\n深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad\n-&gt; AdaDelta -&gt; Adam -&gt; Nadam\n这样的发展历程。Google 一下就可以看到很多的教程文章，详细告诉你这些算法是如何一步一步演变而来的。在这里，我们换一个思路，用一个框架来梳理所有的优化算法，做一个更加高屋建瓴的对比。\n首先定义：待优化参数： \\(w\\)\n，目标函数： \\(f(w)\\) ，初始学习率\n\\(\\alpha\\)\n而后，开始进行迭代优化。在每个 epoch \\(t\\), 执行如下操作：\n\n计算目标函数关于当前参数的梯度： \\(g_t=\\nabla f(w_t)\\)\n 根据历史梯度计算一阶动量和二阶动量：\\(m_t\n= \\phi(g_1, g_2, \\cdots, g_t)\\); \\(V_t\n= \\psi(g_1, g_2, \\cdots, g_t)\\)\n 计算当前时刻的下降梯度： \\(\\eta_t = \\alpha\n\\cdot m_t / \\sqrt{V_t}\\)\n 根据下降梯度进行更新： \\(w_{t+1} = w_t -\n\\eta_t\\)\n\n掌握了这个框架，你可以轻轻松松设计自己的优化算法。\n我们拿着这个框架，来照一照各种玄乎其玄的优化算法的真身。步骤 3、4 对于各个算法都是一致的，主要的差别就体现在 1 和 2 上。\nSGD\n先来看 SGD, SGD 没有动量的概念，也就是说：\n\\(m_t = g_t\\) \\(V_t = I^2\\)\n代入步骤 3，可以看到下降梯度就是最简单的\n\\(\\eta_t = \\alpha \\cdot g_t\\)\nSGD 最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。\nSGD with Momentum\n为了抑制 SGD 的震荡，SGDM\n认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM\n全称是 SGD with\nmomentum，在 SGD 基础上引入了一阶动量：\n\\(m_t = \\beta_1 \\cdot m_{t-1} +\n(1-\\beta_1)\\cdot g_t\\)\n一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近\n\\(1/(1-\\beta_1)\\)\n个时刻的梯度向量和的平均值。\n也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。\n\\(\\beta_1\\)\n的经验值为 0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。\nSGD with Nesterov\nAcceleration\nSGD\n还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。\nNAG 全称 Nesterov Accelerated Gradient，是在 SGD、SGD-M\n的基础上的进一步改进，改进点在于步骤 1。我们知道在时刻 t\n的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG 在步骤\n1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：\n\\(g_t=\\nabla f(w_t-\\alpha \\cdot m_{t-1} /\n\\sqrt{V_{t-1}})\\)\n然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤 2 中当前时刻的累积动量。\nAdaGrad\n此前我们都没有用到二阶动量。二阶动量的出现，才意味着 “自适应学习率” 优化算法时代的到来。SGD\n及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的 embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。\n怎么样去度量历史更新频率呢？那就是二阶动量 —— 该维度上，迄今为止所有梯度值的平方和：\n\\(V_t = \\sum_{\\tau=1}^{t}\ng_\\tau^2\\)\n我们再回顾一下步骤 3 中的下降梯度：\n\\(\\eta_t = \\alpha \\cdot m_t /\n\\sqrt{V_t}\\)\n可以看出，此时实质上的学习率由 \\(\\alpha\\) 变成了 \\(\\alpha / \\sqrt{V_t}\\) 。\n一般为了避免分母为 0，会在分母上加一个小的平滑项。因此 \\(\\sqrt{V_t}\\)\n是恒大于 0 的，而且参数更新越频繁，二阶动量越大，学习率就越小。\n这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为\n\\(\\sqrt{V_t}\\)\n是单调递增的，会使得学习率单调递减至 0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。\nAdaDelta / RMSProp\n由于 AdaGrad\n单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是\nAdaDelta 名称中 Delta 的来历。\n修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：\n\\(V_t = \\beta_2 * V_{t-1} + (1-\\beta_2)\ng_t^2\\)\n这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。\nAdam\n谈到这里，Adam 和 Nadam\n的出现就很自然而然了 —— 它们是前述方法的集大成者。我们看到，SGD-M 在 SGD\n基础上增加了一阶动量，AdaGrad 和 AdaDelta 在 SGD\n基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是\nAdam 了 ——Adaptive + Momentum。\nSGD 的一阶动量：\n\\(m_t = \\beta_1 \\cdot m_{t-1} +\n(1-\\beta_1)\\cdot g_t\\)\n加上 AdaDelta 的二阶动量：\n\\(V_t = \\beta_2 * V_{t-1} + (1-\\beta_2)\ng_t^2\\)\n优化算法里最常见的两个超参数 \\(\\beta_1\\), \\(\\beta_2\\)\n就都在这里了，前者控制一阶动量，后者控制二阶动量。\nNadam\n最后是 Nadam。我们说 Adam 是集大成者，但它居然遗漏了 Nesterov，这还能忍？必须给它加上，按照 NAG 的步骤 1：\n\\(g_t=\\nabla f(w_t-\\alpha \\cdot m_{t-1} /\n\\sqrt{V_t})\\)\n这就是 Nesterov + Adam = Nadam 了。\n说到这里，大概可以理解为什么 j 经常有人说 Adam / Nadam\n目前最主流、最好用的优化算法了。新手上路，先拿来一试，收敛速度嗖嗖滴，效果也是杠杠滴。\n那为什么 Adam 还老招人黑，被学术界一顿鄙夷？难道只是为了发 paper 灌水吗？\nAdam 的两宗罪\n从理论上看，一代更比一代完善，Adam/Nadam\n已经登峰造极了，为什么大家还是不忘初心 SGD 呢？\n举个栗子。很多年以前，摄影离普罗大众非常遥远。十年前，傻瓜相机开始风靡，游客几乎人手一个。智能手机出现以后，摄影更是走进千家万户，手机随手一拍，前后两千万，照亮你的美（咦，这是什么乱七八糟的）。但是专业摄影师还是喜欢用单反，孜孜不倦地调光圈、快门、ISO、白平衡…… 一堆自拍党从不\ncare\n的名词。技术的进步，使得傻瓜式操作就可以得到不错的效果，但是在特定的场景下，要拍出最好的效果，依然需要深入地理解光线、理解结构、理解器材。\n优化算法大抵也如此。在上一篇中，我们用同一个框架让各类算法对号入座。可以看出，大家都是殊途同归，只是相当于在 SGD 基础上增加了各类学习率的主动控制。如果不想做精细的调优，那么 Adam 显然最便于直接拿来上手。\n但这样的傻瓜式操作并不一定能够适应所有的场合。如果能够深入了解数据，研究员们可以更加自如地控制优化迭代的各类参数，实现更好的效果也并不奇怪。毕竟，精调的参数还比不过傻瓜式的 Adam，无疑是在挑战顶级研究员们的炼丹经验！\nAdam 罪状一：可能不收敛\n这篇是正在深度学习领域顶级会议之一 ICLR 2018 匿名审稿中的 On the Convergence of\nAdam and\nBeyond，探讨了 Adam 算法的收敛性，通过反例证明了 Adam 在某些情况下可能会不收敛。(注:\n这篇论文已经成了 2018 ICLR 最佳论文)\n回忆一下上文提到的各大优化算法的学习率：\n\\(\\eta_t = \\alpha / \\sqrt{V_t}\\)\n其中，SGD\n没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad\n的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到 0，模型也得以收敛。\n但 AdaDelta 和 Adam\n则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得\n\\(V_t\\)\n可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。\n这篇文章也给出了一个修正的方法。由于 Adam 中的学习率主要是由二阶动量控制的，为了保证算法的收敛，可以对二阶动量的变化进行控制，避免上下波动。\n\\(V_t = max(\\beta_2 * V_{t-1} + (1-\\beta_2)\ng_t^2, V_{t-1})\\)\n通过这样修改，就保证了 \\(||V_t|| \\geq\n||V_{t-1}||\\) ，从而使得学习率单调递减。\nAdam 罪状二：可能错过全局最优解\n深度神经网络往往包含大量的参数，在这样一个维度极高的空间内，非凸的目标函数往往起起伏伏，拥有无数个高地和洼地。有的是高峰，通过引入动量可能很容易越过；但有些是高原，可能探索很多次都出不来，于是停止了训练。\n近期 Arxiv 上的两篇文章谈到这个问题。\n第一篇就是前文提到的吐槽 Adam 最狠的 The Marginal Value of Adaptive\nGradient Methods in Machine Learning\n。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。\n另外一篇是 Improving\nGeneralization Performance by Switching from Adam to\nSGD，进行了实验验证。他们 CIFAR-10 数据集上进行测试，Adam\n的收敛速度比 SGD\n要快，但最终收敛的结果并没有 SGD 好。他们进一步实验发现，主要是后期 Adam\n的学习率太低，影响了有效的收敛。他们试着对 Adam 的学习率的下界进行控制，发现效果好了很多。\n于是他们提出了一个用来改进 Adam 的方法：前期用 Adam，享受 Adam 快速收敛的优势；后期切换到 SGD，慢慢寻找最优解。这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。这篇文章把这一切换过程傻瓜化，给出了切换 SGD 的时机选择方法，以及学习率的计算方法，效果看起来也不错。\n到底该用 Adam 还是 SGD？\n所以，谈到现在，到底 Adam 好还是 SGD 好？这可能是很难一句话说清楚的事情。去看学术会议中的各种 paper，用 SGD 的很多，Adam 的也不少，还有很多偏爱 AdaGrad 或者 AdaDelta。可能研究员把每个算法都试了一遍，哪个出来的效果好就用哪个了。\n而从这几篇怒怼 Adam 的 paper\n来看，多数都构造了一些比较极端的例子来演示了 Adam\n失效的可能性。这些例子一般过于极端，实际情况中可能未必会这样，但这提醒了我们，理解数据对于设计算法的必要性。优化算法的演变历史，都是基于对数据的某种假设而进行的优化，那么某种算法是否有效，就要看你的数据是否符合该算法的胃口了。\n算法固然美好，数据才是根本。\n另一方面，Adam 之流虽然说已经简化了调参，但是并没有一劳永逸地解决问题，默认参数虽然好，但也不是放之四海而皆准。因此，在充分理解数据的基础上，依然需要根据数据特性、算法特性进行充分的调参实验，找到自己炼丹的最优解。而这个时候，不论是 Adam，还是 SGD，于你都不重要了。\n少年，好好炼丹吧。\n优化算法的选择与使用策略\n在前面两节中，我们用一个框架梳理了各大优化算法，并且指出了以 Adam 为代表的自适应学习率优化算法可能存在的问题。那么，在实践中我们应该如何选择呢？本文介绍 Adam+SGD 的组合策略，以及一些比较有用的 tricks.\n不同优化算法的核心差异\n下降方向从第一篇的框架中我们看到，不同优化算法最核心的区别，就是第三步所执行的下降方向\n\\(\\eta_t = (\\alpha/ \\sqrt{V_t} ) \\cdot\nm_t\\)\n这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。SGD\n算法的下降方向就是该位置的梯度方向的反方向，带一阶动量的 SGD 的下降方向则是该位置的一阶动量方向。自适应学习率类优化算法为每个参数设定了不同的学习率，在不同维度上设定不同步长，因此其下降方向是缩放过（scaled）的一阶动量方向。\n由于下降方向的不同，可能导致不同算法到达完全不同的局部最优点。\nAn empirical analysis of the\noptimization of deep network loss surfaces\n这篇论文中做了一个有趣的实验，他们把目标函数值和相应的参数形成的超平面映射到一个三维空间，这样我们可以直观地看到各个算法是如何寻找超平面上的最低点的。\n\n\noptimizer_compare\n\n上图是论文的实验结果，横纵坐标表示降维后的特征空间，区域颜色则表示目标函数值的变化，红色是高原，蓝色是洼地。他们做的是配对儿实验，让两个算法从同一个初始化位置开始出发，然后对比优化的结果。可以看到，几乎任何两个算法都走到了不同的洼地，他们中间往往隔了一个很高的高原。这就说明，不同算法在高原的时候，选择了不同的下降方向。\nAdam+SGD 组合策略\n正是在每一个十字路口的选择，决定了你的归宿。如果上天能够给我一个再来一次的机会，我会对那个女孩子说：SGD！\n不同优化算法的优劣依然是未有定论的争议话题。据我在 paper 和各类社区看到的反馈，主流的观点认为：Adam 等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的 SGD（+Momentum）往往能够取得更好的最终结果。\n那么我们就会想到，可不可以把这两者结合起来，先用 Adam 快速下降，再用 SGD 调优，一举两得？思路简单，但里面有两个技术问题：\n\n什么时候切换优化算法？—— 如果切换太晚，Adam 可能已经跑到自己的盆地里去了，SGD 再怎么好也跑不出来了。\n切换算法以后用什么样的学习率？——Adam 用的是自适应学习率，依赖的是二阶动量的累积，SGD 接着训练的话，用什么样的学习率？\n\n上一篇中提到的论文 Improving Generalization Performance by Switching\nfrom Adam to SGD 提出了解决这两个问题的思路\n首先来看第二个问题，切换之后用什么样的学习率。Adam 的下降方向是\n\\(\\eta_t^{Adam} = (\\alpha/ \\sqrt{V_t} )\n\\cdot m_t\\)\n而 SGD 的下降方向是\n\\(\\eta_t^{SGD} = \\alpha^{SGD} \\cdot\ng_t\\)\n\\(\\eta_t^{SGD}\\) 必定可以分解为\n\\(\\eta_t^{Adam}\\)\n所在方向及其正交方向上的两个方向之和，如下图所示，这里 p 为 Adam 下降方向，g 为梯度方向，r 为 SGD 的学习率。\n\n\nadam_to_sgd_direction\n\n那么其在 \\(\\eta_t^{Adam}\\)\n方向上的投影就意味着 SGD 在 Adam 算法决定的下降方向上前进的距离，而在\n\\(\\eta_t^{Adam}\\) 的正交方向上的投影是\nSGD 在自己选择的修正方向上前进的距离。\n如果 SGD 要走完 Adam 未走完的路，那就首先要接过 Adam 的大旗 —— 沿着 \\(\\eta_t^{Adam}\\)\n方向走一步，而后在沿着其正交方向走相应的一步。\n这样我们就知道该如何确定 SGD 的步长（学习率）了 ——SGD 在 Adam 下降方向上的正交投影，应该正好等于 Adam 的下降方向（含步长）。也即：\n\\(proj_{\\eta_t^{SGD}} =\n\\eta_t^{Adam}\\)\n解这个方程，我们就可以得到接续进行 SGD 的学习率：\n\\(\\alpha_t^{SGD} =\n((\\eta_t^{Adam})^T\\eta_t^{Adam})/((\\eta_t^{Adam})^Tg_t)\\)\n为了减少噪声影响，作者使用移动平均值来修正对学习率的估计：\n\\(\\lambda_t^{SGD} = \\beta_2 \\cdot\n\\lambda_{t-1}^{SGD}+(1-\\beta_2) \\cdot \\alpha_t^{SGD}\\)\n\\(\\tilde{\\lambda}_t^{SGD}=\\lambda_t^{SGD}/(1-\\beta_2^t)\\)\n这里直接复用了 Adam 的 \\(\\beta_2\\)\n参数。\n然后来看第一个问题，何时进行算法的切换。\n作者的回答也很简单，那就是当 SGD\n的相应学习率的移动平均值基本不变的时候，即：\n\\(|\\tilde{\\lambda}_t^{SGD} -\n\\alpha_t^{SGD}|&lt;\\epsilon\\).\n每次迭代玩都计算一下 SGD 接班人的相应学习率，如果发现基本稳定了，那就 SGD\n以 \\(\\tilde{\\lambda}_t^{SGD}\\)\n为学习率接班前进。\n优化算法的常用 tricks\n最后，分享一些在优化算法的选择和使用方面的一些 tricks。\n\n首先，各大算法孰优孰劣并无定论。如果是刚入门，优先考虑\nSGD+Nesterov Momentum 或者 Adam.（Standford 231n :\nThe two recommended updates to use are either SGD+Nesterov Momentum or\nAdam）\n选择你熟悉的算法 —— 这样你可以更加熟练地利用你的经验进行调参\n充分了解你的数据 —— 如果数据是非常稀疏的，那么优先考虑自适应学习率的算法。\n根据你的需求来选择 —— 在模型设计实验过程中，要快速验证新模型的效果，可以先用 Adam 进行快速实验优化；在模型上线或者结果发布前，可以用精调的 SGD 进行模型的极致优化。\n先用小数据集进行实验。论文 Stochastic\nGradient Descent Tricks\n指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。\n考虑不同算法的组合。先用 Adam 进行快速下降，而后再换到\nSGD 进行充分的调优。切换策略可以参考本文介绍的方法。\n数据集一定要充分的打散（shuffle）。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。\n训练过程中持续监控训练数据和验证数据上的目标函数值以及精度或者\nAUC\n等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练 —— 下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。\n制定一个合适的学习率衰减策略。可以使用定期衰减策略，比如每过多少个\nepoch 就衰减一次；或者利用精度或者 AUC\n等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。\n\n","categories":["机器学习"],"tags":["机器学习","数学","转载"]},{"title":"Binary Indexed Trees 简介","url":"/2016/07/12/Binary%20Indexed%20Trees%20%E7%AE%80%E4%BB%8B/","content":"Binary Indexed\nTrees（中文名为树状数组，下文简称为 BIT）是一种特殊的数据结构，可多用于高效计算数列的前缀和，\n区间和。对于长度为 n 的数组，它可以以 \\(O(logn)\\) 的时间得到任意前缀和 $\n{_{i=1}^{j} a [i],1&lt;=j&lt;=N}$，并同时支持在 $ O (log\nn)$ 时间内支持动态单点值的修改。空间复杂度 \\(O(n)\\)\n\n虽然 BIT 名称中带有 tree 这个词，但是实际存储时是利用两个数组进行存储，记这两个数组为 nums 和\nBIT。假设我们现在需要对原始数组 arr\n进行前缀求和和区间求和，那么可以按照以下步骤进行。\n1. 初始化\n\\(nums[i] = arr[i]\\) \\(BIT[i] = {\\displaystyle \\sum\n_{k=i-lowestbit(i)+1}^{i}arr[k]}\\)\n上面的 lowestbit(i) 指将 i 转为二进制后，最后一个 1 的位置所代表的数值。如 lowestbit(1)=1、lowestbit(6)=2，具体的实现可通过 (i&amp;-i) 获取。\n下图就是初始化后的情况，横轴为数组的下标 (记为 i)，纵轴为下标数值对应的 lowestbit（i&amp;-i），长方形表示 BIT [i] 涵盖的求和的范围\n![][1]\n可以看到每个数组下标的 lowestbit（也就是图中描黑的部分）在形态上构成了一棵树的形状，这也是名称中 tree 的来源。并且对于每个下标的 lowestbit 表示成的 tree\nnode 有以下特性。\n(1) 假如 i 是左子节点，那么其父节点下标为 i+(lowestbit (i))\n(2) 假如 i 是右子节点，那么其父节点下标为 i-(lowestbit (i))\n上面这两个特性非常重要，也是我们进行后文分析的重要基础。\n2. 更新一个数值 假如要修改原始数组 arr\n中的下标为 i 的值，那么需要修改 nums 数组中对应下标的值。除此之外还需要修改 BIT 数组中涵盖了 arr[i] 的值。结合上图可以知道，BIT 数组中涵盖了 arr[i] 的值为下标 i 及其所有父节点，伪代码如下\nwhile i &lt; n:    BIT[i] += new_value    i += (i&amp;-i)\n3. 区间求和\n假如要求 arr 数组下标区间为 [i,j] 的数值之和，那么可以先求下标为 [0,i-1] 的数值之和，再求下标为 [0,j] 的数值之和，然后用后者减去前者即可。\n通过观察上面初始化后的图可以知道求 [0, i] 可以通过下面的方法：\ncount = 0while i&gt;0:    count += BIT[i]    i -= (i&amp;-i)\n通过上面的操作，通过利用额外的两个数数组，将原来的区间求和的操作从时间复杂度 \\(O(n)\\) 变为了 \\(O(logn)\\), 但是更新数组的值的操作的时间复杂度也从原来的 \\(O(1)\\) 变为了 \\(O(logn)\\), 所以这种数据结构更适合用于区间求和频繁的应用场景。\n下面是 [LeetCode][2] 上的一道利用了 BIT 的题目，有兴趣的读者可以尝试做一下，验证刚刚学的理论知识。\n&gt;Given an integer array nums, find the sum of the elements between\nindices i and j (i ≤ j), inclusive.\n\nThe update(i, val) function modifies nums by updating the element at\nindex i to val.\n\n实现的 python 代码如下所示： class NumArray(object):    def __init__(self, nums):        \"\"\"        initialize your data structure here.        :type nums: List[int]        \"\"\"        self.nums = nums[:]        self.count = [0 for i in xrange(len(nums)+1)]        for i in xrange(len(nums)):            self.initialize(i, nums[i])    def initialize(self, i, val):        i += 1        while i &lt; len(self.nums)+1:            self.count[i] += val            i += (i &amp; -i)    def update(self, i, val):        \"\"\"        :type i: int        :type val: int        :rtype: int        \"\"\"        diff = val - self.nums[i]        self.nums[i] = val        self.initialize(i, diff)    def left_sum(self, i):        i += 1        total = 0        while i&gt;0:            total += self.count[i]            i -= (i &amp; -i)        return total    def sumRange(self, i, j):        \"\"\"        sum of elements nums[i..j], inclusive.        :type i: int        :type j: int        :rtype: int        \"\"\"        return self.left_sum(j) - self.left_sum(i-1)# Your NumArray object will be instantiated and called as such:# numArray = NumArray(nums)# numArray.sumRange(0, 1)# numArray.update(1, 10)# numArray.sumRange(1, 2) [1]:\nhttps://wulc.me/imgs/fenwick_tree_binary_index_tree.jpg [2]:\nhttps://leetcode.com/problems/range-sum-query-mutable/\n","categories":["python"],"tags":["python","树"]},{"title":"Attention Is All You Have","url":"/2025/10/03/Attention%20Is%20All%20You%20Have/","content":"2017 年，《Attention Is\nAll You Need》悄然问世，带来了 Transformer\n的架构，这个此后被搜广推领域广泛应用、并成为后来掀起新一轮 AI 革命的 LLM\n的基石。Transformer\n让机器拥有了某种 “超级注意力”：一种可以并行处理全局信息、计算序列中所有元素关联权重的强大机制。它不需要像人类一样逐字逐句地阅读和理解，而是可以一瞬间 “看到” 全部，并找出其中最关键的连接\n但是一个巨大的悖论和困境正在上演。人类创造了看似拥有 “无限可扩展注意力” 的机器，但人类自身所拥有的，却是一套古老而有限的生物注意力系统。机器的核心是 “更多、更快、更全局”，而人类的核心是 “选择、聚焦、做减法”。我们正用着自己这套需要休息、会疲劳、极易分心的心智系统，去对抗一个由 “超级注意力” 算法驱动的、旨在无限捕获我们注意力的科技环境\n对于碳基生物而言，更加残酷的一个事实是 \"Attention Is All You\nHave\"。因为对人类来说，注意力是与生俱来、且每日额度恒定的核心生命资产。而这唯一的货币投向何处，你的生命就投资何处，最终也将到达何处。我们一直在说 “人是环境的产物”，究其原因，也许是人在某个环境中，不得不将其的注意力倾注到这个环境设定的规则里最关注的事情，进而决定了人将变为何种产物\n如果说注意力是人最宝贵的财富，那如何保护自己的注意力便是一个值得探讨的命题。本文尝试对这部分展开一些探索，包括人的注意力为何是有限的，这少得可怜的注意力在当下的注意力经济中是如何被各种争夺，以及如何构建自己的注意力框架。本文是笔者最近感觉自我工作效率低下后的一次寻根问底求法的过程，祝开卷有益～\n\n有限的注意力预算\n看不见的大猩猩\n1999 年，心理学家克里斯托弗・查布里斯和丹尼尔・西蒙斯设计了一个堪称经典的 “看不见的大猩猩”(The Invisible\nGorilla) 实验。这个看似简单的实验设计，却揭示了一个令人不安的真相：我们对自己感知能力的自信，很大程度上是一种错觉\n在这个实验中，参与者被要求观看一段视频，数一数身穿白色球衣的队员一共传球多少次。视频中间，一个穿着大猩猩服装的人走入镜头，捶打胸膛，然后缓缓离开。结果却令人震惊：超过一半的参与者全神贯注于计数，完全没有看到那个明显的大猩猩。而当被告知真相时，许多参与者坚决不相信，甚至怀疑实验者更换了视频。他们对自己的观察能力如此自信，以至于无法接受如此明显的事物竟从自己的意识中完全消失\n这个实验生动地揭示了注意盲视（Inattentional\nBlindness）：当我们的注意力高度集中于某一事物时，会对其他显而易见的信息视而不见。这不是粗心，而是我们大脑信息处理能力的根本瓶颈。后续研究进一步证实，即便是飞行员、放射科医生等经过严格专业训练的人员，在专注执行特定任务时，同样会错过显而易见的关键信息。比如在一项医学影像研究中，83% 的放射科医师在寻找特定病灶时，竟未注意到图像角落处一个明显的外星人贴图\n我们的注意力就像一个聚光灯，光柱之外的一切，都会沉入黑暗。大脑的前额叶皮层就像专业的灯光师，为了确保主要任务的完成质量，会有意抑制次要信息的处理\n认知心理学中的有限容量理论（Limited Capacity\nTheory）指出，人类的注意力是一种有限的认知资源。当同时进行多项任务时，如果总需求超过资源总量，整体的表现就会下降。这个其实跟 “心智带宽” 的概念也有点像，人的注意力通道是一条带宽受限的通道，在执行一项任务时消耗的带宽越多，可用于执行其他任务的带宽就越少，从而导致整体性能下降。这通常通过双任务范式 (Dual-Task\nParadigm) 的实验来证明：让参与者同时执行两项不同的任务（主任务和次任务）来研究认知负荷对任务执行能力的影响。比如一边记忆单词一边做数学题，他们的表现通常会比只做一件事时差\n注意力的反向塑造\n在这个信息爆炸的时代，我们每个人的注意力都成为了一种稀缺资源。但比这更深刻的真相是：注意力不仅是资源，更是一把雕刻刀。你持续关注什么，你的心智、能力甚至大脑结构，就会朝着那个方向演化；这种塑造发生在多个层面，从即时的认知表现到长期的个人发展，背后有坚实的心理学和神经科学依据\n心理学家威廉・詹姆斯在《心理学原理》中提到：“我的经验是由我选择注意的东西决定的”。我们并非体验世界的全部，我们只体验我们关注的那部分。你的注意力过滤器，决定了哪些信息能进入你的意识，从而构建你的个人现实\n伦敦大学学院的神经学研究显示，不同领域的专业训练会诱导特异性的脑结构改变，在这篇文献中《Navigation-related\nstructural change in the hippocampi of taxi\ndrivers》指出，通过脑扫描研究，发现伦敦出租车司机大脑中负责空间记忆的海马体后部显著更大，因为这些司机们需要花费数年时间持续将注意力集中在记忆伦敦复杂的街道网络上。这就像健身增肌，只不过他们锻炼的是大脑中负责导航的特定区域。\n类似的神经可塑性现象在不同领域的专家身上都有体现：\n\n钢琴调音师的左侧听觉皮层灰质密度显著增高，他们的耳朵能分辨出常人无法察觉的微小音高差异\n\n品酒师的嗅觉皮层与眶额叶皮层连接增强，能构建出复杂的风味地图\n\n专业棋手的顶叶皮层更为活跃，能在脑海中预演数十步棋局的演变\n\n这些结构性改变印证了 Hebbian theory\n中提出的 \"神经元同步放电则连接增强\" 原则，当你持续将注意力投入某个领域，相关神经元回路就会被反复激活，最终物理性地强化这些连接，就像小径因频繁行走而变成康庄大道\n然而，这种强大的神经可塑性是一把双刃剑。哲学家尼尔・波兹曼在《娱乐至死》中发出的警告，在今天这个算法时代显得尤为紧迫：媒介的形式本身会潜移默化地塑造我们的思维方式和认知习惯\n在电视时代，一切信息都必须以娱乐的方式出现，碎片化、图像化、追求瞬时刺激的特性，则塑造了浮躁和浅薄的文化。今天的我们身处一个由算法驱动的、比电视强大无数倍的超注意力捕获网络中，如果我们放任自己的注意力被无限切割、被廉价娱乐填满，我们就在主动重塑一个不利于深度思考的大脑。神经科学研究显示，持续的多任务处理和碎片化注意力会：1）强化大脑对快速反馈的依赖，削弱前额叶皮层的抑制控制能力\n2）减少默认模式网络的激活，而这一网络正是创造性思维和深度整合的发生地\n3）导致注意力残留效应，即使切换任务后，大脑仍有一部分资源被先前任务占用\n如同在《工作、体制化与自由》一文中提到的，短视频带来的高频、短期的刺激，代价是词汇量、语义精度和表达能力的下降，是阅读长内容的耐心和能力的丧失，是系统性思考与推理能力的衰退，也是空虚感的重要来源\n无限的注意力争夺战\n在这个信息过载的时代，我们每个人的注意力都成了各方势力争夺的战场。正如经济学家赫伯特・西蒙所预言：“信息的丰富导致了注意力的贫乏”。我们正生活在这个预言的实现中，每个人的认知预算都在被精心设计的系统持续透支\n注意力经济下，你才是被售卖的商品\n我们每日有限的 “注意力币”，已经成为当今世界最炙手可热的商品，由此催生了 “注意力经济（Attention\neconomy）”。这个价值数千亿美元的注意力经济，其商业模式异常清晰：将用户的海量注意力聚合起来，然后 “打包” 卖给广告商。如同在《The Social\nDilemma》中发出的发人深省的警告：If you're not paying for the\nproduct, you are the product\n这个系统的精妙之处在于，它不再是被动地等待你的关注，而是主动地捕捉你的意识。收割我们注意力的武器，正是由《Attention\nIs All You\nNeed》所启发的技术架构。社交媒体、短视频平台、新闻资讯 App 的背后，是复杂精密的推荐算法系统，它们的优化目标极其单纯：最大化用户停留时长和互动率。它们就像永不疲倦的角斗士，为了争夺你的 “注意力币” 而疯狂角力\n这些系统巧妙地利用了人类大脑的进化弱点：\n\n对不确定性的痴迷：下拉刷新机制与老虎机的 \"可变奖励\" 模式同出一辙，每一次刷新都在激活我们大脑中的多巴胺系统，创造着 \"下一次会有更好内容\" 的期待；如同斯金纳箱里那只不断按压开关期待得到奖赏的小白鼠一样\n\n对负面信息的警觉：坏消息、冲突、八卦往往能获得更多点击，因为我们的生存本能要求我们时刻警惕潜在威胁\n\n对社交认同的渴望：点赞、评论、转发的通知提示，，触发了我们大脑中与社会接纳相关的奖励回路\n\n\"无限滚动\"、\"自动播放\"、\"个性化推荐\"—— 这些都不是中性的功能设计，而是精心计算的 \"注意力捕获装置\"。它们像是一条永不停止的传送带，让你在无意识中持续消耗着宝贵的认知预算\n深度工作 vs 碎片化响应\n在 \"注意力经济\" 的持续冲击下，我们正在丧失一种至关重要的核心能力：深度工作（Deep\nWork）。这其实也是计算机科学家卡尔・纽波特在同名的书籍《深度工作》中提出的概念，指的是 “在无干扰的状态下专注进行的职业活动，使个人的认知能力达到极限。这种努力能够创造新价值，提升技能，且难以复制。”\n如程序员编写复杂的核心代码、作家撰写书籍章节、科学家推导关键公式、设计师构思核心创意等；与之相反的是那些对认知要求不高的、逻辑性的事务性任务，通常在受到干扰的情况下开展的工作\n从神经学和心理学角度出发，深度工作都是具备巨大价值的\n\n从神经学角度出发，专注力是一种需要训练的能力。从上面的注意力反向塑造的理论可知，行为会重塑大脑，而深度工作能够强化专注相关的神经回路，从而变得更擅长专注和思考。反之，不断分心会永久性地削弱这种能力\n从心理学的角度出发，深度工作中所达到的理想状态，其实这也是我们常说的心流状态 (Flow)，其特点往往是需要高度专注、创造价值、有门槛，其关键点之一是不受到外界的过多干扰。心流是幸福感、满足感和意义感的重要来源。高质量的工作生活本身就是一种巨大的回报，如果乔帮主在\nStanford\n那场著名的演讲中的提到的一样\n\n\nI’m convinced that the only thing that kept me going was that I loved\nwhat I did. You’ve got to find what you love—and that is as true for\nwork as it is for your lovers. Your work is going to fill a large part\nof your life, and the only way to be truly satisfied is to do what you\nbelieve is great work. And the only way to do great work is to love what\nyou do.\n\n然而事实是，我们已陷入集体性的 \"碎片化响应\" 模式。现代工作环境几乎是与深度专注为敌的。开放式办公空间虽然促进了表面上的协作，却成了持续干扰的温床。我们不断地在邮件、即时消息、会议通知和突发请求之间疲于奔命。更恐怖的是，我们似乎默许并推崇这种 \"忙碌文化\"，将碎片化的响应等同于工作效率。快速回复邮件、同时处理多个任务、频繁参与会议 —— 这些行为获得了组织的即时奖励，却让我们远离了那些真正需要深度专注才能完成的价值创造工作\n但从生物学的角度出发，人类大脑的构造决定了我们本质上是序列处理器，而非并行处理器。所谓的 “多任务处理” 实际上是一个快速、耗能的 “任务切换” 过程，而每一次切换都伴随着显著的生物学代价（switch\ncost）\n1）效率下降与时间浪费。虽然每次切换可能只需要十分之几秒到几秒钟来重新定向注意力，看似短暂，但在一天内成百上千次的切换中累积起来，就是个巨大的时间黑洞。另外就像一壶水每次烧到一半就关掉，重新再烧，整体煮沸的时间远大于一次性烧开\n2）错误率增加与认知残留。当你从任务 A 切换到任务 B 时，关于任务 A 的思维碎片（attention\nresidue）并不会立刻消失。它们会干扰你对新任务 B 的专注，导致你在 B 上犯更多错误，或者感觉思维 “粘滞”。在切换的缝隙中，大脑容易将不同任务的规则混淆。比如，你正在写邮件，然后去回了条微信，再回来写邮件时，可能会不小心把微信的缩写或表情符号用在邮件里，这就是典型的接口错误\n3）加速心智疲劳与压力荷尔蒙。任务切换是一种高能耗的认知活动。每一次 “规则卸载与加载” 都需要消耗大量的葡萄糖（大脑的主要燃料）和氧气。这解释了为什么没做什么 “正经事”，只是不停回邮件、看消息，一天下来也会感觉心力交瘁 —— 你的大脑一直在进行高强度的内部搬运工作。持续的、不可预测的干扰（如 IM 软件消息、邮件）会激活大脑的\n“威胁监测” 网络杏仁核\n，引发低度的应激状态，分泌皮质醇等压力荷尔蒙。长期如此，会导致慢性疲劳、焦虑和创造力枯竭\n4）对学习与长期记忆的深层损害。将短期记忆转化为长期知识，需要海马体的深度处理；深度、不受干扰的专注是这一过程的关键。频繁的任务切换会阻止信息进入深度处理流程，导致你感觉 “学了很多，但什么都没记住”。更可怕的是，长期的多任务习惯会重塑我们的大脑神经回路。它会让我们的大脑习惯于寻求新奇和刺激，变得越来越不耐受无聊和沉默。最终，当你需要长时间专注于一本复杂的书籍或一份困难的报告时，你会发现自己已经失去了这种能力 —— 你的 “专注力肌肉” 已经萎缩了\n我们正在用人类本质上的串行处理大脑，去拙劣地模仿机器并行处理的 \"多头注意力\" 机制。Transformer 模型可以真正地同时处理多个信息流并理解其复杂关联，而人类的所谓 \"多任务\"，只是在浅层次事务上消耗注意力，牺牲了深度思考和创造性产出。我们试图成为机器，却在过程中丧失了身而为人的独特优势\n设计你的 Transformer\n在这个注意力被疯狂争夺的时代，我们需要从被动的注意力消费者转变为主动的架构师。就像\nTransformer\n模型通过精心的结构设计实现高效的信息处理，我们也需要构建属于自己的 \"个人注意力架构\"，从而帮助我们在信息洪流中保持清醒，将宝贵的注意力资源投入到真正重要的事物上\n定义核心 Query\n在 Transformer 中，Query\n是主动发出的提问，它决定了模型要在海量信息（Key）中寻找什么。同样，管理注意力的第一步，是向自己发出一个清晰的\nQuery：我当下最核心的目标和价值是什么？或者用更加大白话来说，我到底要什么？\n说实话要回答这个问题并不容易，知道自己要什么、想成为什么样的人、这一生要怎么过，本身就是一个极度非标的、需要花费大量时间和心力、持续做自我探索才能得到答案，又或者终其一生都没有答案的问题\n虽然长期的目标不那么好定义，单独短期的目标还是相对明确的。比如说每个季度初，可以写下\n1-3\n个最重要的 “灯塔” 目标。它们应像灯塔一样，指引你所有的努力方向，这有点像的不少互联网公司的\nokr 制度\n另外，可以定义每日的 Query\n清单：每天清晨起床，可以拟定下 “我的注意力最需要投注在哪三件事上？”\n这能帮你设定一天的注意力基调\n优化 Key-Value 对\n定义了 Query\n之后，就需要对输入的信息进行过滤和权重分配。这意味着需要强化高价值的 Key\n和弱化低价值的 Key\n\n强化高价值 Key：主动寻找和增加与你的核心 Query\n相关的信息源。例如，如果你的 Query\n是 “提升专业技能”，那么专业的在线课程、行业报告、高质量的书籍就是高价值的\nKey。它们背后的 Value 是职业成长和核心竞争力\n\n弱化低价值 Key：勇敢地对那些与你核心目标无关的 Key\n说不。这相当于在计算注意力权重时，直接给它们乘以一个接近 0\n的衰减系数。无关的微信群、八卦新闻 App、无尽的推送通知，都应被纳入此列\n\n这需要我们对输入信息进行权重分配，建立一个精密的过滤系统。实战中有一个分级过滤策略值得尝试（其中不少理念出自上面的《深度工作》这本书）\n第一层：环境级过滤\n\n卸载那些最消耗你时间的娱乐应用（尤其是短视频和社交媒体 App）。或者可以只在电脑端使用它们，增加使用门槛\n\n减少用网络来消磨碎片时间：排队、等车时不要立刻刷手机获取碎片化，这会让你的大脑习惯于这种 “分心” 的模式。可以做一些连续性强的事情，如听播客\n\n关闭所有非关键通知，只保留电话、短信和极少数重要通讯工具的通知，减少这些信息对你的中断\n使用 Freedom、Cold Turkey 等工具在工作时段屏蔽干扰网站\n\n第二层：内容级过滤\n\n取消关注那些让你焦虑、或总是推送无用信息的公众号、博主和邮件列表\n\n订阅 2-3 个高质量的输入信息源，关注少数几个领域内的顶尖专家，而不是被算法推荐牵着走\n\n建立 \"信息食谱\" 概念：蛋白质（深度内容）、维生素（跨界知识）、碳水化合物（日常资讯）\n\n第三层：时间级过滤\n\n设定固定的工作时间，如每天在固定时间进行深度工作，形成一种习惯和节奏；设定固定的信息处理时段，比如上午 10-11 点处理邮件和企业\nIM 的消息\n\n晚间 10 点后启动 \"数字宵禁\"，停止处理工作信息\n\n每周日上午进行 \"信息斋戒\"，通过冥想等方式给大脑留出消化的时间和空间\n\n“Multi-Head” 策略\nTransformer 的多头注意力（Multi-Head\nAttention）机制，真正在硬件层面同步计算多个注意力函数，然后将结果拼接融合，它的每个\nhead 都拥有独立的计算资源，来并行捕捉信息的不同方面\n但对人类而言，如果前面提到，人类大脑的构造决定了我们本质上是序列处理器，而非并行处理器。所谓的 “多任务” 只是快速切换，并付出巨大的切换成本 —— 包括时间延迟、错误率增加和心智疲劳等\n因此，我们的策略不是成为机器，而是向机器学习其架构的 “专注” 与 “模块化” 思想，通过极致的规划，在时间线上模拟出并行的效果，从而最大化我们串行处理器的效能。如果我们把注意力模块化地分配，可以粗略将注意力划分为\n4 种重要的 head：深度工作、日常事务、创意探索和恢复休息\n\n“深度工作” 头\n\n进入深度工作状态，本质是触发 “心流”(FLow)\n。心流状态下，大脑会释放去甲肾上腺素、多巴胺、内啡肽等神经化学物质，这些物质不仅能提升幸福感，还能显著增强专注力、模式识别能力和创造性思维。这是一种一旦中断就难以重建的高价值状态。\n进入深度工作状态有一些可尝试的建议\n1）设定启动仪式：在深度时间块开始前，进行一个简单的仪式（如泡一杯茶、整理桌面、深呼吸三次），告诉大脑 “准备进入状态了”。这能形成强大的条件反射\n2）明确产出目标：不要只写 “写报告”，而是写 “完成报告引言部分和三个核心论点的初稿”。具体的目标能牢牢锁住你的注意力\n3）环境设计：工作台极简主义，即保持桌面整洁，只留下当前任务所需的物品。多余的物品都是视觉上的干扰源；噪音管理，环境嘈杂下使用降噪耳机，播放白噪音、自然声音或专注音乐（如 Lo-fi），隔绝\nauditory\ndistraction\n4）番茄工作法：对于难以进入深度状态的人，可以从 25 分钟专注 + 5 分钟休息的 “番茄钟” 开始训练。当你深度工作能力增强后，可以尝试延长专注时间至 50 分钟或 90 分钟\n\n“日常事务” 头\n\n如同前面提到的 “注意力残留” 效应，当你从一项未完成的任务切换到另一项时，你的注意力资源并不会立刻完全转移，一部分会 “残留” 在之前的任务上。而批量处理琐事，就是为了将这些残留效应集中并最小化，避免它们污染你的深度工作时间\n具体的建议，可以安排 1-2 个固定时间（如上午 11 点，下午 4 点），批量处理邮件、回复消息、报销等琐碎事务。其他时间绝不查看；同时也为这个时间块设置一个倒计时（如 45 分钟）。时间压力会迫使你更快地决策和行动，避免在琐事上过度完美主义\n\n“创意探索” 头\n\n当你停止专注思考，进入放松、漫无目的的状态时，大脑的 “默认模式网络”\n会被激活。这个网络并非在休息，而是在后台进行信息整合、连接不同记忆、并孵化创意。许多 “Aha!\nmoment” 都发生在此刻，因为它连接了意识思考未能触及的遥远节点\n关于创意探索，也有一些具体的方法论\n1）主动安排 “无聊” 时间：刻意在日程中留白，不安排任何活动，允许自己发呆、散步、望窗外\n2）跨界刺激：阅读与你专业完全无关的书籍、杂志或纪录片。异质性的信息是创意的最佳催化剂\n3）创意捕捉：在 “创意探索” 时间后，安排一个简短的（5-10 分钟）记录时间，将脑海中浮现的任何想法记录下来，无论它们当时看起来多么不靠谱\n\n“恢复休息” 头\n\n这里说的恢复休息，不是上了一周的班之后身心俱疲后，通过胡吃海喝，通宵打游戏追剧等来缓解表层焦虑的行为。高质量的休息不是一个纯被动的接收过程，而是一系列主动为大脑做的优化：系统性清理与重置（睡眠）、硬件升级与优化（运动）、软件调试与抗干扰训练（冥想正念）\n1）睡眠：睡眠远非简单的休息。它将你的大脑从信息收集模式切换到关键的维护模式。在此期间，大脑会进行深度清理，清除白天积累的代谢废物（如 β- 淀粉样蛋白），巩固记忆，并将知识从海马体转移到皮层，形成长期记忆。将睡眠视为非协商性投资，如同对待 “深度工作” 头一样，在日历上锁定睡眠时间，保证睡眠的时长和质量；除了夜间睡眠，中午 20 分钟左右的短睡能有效清空工作记忆，为下午的深度工作块重启认知资源。关于睡眠的原理和重要性，可以参考这期播客《我们为什么要睡觉》：情商和心理治疗都靠睡觉？\n2）运动：身体活动是最高效的健脑术。有氧运动能直接促进脑源性神经营养因子（BDNF）\n的释放，这种物质如同为大脑神经元 “施肥”，能促进神经细胞生长、增强连接，直接提升学习效率、记忆力和专注力的 “硬件基础”。定期运动就是在为你的整个认知系统进行硬件升级\n3）冥想正念：如果说睡眠是清理缓存，运动是升级硬件，那么冥想就是对注意力这款 “核心软件” 的调试和优化。每天 10-15 分钟的正念冥想，其核心价值不在于增加注意力时长，而在于极大地提升你的\n“元认知” 能力 —— 即觉察到注意力涣散（Mind-Wandering）并将其温和而坚定地拉回的能力。这就像是每日为注意力进行的 “健身操”，长期练习能显著增强你对心智的控制力\n正向传播与反向传播\n如果我们把上面构建的注意力系统视为一个需要训练的神经网络。没有反馈的练习只是重复，无法带来精进。唯有通过 “反向传播” 计算误差并调整参数，模型才能变得更聪明。个人的注意力系统也需要持续的训练和迭代优化\n正向传播比较直观，就是在每个规划好的时间块内，心无旁骛地执行单线程的 “正向传播”，即按照上面提到的\nMulti-Head\n策略，实现 “认知闭合”，屏蔽一切干扰，让思维在单一任务上深度推进\n反向传播是最关键的一步。你需要一个反馈回路，可以粗略将其分为每日回顾与每周回顾\n\n每日回顾\n\n每天结束后可以花几分钟回顾一下当天的注意力分配情况，问自己一些关键性问题\n1）一致性分析：“我计划的注意力投向与实际投向一致吗？” →\n如果不一致，是计划不切实际，还是执行时意志力溃散？\n2）漏洞溯源：“今天最大的注意力‘漏洞’是什么？” →\n不要笼统地归咎于 “手机”。追问下去：是微信消息？是突如其来的邮件提醒？还是内心的焦虑感让你主动去寻找分心？\n紧接着的问题就是 “明天如何堵上这个漏洞？”。这里的方法说实话会比较多样，需要针对性地制定微策略，同时也是因人而异。比如说如果是微信，能否将其放入定时开关的 “专注模式”？如果是邮件，能否关闭桌面通知？如果是内心焦虑，能否先花 5 分钟用纸笔梳理焦虑来源？\n\n每周回顾\n\n每周的回顾也是一个阶段性的回顾，比如说设定在工作日的最后一天如周五下午，可以从以下方面来梳理清楚过去一周的精力花费在了哪些方向上\n1）审视 “注意力预算” 的分配：回顾过去一周的时间记录或日程表。你投入在不同 “注意力头”（深度、事务、创意、休息）上的时间比例是否健康？是否在低回报的浮浅工作上超支？\n2）识别高价值活动：过去一周，哪些 “正向传播” 带来了最大的成就感和实际产出？下周如何复制并扩大这类活动？\n3）优化日程模板：根据本周的洞察，为下一周设计一个更合理的 “串行多头” 日程模板。例如，发现下午 3 点后精力难以集中，就把 “深度工作头” 全部调整到上午\n记录的工具比较多，市面上有比较多的 APP，对于 Mac\n笔记本的用户，笔者推荐的其实是自带的日程软件，跟 《时间贫困》：一个人的时间花在哪里，是看得见的\n里提到的时间记账法比较类似\n你是你所有注意力的总和\n从《Attention Is All You Need》到 “Attention Is All You\nHave”，这不仅仅是一次文字的戏仿，更是一个时代命题的转换。我们见证了机器如何凭借 “超级注意力” 崛起，也亲历了自身有限注意力在洪流中的飘摇。但本文的目的，并非制造如同纺织工人面对蒸汽机时、我们面对已经或即将取代大量工作岗位的\nAI\n时的那份焦虑和恐惧，是揭示一个更为根本的真相：你的注意力流向何处，你的生命就走向何处\nTransformer\n模型通过精妙的架构，将有限的算力精准投注于最有价值的信息关联上。这恰恰是我们最需要向机器学习的智慧 —— 不是学习它的 “无限”，而是学习它的 “专注” 与 “效率”。我们无法扩展注意力的总量，但可以像优化一个精密模型那样，去设计它的分配\n你定义的核心目标，是你向世界发出的\nQuery；你精心筛选的信息环境，是你为自己构建的 Key-Value\n仓库；而你模块化管理的日程与精力，则是你独一无二的 Multi-Head\n策略。每日的反省与每周的复盘，就是你这个系统的反向传播算法，在误差中学习，在迭代中进化\n我们守护和优化注意力，不是为了更高效地完成工作，而是为了更清醒地度过这一生。你的注意力，是你用来雕刻自我和塑造现实的唯一刻刀。你将它持续投向哪里，哪里就会在你生命中生长、壮大。你关注知识，便成为智者；你关注创造，便成为创造者；你关注所爱之人，便构筑了深厚的关系\n在这个万物都在争夺你注意力的时代，能够清醒地决定自己要看什么、想什么、爱什么，或许就是我们这个时代最珍贵的自由与最大的力量。毕竟\nAttention is all you have, and finally all you are\n\n本文的一些参考内容\n\n《Attention Is All You\nNeed》\n\n《心理学原理》\n\n《Navigation-related\nstructural change in the hippocampi of taxi drivers》\n\n《娱乐至死》\n\n《工作、体制化与自由》\n\n《The\nSocial Dilemma》\n\n《深度工作》\n‘You’ve\ngot to find what you love,’ Jobs says\n\n《我们为什么要睡觉》：情商和心理治疗都靠睡觉？\n\n《时间贫困》：一个人的时间花在哪里，是看得见的\n\nThe\nInvisible Gorilla\n\nInattentional\nBlindness\n\nLimited Capacity\nTheory\nHebbian\ntheory\n\nAttention\neconomy\n\nFlow\nAttention\nResidue\n\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"CPU 缓存","url":"/2015/11/21/CPU%E7%BC%93%E5%AD%98/","content":"下面是 CPU 缓存的一些概念，所用命令均是在 Linux 平台下\n\n可通过命令 getconf -a| grep CACHE | grep size\n查看 CPU 的各级缓存大小\n\n也可以通过命令 lscpu | grep ^L 查看\n\nCPU 缓存以行（line）单位，主内存以页（page）为单位，磁盘以块（block）为单位\n\nCPU 缓存一般分为指令缓存（I-Cache）和数据缓存（D-Cache），且两者一般都是分开的\n\n缓存控制器（cache\ncontroller）判断 CPU 要获取的指令和数据是否在 CPU 缓存中，从一级缓存往下找，直到主内存和磁盘，且从找到的那一级开始往上面所有级缓存 , 如下图所示：\n\n\n\n评判软件优秀与否的一种标准：对 cpu 缓存的命中率\n\n","categories":["操作系统"],"tags":["操作系统"]},{"title":"C++ 一些基本语法","url":"/2015/11/24/C++%E5%81%9AOJ%E6%97%B6%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9/","content":"本文主要涉及到 C++ 一些基本语法，在做 oj 时经常用到，特此记录。\n\n字符和字符串\n\n数字转字符串：std::to_string (int)\n\n字符串转数字：std::stoi (string)\n\n上面两个函数均需要 #include&lt;string&gt;\n\n字符串可以用数组方式来访问\n\n字符串长度可用其 length () 函数获取\n\n字符串可以通过 substr (i，n) 方法来提取子字符串，表示从第 i 个字符开始提取 n 个字符（包括 i）\n\n字符串大小写转换：利用 STL 中的 transform 函数，见下面的例子 \n\n#include &lt;iostream&gt;  #include &lt;algorithm&gt;  #include &lt;string&gt;  using namespace std;int main(){    string s = \"abcdEFg\";    transform(s.begin(), s.end(), s.begin(), ::toupper);    cout &lt;&lt; s&lt;&lt;endl;   //输出ABCDEFG    transform(s.begin(), s.end(), s.begin(), ::tolower);    cout &lt;&lt; s &lt;&lt; endl; //输出abcdefg    return 0;  }  \n\n单个字符大小写转换\n\n需要记住 ASCII 码中 A-65，a-97\n\n将 char c 从大写转为小写可以通过下面代码 \n\n\nif(c&gt;='A' &amp;&amp; c&lt;='Z')       c=char(c+32);  ```  - 数字从char类型转为int类型    - 需要记住0对应的ASCII码为48    - 一个简单的例子如下```cpp  char c='0';  int a=int(c);  cout&lt;&lt;a&lt;&lt;endl; //输出48  int b=int(c)-48;  cout&lt;&lt;b&lt;&lt;endl; //输出0  \n数组\n初始化数组（不初始化时为随机的地址值）\n\n方法一：直接用数值初始化，见下面代码\nint a[3]={1,2,3} //元素依次为1,2,3  int a[3]={0}     //元素依次为0,0,0  int a[3]={1}     //元素依次为1,0,0  ```  采用这种方法初始化时如果{}里面的元素的个数小于数组长度，则不足长度的元素默认值为0- 方法二：for循环  ### 动态分配数组长度方法(一维)    - 方法一：通过vector实现，需要#include &lt; vector  &gt;    - 方法二：通过malloc和free实现，如下面例子就初始化了一个长度为n的数组，且数组的值为从0到n-1（这个是继承了C分配内存的特性，C++可通过`new`和`delete`来实现，见方法三）```cpp  cin&gt;&gt;n;  int *a=(int *)malloc(sizof(int)*n);  for(int i=0;i&lt;n;i++)    a[i]=i;  free(a);  \n方法三：通过 new 和 delete 来实现，见下面代码 \n\ncin&gt;&gt;n;  int *a=new int[n];  for(int i=0;i&lt;n;i++)      a[i]=i;  delete []a;  \n容器\nvector\n\n需要 #include &lt;vector&gt;\n\nvector 的方法：\n\nvector 中的元素可以以数组下标访问\n\npush_back ( ) 将一个元素放到 vector 中\n\nvector.size ( ) 获取 vector 的大小\n\n查找元素 t 是否在 vector 中 \n\n\nT t;  vector&lt;T&gt;::iterator it=find(vector.begin( ),vector.end( ),t);  if(it == vector.end( ) )   cout&lt;&lt;\"not found\";  \n\n其他方法（需要 #include &lt;algorithm&gt;）：\n\nsort (vector.begin ()，vector.end ( ) )\n// 针对 vector 数据类型的排序\n\nreverse (vector.begin ()，vector.begin ( )+5 ) // 针对 vector 数据类型的\n反转，注意：reverse (vector.begin (),vector.begin ()+5)\n仅仅对 5 个元素进行 reverse 操作，不包括 vector.begin ()+5\n\n\nmap\n\n需要 #include &lt;map&gt;\n\n用数组下标的形式往 map 中添加元素和查找元素\n\n当 map 的 key 为结构体类型时，可通过重载 &lt;\n来判断该如何排序，见下面的代码 \n\n#include &lt;iostream&gt;  #include &lt;map&gt;  using namespace std;struct stu{  \tstring name;  \tint sco;      /*重载运算符 &lt;,达到从大到小或从小到大的排序效果，下面的代码是从小到大，如改成 return a.sco &gt;b.sco 则是从大到小 */  \tfriend bool operator &lt; (const stu &amp;a, const stu &amp;b){  \t\treturn a.sco &lt; b.sco;  \t}  };int main(){  \tmap&lt;stu, int&gt; score;  \tstu tmp;  \tfor (int i = 0; i &lt; 10; i++){  \t\ttmp.name = to_string(i);  \t\ttmp.sco = i;  \t\tscore[tmp] = i+1;  \t}  \t//获取顺序排列时的第一个元素，从大到小还是从小到大要看重载的&lt;  \tmap&lt;stu, int&gt;::iterator it = score.begin();  \tcout &lt;&lt; it-&gt;first.name &lt;&lt; ' '&lt;&lt;it-&gt;second &lt;&lt; endl;  \t//获取逆序排序的第一个元素，从大到小还是从小到大要看重载的&lt;  \tmap&lt;stu,int&gt;::reverse_iterator rit = score.rbegin();  \tcout &lt;&lt; rit-&gt;first.name &lt;&lt; ' ' &lt;&lt; rit-&gt;second &lt;&lt; endl;  \treturn 0;  }  \n\n遍历 map\n\nmap&lt;T,T&gt; m;  map&lt;T,T&gt;::iterator it;  for(it=m.begin();it!=m.end();it++)      cout&lt;&lt;it-&gt;first&lt;&lt;':'&lt;&lt;it-&gt;second&lt;&lt;endl;  \n\n输入输出\n\nprintf 函数格式化数字的输出\n\n数字前补 0 到达指定位数:\n\n如\nint a=3,b=4;  printf(\"%04d %05d  %d\",a,b,b) //输出为 `0003 00004 4`  \n\n可通过 scanf 从输入的一定格式的字符串中提取数字如：\n\nint year,month,day;  scanf(\"%d/%d/%d\",&amp;year,&amp;month,&amp;day);   //输入2014/09/06时，year=2014，month=9,day=6  \n\ncin 或 cout 的类型为 string 时需要 include \n\n结构体的数据类型可以是别的结构体，也可以是自身结构体的指针\n\n结构体内部也可以放函数，函数可用来初始化一个结构体\n\n\n","categories":["C++"],"tags":["C++"]},{"title":"An Overview Of An Ad System","url":"/2021/05/05/An%20Overview%20Of%20Ad%20System/","content":"从实习到工作，接触过一些大大小小的广告系统，有麻雀虽小但五脏俱全的小\ndsp，也有把 ssp、adx、dsp 都打包了的大媒体\n，算是对业界的广告系统有了一个初步的了解。趁着五一放假这几天，简单地梳理一下当前了解到的广告系统知识，主要是想对零散的知识做个整理，由于广告系统这个概念非常的大，涉及到的部分非常的多，无法面面俱到，所以本文主要是从几个视角（技术、业务、产品）言简意赅的描述一下笔者比较关心的几个部分，中间内容可能不全，欢迎交流指正。\n特意声明，本文内容与笔者雇主无关，主要是基于笔者当前的认知梳理的内容；在撰写过程中只会引用公开的内容，不会涉及到笔者雇主内部未公开的信息；如相关同学觉得有敏感内容，可联系删除。而其实在崇尚开源、paper\n漫天飞、人员流动越来越快的如今，笔者觉得这些通用技术并不是最核心的地方，数据 + 对业务的理解 + 灵活组装这些通用的技术才是。\n\n技术视角\n这里的技术视角主要是技术同学日常迭代较多的模块，总体的架构上可将其划分为\n召回 + 精排，而在这个结构里的两个主要关注的地方是出价和模型。\n召回与精排\n基本有规模的的推荐或广告系统，每次请求中选出 topk 个候选的过程都是\n“召回 + 精排”，中间还可能会插入一个粗排，其原因都是候选集过于庞大，需要在工程与效果的\ntrade-off，而如果候选不多，延迟允许的情况下，对全量候选直接做精排，效果上无疑是最好的。\n关于召回和排序，这篇文章讲得比较全面，推荐读一下， 推荐系统技术演进趋势：从召回到排序再到重排\n当前的召回基本都是采用多路召回的模式，每一路的召回都有其存在的业务意义和目标，且往往各路之间是互补关系，有点类似\nensemble 里的 bagging 的思想。且考虑到效率，当前采用的基本都是 ANN\n召回，其基本思想就是先把全量的候选划分在 \\(m\\) 个子空间中，召回时通过特定方式选出\n\\(n\\) （\\(m\n\\gt n\\)）子空间，然后将 query 与这 \\(n\\) 个子空间的候选的相似性最高的 topk\n个候选进入精排。关于 ANN 召回可参考这篇文章 图像检索：再叙 ANN\nSearch，其中的 IVF-PQ\n是业界比较常用的一种召回方式。另外，FaceBook 发表的 Embedding-based\nRetrieval in Facebook Search\n也值得一读，整篇文章读下来就像一个奋战在一线的工程师向你娓娓道来他们是怎么从\n0 到 1 构建一个召回系统，笔者也针对这篇 paper 写了 Embedding-based\nRetrieval in Facebook Search 阅读笔记，供参考。\n召回的目标是尽可能快地选出较好的候选，往往采用的特征和模型都会比较简单；而精排相对于召回，在特征和模型上可以做得更为复杂，因为精排面临的候选相对于召回少了非常多，各种交叉特征、复杂的结构都可以精排中尝试，精排基本上就是在根据业务去挖特征，而这里面笔者认为比较值得学习的是\nAirbnb 的这篇文章 Real-time Personalization using Embeddings for Search\nRanking at\nAirbnb，里面没有涉及到比较玄学的模型结构、超参等；而是看到了作者对业务有较为深刻的理解，同时通过各种方式将这些理解融入到模型中，笔者也针对这篇\npaper 写了 Real-time\nPersonalization using Embeddings for Search Ranking at Airbnb\n阅读笔记，供参考。\n此外，这里需要注意的是，将一个排序的过程拆成了多个阶段来选择\ntopk\n个候选，带来的问题是每个阶段的排序优化目标可能会有割裂，而这在上面的文章《推荐系统技术演进趋势：从召回到排序再到重排》中也有提出\n\n如果在召回阶段使用模型召回，理论上也应该同步采用和排序模型相同的优化目标，尤其是如果排序阶段采用多目标优化的情况下，召回模型也应该对应采取相同的多目标优化。同理，如果整个流程中包含粗排模块，粗排也应该采用和精排相同的多目标优化，几个环节优化目标应保持一致。因为召回和粗排是精排的前置环节，否则，如果优化目标不一致，很可能会出现高质量精排目标，在前置环节就被过滤掉的可能，影响整体效果\n\n如果说上面的观点是召回应该去适配精排，则 FaceBook 在\nEmbedding-based Retrieval in Facebook Search\n中提出的是让精排适配新的召回，paper 中指出新的 ANN\n召回的结果可能并不会被精排认可，paper\n中描述如下，也提出了一些解决方法\n\nsince the current ranking stages are designed for existing retrieval\nscenarios, this could result in new results returned from embedding\nbased retrieval to be ranked sub-optimally by the existing rankers\n\n模型\n模型的重要性无需强调了，往往离线指标的几个千分点的提升，就能带来线上比较显著的收益，而且几乎系统的规则约束的东西都可以被模型取代。这里主要讲一些笔者认为对模型效果比较重要的几部分：数据、训练、预估纠偏。\n数据\n数据是可以说是对模型效果影响最大的因素，这里可将其分为数据流和特征工程两部分\n数据流比较核心的目标是及时且准确地获取各种事件 (click,convert) 的\nground\ntruth；“及时” 指的是发生的事件需要尽可能快地喂给模型，“准确” 指的是这些事件需要被正确打上 label。\n数据流与转化归因 (conversion\nattribution) 这个领域密切相关，归因可以理解为 label 的获取与拼接，\n当前最常见的是 last touch 方式的归因，也有一些其他的归因方式如\nmulti-touch 归因等，\n通常涉及到广告主的上报和实际的拼接两部分，这里就不详细展开讲了，详细可参考这个\nrtb-papers\n里相关的\npaper。这里主要讲几个笔者认为比较值得关注的问题：delayed\nfeedback、bias、全渠道数据。\n\ndelayed feedback\n\n在广告场景下，cvr\n模型是这个问题的典型例子，因为转化是有延迟的，即在点击发生后过一段时间用户可能才会发生转化，且往往转化漏斗越深，延迟的时间越长\n这时候有两种选择，一种是等待事件的 label\n完全回流再进行训练，比如说事件的真实 label\n能在一天内完全回流，做天级训练即可，但是这不符合上面提到的 “及时” 的原则；另一种则是实时把数据送入模型做\nonline training，但是这不符合上面提到的 “准确” 的原则，因为有些 label\n可能还没回流。而实际上，实时性和准确性也是一个 trade-off 的关系。\n这个问题也被归纳为一个 delayed feedback 的问题，这部分内容可以参考 Delayed\nFeedBack In Computational Advertising，里面介绍了一些 paper\n里的解决方法，基本都是解决在 online-training 模式下如何解决 label\n回传不及时的问题，如利用 importance sampling\n等方法对样本做加权，或者让样本多次进模型，然后从统计意义推导出新的概率表达，从而保证样本是无偏的。\n\nbias\n\n训练数据可能存在各种 bias，常见的有 exposure bias 和 position\nbias\n(1) exposure bias： 针对的问题是只有被曝光的样本才能进入训练集，导致\ntraining 阶段能获取到的样本只是 serving 时很小的一部分\n(2) position bias： 针对的问题是位置越显眼的广告位被点击的概率越高\n关于 exposure bias 可参考文章 Exposure\nBias In Machine Learning，里面介绍了一些 paper\n的解决方法，笔者将其总结为 Data Augmentation、IPS 和 Domain Adaption\n三大类的方法\nexposure bias\n在召回上往往也会被简化成一个负例选择的问题，这篇文章 SENet 双塔模型：在推荐领域召回粗排的应用及其它\n最后一部分列了一些可能的负例选择方法，也具备一定的实践指导意义\n\n选择 1: 曝光未点击数据\n这就是上面说的导致 Sample Selection\nBias 问题的原因。我们的经验是，这个数据还是需要的，只是要和其它类型的负例选择方法，按照一定比例进行混合，来缓解 Sample\nSelection\nBias 问题。当然，有些结论貌似是不用这个数据，所以用还是不用，可能跟应用场景有关。\n选择 2: 全局随机选择负例\n就是说在原始的全局物料库里，随机抽取做为召回或者粗排的负例。这也是一种做法，Youtube\nDNN 双塔模型就是这么做的。从道理上讲，这个肯定是完全符合输入数据的分布一致性的，但是，一般这么选择的负例，因为和正例差异太大，导致模型太好区分正例和负例，所以模型能学到多少知识是成问题的。\n选择 3:Batch 内随机选择负例\n就是说只包含正例，训练的时候，在 Batch 内，选择除了正例之外的其它 Item，做为负例。这个本质上是：给定用户，在所有其它用户的正例里进行随机选择，构造负例。它在一定程度上，也可以解决 Sample\nSelection Bias 问题。比如 Google 的双塔召回模型，就是用的这种负例方法。\n选择 4: 曝光数据随机选择负例\n就是说，在给所有用户曝光的数据里，随机选择做为负例。这个我们测试过，在某些场景下是有效的。\n选择 5: 基于 Popularity 随机选择负例\n这种方法的做法是：全局随机选择，但是越是流行的 Item，越大概率会被选择作为负例。目前不少研究证明了，负例采取 Popularity-based 方法，对于效果有明显的正面影响。它隐含的假设是：如果一个例子越流行，那么它没有被用户点过看过，说明更大概率，对当前的用户来说，它是一个真实的负例。同时，这种方法还会打压流行 Item，增加模型个性化程度。\n选择 6: 基于 Hard 选择负例\n它是选择那些比较难的例子，做为负例。因为难区分的例子，很明显给模型带来的 loss 和信息含量比价多，所以从道理上讲是很合理的。但是怎样算是难的例子，可能有不同的做法，有些还跟应用有关。比如 Airbnb，还有不少工作，都是在想办法筛选 Hard 负例上。\n以上是几种常见的在召回和粗排阶段选择负例的做法。我们在模型召回阶段的经验是：比如在 19 年年中左右，我们尝试过选择 1 + 选择 3 的混合方法，就是一定比例的 “曝光未点击” 和一定比例的类似 Batch 内随机的方法构造负例，当时在 FM 召回取得了明显的效果提升。但是在后面做双塔模型的时候，貌似这种方法又未能做出明显效果。全局随机，则无论是 FM 召回还是后来的双塔，都没做出效果，有时甚至负向明显。但是你又能看到一些报道采用的是全局随机做为负例。所以，我目前的感觉，负例这块是个宝藏，值得深入探索下，包括不同方法的混合，但是到底哪种方法是有效的，貌似很难有统一的定论，带有一定艺术性。\n\n\n全渠道数据\n\n除了自身的数据外，还需要关注如何利用全渠道的数据提升的效果，其核心是将被\ncvr\n被低估的用户通过全渠道数据捞回来。提到全渠道的数据的概念，不得不说一下广告的模式，传统的广告模式将广告的竞价划分为\nssp &lt;-&gt; adx &lt;-&gt; dsp\n三大块，而当前的一些大媒体基本上都包含这三大模块，如微信、抖音等，自身是媒体 (ssp)，也是拍卖平台 (adx),\n同时也承担着广告主投放的任务 (dsp)。基本上面说的相关模型都数据媒体侧自身的\ndsp 这一部分\n媒体利用的数据往往只是用户在其自身产生的数据，如微信只能拿到用户在微信上的行为数据，拿不到用户在抖音上的相关数据；但广告主往往会在多个媒体上投放，可能用户在媒体\nA 点击了某个广告，最终转化是在媒体 B 上，如果媒体 A\n只用其自身的数据，会把 A 当做一个负例，cvr\n会被低估，相当于放弃了这个转化高潜用户，而利用媒体 B\n的数据则能让模型学习到这个用户真实的 cvr 情况；\n怎么拿到这部分数据呢？一种是广告主将媒体 A 的数据归因后直接回传给媒体\nB，另一种则是媒体之间相互合作，如笔者做过的联邦学习在广告的落地应用就是属于这部分。\n拿到这部分数据后便可进行建模，最简单就是加到原来的模型中，除此之外，还可以独立建模，然后作用到出价或原始\ncvr 上。\n\n特征工程\n\n如果说数据流决定用哪部分数据，特征工程则是决定了是否能够完全挖掘这部分数据的价值，即常说的 “数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。\n特征工程基本上做的就是根据业务特点去挖掘可能有用的特征，然后开 ab\n实验验证效果；虽然说特征跟业务强相关，但是具体的挖掘也有一些套路，常用的有属性特征、统计特征、序列特征等\n\n属性特征：用户 / 广告本身的一些属性，如用户的年龄、性别；广告的类别、样式等\n\n统计特征：用户在特定时间范围 (如过去 7d、 3d、 12h、1h\n等) 对当前广告的特定的维度 (如特定类别、特定位置、特定广告主等) 的广告进行了特定操作 (如点击、浏览等) 的次数；\n\n序列特征：即用户在一段时间内的行为序列，如最近 30\n个点击过的广告 / 商品；典型的应用可参考阿里的 DIN\n\n特征工程另一重要部分是特征筛选，特征筛选最简单的做法是删除指定特征，然后重新训练模型并评估，但是这种做法在训练时长较长时开销是比较大的；因此更合理的做法是在训练过程中便能得到每个特征的重要性，对于\nlogistics regression、tree-model\n这种解释性较好的模型能够做到这点，但是在 embedding-based 的 dnn\n中，每个特征都只能拿到一个\nembedding，怎么能做较好的特征选择的？比较常见的方法有\n\nattention unit：为每个 feature 增加一个 attention unit（如 DIN 中的\nactivation unit），训练过程中通过这个 attention unit\n输出的值来衡量每个特征重要性；其假设是对于重要特征，模型能够自动学习出其权重情况\n\nembedding weight：根据每个 feature 对应的 embedding\n的值大小来判断重要性， 常见的有基于 embedding 的 L1 norm，L2 norm\n等；其假设是对于重要特征，其对应的 embedding 的值越大，类比 lr，其实 nn\n可以近似认为是多个 lr 组成的\n\n上面的两种方法都存在着较强的假设，有没有更直接一点的方法呢？回到最开始的想法，即删除指定特征，然后重新训练模型并评估，能否在训练过程中便达到这一目的呢？其实是可以的，在训练过程中通过\nmask 的方式近似将 embedding 全置为 0，近似当做 drop 掉这个特征，然后通过\nmulti-head 的方式计算这部分 auc 即可。\n另外，在召回阶段，出于效率的考虑，线上往往只会抽取 user/ad\n的单侧特征，一些交叉特征虽然效果好，但是无法直接在线上抽取，这时可以考虑一些其他方法，如蒸馏，可参考阿里的\nPrivileged Features Distillation for E-Commerce Recommendations 或 Distillation\n简介\n训练\n在数据决定上限后，怎么通过模型逼近这个上限呢？\n以下几部分或许值得关注一下：模型结构，初始化，优化器和损失函数；这里说的模型主要还是针对\nDeepLearning 中的 NN 模型，在算力和数据量都达到一定标准后，NN\n的确能够取得 sota 的效果，\n\n模型结构\n\n这里的模型结构针对是 NN 这一类模型，NN\n里的各种结构可以说是百花齐放，如推荐领域的\nWide&amp;Deep、DeepFM、DIN、DCN 等，CV 领域的\nAlexNet、VGG、Inception、ResNet 等，NLP 领域\ntransformer、Bert 等；这些还是笔者读书那会就已经发表的 paper\n里的结构，如果算上近年发表的相关结构等，那就更多了\n而由于深度学习的不可解释性，使得模型的构建这一过程基本没有什么理论基础，基本上只能尝试，然后根据效果推测有效的原因，所以也出现了\nautoML\n这种自动化去搜寻结构和参数的方法。但是抛开那些玄学的部分，在实际应用中模型结构还是有一些可以借鉴的经验，\n\nVC\nDimension: VC Dimension\n本质上是在描述数据量大小和模型大小的关系，即模型参数量应该与数据量大小成正比关系，否则容易出现过拟合或欠拟合的问题\n\nattention: attention\n用最直白的话来说就是动态加权，而这也很符合直觉，如每个特征的重要性不一样，应该对重要性高的给予更高的权重；在\nNN 模型中，attention 常用在两个地方: embedding 和 hidden unit；\nembedding 的 attention 策略可参考这篇文章 SENet 双塔模型：在推荐领域召回粗排的应用及其它；而针对\nhidden unit 的 attention 则可参考这篇 paper，Learning Hidden Unit\nContributions for Unsupervised Acoustic Model Adaptation\n\nmultitask: multitask 结构有两个常见的用处，第一种认为多个 task\n之间有关联，联合训练能增加数据量，同时提升效果；第二种则是对于预估值的准确性有要求的场景，如果广告的\nctr、cvr\n的预估，往往数据流中混合了多个场景的数据，且每个场景对应的数据的后验不同的，为了保值预估值的准确性，需要将后验值不同的数据分到不同的\nhead 中\n\n\n模型训练\n\n模型训练的过程，笔者认为可分为三大部分：初始化，优化器和损失函数\n初始化对效果有影响，而这个问题可以从几个角度去理解，从最优化的角度理解，是因为\nNN 的优化往往是一个 non-convex 的问题，\n如果初始化不好，一开始可能就处于一个不好的位置；从 bp\n的角度理解，初始化的值过小或过大，容易导致梯度消失会梯度爆炸，关于这部分，deeplearning.ai\n上的 Initializing\nneural networks 讲得比较好了，还辅以实践，推荐读一下。\n训练过程本质上就是个优化问题，通过 bp 过程不断修正初始化的\nparameter，从而达到损失函数最小的目标，更详细的描述可以参考 Parameter\noptimization in neural networks；中间涉及到了各种超参数的选择：如\nlearning rate、batch size、optimizer 等；其中 optimizer\n也有非常多的选择，其中 optimizer\n的选择往往又是一个值得考量的地方，关于各类 optimizer\n的区别可以参考这篇文章，一个框架看懂优化算法之异同\nSGD/AdaGrad/Adam\n损失函数基本都是通过 MLE 或 MAP\n推导出来的，其思想都是假设训练样本都是服从某些分布生成的，而训练的目标是让这些样本的联合概率最大；如\nmse 的 assumption 是模型预估值与 ground truth 的误差服从正态分布，cross\nentropy 的 assumption 是预估值服从伯努利分布；而这两个其实也能被统一到\nGLM 这个框架下。\n目前在业界更常见的做法是把问题转为分类问题，对应的 loss 即为 cross\nentropy，而其实一些回归的 loss 也能通过 weighted logistics regression\n转化为分类的问题，比较经典的就是 youtube 的 Deep\nNeural Networks for YouTube Recommendations 中 Modeling Expected\nWatch Time 部分；基于最原始 cross entropy 衍生出来的 loss\n主要有两种形式\n\nreweight, 即对样本进行各种加权，\n包括但不限于根据物理含义直接加权 (如观看时长)、通过 importance sampling\n等方式推导出来的 loss，其最终形式也是 reweight 的模式\n\nauxiliary/regularization, 即在原始的 task\n上增加一些辅助任务或正则项，如 center loss 等\n\n预估纠偏\n在推荐场景下，往往只要序准确就可以了，而总体高估或低估理论上不会影响总体的排序；但是广告场景下涉及到计费环节，需要保证计费时的准确性，因此在广告场景下还需要关注模型预估准确性的问题，假设模型高估了，相当于多收了广告主的钱，广告主吃亏，反之平台吃亏。\n因此，在广告场景下，模型需要关注的指标不仅仅是\nauc，还需要关注预估偏差；为了保证纠偏后不影响总体的排序即 auc\n指标，预估纠偏往往采用的是保序回归，关于预估纠偏可参考这篇文章：使用\nIsotonic Regression 校准分类器\n另外，上面在 loss 中对样本进行 reweight\n的方式，会影响正负样本的分布，导致统计意义上预估值的就是有偏的，应对的策略可以在训练阶段就进行纠偏，或者在预估值上直接做一个转换，这部分内容可参考这篇文章\nDelayed\nFeedBack In Computational Advertising 的 Fake negative\ncalibration\n出价\n上面的无论是召回 + 精排的架构，还是模型相关的部分，在推荐和广告基本都是通用的；而出价 (bidding) 这个领域，\n则是笔者认为是广告相对于推荐的最大的不同；因为在广告场景中引入了广告主 (advertiser) 这一角色，\n出价则是提供了一种方式供广告主表达其对付费诉求，即对一次曝光 / 点击 / 转化愿意付多少钱。\n其次，广告平台往往需要提供多种产品形态来满足广告主的各种需求，如最常见的是保成本类产品、还有一些更追求跑量、品牌广告则是对保量有严格的要求；针对这些不同的产品形态，基本上最终的策略都落在了出价上。因为在基于\necpm 排序的广告系统中，基本上能比较灵活地改动的就是 bid 这个因素了\n而针对上面各种产品需求，其实都是可以通过相同的最优化建模的方式来解决的，阿里之前发表过一篇\npaper\n来描述这个问题，对于出价公式的推导和控制器的构建都有较好的指导意义；笔者针对这篇\npaper 也写了一篇文章 《Bid\nOptimization by Multivariable Control in Display\nAdvertising》阅读笔记，供参考。\n业务视角\n这里的业务视角，其实就是在实际迭代中直接面临的几个问题，也是会直接影响到客户体验与平台收入的几个问题；上面提到的技术可以说都是为了这些业务服务的。笔者将其总结为:\n起量、成本、持续跑量和冷启动四大块。\n起量\n起量可以说是投放面临的第一个难题，素材过审了、计划建好了、预算也准备好，计划是不是就能够如期地起量呢？答案肯定不是的\n广告投放不起量都是怎么解决的？\n就从优化师的角度，描述了为了让计划起量而想出的各种 “奇技淫巧”，总的来说，有以下几个方向：优化素材、放开定向、提高出价 &amp; 预算、堆计划等。\n那么从媒体 / 平台侧的角度来说，有什么手段去缓解广告主起量难的问题么？\n一般来说，起量难的问题会随着广告候选变多而愈发严峻，而这也可以从\nE&amp;E 的角度去解读，因为在 dau\n基本稳定的情况下，媒体展示广告的次数是有限的，如果让更多的新计划得到展示机会，势必会挤压老计划，而且新计划起量后，后续能否持续跑量也是个问题，属于\nexplore 的部分。\n因此，一个朴素的思想是固定一些 explore 的\nquota，专门用于处于起量阶段的计划，相当于给这些起量阶段的计划开的绿色通道；有了绿色通道后，需要考虑的第二个问题是：哪些计划能通过这些绿色通道？每个计划都给同等的机会显然不是最优的，因为不同计划起量后的表现不一样，因此一种更合理的做法是建模判断这些计划在起量后的表现能力，然后根据表现能力决定计划是否能进绿色通道，同时还需要考虑那些堆计划的\ncase，避免对相同或只做了微小改动的计划重复进入绿色通道，给更多广告主以探索的机会。\n成本\n大部分客户投放广告关注的是其\nroi（品牌广告其实也可以认为追求的是长期的\nroi），而近年各个媒体平台上也出现了各种 roi 类的产品，但是 roi\n类的产品要求广告主把付费金额等敏感数据回传，因此大范围推广还需要时间；当前使用更多的是成本类产品，即认为广告主出价是\ntruthful bidding\n的，可以将广告主的出价作为成本，投放过程中尽量让实际成本贴近广告主填的出价。\n由于当前的广告系统都是基于 ecpm 排序和扣费的，因此构成 ecpm\n的几个元素 (bid, ctr,\ncvr) 的值必须要准确，才能保证成本不会过高 (广告主亏) 或过低 (媒体亏)\n首先需要重点关注的是 ctr、cvr\n预估的准确性，而这个问题的难点在于能拿到的训练数据的 label 是\n0 和 1 (代表是否点击 / 转化)，但是实际中需要预估的是一个\nrate，而这导致了没有一个绝对准确的 ground\ntruth，退化成只能通过训练来逼近训练样本中的正负样本的比例，这也是为什么改变了训练样本的分布需要在\nloss function\n或预估值上做纠偏。另外，从概率论出发，大数定律告诉我们：当样本数量越多，则其算术平均值就有越高的概率接近期望值，但问题是很多计划的的\nclick、convert\n数量非常少，所以训练也没法很好的学习出各个计划的期望 cvr。\n因此，面对这么一个没有绝对 ground\ntruth，同时大数定律也不完全适用的而带来的预估偏差的问题，需要有额外的策略来应对，最常见的就是上面提到的保序回归，这个也是基于后验数据的统计对预估值做\ncalibration，因此也需要考虑纠偏粒度上的后验数据是否过少的问题。\n另一个关键因素就是出价了，如果预估完全准确的情况下，按照广告主给定的出价来投放是最优的，但这显然是不太可能，因此，才有了控制器不断地调价来控制成本，满足保成本的诉求，可以认为出价是预估不准的兜底策略。\n冷启动\n无论是在广告还是推荐，冷启动都是一个长期存在的问题，其原因是新用户 / 计划缺少历史数据，模型 / 策略对其学习不够充分，从而效果表现得很差；而在广告场景中，新计划比起新用户的冷启动往往是更常见且严峻的，因为对于成熟的媒体而言，dau/mau\n等基本都是稳定的，但广告主会不断地新建计划。\n冷启动往往会加剧上面提到的各类问题，如在模型上，预估值的准确性更难保证；在出价上，成本更难控制等；而针对冷启动的问题，往往也会从两个方面去优化，即模型和策略。\n在模型上，有不少针对冷启动的 paper，且基本都是针对 nn 这一类\nembedding-based 的模型，其基本思路都是让冷启动 item 的 embedding 贴近\nwarm-up 阶段的状态，笔者将这类方法归纳成两类 (1) 利用 meta-network\n为冷启动的 item 生成一个 id embedding (2) 基于 MAML 的方法训练模型，让\nembedding 更快收敛\n第一种方法的基本思想是利用 item 的 meta 信息 (即使冷启动 item\n也有) 通过一个小网络 (即 mata-network) 生成 embeddding，然后增加一些\nauxiliary task 来训练这个小网络，这些 task\n就有很多选择了，更多是对业务的理解，如可以让 meta-network 吐出来的\nembedding 与 item 在成熟期的 embedding\n误差尽量小 (针对已经步入成熟期的 item 的样本)，也可以利用冷启动 item\n对网络进行二次的训练等\n这种方法的两篇比较典型的 paper 可参考\n\nWarm Up Cold-start\nAdvertisements: Improving CTR Predictions via Learning to Learn ID\nEmbeddings\n\nLearning to Warm Up\nCold Item Embeddings for Cold-start Recommendation with Meta Scaling and\nShifting Networks\n\n第二中方法基本上就是基于 meta learning 的思想，让模型能够更好的 learn\nto learn，即使对于样本量很少的 item 也能较快学习到，代表方法就是\nMAML，可以参考下面两篇 paper，另外，关于 MAML 更通俗的介绍可参考知乎上这篇文章\n\nModel-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks\n\nMeLU- Meta-Learned\nUser Preference Estimator for Cold-Start Recommendation\n\n上面的方法基本都是让冷启动 item 的 id embedding\n能更快收敛，还有另一种方法是将冷启动的 item\n的一些更泛化的特征直接加入到模型中，如图像、文本等描述性特征，通过一些\npretrained model (如 VGG、Bert 等) 将其转为 embedding\n的模型加入模型中，也是一种常见的套路。\n除了模型，策略上往往也需要对冷启动的计划有额外的举措；如在冷启动阶段，可以给计划 \"绿色通道\",\n即一定 explore 的\nquota，如同上面提到的起量问题一样 (起量其实也可以算是一个冷启动问题)，而这也涉及到老生常谈的\nE&amp;E 问题了，一个跟 deep learning\n的玄学程度不相上下的领域，这部分内容介绍可参考 EE\n问题概述。同样地，这些 explore quota\n也应该根据每个计划的预期表现给予个性化的分配；即需要识别计划未来的表现，毕竟往往个性化分别比起均分都是最优的。此外，这些\nquota 必然会对成熟期的计划造成一定的积压，所以也需要考虑冷启动和成熟期的\ntrade-off。\n除了额外的扶持，冷启动的出价方法也需要额外考虑，因为此时计划的后验数据基本是空的，那怎么才能获取出价调控需要的这些数据呢？这里也有一些思路，比如可不可以利用相似计划的后验数据？或者信任预估，直接对预估值取\nsum？或者产品层面就不对冷启动计划做苛刻的成本要求？\n同时，在实际产品中，往往也需要联合产品教育广告主，冷启动期间就是会存在不稳定性，更容易出现超成本等问题，而如果客户对平台粘性不高或者整个市场有其他更有竞争力的竞品，这些超成本所带来的损失往往也是需要有平台来承担。\n持续跑量\n持续跑量可以认为是计划渡过冷启动后，亦即计划进入了成熟期需要面临的问题，因为广告主在投放追求的往往是两个东西：成本和跑量，在成本能控住的前提下，跑量一般是越多越好的，但现状是在不少的媒体上，计划在进入成熟期不久就会掉量，而且往往是掉了之后就再也起不来了，也就是说计划的生命周期较短。\n造成这个问题原因有很多，包括但不限于\n\n广告主频繁地修改出价、定向、预算等有可能改变计划的稳态\n\n模型预估的不准确、出价调控不够稳定，可能导致计划突然爆量或掉量\n\n如果竞价环境比价激烈，那么对于有限的展示机会和不断增加的计划数，部分老计划掉量也是不可避免的\n\n自然衰减，如计划圈定的人群基本都曝光了，或者创意的自然衰减\n\n这个问题会往往会导致广告主为了跑量而不断复制新建计划，进一步加剧起量、冷启动等问题，也直接导致竞争环境变得更激烈，系统的机器负载更大，因此，保证计划的持续跑量是所有广告系统都需要解决的重要问题。\n抛开第三个比较难改变的因素，针对其他几个原因，有一些思路也许值得借鉴\n\n广告主要减少频繁修改计划的定向、出价等操作，这些需要平台教育广告主，至于频率多大算频繁，定向又应该放开到何种程度，需要平台同时实验等手段测试出来，最好能给广告主提供一个参考值\n\n要尽量减少系统波动等因素对计划的影响，即要减少各中工程和算法的事故的影响，如尽量保证各种\ninfra 服务的高可用性，AB\n实验要更加谨慎，因为这些操作都是有可能影响计划的稳态\n\n对于要掉量的计划生效额外策略；这里面又可分为两个问题，如识别掉量计划以及对计划做何种策略，这个问题跟冷启动的扶持也很相似\n\n总的来说，从业务视角来看的几个问题：起量、冷启动、成本、持续跑量，都是广告主非常关心的几个问题，也是直接衡量平台给广告主带来的价值的几个方面，值得重点关注；而且这里面也不像第一部分那么偏技术导向了，需要更多地考虑产品形态、如何更好地服务客户等。\n产品视角\n这里的产品视角也可以理解为广告主的视角，亦即平台披露给广告主的产品形态；一般可将其分为划分为品牌广告、效果广告两大类，而这两大类下面又有很多细分；这里就简单概述一下这两大类广告的一些基本知识。\n品牌广告\n品牌广告往往是一个广告系统发展初期的广告模式，基本流程是广告主先付钱，平台保证曝光，如\ncpt，gd\n就是典型的品牌广告，前者没什么好说的，固定的广告位在特定位置强出就好了；后者则一般会涉及到库存预估和库存分配两个问题\n\n库存预估：gd\n承诺的是未来的曝光量，因此需要保证当前售卖的库存不能超过未来的曝光量；一般可以通过时序预估模型来进行预估\n\n库存分配：将库存和 gd 计划作为 supply side 和 demand side\n构造二部图，然后通过分配算法进行分配，常见的分配算法有 HWM 和 SHALE\n\n上面的做法只是在解决 gd\n广告的保量问题，但是随着优化的精细化，除了保量，还需要考虑一些其他问题，如\n\ngd\n广告和效果广告往往存在竞争关系 (因为曝光的机会是共享且数量是基本固定的),\n需要联合效果广告建模使得利益最大化\n\ngd\n广告除了保量，往往客户也会提出效果的要求，否则平台可以把低质流量直接给到\ngd 计划；因此，从建模上这成了一个多约束的优化问题\n\n如果额外考虑以上这两点，上面传统的分配算法就有点问题了，首先库存预估出来的量往往是总体的曝光量，gd\n能占用其中多少的量需要拍一个数，或者根据 cpm\n分配；其次，上面分配算法是直接把这个曝光给这个 gd\n广告的，不会判断质量的好坏，但我们实际是不希望把太差的量给广告主的，同时也不能对竞价有过多的挤压，因此需要判断流量对于\ngd 计划的质量，需要为 gd 计划考虑一种更加灵活的拿量方法。\n综上，gd\n广告需要考虑保量、效果、溢价率以及对效果广告的挤压，因此建模时也需要把这些因素考虑进去。\n效果广告\n效果广告占了广告市场的大部分的份额，因为绝大部分广告主关心的是投放的\nroi；而上面提到的各种技术，很大一部分都是为效果广告服务的，如保成本、跑量等；这里就不赘述了。\n深度转化产品\n随着优化的深入，常见的效果广告的优化目标已经不能满足所有广告主的需求了，比如部分行业广告主会希望直接优化到\nroi 而不仅仅是成本，同时也能提供相应的数据\n这个时候就需要考虑为这个广告主进行额外建模了，这里又有两种选择，一种是广告主直接把数据回传到平台进行常规的模型训练，另一种则是广告主不愿意把\nlabel 直接回传到平台，这中情况下可以通过 Federated Learning\n的方法对其进行优化，笔者在实际中对这部分也有一定实践经验，详细可参考 字节跳动联邦学习平台 Fedlearner：4 个月落地开源，投放增效 200%+\n建模后往往会通过出价的方式应用模型，除非广告主能够提供直投深度目标的\nbid；而这往往又是一个多目标约束的优化问题，具体的策略可以看上面出价部分\n小结\n综上，本文主要从技术视角、业务视角和产品视角总结了一些笔者比较关注的计算广告领域的知识\n\n技术视角：基本是 “召回 + 精排”\n的结构，里面两个值得关注的部分是模型和出价\n\n业务视角：主要面临 4 个问题：起量、成本、持续跑量、冷启动\n\n产品视角：主要可分为品牌广告、效果广告和深度转化广告三大类\n\n内容很多也很乱 &gt;_&lt;,\n但在整个广告系统下也只是冰山一角；同时因为保密性原因，很多具体方法没有在文中提及，但是相信提及的思想都是普适的，欢迎交流指导。\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"CTR 预估模型简介 -- 非深度学习篇","url":"/2018/07/15/CTR%20%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B--%E9%9D%9E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AF%87/","content":"本文主要介绍 CTR 预估中常用的一些模型，主要是非深度学习模型，包括\nLR、GBDT+LR、FM/FFM、MLR。每个模型会简单介绍其原理、论文出处以及其一些开源实现。\n\nLR(Logistic Regerssion)\nLR + 海量人工特征\n是业界流传已久的做法，这个方法由于简单、可解释性强，因此在工业界得到广泛应用，但是这种做法依赖于特征工程的有效性，也就是需要对具体的业务场景有深刻的认识才能提取出好的特征。\n原理\nLR 是一个很简单的线性模型，其输出的值可认为是事件发生 (\\(y=1\\)) 的概率，即输出值如下式所示\n\\[\\begin{align}  h(x) = p(y=1|x) =\n\\sigma(w^Tx+b) \\end{align}\\]\n其中 \\(w\\) 为模型参数，\\(x\\) 为提取的样本特征，两者均为向量，\\(b\\) 是偏置项。\\(\\sigma\\) 为 sigmoid 函数，即 \\(\\sigma(x) = 1/(1+e^{-x})\\)\n有了事件发生的概率，则事件不发生的概率为 \\(p(y=0|x) =\n1-h(x)\\), 将这两个概率通过如下一条公式表示为\n\\[\\begin{align} p(y|x) =\nh(x)^y(1-h(x))^{1-y} \\end{align}\\]\n有了这个概率值，则给定 \\(n\\)\n个样本，便可通过极大似然估计来估算模型参数，即目标函数为\n\\[\\begin{align} \\max\n\\prod_{i=1}^np(y_i|x_i) \\end{align}\\]\n通常我们还会对概率取 log，同时添加负号将 max\n改成 min，则可将目标函数改写成如下的形式\n\\[\\begin{align} \\min -\\sum_{i=1}^ny_i\\log\nh(x_i)+(1-y_i)\\log (1-h(x_i)) \\end{align}\\]\n上面的损失函数也叫作 log loss，实际上多分类的\ncross entropy 也同以通过极大似然估计推导出来。\n有了损失函数，便可通过优化算法来求出最优的参数，由于这是个无约束的最优化问题，可选用的方法很多，最常用的就是\ngradient\ndescent，除此之外，另外还有基于二阶导数的牛顿法系列，适用于分布式中的\nADMM，以及由 Google 在论文 Ad\nClick Prediction: a View from the Trenches 中提出的 FTRL\n算法，目前也是业界普遍采用的方法，该算法具有 online learning\n和稀疏性的良好特性，online learning 指的是其更新方式与 SGD (stochastic\ngradient descent)\n相似，稀疏性指的是该算法能够解决带非光滑的 L1 正则项的优化问题。由于这里这篇文章主要讲述各种\nCTR 预估模型，因此这里不对优化算法做展开了。\n上面提到了 L1 正则项，就是在原来的损失函数基础上加上了 \\(C\\sum_{i=1}^m |w_i|\\) 这一项，\n表示各个参数的绝对值的和乘上常数 \\(C\\)；加上这一项后能够使得最终的求解出来的参数中大部分的\n\\(|w_i|\\)\n为 0，这也是稀疏性的名称来源。稀疏性使得模型的复杂度下降，缓解了过拟合的问题，同时具有有特征筛选的能力。因为\nLR 模型可以理解为对各个特征进行加权求和，如果某些特征的权重即 \\(w_i\\)\n为 0，则可认为这些特征的重要性不高。在 CTR 预估中输入的是海量人工特征，因此添加\nL1 正则化就更有必要了。\n由于 L1 正则项不再是处处光滑可导的函数，因此在优化损失函数时。原来的\ngradient descent 不能够直接使用，而是要通过 subgradient\n的方法或前面提到的 FTRL 算法进行优化。\n上面涵盖了 LR 模型的基本原理。而在 CTR 预估中，应用 LR\n模型的重点在于特征工程。LR 模型适用于高维稀疏特征。对于\ncategorical 特征，可以通过 one-hot 编码使其变得高纬且稀疏。而对于\ncontinious 特征，可以先通过区间划分为 categorical 特征再进行 one-hot\n编码。同时还需要进行特征的组合 / 交叉，以获取更有效的特征。\n一些问题\n上面介绍过程中有一些结论我们直接就使用了，下面对于上面提到的某些结论做出一些解释\n1. LR 的输出为什么可以被当做是概率值？\n这部分涉及到广义线性模型 (GLM，Generalized\nlinear model) 的知识，这里略过复杂的推导，直接给出结论。简单来说，LR\n实际上是一个广义线性模型，其假设是二分类中 \\((y|x,\\theta)\\)\n服从伯努利分布 (二项分布)，即给定输入样本 \\(x\\) 和模型参数 \\(\\theta\\),\n事件是否发生服从伯努利分布。假设伯努利分布的参数 \\(\\phi\\) ，则 \\(\\phi\\) 可作为点击率。通过\n广义线性模型的推导，能够推出 \\(\\phi\\)\n的表示形式如下\n\\[\\begin{align} \\phi = 1/(1+e^{-\\eta})\n\\end{align}\\]\n从上面的式子可知，LR 中的 sigmoid\n函数并不是凭空来的，而式子中的 \\(\\eta\\) 也被称为连接函数（Link function),\n是确定一个 GLM 的重要部分，在 LR 中为简单的线性加权。\n另外，如果将输出值与真实值的误差的分布假设为高斯分布，那么从 GLM\n可推导出 Linear Regression，关于 GLM 详细的推导可参考这篇文章 广义线性模型（GLM）。\n2. 为什么 L1 正则项能够带来稀疏性？\n这里有个很直观的回答，l1 相比于\nl2\n为什么容易获得稀疏解？，简单来说，就是当不带正则项的损失函数对于某个参数\n\\(w_i\\) 的导数的绝对值小于 l1\n正则项中的常数 \\(C\\) 时，这个参数 \\(w_i\\) 的最优解就是 0。\n因为求解某个参数 \\(w_i\\)\n使得损失函数取极小值时可分两种情况讨论 (下面的 \\(L\\) 为不带正则项的损失函数) 1）\\(w_i&lt;0\\) 时，\\(L+C|w_i|\\) 的导数为 \\(L'- C\\) 2) \\(w_i&gt;0\\) 时，\\(L+C|w_i|\\) 的导数为 \\(L'+C\\)\n当 \\(w_i&lt;0\\) 时，令 \\(L'- C &lt; 0\\), 函数在递减；而当 \\(w_i&gt;0\\) 时，令 \\(L'+C &gt; 0\\), 函数在递增，则 \\(w_i=0\\)\n便是使得损失函数最小的最优解，且结合 \\(L'-\nC &lt; 0\\) 和 \\(L'+C &gt;\n0\\)，可得 \\(C &gt;\n|L'|\\)。这便是我们上面得到的结论，上面是针对某一个参数，实际上也可以推广到所有参数上。事实上，通过\nsubgradient descent 求解这个问题时也能够得到相同的结论。\n3. 连续特征为什么需要离散化？\n参考这个问题：连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？\n离散化后有以下几个好处：\n\n稀疏向量内积乘法运算速度快，计算结果方便存储\n离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄 &gt; 30 是 1，否则 0。如果特征没有离散化，一个异常数据 “年龄 300 岁” 会给模型造成很大的干扰；\n逻辑回归属于广义线性模型，表达能力受限；单变量离散化为 N 个后，可以通过\none-hot\n编码为每个变量设置单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；\n离散化后可以进行特征交叉，由 M+N 个变量变为 M*N 个变量，进一步引入非线性，提升表达能力；\n特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30 作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间要取决于具体的场景\n\n4.1 为什么要对 categorical 特征做 One-hot 编码后再输入\nLR？\n参考这篇文章 One-Hot 编码与哑变量，简单来说，就是 LR 建模时，要求特征具有线性关系，而实际应用中很少有满足这个假设关系的，因此 LR 模型效果很难达到应用要求。但是通过对离散特征进行\none-hot 编码，LR\n可以为某个特征中所有可能的值设置一个权重，这样就能够更准确的建模，也就能够获得更精准的模型。而\none-hot 编码后特征实际上也是做了一个 min-max\n归一化，能够克服不同特征的量纲差异，同时使模型收敛更快。\n开源实现\n由于 LR 模型的广泛性，基本上每个机器学习库或者框架都有相关实现，如\nsklearn 提供了单机版的实现，spark\n提供了分布式版本的实现，腾讯开源的\nParameter Server Angel\n中也提供了 LR+FTRL\n的实现，Angel 支持 Spark，目前也还在开发中 。除此之外，Github\n上也有很多个人开源的实现，这里不再列举。\nLS-PLM(Large Scale\nPiece-wise Linear Model)\nLS-PLM (也叫作 MLR, Mixture of Logistics Regression) 是阿里妈妈在 2017\n年在论文 Learning Piece-wise\nLinear Models from Large Scale Data for Ad Click Prediction\n中公开的，但是模型早在 2012 年就在阿里妈妈内部使用。这个模型在 LR\n基础上进行了拓展，目的是为了解决单个 LR 无法进行非线性分割的问题。\n原理\nLR\n是一个线性模型，模型会在数据空间中生成一个线性分割平面，但是对于非线性可分的数据，这一个线性分割面显然无法正确分割这些数据。以下图为例（摘自上面的论文），A）为一组非线性训练数据的正负样本分布；对于该问题，LR 会生成\nB）中的分割平面，C) 图展示的 LS-PLM 模型 则取得了较好的效果。\n\n\nLS-PIC\n\n在 CTR 问题中，划分场景分别建模是一种常见的手法。例如，同一产品的 PC/APP 端，其用户的使用时间和习惯差异可能很大；比如 PC 可能更多是办公时间在看，而手机则是通勤时间或者临睡前使用更多。假设有 hour 作为特征，那么 “hour=23” 对于 APP 端更加有信息量，而对于 PC 可能意义不大。因此，区分 PC/APP 端分别建模可能提升效果。\nLS-PLM\n也是采用这个思想的，不够这里不是划分场景，而是划分数据，通过将数据划分不同的 region、然后每个 region 分别建立\nLR。\n这里需要注意的是这里一个样本并不是被唯一分到了一个 region，而是按权重分到了不同的 region。其思想有点像\nLDA (Latent Dirichlet allocation) 中一个单词会按照概率分到多个 topic\n上。\n论文中的公式如下\n\\[\\begin{align} p(y=1|x) = g (\n\\sum_{j=1}^m \\sigma(\\mu_j^T x)\\eta(w_j^Tx)) \\end{align}\\]\n公式中的符号定义如下：\n参数定义如下：\n\n\\(m\\) : region\n的个数 (超参数：一般是 10~100)\n\\(\\Theta=\\{\\mu_1,\\dots,\\mu_m,\nw_1,\\dots,w_m \\}\\): 表示模型的参数，需要训练\n \\(g(\\cdot)\\)：为了让模型符合概率定义 (概率和为 1) 的函数\n \\(\\sigma(\\cdot)\\)：将样本分到\nregion 的函数\n \\(\\eta(\\cdot)\\)：在 region\n中划分样本的函数\n\n前面提出的公式更像个框架，在论文中，只讨论了 \\(g(x) = x\\), \\(\\sigma\\) = softmax ，\\(\\eta\\) = sigmoid\n的情形，而且因此，上面的公式可写成如下的形式\n\\[\\begin{align} p(y=1|x) = \\sum_{i=1}^m\n\\frac{e^{\\mu_i^Tx}}{\\sum_{j=1}^m e^{\\mu_j^Tx}}\\frac{1}{1+e^{-w_i^Tx}}\n\\end{align}\\]\n这个公式其实已经变成了通过多个 LR 模型进行加权求和的 bagging\n模式，只是这里每个模型的权重是学习出来而不是事先确定的。\n写出了概率函数，后面的推导跟前面的 LR\n其实是一样的，也是先通过极大似然估计得到 \\(\\max\\) 问题，添加负号后转为损失函数求 \\(\\min\\) 问题。这里不做详细的推导了。\n在 LS-PLM 中也是需要添加正则项的，除了在 LR 中提到的 L1\n正则化，论文还提出了 \\(L_{2,1}\\)\n正则项，表示如下\n\\[\\begin{align} ||\\Theta||_{2,1} =\n\\sum_{i=1}^d \\sqrt {\\sum_{j=1}^m(\\mu_{ij}^2+w_{ij}^2)}\n\\end{align}\\]\n上式中的 \\(d\\) 表示特征的维数，其中\n\\(\\sqrt\n{\\sum_{j=1}^m(\\mu_{ij}^2+w_{ij}^2)}\\)\n表示对某一维特征的所有参数进行 L2 正则化，而外侧的 \\(\\sum_{i=1}^d\\) 表示对所有的 feature 进行 L1\n正则化，由于开方后的值必为正，因此这里也不用添加绝对值了。由于结合了 L1\n和 L2 正则项，所以论文也将这个叫做 \\(L_{2,1}\\) 正则项。\n由于损失函数和正则项都是光滑可导的，因此优化方面比带 L1 正则的 LR\n更加简单，可选的优化方法也更多。\nMLR 适用的场景跟 LR 一样，也是适用于高纬稀疏特征作为输入。\n开源实现\n前面提到的腾讯的 PS Angel 实现了这个算法，具体可参考这里；Angel\n是用 Scala 开发的。也有一些个人开源的版本如 alphaPLM，这个版本是用\nC++ 写的，如果需要实现可以参考以上资料。\nGBDT+LR(Gradient\nBoost Decision Tree + Logistic Regression)\nGBDT + LR 是 FaceBook 在这篇论文 Practical\nLessons from Predicting Clicks on Ads at Facebook\n中提出的，其思想是借助 GBDT 帮我们做部分特征工程，然后将 GBDT 的\n输出作为 LR 的输入。\n原理\n我们前面提到的无论 LR 还是\nMLR，都避免不了要做大量的特征工程。比如说构思可能的特征，将连续特征离散化，并对离散化的特征进行\nOne-Hot\n编码，最后对特征进行二阶或者三阶的特征组合 / 交叉，这样做的目的是为了得到非线性的特征。但是特征工程存在几个难题：\n\n连续变量切分点如何选取？\n离散化为多少份合理？\n选择哪些特征交叉？\n多少阶交叉，二阶，三阶或更多？\n\n而 GBDT + LR 这个模型中，GBDT 担任了特征工程的工作，下面首先介绍一下\nGBDT。\nGBDT 最早在这篇论文 Greedy Function\nApproximation：A Gradient Boosting Machine 中提出； GBDT\n中主要有两个概念：GB (Gradient\nBoosting) 和 DT (Decision Tree)，Gradient Boosting 是集成学习中\nboosting 的一种形式，Decision Tree\n则是机器学习中的一类模型，这里不对这两者展开，只讲述在 GBDT\n中用到的内容。关于决策树的介绍可参考这篇文章 决策树模型\nID3/C4.5/CART 算法比较。\n在 GBDT 中采用的决策树是 CART (Classification And Regression\nTree)，将其当做回归树使用，这里的回归树是一棵在每个树节点进行分裂的时候，给节点设定其在某个特征的的值，若样本对应的特征的值大于这个给定的值的属于一个子树，小于这个给定的值的属于另一个子树。\n那么，构建 CART 回归树是\n的关键问题就在于选择具体的特征还有这个特征上具体的值了。选择的指标是平方误差最小化准则。对于任意一个切分，其平方误差计算方式如下\n\n假设切分后左子树有 \\(m\\)\n个样本，右子树有 \\(n\\) 个\n计算左子树样本的目标值的均值为 \\(y_m =\n\\frac{1}{m}\\sum_{i=1}^{m}y_i\\),\n同样计算右子树样本的目标值的均值为 \\(y_n =\n\\frac{1}{n}\\sum_{j=1}^{n}y_j\\)\n 平方误差和为 \\(L = \\sum_{i=1}^m(y_i -\ny_m)^2 + \\sum_{j=1}^n(y_j - y_n)^2\\)\n 对于每一个可能的切分值，我们都可计算其平方误差和 \\(L\\)，选择使得 \\(L\\) 最小的切分点即可。\n\n上面便是 GBDT 中的 “DT”\n部分，用于解决一个回归问题，也就是给定一组样本，我们可以通过上面的方式来构建出一棵\nCART 来拟合这组样本。下面我们来讲一下 GBDT 中的 “GB” 部分。\n简单来说，gradient boosting\n就是将若干个模型的输出进行叠加作为最终模型的输出。如下图是一个简单的例子 (图片来源于提出\nxgb 的论文：XGBoost: A\nScalable Tree Boosting System)\n\n\nxgboost\n\n下式就是叠加了 \\(T\\) 个 \\(f_t(x)\\) 模型作为最终的模型，\\(f_t(x)\\) 在 GBDT 中就是一棵 CART，当然\n\\(f_t(x)\\) 不限于树模型。\n\\[\\begin{align} F(x) = \\sum_{t=1}^Tf_t(x)\n\\end{align}\\]\n在构建每棵树的时候，输入的样本不同的地方在于每个样本的目标值 \\(y\\)；如构建第 \\(k\\) 棵树，对于原始样本 \\((x_i, y_i)\\), 其目标值变为\n\\[\\begin{align} y_{ik} = y_i -\n\\sum_{t=1}^{k-1}f_t(x_i) \\end{align}\\]\n即输入第 \\(k\\) 棵树的样本变为 \\((x_i, y_{ik})\\)，所以在构建第 \\(k\\) 棵树的时候，实际上是在拟合前 \\(k-1\\)\n棵树的输出值的和与样本真实值的残差。\n回到我们的 GBDT + LR 模型，首先通过前面提到的 GBDT\n训练出一批树模型，然后样本输入每棵树后最终都会落到一个具体的叶子节点上，那我们就将这个节点标为\n1，其他叶子节点标为 0，这样每棵树输出的就相当于是一个 one-hot\n编码的特征。如下图是摘自 FaceBook\n原始论文的图，里面有两棵树，假如输入 \\(x\\)\n在第一棵树中落入第一个叶子节点，在第二棵树种落入第二个叶子节点，那么输入\nLR 的特征为 [1, 0, 0, 0, 1].\n\n\nGBDT + LR\n\nGBDT+LR\n方案中每棵决策树从根节点到叶节点的路径，会经过不同的特征，此路径就是特征组合，而且包含了二阶，三阶甚至更多，因此输出的\none-hot\n特征是原始特征进行交叉后的结果。而且每一维的特征其实还是可以追溯出其含义的，因为从根节点到叶子节点的路径是唯一的，因此落入到某个叶子节点表示这个特征满足了这个路径中所有节点判断条件。\nGBDT 适用的问题刚好与 LR 相反，GBDT\n不适用于高纬稀疏特征，因为这样很容易导致训练出来的树的数量和深度都比较大从而导致过拟合。因此一般输入 GBDT\n的特征都是连续特征。\n在 CTR 预估中，会存在大量的 id 特征，对于这种离散特征，一般有两种做法\n1) 离散特征不直接输入到 GBDT 中进行编码，而是做 one-hot\n编码后直接输入到 LR 中即可；对于连续特征，先通过 GBDT\n进行离散化和特征组合输出 one-hot 编码的特征，最后结合这两种 one-hot\n特征直接输入到 LR。大致框架如下所示\n\n\nReal GBDT\n\n\n 将离散的特征也输入 GBDT\n进行编码，但是只保留那些出现频率高的离散特征，这样输入 GBDT\n中的 one-hot 特征的维度会遍地，同时通过 GBDT 也对原始的 one-hot\n特征进行了组合和交叉。\n\n一些问题\n1. GBDT 中的 gradient 在哪里体现了？\n推导到现在，好像也没有提及到\ngradient，其实前面拟合残差时已经用到了 gradient\n的信息。\n首先，我们要转换一下思维，我们一般在优化中使用的 gradient descent\n都是对某个参数进行的，或者说是在参数空间中进行的，但是除了参数空间，还可以在函数空间中进行。如下图所示对比了两种方式 (下面两张图均摘自 GBDT 算法原理与系统设计简介)\n\n\ngredient descent v.s. gradient\nboosting\n\n在函数空间中，是对函数直接进行求导的，因此 GBDT 算法的流程如下\n\n\nGBDT\n\n上图中的 \\(\\tilde{y_i}\\)\n就是我们前面说的第 \\(i\\)\n个样本的残差，当损失函数为平方损失即 \\[L(y,F(x)) = \\frac{1}{2}(y-F(x))^2\\]\n对 \\(F(x)\\) 求导得出的残差为\n\\[\\begin{align} \\tilde{y_i} = y_i - F(x_i)\n\\end{align}\\]\n这正是我们前面说的样本的真实值与前面建的树的输出和的差。如果损失函数改变，这个残差值也会进行相应的改变。\n2. GBDT 怎么处理分类问题？\n上面我们讲的 GBDT 是处理回归问题的，但是对于 CTR\n预估这一类问题，从大分类上其实还是一个分类问题。那 GBDT\n是怎么处理这个问题？\n在回归问题中，GBDT 每一轮迭代都构建了一棵树，实质是构建了一个函数\n\\(f\\)，当输入为 x 时，树的输出为 \\(f(x)\\)。\n在多分类问题中，假设有 \\(k\\)\n个类别，那么每一轮迭代实质是构建了 \\(k\\) 棵树，对某个样本 \\(x\\) 的预测值为 \\(f_{1}(x), f_{2}(x), ..., f_{k}(x)\\),\n在这里我们仿照多分类的逻辑回归，使用\nsoftmax 来产生概率，则属于某个类别 \\(j\\)\n的概率为\n\\[\\begin{align} p_{c} =\n\\frac{\\exp(f_{j}(x))}{ \\sum_{i=1}^{k}{exp(f_{k}(x))}}\n\\end{align}\\]\n通过上面的概率值，可以分别计算出样本在各个分类下的 log loss，根据上面\nGBDT 在函数空间的求导，对 \\(f_1\\) 到\n\\(f_k\\)\n都可以算出一个梯度，也就是当前轮的残差，供下一轮迭代学习。也就是每一轮的迭代会同时产生\nk 棵树。\n最终做预测时，输入的 \\(x\\) 会得到\n\\(k\\)\n个输出值，然后通过 softmax 获得其属于各类别的概率即可。\n更详细的推导可参考这篇文章：当我们在谈论 GBDT：Gradient\nBoosting 用于分类与回归\n开源实现\n直接实现 GBDT + LR\n的开源方案不多，但是由于两者的耦合关系并不强，因此可以先训练\nGBDT，然后将原始特征通过 GBDT 转换后送入到 LR 中，GBDT\n有多个高效的实现，如 xgboost，LightGBM。\nFM(Factorization Machine)\nFM（Factorization Machine）是于 2010 年在论文 Factorization\nMachines\n中提出，旨在解决稀疏数据下的特征组合问题。其思想是对组合特征的参数所构成的参数矩阵进行矩阵分解，从而得到每个原始特征的隐向量表示，更新特征的隐向量对数据的稀疏性具有鲁棒性。关于\nFM 和 FFM ，美团点评这篇文章：深入 FFM 原理与实践\n其实已经写得很详细了，本文主要参考该文章进行修改。\n原理\nFM 可以认为是在 LR\n的基础上加入特征的二阶组合，即最多有两个特征相乘，则模型可表示成如下形式\n\\[\\begin{align} y(\\mathbf{x}) = w_0+\n\\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_i\nx_j  \\end{align}\\]\n从模型也可以看出，其实 FM 是在 LR 基础上增加了最后的二阶交叉项。\n从上面的公式可以看出，组合特征的参数一共有 \\(\\frac{n(n−1)}{2}\\)\n个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的，原因是每个参数\n\\(w_{ij}\\) 的训练需要大量 \\(x_i\\) 和 \\(x_j\\)\n都非零的样本；由于样本数据本来就比较稀疏，满足 \\(x_i\\) 和 \\(x_j\\)\n都非零的样本将会非常少。训练样本的不足，则会导致参数 \\(w_{ij}\\)\n不准确，最终将严重影响模型的性能。\n如何解决这个问题？FM 中借鉴了矩阵分解的思想，在推荐系统中，会对\nuser-item 矩阵进行矩阵分解，从而每个 user 和每个 item\n都会得到一个隐向量。如下图所示\n\n\nmatrix\n\n类似地，所有二次项参数 \\(w_{ij}\\)\n可以组成一个对称阵 \\(W\\)，那么这个矩阵就可以分解为 \\(W=V^TV\\)，\\(V\\) 的第 \\(j\\) 列便是第 \\(j\\)\n维特征的隐向量。换句话说，每个参数可表示成两个隐向量的内积的形式。即\n\\(w_{ij}=&lt;v_i,v_j&gt;\\)，\\(v_i\\) 表示第 \\(i\\)\n维特征的隐向量，这就是 FM 模型的核心思想。因此，可以将上面的方程改写成如下形式\n\\[\\begin{align}y(\\mathbf{x}) = w_0+\n\\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n\n&lt;v_i, v_j&gt;x_i x_j \\end{align}\\]\n假设隐向量的长度为 \\(k(k&lt;&lt;n)\\)，二次项的参数数量减少为\n\\(kn\\) 个，远少于多项式模型的参数数量。另外，参数因子化使得\n\\(x_hx_i\\) 的参数和 \\(x_ix_j\\)\n的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计 FM 的二次项参数。\n具体来说，\\(x_hx_i\\) 和 \\(x_ix_j\\) 的系数分别为 \\(&lt;v_h,v_i&gt;\\) 和 \\(&lt;v_i,v_j&gt;\\)，它们之间有共同项 \\(v_i\\)。也就是说，所有包含 \\(x_i\\) 的非零组合特征（即存在某个 \\(j≠i\\)，使得 \\(x_ix_j≠0\\)）的样本都可以用来学习隐向量\n\\(v_i\\)，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，\\(w_{hi}\\) 和 \\(w_{ij}\\) 是相互独立的。\n另外，原始论文还对特征交叉项计算的时间复杂度做了优化，具体见如下公式\n\\[\\begin{align} \\sum_{i=1}^n\n\\sum_{j=i+1}^n \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j =\n\\frac{1}{2} \\sum_{f=1}^k \\left(\\left( \\sum_{i=1}^n v_{i, f} x_i\n\\right)^2 - \\sum_{i=1}^n v_{i, f}^2 x_i^2 \\right)\n\\end{align}\\]\n从公式可知，原来的计算复杂度为 \\(O(kn^2)\\)，而改进后的时间复杂度为 \\(O(kn)\\)\n在 CTR 预估中，对 FM 的输出进行 sigmoid 变换后与 Logistics Regression\n是一致的，因此损失函数的求解方法以及优化算法都基本一致，这里不再详细展开。\n由于 FM 可以看做是 LR\n基础上加上二阶特征组合的模型，同时模型本身对稀疏性有较好的鲁棒性，因此\nFM\n适用范围跟 LR 一样，都适用于输入的特征是高纬度稀疏特征。\n开源实现\nFM 在 github 上有单机版本的开源实现 fastFM和 pyFM， fastFM\n是一个学术项目，发表了相关论文 fastFM: A Library for\nFactorization Machines, 对 FM 进行了拓展；同时我们前面提到的腾讯的 PS\nAngel 中也实现了这个算法，可参考这里。\nFFM(Field-aware\nFactorization Machine)\nFFM 发表于论文 Field-aware\nFactorization Machines for CTR Prediction， 是台大的学生在参加 2012\nKDD Cup 时提出的，这个论文借鉴了论文 Ensemble\nof Collaborative Filtering and Feature Engineered Models for Click\nThrough Rate Prediction 中的 field 的 概念，从而提出了 FM\n的升级版模型 FFM。\n原理\n通过引入 field 的概念，FFM 把相同性质的特征归于同一个 field。简单来说，同一个 categorical 特征经过 One-Hot 编码生成的数值特征都可以放到同一个 field，包括用户性别、职业、品类偏好等。\n在 FFM 中，每一维特征 \\(x_i\\)，针对其它特征的每一种 field \\(f_j\\)，都会学习一个隐向量 \\(v_{i,f_j}\\)。因此，隐向量不仅与特征相关，也与 field 相关。也就是说，假设有\n\\(f\\) 个 field，那么每个特征就有 \\(f\\) 个隐向量，与不同的 field\n的特征组合时使用不同的隐向量，而原来的 FM\n中每个特征只有一个隐向量。\n实际上，FM 可以看作 FFM 的特例，是把所有特征都归属到一个 field\n时的 FFM 模型。根据 FFM 的 field 敏感特性，同样可以导出其模型方程如下\n\\[\\begin{align} y(\\mathbf{x}) = w_0 +\n\\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle\n\\mathbf{v}_{i, f_j}, \\mathbf{v}_{j, f_i} \\rangle x_i\nx_j  \\end{align}\\]\n其中，\\(f_j\\) 是第 \\(j\\) 个特征所属的 field。如果隐向量的长度为\n\\(k\\)，那么 FFM 的二次参数有 \\(nfk\\) 个，远多于 FM 模型的 \\(nk\\) 个。此外，由于隐向量与 field\n相关，FFM 二次项并不能够化简，其预测复杂度是 \\(O(kn^2)\\)。\n其实，FFM 是在 FM\n的基础上进行了更细致的分类，增加了参数的个数使得模型更复杂，能够拟合更复杂的数据分布。但是损失函数的推导以及优化的算法跟前面的\nFM 还有 LR 都是一样的，因此这里不再赘述。\nFFM 适用的场景跟 FM 和 LR\n一样，适用于输入的特征是高维稀疏特征。\n开源实现\nFFM 最早的开源实现是台大提供的 libffm，去年开源的 xlearn\n中也提供了该算法的实现，提供的 api 比 libffm 更加友好。\n另外，由于 FM/FFM 可以看做是 LR\n加了特征交叉的增强版本，对输入的特征的特点要求一致，因此上面的 GBDT+LR\n也可以直接套到 GBDT+FM/FFM 上，值得一提的是，还是台大的学生，在 2014 由\nCriteo 举办的比赛上，通过 GBDT+FFM 的方案夺冠，其实现细节可参考 kaggle-2014-criteo。\n小结\n在非深度学习中，可以看到主流的几个模型基本都是基于 LR 进行的拓展或将\nLR 与其他模型结合。原因是 LR\n模型简单，具有良好的理论基础，可解释性强，能够获取各个特征的重要性，且能够直接输出概率值。但是应用\nLR 过程中无法避免且最为重要的一点就是人工特征工程，特征决定了上限，虽然\nFM/FMM 和 GBDT+LR\n在一定程度上起到了自动特征工程的作用，但是需要人工特征还是占主要部分。\n后面要讲的深度学习的方法在一定程度上能够缓解这个问题，因为深度学习能够通过模型自动学习出有效特征，因此，深度学习也被归类为表示学习 (\nRepresentation\nLearning) 的一种；但是，没有免费午餐的，特征工程的便利性带来的是特征的不可解释性，所以怎么选取还是要根据具体的需求和业务场景。\n","categories":["机器学习"],"tags":["计算广告","机器学习"]},{"title":"C++ 编译初步","url":"/2018/11/24/C++%20%E7%BC%96%E8%AF%91%E5%88%9D%E6%AD%A5/","content":"文章为转载，转载自 Compiling\nCpp，主要涉及到 C++ 在 linux 下通过 g++\n编译的一些基础知识，包括编译单个源文件、多个源文件、创建并使用静态库等。\n\n关于程序的编译和链接\n一般来说，无论是\nC、C++、还是 pas，首先要把源文件编译成中间代码文件，在 Windows 下也就是\n.obj 文件，UNIX 下是 .o 文件，即 Object\nFile，这个动作叫做编译（compile）。然后再把大量的 Object\nFile 合成执行文件，这个动作叫作链接（link）。\n编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在 C/C++ 文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件应该对应于一个中间目标文件。都如果函数未被声明，编译器会给出一个警告，但可以生成 Object\nFile。但在链接程序时，链接器会在所有的 Object\nFile 中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker\nError）\n链接时，主要是链接函数和全局变量，链接器并不管函数所在的源文件，只管函数的中间目标文件，在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在 Windows 下这种包叫 “库文件”（Library\nFile)，也就是 .lib 文件，在 UNIX 下，是 Archive File，也就是\n.a 文件。\n单个源文件生成可执行程序\n下面是一个保存在文件 helloworld.cpp 中一个简单的 C++ 程序的代码：\n/* helloworld.cpp */#include &lt;iostream&gt;int main(int argc,char *argv[]) {  std::cout &lt;&lt; \"hello, world\" &lt;&lt; std::endl;  return(0);}\n程序使用定义在头文件 iostream 中的\ncout，向标准输出写入一个简单的字符串。该代码可用以下命令编译为可执行文件：\n$ g++ helloworld.cpp\n编译器 g++ 通过检查命令行中指定的文件的后缀名可识别其为 C++\n源代码文件。编译器默认的动作：编译源代码文件生成对象文件 (object\nfile)，链接对象文件和 libstdc++ 库中的函数得到可执行程序，\n然后删除对象文件。\n由于命令行中未指定可执行程序的文件名，编译器采用默认的\na.out。程序可以这样来运行：\n$ ./a.outhello, world\n更普遍的做法是通过 -o\n选项指定可执行程序的文件名。下面的命令将产生名为 helloworld\n的可执行文件：\n$ g++ helloworld.cpp -o helloworld\n在命令行中输入程序名可使之运行：\n$ ./helloworldhello, world\n程序 g++ 是将 gcc 默认语言设为 C++\n的一个特殊的版本，链接时它自动使用 C++ 标准库而不用 C\n标准库。通过遵循源码的命名规范并指定对应库的名字，用 gcc\n来编译链接 C++ 程序是可行的，如下例所示：\n$ gcc helloworld.cpp -l stdc++ -o helloworld\n选项 -l (ell) 通过添加前缀 lib 和后缀 .a\n将跟随它的名字变换为库的名字\nlibstdc++.a。而后它在标准库路径中查找该库。gcc 的编译过程和输出文件与\ng++ 是完全相同的。\n在大多数系统中，GCC 安装时会安装一名为 c++ 的程序。如果被安装，它和\ng++ 是等同，如下例所示，用法也一致：\n$ c++ helloworld.cpp -o helloworld\n多个源文件生成可执行程序\n如果多于一个的源码文件在 g++\n命令中指定，它们都将被编译并被链接成一个单一的可执行文件。下面是一个名为\nspeak.h 的头文件；它包含一个仅含有一个函数的类的定义：\n/* speak.h */#include &lt;iostream&gt;class Speak { public:  void sayHello(const char *);};\n下面列出的是文件 speak.cpp 的内容：包含 sayHello () 函数的函数体：\n/* speak.cpp */#include \"speak.h\"void Speak::sayHello(const char *str) {  std::cout &lt;&lt; \"Hello \" &lt;&lt; str &lt;&lt; \"\\n\";}\n文件 hellospeak.cpp 内是一个使用 Speak 类的程序：\n/* hellospeak.cpp */#include \"speak.h\"int main(int argc,char *argv[]){    Speak speak;    speak.sayHello(\"world\");    return(0);}\n下面这条命令将上述两个源码文件编译链接成一个单一的可执行程序：\n$ g++ hellospeak.cpp speak.cpp -o hellospeak\nPS：这里说一下为什么在命令中没有提到 speak.h\n这个文件，原因是在 speak.cpp 中包含有\n#include\"speak.h\"\n这句代码，它的意思是 搜索系统头文件目录之前将先在当前目录中搜索文件\nspeak.h, 而 speak.h\n正在该目录中，不用再在命令中指定了。\n源文件生成对象文件\n选项 -c\n用来告诉编译器编译源代码但不要执行链接，输出结果为对象文件。文件默认名与源码文件名相同，只是将其后缀变为\n.o。例如，下面的命令将编译源码文件 hellospeak.cpp 并生成对象文件\nhellospeak.o：\n$ g++ -c hellospeak.cpp\n命令 g++ 也能识别 .o\n文件并将其作为输入文件传递给链接器。下列命令将编译源码文件为对象文件并将其链接成单一的可执行程序：\n$ g++ -c hellospeak.cpp $ g++ -c speak.cpp $ g++ hellospeak.o speak.o -o hellospeak\n选项 -o\n不仅仅能用来命名可执行文件。它也用来命名编译器输出的其他文件。例如：除了中间的对象文件有不同的名字外，下列命令生将生成和上面完全相同的可执行文件：\n$ g++ -c hellospeak.cpp -o hspk1.o $ g++ -c speak.cpp -o hspk2.o $ g++ hspk1.o hspk2.o -o hellospeak\n编译预处理\n选项 -E 使 g++\n将源代码用编译预处理器处理后不再执行其他动作。下面的命令预处理源码文件\nhelloworld.cpp 并将结果显示在标准输出中：\n$ g++ -E helloworld.cpp\n本文前面所列出的 helloworld.cpp\n的源代码，仅仅有六行，而且该程序除了显示一行文字外什么都不做，但是，预处理后的版本将超过\n1200 行。这主要是因为头文件 iostream\n被包含进来，而且它又包含了其他的头文件，除此之外，还有若干个处理输入和输出的类的定义。\n预处理过的文件的 GCC 后缀为 .ii，它可以通过\n-o 选项来生成，例如：\n$ gcc -E helloworld.cpp -o helloworld.ii\n生成汇编代码\n选项 -S\n指示编译器将程序编译成汇编语言，输出汇编语言代码而後结束。下面的命令将由\nC++ 源码文件生成汇编语言文件 helloworld.s：\n$ g++ -S helloworld.cpp\n生成的汇编语言依赖于编译器的目标平台。\n创建静态库\n静态库是编译器生成的一系列对象文件的集合。链接一个程序时用库中的对象文件还是目录中的对象文件都是一样的。库中的成员包括普通函数，类定义，类的对象实例等等。静态库的另一个名字叫归档文件 (archive)，管理这种归档文件的工具叫\nar 。\n在下面的例子中，我们先创建两个对象模块，然后用其生成静态库。\n头文件 say.h 包含函数 sayHello () 的原型和类 Say 的定义：\n/* say.h */#include &lt;iostream&gt;void sayhello(void);class Say { private:  char *string; public:  Say(char *str){    string = str;  }  void sayThis(const char *str){    std::cout &lt;&lt; str &lt;&lt; \" from a static library\\n\";  }  void sayString(void);};\n下面是文件 say.cpp\n是我们要加入到静态库中的两个对象文件之一的源码。它包含 Say 类中\nsayString () 函数的定义体；类 Say 的一个实例 librarysay\n的声明也包含在内：\n/* say.cpp */#include \"say.h\"void Say::sayString() {  std::cout &lt;&lt; string &lt;&lt; \"\\n\";}Say librarysay(\"Library instance of Say\");\n源码文件 sayhello.cpp\n是我们要加入到静态库中的第二个对象文件的源码。它包含函数 sayhello ()\n的定义：\n/* sayhello.cpp */#include \"say.h\"void sayhello(){  std::cout &lt;&lt; \"hello from a static library\\n\";}\n下面的命令序列将源码文件编译成对象文件，命令 ar\n将其存进库中\n$ g++ -c sayhello.cpp$ g++ -c say.cpp$ ar -r libsay.a sayhello.o say.o\n程序 ar 配合参数 -r 创建一个新库 libsay.a\n并将命令行中列出的对象文件插入。采用这种方法，如果库不存在的话，参数 -r\n将创建一个新的库，而如果库存在的话，将用新的模块替换原来的模块。\n下面是主程序 saymain.cpp，它调用库 libsay.a 中的代码：\n/* saymain.cpp */#include \"say.h\"int main(int argc, char *argv[]){  extern Say librarysay; // 使用库的对象  Say localsay = Say(\"Local instance of Say\"); //使用库的类定义  sayhello(); // 使用库的普通该函数  librarysay.sayThis(\"howdy\");  librarysay.sayString();  localsay.sayString();  return(0);}\n该程序可以下面的命令来编译和链接：\n$ g++ saymain.cpp libsay.a -o saymain\n程序运行时，产生以下输出：\nhello from a static libraryhowdy from a static libraryLibrary instance of SayLocal instance of Say\n小结\n上面介绍了手动通过命令编译 C++\n源文件，但是面对一些大工程，源码文件数量多且依赖关系复杂时，手动编译不太现实，这时候就要依赖\nmake 和 Makefile 对程序进行自动编译了，简单来说就是把编译规则写好在\nMakefile 里，然后通过 make 进行自动编译，具体细节可参考这个教程 跟我一起写 Makefile。\n","categories":["C++"],"tags":["C++"]},{"title":"CentOS 下安装 python 的 sklearn 模块","url":"/2015/10/23/CentOS%E4%B8%8B%E5%AE%89%E8%A3%85python%E7%9A%84sklearn%E6%A8%A1%E5%9D%97/","content":"sklearn 是 python 的一个机器学习库，封装了很多常用的机器学习算法，在数据挖掘中经常会用到这个库。在 MacOS 上安装只需要一条命令，可以说是最简单的。在 windows 下也可以直接安装封装好这些模块的程序如 winpython 等。但是我在 CentOS 下的安装可谓痛苦，用 pip 安装会有各种依赖，用 yum 解决依赖又会因为支持 sklearn 的库（如 numpy，scipy）版本不够新而导致 sklearn 无法安装。\n\n最后还是在官方的安装指南中找到了解决方法，所以官方文档真的是非常有参考价值，而且权威性也是很好的。下面说说具体的解决方法。\n解决方法实际上是安装一个类似于 Windows 下 winpython 的程序，在 Linux 中就是 Anaconda, 可以认为 Anaconda 是封装了 python 和 sklearn 等第三方库的一个程序。下面讲一下安装步骤以及注意事项。\n安装非常简单：\n1. 下载安装文件\n安装可以选择 32 位或者 64 位以及 python 的版本（提供 2.7 和 3.5），下面下载的是 64 位的 python\n2.7\n版本的 andconda，若要下载其他版本的请移步到 https://www.continuum.io/downloads\nwget https://3230d63b5fc54e62148e-c95ac804525aac4b6dba79b00b39d1d3.ssl.cf1.rackcdn.com/Anaconda2-2.5.0-Linux-x86_64.sh  \n2. 运行安装文件\nbash Anaconda2-2.5.0-Linux-x86_64.sh  \n在安装过程中会询问安装的目录，默认是当前目录下创建 anaconda2 目录，也可以自己在询问时输入指定目录。安装到最后还会问是否要在～/.bashrc 中添加环境变量，添加环境变量的作用是为了方便某些命令的输入如 python、conda 等。连配置环境变量的功夫都省去了。\n3. 安装第三方模块\n因为 Anaconda 采用了 conda作为包管理程序，所以更新或卸载已安装的模块、安装其他模块都可以用 conda 命令。\n如 conda update scikit-learn 可以更新已经安装了的 sklearn，conda remove scikit-learn 可以卸载已安装的 sklearn。conda install 可以安装新的第三方模块。简直就是一个加强版的 python\nshell。\n到这里安装就结束了，比起之前折腾的一个一个包来安装的要方便得多。\n","categories":["Linux"],"tags":["python","Linux"]},{"title":"CTR 预估模型简介 -- 深度学习篇","url":"/2018/07/16/CTR%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AF%87/","content":"本文主要介绍 CTR 预估中一些深度学习模型，包括\nFNN、Wide&amp;Deep、PNN、DIN、\nDeep&amp;Cross 等。每个模型会简单介绍其原理、论文出处以及其一些开源实现。\n\nFNN(Factorization-machine\nsupported Neural Network)\n模型结构\nFNN 是伦敦大学于 2016 在一篇论文中发表的，模型的结构如下\n\n\nFNN\n\nFNN 假设输入数据的格式是离散的类别特征 (表示为 one-hot\n编码)，且每个特征属于一个 field，通过 embedding\n层将高纬稀疏特征映射成低维稠密特征后，再作为多层感知机 (MLP) 的输入。\n一般来说，embedding 层的参数可以随机初始化，但是在 FNN 中，初始化\nembedding 是采用通过 FM\n预训练得到的每个特征的隐向量，这样初始化的好处是将预训练的向量作为初始化参数时，能够让模型的参数在初始化的时候就处于较优的位置 (训练的目的其实就是为了得到最优的模型参数)，能够加快收敛的过程，至于效果方面，则不一定会优于随机初始化的情况，因为随机初始化经过多轮的迭代也可能会收敛同样的效果。\n相关论文\n提出 FNN 的论文 Deep\nLearning over Multi-field Categorical Data: A Case Study on User\nResponse Prediction是张伟楠博士在伦敦大学时发表的，张伟楠博士还有很多与\nRTB 相关的论文，具体可参看其主页。\n开源实现\n论文作者在 github 上的 deep-ctr 这个仓库中提供了\nFNN 的代码，但是是 Theano 实现的；后来作者又将代码更新为 Tensorflow\n框架实现的，详见 product-nets，这个仓库也包含了后面要介绍的\nPNN 的实现代码。\nWide&amp;Deep\n模型结构\nWide &amp; Deep 是 Google\n在 2016 年 6 月中发布的。模型结合了传统的特征工程与深度模型：既有 Wide 的 LR\n模型，也有 Deep 的 NN 模型。\n其结构如下所示\n\n\nmodel structure\n\nwide 部分其实就是 LR，deep 部分其实就是 FNN，只是 deep 部分中的\nembedding 层不用 FM 训练得到的隐向量初始化。根据论文的描述，wide\n部分主要负责 memorization， deep 部分主要负责\ngeneralization；memorization\n主要指的是记住出现过的样本，可以理解为拟合训练数据的能力，generalization\n则是泛化能力。\n根据论文的实验，wide &amp; deep 比起单纯的 wide 或 deep\n都要好，但是根据我后面的实验以及网上的一些文章，wide\n部分仍然需要人工设计特征，在特征设计不够好的情况下，wide&amp;deep\n整个模型的效果并不如单个的 deep 模型。\nWide&amp;Deep 中还允许输入连续的特征，这点与 FNN\n不同，连续特征可以直接作为 Wide 部分或 Deep 部分的输入而无需 embedding\n的映射，具体如下图所示。\n\n\ninput feature\n\n相关论文\nWide&amp;Deep 是 Google 在论文 Wide &amp; Deep Learning for\nRecommender Systems 中提出的，论文原来是用于 Google Play\n的推荐中，但是推荐和 CTR 实际上是同一类的问题：排序问题，所以也可以迁移到 CTR 预估的领域。\n开源实现\n由于 Wide&amp;Deep 是 google 提出的，因此在自家的框架 Tensorflow\n中提供了 Wide&amp;Deep API，具体的使用方法可参考官方的文档 TensorFlow\nWide &amp; Deep Learning Tutorial。\nPNN(Product-based Neural\nNetworks)\n模型结构\nPNN 是上海交大在 2016 年发表的，FNN 是在 PNN\n的基础上进行了改进，就是增加了特征的二阶交叉项。因此，FNN 和 PNN\n的关系，类似于 LR 和 FM 的关系，只是 FNN 和 PNN 均是对原始特征进行了\nembedding 映射。PNN 模型的结构如下所示\n\n\nPNN\n\n特征经过 embedding\n层映射后，有两种乘积的操作，第一种是跟 1 做外积，实际上就是将映射后的特征进行拼接，\n得到了上图中的 z\n向量部分；第二种是与其他特征分别两两进行内积，得到了上图中的 p\n向量部分，这个操作其实就相当于进行了特征交叉，只是这种交叉是在 embedding\n映射后。再后面的结构其实又是一个多层感知机了。\n相关论文\nPNN 是在上海交大于 2016 年在这篇论文 Product-based Neural Networks\nfor User Response Prediction 中提出。\n开源实现\nPNN 的作者在 github 上的 product-nets\n上开源了其代码，通过 Tensorflow 实现，代码里面也包含了 FNN，DeepFM\n等一些其他模型的实现。\nDeepFM\n模型结构\nDeepFM 是华为诺亚方舟实验室在 2017 提出的用于 CTR 预估的模型，DeepFM\n其实就是模仿 Wide&amp;Deep，只是将 Wide 部分替换成了\nFM，所以创新性并不算大。其结构如下所示，\n\n\nDeepFM\n\n相关论文\nDeepFM 是在这篇论文中提出的 DeepFM: A Factorization-Machine\nbased Neural Network for CTR Prediction\n开源实现\n作者没有公开源码，上面提到的 product-nets\n提供了这个模型的实现代码，同时 tensorflow-DeepFM\n也提供了一个 tensorflow 实现的版本，star 数是 github 上较高的了。\nDIN(Deep Interest Network)\n模型结构\n从之前提到的几个模型可知，CTR 预估中的深度学习模型的基本思路是将原始的高维稀疏特征映射到一个低维空间中，也即对原始特征做了 embedding 操作，之后一起通过一个全连接网络学习到特征间的交互信息和最终与 CTR 之间的非线性关系。这里值得注意的一点是，在对用户历史行为数据进行处理时，每个用户的历史点击个数是不相等的，我们需要把它们编码成一个固定长的向量。以往的做法是，对每次历史点击做相同的 embedding 操作之后，将它们做一个求和或者求最大值的操作，类似经过了一个 pooling 层操作。提出\nDIN 的论文认为这个操作损失了大量的信息，于是引入了 attention\n机制 (其实就是一种加权求和)。\nDIN 是阿里妈妈在 2017 年提出的，其模型的结构如下所示\n\n\nDeep Interest Network\n\nActivation Unit 的结构如下所示\n\n\nActivation Unit\n\nDIN 模型在对用户的表示计算上引入了 attention network\n(也即图中的 Activation Unit)\n。DIN 把用户特征、用户历史行为特征进行 embedding 操作，视为对用户兴趣的表示，之后通过 attention\nnetwork，对每个兴趣表示赋予不同的权值。这个权值是由用户的兴趣和待估算的广告进行匹配计算得到的，如此模型结构符合了之前的两个观察 —— 用户兴趣的多样性以及部分对应。attention\nnetwork 的计算公式如下， \\(V_u\\)\n代表用户表示向量， \\(V_i\\)\n代表用户兴趣表示向量， \\(V_a\\)\n代表广告表示向量，\\(w_i\\)\n表示各个用户兴趣表示向量的权重，\\(g\\)\n是 Activation Unit 的逻辑，论文中提出了一种如上图的 Activation Unit\n所示，当然也可自行设计新的 Activation 方法。\n\n\nDIN\n\n相关论文\nDIN 是在论文 Deep Interest\nNetwork for Click-Through Rate Prediction 中提出的。\n开源实现\n论文作者在 github 上的仓库 DeepInterestNetwork\n开源了其代码，通过 Tensorflow 实现。\nDeep&amp;Cross\n模型结构\nPNN\n进行了特征的二阶交叉，目前是为了获得信息量更多的特征，除了二阶，三阶四阶甚至更高阶的特征会更加有区分度；Deep&amp;Cross\n就是一个能够进行任意高阶交叉的神经网络。\nDeep&amp;Cross 是 StandFord 和 Google 与 2017 年 提出的，类似于\nWide&amp;Deep，模型也是由两部分组成，分别是 Deep network 和 Cross\nnetwork，该模型结构如下所示\n\n\nDeep &amp; Cross\n\n\\(x_i\\) 表示可由如下公式确定\n\n\nx0\n\n\n\nxl\n\n从上面两条公式可知，Cross network 中的第 \\(l+1\\) 层的神经元由最原始的输入和第 \\(l\\) 层的神经元共同决定，因此第 \\(l\\) 层相当于对原始特征进行了 \\(l\\) 阶交叉。\n相关论文\nDeep&amp;Cross 是在这篇论文 Deep &amp; Cross Network for Ad\nClick Predictions 中提出的。\n开源实现\n论文没有公开代码，DeepCTR 中提供了\nDeep&amp;Cross 的 tensorflow 实现，可供参考。\n总结\n在 CTR 预估中，模型适用传统方法还是深度学习方法，其实是一个海量离散特征 + 简单模型\n和 少量连续特征 + 复杂模型\n的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。特征与模型往往是对偶的，前者容易，而且可以 n 个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。\n","categories":["机器学习"],"tags":["计算广告","机器学习"]},{"title":"Centos 下安装 python2.7","url":"/2015/03/23/Centos%E4%B8%8B%E5%AE%89%E8%A3%85python2-7/","content":"Centos 是一个 Linux\n发行版，因为稳定性得到了比较广泛的应用，但是存在着软件版本不够新的问题。比如说 python 版本为 2.6，但是 python\n2.7 对第三方模块的支持往往更好，下面就说一下怎么在 Centos 下安装 python2.7。\n\n简介\n需要注意的是，系统内部的一些命令依赖 python 环境运行（比如说 yum），所以假如卸载系统自带的 python 环境会导致这些程序不能运行，所以建议不要动原来系统自带的 python\n而在另外一个路径安装 python\n2.7，调用 python 命令时调用这个安装路径的 python 路径即可。\n安装利用了 miniconda, 里面集成了 python\n和 conda，conda\n可以简单认为是一个包管理系统，允许在同一台机器上安装同一软件的多个版本。\n安装步骤\n下载安装脚本\nwget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh  \n运行安装脚本\nsh Miniconda-latest-Linux-x86_64.sh -b -p /usr/local/miniconda  ```  `-p` 参数会指定安装的目录,可以根据自己的修改### 修改环境变量因为系统自带的 python 2.6 命令路径为 `/usr/bin/python`,而`/usr/bin`本来就存在系统的环境变量PATH中，所以**假如输入python的时候希望进入python 2.7，那么python2.7 的环境变量就必须添加在PATH前，因为系统是从前往后读PATH变量的。**修改`~/.bashrc` 或 `/etc/profile`（前者针对的是当前用户，后者针对的是全部用户，可参考[这篇文章][3]） , 在文件末尾添加环境变量如下:```sh  export PATH=/usr/local/miniconda:$PATH  \n/usr/local/miniconda 是你安装 python 的目录，一定要添加在 $PATH 前，否则输入 python 还是会跑回原来 2.6 版本的 python。\n然后输入 source ~/.bashrc 或 source /etc/profile 让配置生效。\n这时输入 python 应该就能看到 python 2.7 了。\n安装第三方模块\n因为上面安装的 miniconda 中除了 python 还安装了 conda，而 conda 提供了包管理机制，所以可以通过 conda 安装第三方模块。\n如 conda install numpy 就安装了 numpy 模块。更详细内容参考 conda 官方文档。\n","categories":["Linux"],"tags":["python","Linux"]},{"title":"Code Complete 阅读笔记 - 创建高质量的代码 (2)","url":"/2018/10/31/Code%20Complete%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E5%88%9B%E5%BB%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E4%BB%A3%E7%A0%81(2)/","content":"本文主要是 Code Complete 中创建高质量的代码部分的的两章笔记：第 8\n章（防范式编程）、第 9\n章（伪代码编码过程），介绍了如何进行防范式编程（defensive\nprogramming），即保护程序免遭非法输入数据的破坏，目的其实就是增强程序的鲁棒性；同时介绍了如何通过伪代码编码方法来创建类和子程序。\n\n防御式编程（defensive\nprogramming）\n这里的防御式编程的主要思想是：子程序不应该因为传入错误的数据而被破坏，哪怕是由其他子程序产生的错误数据。下面主要就是讲述一些方法来处理这类问题\n断言（assertion）\nassert 关键字在多门语言中均有出现，如 Python， Java，c++\n等；其目的就是非常肯定某个条件表达式是成立的，否则就是出错了，如确保分母不为\n0 等。而应用在防范式编程中，可以用来检查如下条件\n\n输入参数和输出参数的取值处于预期的范围内\n子程序开始（或结束）执行时，文件或流是打开（或关闭）的状态\n子程序开始（或结束）执行时，文件或流的读写位置处于开头（或结尾）的状态\n子程序开始（或结束）执行时，某个容器是空的（满的）\n文件或流已用只读、只写或可读可写的方式打开\n仅用于输入的变量的值没有被子程序修改\n指针非空\n传入子程序的数组或其他容器的 size 能容纳设定的数据元素个数\n........\n\n需要注意的是，断言只是在开发阶段被编译到目标代码中，而在生成产品代码是并不编译进去，以降低系统的性能。\n关于使用断言，有如下的建议\n\n用错误处理代码来处理预期会发生的情况，而用断言来处理绝对不应该发生的状况\n避免把执行代码放到断言中，因为这样会导致关闭断言时，编译器很可能就把这些代码排除在外，正确的做法是先将执行代码的结果在断言外用变量存起来，如下所示\n// badassert PerformAction() == True//goodresult = PerformAction() assert result == True\n用断言来注解并验证前条件（precondition）和后条件（postcondition）。简单来说，前条件就是在执行函数前需要为函数准备好的条件，后条件则是在函数执行后要完成的任务。\n\n错误处理技术\n上面提到用断言来处理绝对不应该发生的状况，而用错误处理代码来处理预期会发生的情况，如网络阻塞等。那么该怎么处理那些可能发生的错误呢？本章给出了如下可行的方法\n\n返回中立值。如数值计算结果返回 0，字符串可以返回空字符，指针操作可以返回一个空指针等。\n返回下一个正确的数据。如在处理数据流的时候，如果发现某一条记录已经损坏，可以继续读下去知道又找到一条正确记录为止，比如说以每秒\n100 次的速度读取体温计的数据，那么如果某一次得到的数据有误，只需再等上\n1/100 秒然后继续读取即可。\n返回与前一次相同的数据。同样是上面的体温计的例子，如果在某次读取中没有获得数据，可以简单地返回前一次的读取结果，这是根据实际的应用情况决定的，在某些变化较大的场景下不能这么使用。\n使用最接近的合法值。比如说汽车的速度盘，倒车时无法显示负值的速度，因此简单地显示 0，即最接近的合法值。\n把警告信息记录到日志文件中，然后继续执行。\n返回一个错误码。即只让系统的某些部分处理错误，其他部分不在本地处理错区，而是简单地报告说有错误发生。\n调用错误处理子程序或对象。把错误处理都集中在一个全局的错误处理子程序或对象中。\n....\n\n异常\n异常是把代码中的错误或异常事件传递给调用代码的一种特殊手段。异常的基本结构是：子程序使用\nthrow 跑出一个异常对象，再被调用链上层其他子程序的\ntry-catch 语句捕获。\n使用异常时有以下建议\n\n只有在真正例外的情况下才抛出异常。也就是说假如子程序局部能够处理这个错误就不要抛出异常；因为异常虽然能够增加程序的鲁棒性，但是会使程序的复杂性增加。调用子程序的代码需要了解呗调用的代码中可能会抛出的异常，因此异常弱化了封装性。\n避免在构造函数和析构函数中抛出异常。比如在 C++ 里只有当对象完全构造后才可能调用析构函数，也就是说，如果在构造函数的代码里抛出异常，就不会调用析构函数，从而造成潜在的资源泄露问题。\n在合适的抽象层次抛出异常。即抛出的异常应该与子程序接口的抽象层次一致的。如下所示，第一个例子中\nGetTaxId() 将更底层的 EOFException\n返回给调用方，\n这样破坏了封装性。与之相反的是第二个例子，GetTaxId()\n里的异常处理代码可能只要把一个 io_disk_not_ready 异常映射为\nEmployeeDataNotAvailable 异常就好了。\n\n// 抛出抽象层次不一致的异常的类class Employee {    ....    public TaxId GetTacId() throws EOFException {        ....    }}// 一个在一致的抽象层次上抛出的异常的类class Employee {    ....    public TaxId GetTacId() throws EmployeeDataNotAvailable {        ....    }}\n\n把项目中对异常的使用标准化。为了保持异常处理尽可能便于管理，可以用以下几种途径把对异常的使用标准化\n\n某些语言允许抛出的类型多种多样，如 C++ 就可以抛出对象、数据以及指针，\n因此应该为可以抛出哪些种类的异常建立一个标准；可以考虑只抛出从\nstd::exception 基类派生出来的对象\n考虑创建项目的特定异常类，用作项目所有可能抛出的异常的基类\n规定在何种场合先允许代码使用 throw-catch\n语句在局部对错误进行处理\n规定在何种场合允许代码抛出不在局部进行处理的异常\n\n\n隔离程序\n隔栏（barricade）是一种容损策略，与防火墙类似，当火灾发生时，防火墙能阻止火势从建筑物的一个部位向其他部位蔓延。而以防御式编程为目的而进行隔离的一种方法，就是把某些接口选定为\n“安全”\n区域的边界。对穿越安全区域边界的数据进行合法性校验，并当数据非法时做出对策，如下图所示\n\n\n隔离程序\n\n同样的可以在类的层次中使用这种方法，类的公用方法可以假设数据是不安全了，需要负责检查数据并进行清理。一旦类的公用\n方法接受了数据，那么类的私有方法就可以假定数据都是安全的了。\n隔栏的使用使断言和错误处理有了清晰的区分，隔栏外部的程序应使用错误处理技术，在哪里对数据做任何假定都是不安全的；而在隔栏的内部的程序就应该使用断言技术，因为传进来的数据应该已在通过隔栏时被清理过了。\n小结\n防御式编程能够让错误更容易发现和修改，并减少错误对产品代码的破坏，增加程序的鲁棒性，但是过度的使用也会引起问题。如果在每一个能够想到的提防用一种能想到的方法检查从参数传入的数据，那么程序将会变得臃肿而缓慢，而且引入了额外的代码增加了软件的复杂度。因此需要考虑好在那些重要的地方进行防御，然后因地制宜地调整进行防御式编程的优先级。\n伪代码编码过程\n这一章主要关注创建类及其子程序的一种方式：伪代码编码。伪代码编程过程是一种通过书写伪代码而更加高效的创建程序代码的专门方法。\n伪代码\n关于使用伪代码有以下指导原则\n\n用类似英语的语言来精确描述特定操作\n避免使用目标编程语言中的语法元素，而应该在一个比代码本身略高的层次上进行设计\n在意图层面上编写伪代码，即用伪代码去描述解决问题的方法的意图，而不是写如何在目标语言中实现这个方法\n\n如下是一段违背了上面的指导原则的伪代码，这段代码的意图不明确，而且包含了\nC 语言的具体语法以及编码细节（返回 1 表示 null）\nincrement resource number by 1allocate a dlg struct using mallocif malloc() returns NULL then return 1invoke OSrsrc_init to initialize a resource for the operating system*hRsrcPtr = resource numberreturn 0\n下面是针对同样功能所写的伪代码，比起上面的就要好很多了\nKeep track of current number of resources in use    If another resource is available        Allocate a dialog box structure        If a dialog box structure could be allocated            Note that one more resource is in use            Initialize the resource            Store the resource number at the location provided by the caller        Endif    EndifReturn true if a new resource was created; else return false\n使用这种风格的伪代码能够带来以下好处\n\n伪代码使得评审更加容易。无须检查源代码就可以评审细节设计\n伪代码支持反复迭代精化的思想。从高层设计开始，将其精化为伪代码，然后再把伪代码精化为源代码。这样持续不断的小步精化，可以在推向更低的细节层次的同时，不断检查已形成的设计；及时修复各个层次的错误\n伪代码使变更更加容易。这跟在产品最具可塑性的阶段进行变动的原则是相同的\n伪代码比其他形式的设计文档更加容易维护。使用其他方法时，设计和代码是分离的，当其中之一变动时，两者就不再一致，而使用伪代码编程时，伪代码中的语句将会变为代码中的注释。\n\n通过伪代码创建子程序\n通过伪代码创建子程序主要包括以下步骤\n\n设计子程序\n编写子程序的代码\n检查代码\n收尾工作\n按照需要重复上述步骤\n\n设计子程序\n设计子程序可以从以下角度出发\n（1）检查先决条件。即检查子程序与整体设计是否匹配，是否是真正必需的，至少是间接需要的\n（2）定义子程序要解决的问题。应该详细说明如下问题\n\n子程序将要隐藏的信息\n传给这个子程序的各项输入\n从该子程序得到的输出\n调用程序前确保有关的前条件成立\n在子程序将控制权交回给调用方之前，确保其后条件的成立\n\n（3）为子程序命名，这一部分在上一篇笔记有提及\n（4）决定如何测试子程序\n（5）在标准库中搜寻可用的功能。即如果在标准库中已经有该子程序特定的功能的实现，可以直接使用而不重复造轮子\n（6）研究算法和数据类型\n（7）编写伪代码。首先为子程序编写一般性注释，然后为子程序编写高层次的伪代码。如下所示\n// 一般性注释This routine outputs an error message based on an error codesupplied by the calling routine. The way it outputs the messagedepends on the current processing state, which it retrieveson its own. It returns a value indicating success or failure.// 伪代码set the default status to \"fail\"look up the message based on the error codeif the error code is validif doing interactive processing, display the error messageinteractively and declare successif doing command line processing, log the error message to thecommand line and declare successif the error code isn't valid, notify the user that an internal errorhas been detectedreturn status information\n编写子程序代码\n主要过程就是在伪代码的每一句话下填入代码。如下所示\n/* This routine outputs an error message based on an error codesupplied by the calling routine. The way it outputs the messagedepends on the current processing state, which it retrieveson its own. It returns a value indicating success or failure.*/Status ReportErrorMessage(    ErrorCode errorToReport    ) {    // set the default status to \"fail\"    Status errorMessageStatus = Status_Failure;    // look up the message based on the error code    Message errorMessage = LookupErrorMessage( errorToReport );    // if the error code is valid    if ( errorMessage.ValidCode() ) {        // determine the processing method        ProcessingMethod errorProcessingMethod = CurrentProcessingMethod();        // if doing interactive processing, display the error message        // interactively and declare success        if ( errorProcessingMethod == ProcessingMethod_Interactive ) {            DisplayInteractiveMessage( errorMessage.Text() );            errorMessageStatus = Status_Success;        }        // if doing command line processing, log the error message to the        // command line and declare success        else if ( errorProcessingMethod == ProcessingMethod_CommandLine ) {            CommandLine messageLog;            if ( messageLog.Status() == CommandLineStatus_Ok ) {                messageLog.AddToMessageQueue( errorMessage.Text() );                messageLog.FlushMessageQueue();                errorMessageStatus = Status_Success;            }        else {            // can't do anything because the routine is already error processing        }        else {            // can't do anything because the routine is already error processing        }    }    // if the error code isn't valid, notify the user that an    // internal error has been detected    else {        DisplayInteractiveMessage(            \"Internal Error: Invalid error code in ReportErrorMessage()\"        );    }    // return status information    return errorMessageStatus;}\n检查代码\n主要包含以下几个步骤\n1. 人肉运行代码\n2. 编译子程序。把编译器的告警级别调到最高；消除产生错误消息和警告的所有根源\n3. 在调试器中逐行执行代码 4. 测试代码，编写测试用例来测试代码\n5. 消除程序中的错误\n收尾工作\n收尾工作就是重新审视整个子程序代码来确保子程序的质量合乎标准\n\n检查子程序的接口。确保所有的输入、输出数据都参与了计算，且所有的参数也都用到了\n检查整体的设计质量。确认子程序只干一件事；子程序之间的耦合是松散的；子程序采用了防御式编程；\n检查子程序中的变量。检查是否存在不准确的变量名称、未被用到的对象、未经声明的变量、未经初始化的对象等\n检查子程序的布局。确保正确地使用了空白来明确子程序、表达式及参数列表的逻辑结构\n检查子程序的文档，确认有伪代码转化而来的注释仍是准确无误的。\n出去冗余的注释\n。。。\n\n小结\n伪代码编码是创建类和子程序的一个有效途径，在编写时需要使用易懂的英语，避免使用特定编程语言中才有的特性，同时要在意图层面上写伪代码，即描述该做什么，而不是怎么去做。\n","categories":["编程"],"tags":["编程"]},{"title":"Code Complete 阅读笔记 - 创建高质量的代码 (1)","url":"/2018/10/18/Code%20Complete%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E5%88%9B%E5%BB%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E4%BB%A3%E7%A0%81(1)/","content":"最近在看 Code\nComplete（中文译作代码大全），一本关于代码构建的书。虽然研究生阶段做的东西与算法结合比较紧密，找工作的岗位也叫算法工程师，但是始终觉得算法工程师首先也得是个工程师，而不应该仅仅是调参师，因此一些基本的工程能力还是不可或缺的。本文主要是创建高质量的代码部分的的两章笔记：第\n6 章（可以工作的类）、第 7\n章（高质量的子程序），主要给出了在构建类和子程序过程中的一些建议。\n\n可以工作的类（Working class）\n良好的类接口\n创建高质量的类，第一步也可能是最重要的一步就是创建一个良好的接口，而这又涉及到两部分：抽象和封装\n对于抽象，有以下建议\n\n类的接口应该展示一致的抽象层次 ,\n也就是说如果某个类实现了不止一个 ADT (abstract data\ntype)，那么就应该把这个类重新组织为一个或多个定义更加明确的\nADT，如下所示的 cpp 代码中混合了不同层次抽象的类接口 \n\nclass EmployeeCensus: public ListContainer { public:  ...  // The abstraction of these routines is at the “employee” level.  void AddEmployee( Employee employee );  void RemoveEmployee( Employee employee );  //The abstraction of these routines is at the “list” level.  Employee NextItemInList();  Employee FirstItem();  Employee LastItem();  ... private:  ...};\n这个类展现了两个 ADT： Employee 和\nListContainer，原因是使用容器类或其他类库来实现内部逻辑，但是却没有把使用容器类或其他类库这一事实隐藏起来，如下是修改过有着一直抽象层次的类接口\nclass EmployeeCensus { public:  ...  // The abstraction of all these routines is now at the “employee” level.  void AddEmployee( Employee employee );  void RemoveEmployee( Employee employee );  Employee NextEmployee();  Employee FirstEmployee();  Employee LastEmployee();  ... private:  // That the class uses the ListContainer library is now hidden.  ListContainer m_EmployeeList;  ...};\n没有采取 EmployeeCensus 继承 ListContainer 的方式，是因为\nEmployeeCensus 与 ListContainer 不是 “is a” 的关系 ,\n因为 EmployeeCensus 中可能还会有排序、统计等 ListContainer\n不支持的操作。\n\n提供成对的服务。大多数操作都有与其相对应、相等以及相反的操作，如上面的第一个和最后一个，添加和删除等。所以在设计一个类的时候，要检查每一个公共子程序，决定是否需要另一个与其互补的操作，但是也不要盲目地创建\n把不相关的信息转移到其他类。如果某个类中一半子程序使用着该类的一半数据，而另一半子程序则使用另一半的数据，这时其实已经把两个类混在一起使用了，需要拆开。\n\n抽象通过提供一个可以让你忽略实现细节的模型来管理复杂度，而封装则强制阻止你看到细节。两个概念比较相似，关于封装，有以下建议\n\n尽可能限制类和成员的可访问性。当犹豫着子程序的可访问性应设为\npublic、protected、private\n中哪一种时，经验之举是采用最严格且可行的访问级别。\n不要公开暴露成员数据。即不要直接访问成员数据，而是通过\nGet 、 Set 子程序进行访问和修改。\n留意过于紧密的耦合关系。耦合指的是两个类之间关联的紧密程度，这种关联通常是约松越好，因此可以有以下一些指导建议\n\n\n尽可能限制类和成员的可访问性\n避免友元类 / 友元函数\n避免在公开接口中暴露成员数据\n\n设计和实现问题\n包含 (has a) 与继承 (is a)\n从英文上便可区分两者，包含指的是将某个对象作为类的成员，而继承则是在原来的对象之上进行拓展。\n关于包含，需要警惕有超过约 7\n个数据成员的类，因为某些研究表明人们在做事情时能记住的离散项目的个数是\n7 ± 2，如果一个类包含超过约 7\n个数据成员，可考虑将其分解为几个更小的类。\n关于继承，其目的是通过定义能为两个或更多的派生类提供共有元素的基类从而写出更精炼的代码，在使用继承时，需要考虑\n（1）成员函数是否应对派生类可见？是否应该有默认实现？默认实现能够被覆盖？\n（2）继承需要遵循 Liskov 替换原则 (Liskov Substitution Principle，LSP),\n简单来说就是对于基类中定义的所有子程序，用在它的任何一个派生类中时的含义都应该是相同的，而不应该存在着不同派生类在使用同一个基类方法时需要区分其返回的值的单位等细节。\n（3) 派生类中的成员函数不要与基类中不可覆盖的成员函数重名\n（4）使用多态来避免类型检查，对于下面的代码，应该用基类的 shape.Draw ()\n的方法来替代 shape.DrawCircle() 和\nshape.DrawSquare()，从而避免这些类型检查。 switch ( shape.type ) {  case Shape_Circle:    shape.DrawCircle();    break;  case Shape_Square:    shape.DrawSquare();    break;    ...}\n（5）避免让继承体系过深，建议继承的层次在 2-3 层，派生类的个数在 7±2\n（6）让所有数据都是\nprivate，如果派生类需要访问基类的属性，应该提供 protected 的\naccessor function。\n那么，何时使用包含，何时使用继承？\n\n如果多个类共享数据而非行为，应该创建这些类可以包含的共用对象\n如果多个类共享行为而非数据，应该让它们从共同的基类继承而来，并在基类里面定义公用的子程序\n如果过多个类既共享数据也共享行为，应该让它们从一个共同过的基类继承，并在基类里面定义共用的数据和子程序\n\n成员函数、数据成员、构造函数\n关于成员函数和数据成员有以下建议\n（1) 让类中的子程序的数量尽可能少 （2)\n减少类调用的不同子程序的数量（也叫扇入 /fan\nin，因为类用到其他类的数量越高，其出错率也越高 （3)\n对其他类的子程序的间接调用要尽可能少，比图说 A 对象中创建了 B\n对象，应该避免 A 对象直接调用 B 对象中的方法，即\nA.B().b_action() （4)\n减少类和类之间的互相合作的范围，应尽量让下面这些数字尽可能小，包括实例化的对象的种类、实例化对象上直接调用的不同的子程序的数量、调用由其他对象返回的对象的子程序的数量。\n关于构造函数有以下建议\n（1）应该尽可能在构造函数中初始化所有的数据成员 （2）用\nprivate\n构造函数来强制实现单例模式。单例模式指的是一个类只有一个对象，实现的具体方法是把类的所有构造函数都隐藏起来，然后对外提供一个 static\n的 GetInstance() 子程序，如下所示（Java 示例）：\n//Java Example of Enforcing a Singleton with a Private Constructorpublic class MaxId {  // Private Constructor  private MaxId() {    ...  }  ...  public static MaxId GetInstance() {    return m_instance;  }  ...  // Here is the single instance.   private static final MaxId m_instance = new MaxId();...}\n（3）优先使用 deep copies，除非论证可行，才采用 shallow copies。因为\ndeep copies 在开发和维护方面都比 shallow copies 简单，shallow copies\n需要增加很多代码用于引用计数、确保安全地复制对象、安全地比较对象以及安全地删除对象等。而这些代码是很容易出错的，除非有充分的理由，否则就应该避免它们。\n高质量的子程序（High-Quality\nRoutines）\n创建子程序的必要性不言而喻：能够重用代码、提高可移植性、良好子程序命名甚至能够达到自我注解的作用等等。\n内聚性\n对子程序而言，内聚性指的是子程序中各种操作之间联系的紧密程度。像\nCosine ()\n（余弦函数）这样的函数就是内聚性很强的，因为整个程序只完成了一项功能；而 CosinAndTan ()\n(余弦与正切)\n这个函数的内聚性就比较弱，因为它完成了多余一项的操作。我们的目标是让每一个子程序只把一件事做好，不再做其他事情，这也是功能上的内聚性，虽然也有一些其他的内聚性，但是功能上的内聚性是最佳的一种内聚性。\n命名\n好的子程序命名非常重要，命名应该遵循以下原则\n（1）描述子程序所做的事情，一般采用动宾结构；除了面向对象语言中的类可以忽略宾语，因为对象本身已经包含在调用语句总了，如\ndocument.Print(), orderInfo.Check() 等。\n（2）避免使用无意义、模糊或表达不清的动词。有些动词的含义非常灵活，可以延伸到涵盖几乎任何含义。像 HandleCalculation(), PerformServices(), OutputUser(), ProcessInput(), DealWithOutput()\n这些子程序名称根本不能说明子程序是做什么的。这时候要采用更具体的词语，比如说将 HandleOutput\n改成 FormatAndPrintOutput\n就会清晰很多；假如是子程序本身的设计问题而导致了无法采用更具体的词，那么就需要重新组织这个子程序了。\n（3）给函数命名时要对返回值有所描述。这里的有所描述并不是显式地描述返回值类型，而是通过函数名体现，如\ncustomerID.next(),printer.isReady() 等都较好地体现了返回值\n（4）准确适用对仗词。命名时遵循对仗词的命名规则有助于保持一致性，从而也提高可读性。下面是一些通用的对仗词。\n\n\n对仗词\n\n（5）给常用的操作建立命名规则。如下是作者列举的某个例子，这些方法是某个工程里面获取对象\nid\n的所有方法，其作用一致，但是到了后来，没人能记住哪个对象应该用哪些子程序了。所以应该一开始就应该统一获取\nid 的子程序名称，\nemployee.id.Get()dependent.GetId()supervisor()candidata.id()\n参数与返回值\n关于参数和返回值有以下建议\n（1）按照输入 - 修改 - 输出的顺序排列参数。而不是按照字母顺序排列，还可以考虑采用某种表示输入、修改、输出的命名规则，如可以给这些参数名字加上\ni_, m_, o_ 前缀。\n（2）如果几个子程序都用了类似的一些参数，应该让这些参数的排列顺序保持一致。这样可以产生一定的记忆效应\n（3）把状态变量或错误变量放到最后。就是将那些表明发生了错误的变量放到函数的最后，这些参数只是附属于主程序的主要功能，而且是仅用于输出的参数。\n（4）不要把子程序的参数用于工作变量。如下的代码中 inputVal\n就不应该被这么用，在 C++ 中可以用 const\n参数来做这一限制。\nint Sample( int inputVal ) {  inputVal = inputVal * CurrentMultiplier( inputVal );  inputVal = inputVal + CurrentAdder( inputVal );  ...  return inputVal;}\n\n关于返回值，要检查所有可能的返回路径。也就是要确保在所有可能的情况下该函数都会返回值。在函数开头用一个默认值来初始化返回值是一个很好的做法，这种方法能够在未正确地设置返回值时提供一张保险网。\n\n","categories":["编程"],"tags":["编程"]},{"title":"Delayed FeedBack In Computational Advertising","url":"/2020/12/05/Delay%20FeedBack%20In%20Computational%20Advertising/","content":"转化是有延迟的，即在点击发生后过一段时间用户可能才会发生转化，且往往转化漏斗越深，延迟的时间越长；在计算广告中，delayed\nfeedback 主要影响下面两个场景\n\nCVR 模型的训练\n\n基于后验的调价策略\n\n对于场景 1，影响体现在（1）过早把样本送入模型，把最终会转化但是还没回传\nlabel\n的事件当做负例，导致模型低估（2）过晚把样本送入模型，即让所有样本都等待一个足够长的时间才送入模型，导致模型没能及时更新\n对于场景 2，影响体现控制器控制 cost/value=target\n时，分母会小于实际值，导致控制的不稳定\n本文主要介绍三篇 paper 针对这个问题在场景 1\n的一些解决思路，其中涉及到的一些方法也能应用到场景 2 中（而如果问题 1\n能被较好地解决，也能基于预估值而不是后验数据进行调价）\n\n第一篇 paper\n是较早提出的一个解决方法，引入了一个隐变量辅助建模，同时对延迟时间单独建模，总体建模思路比较值得学习\n第二篇 paper 是对第一篇 paper 的拓展，第一篇 paper\n是通过指数分布建模回传延迟的时间的，而这篇 paper 则借鉴了 KDE (Kernel\ndensity estimation) 来学习任意分布去建模回传延迟时间\n第三篇 paper\n则是比较了几种方法在离线数据和在线实验的效果，改动主要是在 loss function\n上；其中就包括了第一篇 paper 的方法，以及其他一些利用了\npositive-unlabeled learning/importance sampling 等方法推导出来出新的\nloss\nModeling\nDelayed Feedback in Display Advertising(2014)\n这篇 paper 是 criteo 在 2014 年发表的一篇对 delayed feedback 建模的\npaper，建模思路比较值得学习。针对这篇 paper 可参考之前写过的一篇文章\n《Modeling\nDelayed Feedback in Display Advertising》 阅读笔记\nA\nNonparametric Delayed Feedback Model for Conversion Rate\nPrediction(2018)\n这篇 paper 是在第一篇 paper\n基础上对回传延迟函数做了改进，第一篇回传延迟使用了指数分布来建模，这里则借助了\nKDE（Kernel\ndensity estimation） 的思想用任意分布来代替第一篇 paper\n的指数分布，而这正是 paper 名字中 nonparametric\n意思的含义。\n基本符号定义跟第一篇 paper 差不多\n\n\nnotion\n\nPaper 中主要通过 Survival\nanalysis 的方法来推导，简单来说，survival analysis\n主要研究如下领域\n\nSurvival analysis is a branch of statistics for analyzing the\nexpected duration of time until one or more events happen, such as death\nin biological organisms and failure in mechanical systems.\n\nPaper 中使用到的 survival analysis 概念和推论如下\n\n\nassumption\n\n第一篇 paper 其实也用了上面的推论，\\(f(t)\\) 是指数分布的 pdf，\\(s(t)\\) 是指数分布的 cdf，而 \\(h(t)\\) 则是第一篇 paper 中的 \\(\\lambda(x)\\)\n而从上面的表达可知，求解出 \\(s(t)\\)\n的关键是求出 hazard function \\(h(x)\\),\nhazard function 的含义是当前时刻第一次发生事件的 event rate;\n相比于第一篇直接令 \\(h(x) = \\lambda(x) =\nw\\_dx\\), 这里借鉴了 Kernel density estimation 的思想将 hazard\nfunction 写成如下形式（\\(L\\) 是一个超参，表示时间轴上的 \\(L\\) 个 pseudo-points）\n\\[\\begin{align} h(d_i; x_i, V) =\n\\sum_{l=1}^{L}\\alpha_l(x_i;V)k(t_l, d_i) \\end{align}\\]\n下图更直观地描述了这种形式是如何拟合任意分布的\n\n\nKDE\n\n上面中的 \\(\\alpha_l,\nk\\) 定义如下，直观来看，\\(\\alpha_l\\) 表示当前的 pseudo-point 对总体的\nhazard function 的贡献，\\(k(t_l,\n\\tau)\\) 表示离当前的 pseudo-point \\(t_l\\) 越近的训练样本，对当前的 pseudo-point\n\\(t_l\\) 的参数 \\(V_l\\) 的影响越大（想象 bp 时各个 \\(V_l\\) 的 gradient）\n\\[\\begin{align} \\alpha_l(x_i; V) =\n(1+\\exp(-V_{l}^{T}x_i))^{-1} \\end{align}\\]\n\\[\\begin{align} k(t_l, \\tau) =\n\\exp(-\\frac{(t_l-\\tau)^2}{2h^2}) \\end{align}\\]\n写出了 hazard function 后，根据公式（2）有\n\n\nsurvival function\n\n则 paper1 的的回传延迟概率可写成如下形式\n\\[\\begin{align} p(d_i|x_i, c_i = 1) =\ns(d_i; x_i, V )h(d_i; x_i, V) \\end{align}\\]\n\n\nprobability\n\n训练的算法就跟 paper1 中的 EM 算法一样\n\n\nEM\n\n上面这种形式是借鉴了 KDE 的思想，KDE 可以理解为一种可写出任意分布的\npdf 的方法，关于这部分的推导这里有一个直观的回答：什么是核密度估计？如何感性认识？\nAddressing\nDelayed Feedback for Continuous Training with Neural Networks in CTR\nprediction(2019)\n这是 Twitter 发的一篇 paper，主要对比了几种方法在 delayed feedback\n问题上的效果\nFaked nagative weighted\nDelay feedback 的问题也可以从另一个角度去理解，观察到的样本的分布是\nbiased distribution \\(b\\), 但是需要求解真实的样本的分布 \\(p\\) 的期望；而 importance\nsampling 是解决这个问题的一个方法，这个方法在 Wikipedia\n上的简单介绍如下\n\nIn statistics, importance sampling is a general technique for\nestimating properties of a particular distribution, while only having\nsamples generated from a different distribution than the distribution of\ninterest\n\nzhihu 上一个更通俗的解释见 重要性采样（Importance\nSampling），则 paper 中的可将真实分布的期望写成如下形式\n\\[\\begin{align} E_p[\\log f_{\\theta}(y|x)]\n= E_b[\\frac{p(x,y)}{b(x,y)} f_{\\theta}(y|x)] \\end{align}\\]\n即在观察到的 biased distribution \\(b\\) 上对样本进行加权 \\(w(x,y) = \\frac{p(x,y)}{b(x,y)}\\)，则 loss\nfunction 可写成如下形式\n\n\nloss func\n\n这个方法会将所有的样本在一开始都当做负样本，然后在正样本回传时多传一个正样本，假设\n\\(N\\) 是全部样本，\\(M\\) 是其中的正样本，则有\n\\[\\begin{align} b(y=1|x) = \\frac{M}{M+N} =\n\\frac{\\frac{M}{N}}{1+\\frac{M}{N}} = \\frac{p(y=1|x)}{1+p(y=1|x)}\n\\end{align}\\]\n\\[\\begin{align} b(y=0|x) = 1- b(y=1|x)=\n\\frac{1}{1+p(y=1|x)} \\end{align}\\]\n即上面公式 (7) 可被写成下面公式 (10), 即需要给分布 \\(b\\) 中的正样本加的权重为 \\(1+p(y=1|x)\\), 负样本的权重为 \\(p(y=0|x)(1+p(y=1|x))\\)\n\n\nloss func1\n\n但是这里的 \\(p\\) 是未知的，因此\npaper 中利用了模型的预估值 \\(f_{\\theta}\\) 来近似\n此外，importance sampling 也被应用在强化学习中的 off-policy\n策略中，简单来说就是通过行为策略 (behavior strategy) \\(b\\) 产生的序列分布来优化目标策略 (target\nstrategy) \\(π\\) 价值函数，关于\non-policy 与 off-policy 的区别可参考这个回答 强化学习中 on-policy\n与 off-policy 有什么区别？ ）\nFake negative calibration\n这个方法直接让模型学习 biased distribution \\(b\\), 然后基于上面 Faked nagative weighted\n中推导出来的公式反推出 \\(p(y=1|x)\\) 的概率，即由\n\\[\\begin{align} b(y=1|x) = \\frac{M}{M+N} =\n\\frac{\\frac{M}{N}}{1+\\frac{M}{N}} =\n\\frac{p(y=1|x)}{1+p(y=1|x)}  \\end{align}\\]\n可推出\n\\[\\begin{align} p(y=1|x) =\n\\frac{b(y=1|x)}{1-b(y=1|x)}  \\end{align}\\]\n这种方法可以认为是一种 post\ncalibration，除此之外，也有一种方法是在训练的时候就做了纠正的，也叫做\nprior correction，基本思路就是在训练的时候对预估的 logit\n做一层转换，简单推导如下\n假设真实的分布下，正样本的数量是 \\(P\\)，负样本的数量是 \\(N\\)，\n则 serving 是输出的概率值应该是\n\\[\\begin{align} \\frac{P}{P+N} =\n\\frac{1}{1+e^{-x}} \\end{align}\\]\n但是在 training\n时，由于上面的机制使得每条样本都会以负样本的形式出现一次，\n同时假设会对负样本采样，采样率为 \\(r\\),\n则有\n\\[\\begin{align} \\frac{P}{r*(P+N)+P} =\n\\frac{1}{1+e^{-x^{*}}} \\end{align}\\]\n通过对上面的变换有\n\\[\\begin{align} \\frac{P}{r*(P+N)+P}\n=  \\frac{P/(P+N)}{r+P/(P+N)} =  \\frac{1/(1+e^{-x})}{r + 1/(1+e^{-x})}\n\\end{align}\\]\n则令\n\\[\\begin{align} \\frac{1/(1+e^{-x})}{r +\n1/(1+e^{-x})}=\\frac{1}{1+e^{-x^{*}}} \\end{align}\\]\n可推导出\n\\[\\begin{align} x^{*} = -(\\ln r+\n\\ln(1+e^{-x})) \\end{align}\\]\n即在训练时计算 logit，不使用 \\(\\frac{1}{1+e^{-x}}\\) 而是使用 \\(\\frac{1}{1+e^{-x^{*}}}\\)\nPositive-Unlabeled Learning\npaper 关于里面基本没有推导，详细的推导可参考 Learning Classifiers\nfrom Only Positive and Unlabeled Data,\n基本思路跟第一篇比较像，这里就不详细展开了，主要引用了里面一些关键的推导步骤\n\nA key assumption about the training data is that they are drawn\nrandomly from \\(p(x,y,s)\\), and for\neach tuple \\(&lt; x,y,s &gt;\\) that is\ndrawn, only \\(&lt; x,s &gt;\\) is\nrecorded. Here \\(s\\) is the observed\nlabel and \\(y\\) is the actual label,\nwhich might not have occurred yet. Along with this, it is assumed that\nlabeled positive examples are chosen completely randomly from all\npositive examples, i.e. \\(p(s = 1|x, y = 1) =\np(s = 1|y = 1)\\)\n\n基于上面的 assumption 有\n\n\nassumption\n\n\nThe value of the constant \\(c = p(s = 1|y =\n1)\\) can be estimated using a trained classifier \\(g\\) and a validation set of examples. Let\n\\(V\\) be such a validation set that is\ndrawn from the overall distribution \\(p(x, y,\ns)\\) in the same manner as the nontraditional training set. Let\n\\(P\\) be the subset of examples in\n\\(V\\) that are labeled (and hence\npositive). The estimator of \\(p(s = 1|y =\n1)\\) is the average value of \\(g(x)\\) for \\(x\\) in \\(P\\). That is \\(\\frac{1}{n}\\sum_{x \\in P}g(x)\\)\n\n求解环节，所有样本的 期望 /likelihook 可写成如下形式\n\n\nMLE\n\n则最终的 loss 函数形式如下\n\n\npu-loss\n\n总结\n本文主要介绍了对 delay feedback 问题的几种解决方法，其中第一篇 paper\n引入了一个隐变量，同时对转化延迟和转化分别建模，建模思路比较值得学习；第二篇\npaper 则是对第一篇 paper\n的转化延迟模型做了改进；第三篇则介绍了几种在一开始将所有样本都当做负样本，并在正样本回传的时候再进一次模型，但是这样会导致\ntraining 和 serving 不一致（正负样本比例不一致），对此 paper\n也提出了几种解决方法。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"Distillation 简介","url":"/2020/03/01/Distillation%20%E7%AE%80%E4%BB%8B/","content":"本文简单了描述机器学习中的蒸馏（distillation）技术的原理，distillation\n可简单分为 model distillation 和 feature\ndistillation。顾名思义，蒸馏是对原来的模型 / 特征进行了压缩，其原因可能是为了减少模型的大小（model\ndistillation）、或者某些特征只能在 training 时获取，serving\n无法获取 (feature\ndistillation)；在实际业务中可根据具体场景灵活地应用这两类技术。\n\n基本原理\nDistillation 可分为 Model Distillation 和 Feature\nDistillation，其思想都是在训练时同时训练两个模型：teacher 模型和 student\n模型，而在 serving 时只用 student 模型。这里的假设是：teacher\n模型比起 student 模型，在模型结构上更复杂 (Model Distillation)\n，或在特征集上更为丰富 (Feature Distillation) ；因此其准确率也会比\nstudent 模型要好。\n如下图所示是 Model Distillation 和 Feature Distillation 示例\n（下面的图和公式基本摘自 Privileged Features\nDistillation for E-Commerce Recommendations）\n\n\nMD vs FD\n\n那如何利用 teacher 模型指导 student\n模型学得更好？基本的做法是将 teacher 模型的输出作为 soft\nlabel (相对于作为 ground truth 的 hard label）, 为 student 模型添加额外的\nloss 项；如下公式 (1) 所示\n\\[\\begin{align} \\min_{W_s}\n(1-\\lambda)L_s(y, f_s(X;W_s))+\\lambda*L_d(f_t(X;W_t),f_s(X;W_s)) \\tag{1}\n\\end{align}\\]\n上式中各项符号含义如下\n\n\\(f_s(X; W_s)\\) ：student\n模型的预估值\n \\(f_t(X;W_t)\\) ： teacher\n模型的预估值\n \\(L_s\\) ：student 模型原始的\nloss\n\\(L_d\\) ：利用 teacher\n模型预估值输出作为 soft label 计算的 distillation loss;\n\\(\\lambda\\)：平衡 \\(L_s\\) 和 \\(L_d\\) 的超参\n\n上面公式（1）是 Model Distillation 的典型做法，可以看到输入 teacher\n模型和 student 模型的特征都是相同的即 \\(X\\) ；而公式 (2) 描述的 Feature Distillation\n则认为 teacher 模型的特征（\\(X^*\\)）比\nstudent 模型的特征 (\\(X\\))\n更为丰富，\n\\[\\begin{align} \\min_{W_s}\n(1-\\lambda)L_s(y, f_s(X;W_s))+\\lambda\\*L_d(f_t(X^\\*;W_t),f_s(X;W_s))\n\\tag{2} \\end{align}\\]\n上面两条公式是 Distillation\n的核心思想了，且在使用理论上应该首先训练好 teacher 网络，再训练\nstudent 网络；但是在实际训练的时候，为了加快训练速度，会令 teacher\n模型和 student\n模型同时进行训练；因此最终的损失函数变为了如下公式 (3)\n形式，其中 \\(L_s\\) 和 \\(L_t\\) 是 logloss， 而 \\(L_d\\) 是 cross entropy loss\n\\[\\begin{align} \\min_{W_s, W_t}\n(1-\\lambda)L_s(y, f_s(X;W_s)) +  \\lambda\\*L_d(f_t(X^\\*;W_t),f_s(X;W_s))\n+ L_t(y, f_t(X^\\*;W_t))\\tag{3} \\end{align}\\]\n综上，在 training 和 serving 时的模型结构分别如下所示\n\n\ntrainingAndServing\n\n训练注意事项\n上面提到，distillation 需要训练 teacher 和 student\n两个网络，因此也有两种训练模式：\n（1）先训练 teacher 网络，再训练 student 网络，也被称为 asynchronous\ntraining （2）同时训练 teacher 网络和 student 网络，也被称为 synchronous\ntraining\n理论上应该采用方式 (1),\n但是由于需要串行训练两个模型，会导致训练的时间过长 ,\n因此才提出了方式（2）的方法；而方式 (2) 会带来训练效果不稳定的问题，\n其原因是在 teacher 在训练初期，其效果往往还不好，而将其输出结果作为\nlabel 很容易导致 student 网络学飞了\n因此更常用的做法在这两个之间做个权衡，基本做法就是在训练的初期，将公式 (3)\n中的 \\(\\lambda\\)\n设为 0，然后后面逐渐增大 \\(\\lambda\\)\n这个值\n上面提到的 paper 在这点上提出了一个更简单策略，就是在 \\(k\\) 个 step 后才让 teacher 网络的输出作为\nloss 影响 student 网络，\\(k\\)\n是一个拍定的超参，因此其详细训练方式如下\n\n\nsynchronous training\n\n实现\ntensorflow 提供的一个 distillation 的实现 distillation.py，使用见\nstack-overflow 上的 这个回答\n核心代码如下所示，注释写得已经非常清晰了，下面默认的模式是先训练好了\nTeacher 网络，再训练 Student 网络，也就是上面提到的 asynchronous\ntraining 模式；但是也可以比较容易将下面的逻辑改成 synchronous training\n的。\n### Teacher Networkwith tf.variable_scope(\"teacher\"):  teacher_outputs = self.teacher_model.body(features)  tf.logging.info(\"teacher output shape: %s\" % teacher_outputs.get_shape())  teacher_outputs = tf.reduce_mean(teacher_outputs, axis=[1, 2])  teacher_logits = tf.layers.dense(teacher_outputs, hp.num_classes)  teacher_task_xent = tf.nn.softmax_cross_entropy_with_logits_v2(      labels=one_hot_targets, logits=teacher_logits)  outputs = teacher_logitsif is_distill:  # Load teacher weights  tf.train.init_from_checkpoint(hp.teacher_dir, {\"teacher/\": \"teacher/\"})  # Do not train the teacher  trainable_vars = tf.get_collection_ref(tf.GraphKeys.TRAINABLE_VARIABLES)  del trainable_vars[:]### Student Networkif is_distill:  with tf.variable_scope(\"student\"):    student_outputs = self.student_model.body(features)    tf.logging.info(        \"student output shape: %s\" % student_outputs.get_shape())    student_outputs = tf.reduce_mean(student_outputs, axis=[1, 2])    student_logits = tf.layers.dense(student_outputs, hp.num_classes)    student_task_xent = tf.nn.softmax_cross_entropy_with_logits_v2(        labels=one_hot_targets, logits=student_logits)    teacher_targets = tf.nn.softmax(teacher_logits / hp.distill_temperature)    student_distill_xent = tf.nn.softmax_cross_entropy_with_logits_v2(        labels=tf.stop_gradient(teacher_targets),        logits=student_logits / hp.distill_temperature)    # scale soft target obj. to match hard target obj. scale    student_distill_xent *= hp.distill_temperature**2    outputs = student_logits    # Summaries    tf.summary.scalar(\"distill_xent\", student_distill_xent)if not is_distill:  phase_loss = teacher_task_xentelse:  phase_loss = hp.task_balance * student_task_xent  phase_loss += (1 - hp.task_balance) * student_distill_xentlosses = {\"training\": phase_loss}outputs = tf.reshape(outputs, [-1, 1, 1, 1, outputs.shape[1]])return outputs, losses\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"EE 问题概述","url":"/2019/01/05/EE(Exploitation%20Exploration)%20%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/","content":"EE (Exploitation &amp; Exploration)\n问题在计算广告 / 推荐系统中非常常见，甚至在更广义的范围上，任意决策问题都会牵涉到\nEE\n问题。简单来说，这个问题就是要解决的是在决策时到底是根据已有经验选择最优的策略 (Exploitation)，还是去探索一些新的策略来提升未来的收益 (Exploration)。本文主要介绍解决这个问题的三种比较常见的方法：随机方法，UCB\n方法，Thompson sampling 方法，侧重于方法的具体流程和基本思想。\n\nMAB 建模\nEE 问题一般会通过 MAB (Multi-Armed Bandit) 进行建模，如下所示，所有\narm 就是每次决策中可作出的选择，拉下某个 arm 表示作出了相应的选择。\n\n\nMAB\n\nMAB 符号化表述如下\n\nMAB 可表示为一个二元组 &lt;\\(A,\nR\\)&gt;\n\\(A\\) 表示为一系列可能的动作，\\(R(r|a)\\)\n则表示给定动作下的奖赏的分布，\n每一时刻根据给定策略从 \\(A\\)\n选择动作 \\(a_t\\), 同时环境根据分布\n\\(R(r|a)\\) 生成奖赏 \\(r_t\\)\n 目标是最大化奖赏之和 \\(\\sum_{t=1}^T\nr_t\\)\n\n上面的第 3 步的策略就是下面要介绍的 EE\n问题的解决方法，除去随机方法，UCB 方法和 Thompson sampling\n方法的思想均是通过定义每个 arm 的收益的期望，然后选择收益期望最大的\narm。UCB 是频率学派的思想，认为每个 arm\n的收益期望是固定的，通过试验记录得到其历史收益状况，然后加上一个 bound\n构成了收益期望；Thompson sampling\n则是贝叶斯学派的思想，认为 arm\n的收益期望服从一个特定的概率分布，通过试验记录更新分布的参数，然后从每个\narm 的分布中产生收益期望。\n而根据一个 arm\n的历史试验记录判断其优劣又有两种方法，因而也衍生了两类 bandit\n问题：Bernoulli Bandit 和 Contextual Bandit。在 Bernoulli\nBandit 中，认为每个 arm\n的优劣（即当前试验是否产生收益）是服从伯努利分布的，而分布的参数可以通过历史收益状况求解；而在\nContextual Bandit 中，没有直接定义出一个概率分布来描述每个 arm 的优劣，\n而是假设了 arm 的优劣和描述 arm 的特征组成的向量 \\(x\\) 存在一个线性关系：\\(x^T \\theta\\)，参数 \\(\\theta\\) 可通过历史样本求解和更新。\nUCB 方法和 Thompson sampling 方法均可解决这两类问题，UCB 解决\nBernoulli Bandit 的方法有 UCB1，UCB2 等，解决 Contextual Bandit 的方法有\nLinUCB 等；而 Thomson Sampling 解决 Bernoulli Bandit 时采用了 Bernoulli\n分布和 Beta 分布，解决 Contextual Bandit\n时采用了两个正态分布。后面会详细介绍这些方法。\n随机方法\n\\(\\epsilon\\)-greedy\n\\(\\epsilon\\)-greedy\n是一种最简单的随机方法，原理很简单：每次决策时，以 1 - \\(\\epsilon\\) 的概率选择最优的策略，以 \\(\\epsilon\\) 的概率随机选择任意一个策略；\n并且在每次做出决策获取到真实的 reward\n后更新每个决策的收益情况（用于选择最优策略）。伪代码实现可参考 Multi-Armed Bandit:\nepsilon-greedy\n\\(\\epsilon\\)-greedy\n存在着以下几个比较显著的问题\n\n\\(\\epsilon\\)\n是个超参数，设置过大会导致决策随机性过大，设置过小则会导致探索性不足\n \\(\\epsilon\\)-greedy\n策略运行一段时间后，对各个 arm\n的收益情况有所了解，但没有利用这些信息，仍然不做任何区分地随机\nexploration（会选择到明显较差的 item）\n\\(\\epsilon\\)-greedy\n策略运行一段时间后，但仍然花费固定的精力去\nexploration，浪费了本应该更多进行 exploitation 机会\n\n针对第 2 个问题，可以在 \\(\\epsilon\\)-greedy\n策略运行一段时间后，选择出收益最高的前 \\(n\\) 个 arm，然后 exploration 时从这 \\(n\\) 个 arm 中随机选择。\n针对第 3 个问题，可以设置进行 exploration 的概率 \\(\\epsilon\\)\n随着策略进行的次数而逐渐下降，比如说可以取如下的对数形式，\\(m\\) 表示目前进行了 \\(m\\) 次的决策\n\\[\\begin{align} \\epsilon = \\frac{1}{1 +\n\\log(m+1)} \\end{align}\\]\nSoftmax\n通过 Softmax 进行的 Exploration 也称为 Boltzmann\nExploration，这个方法通过一个温度参数来控制 exploration 和 exploitation\n的比例，假设各个 arm 的历史收益为 \\(\\mu_0\\), \\(\\mu_1\\), ......, \\(\\mu_n\\), 温度参数记为 \\(T\\)，则选择某个 arm 时参考的指标为\n\\[\\begin{align} p_i =\n\\frac{e^{\\mu_i/T}}{\\sum_{j=0}^{n} e^{\\mu_j/T}}(i=0, 1,...., n)\n\\end{align}\\]\n当温度参数 \\(T=1\\),\n上面的方法就是纯粹的 exploitation；而当 \\(T\n\\to \\infty\\) 时，上面的方法就是纯粹的 exploration，因此，可以控制\n\\(T\\) 的范围来控制 exploration 和\nexploitation 的比例。某些文献也会将 \\(1/T\\) 称为学习率。一个很直观的想法就是让\n\\(T\\)\n随着策略运行次数的增加而下降，这样便可让策略从偏向 exploration 转为偏向\nexploitation。\n但是，这篇 paper Boltzmann Exploration Done\nRight 证明了单调的学习率（即 \\(1/T\\)）会导致收敛到局部最优，并提出了一种针对不同的\narm 采用不同的学习率的方法，但是形式已经不是上面的 softmax\n形式了。文章涉及的证明和公式符号较多，这里不再展开阐述，感兴趣读者可自行参考。\nUCB 方法\n假如能够对每个 arm\n都进行足够多次的试验，根据大数定律，次数越多，这些试验结果统计得到的收益便会约接近各个\narm 真实的收益。然而在实际中，只能对各个 arm\n进行有限次的试验，因此这会导致根据统计得到的收益跟真实的收益存在一个误差，UCB\n的核心就在于如果预估这个误差 (也就是 UCB 中的 B (bound))，然后将 arm\n统计的收益加上其通过 UCB 方法计算出来的 bound\n进行排序，选择最高的那个。\nUCB1\nUCB1 方法的理论基础是 Hoeffding's\ninequality，该不等式的定义如下\n\n假设 \\(X_1, X_2...X_n\\)\n是同一个分布产生的 \\(n\\)\n个独立变量，其均值为 \\(\\overline{X} =\n\\frac{1}{n}\\sum_{i=1}^n X_i\\), 则如下公式成立 \\[\\begin{align} p(|E[X] - \\overline{X}| \\le \\delta)\n\\ge 1 - 2e^{-2n\\delta^2} \\end{align}\\]$$\n\n更直观地说，该不等式表明了 \\(n\\)\n个独立同分布的变量的均值与该变量的真实期望的误差小于某个预设的阈值 \\(u\\) 会以概率 \\(1\n- e^{-2nu^2}\\) 恒成立。\n回到我们的问题，可以将 \\(X_1,\nX_2...X_n\\) 看做某个 arm 在 \\(n\\)\n次试验中获得的收益，则通过上面的式子可以设定一个 \\(\\delta\\) 使得公式成立，然后用 \\(\\overline{X} + \\delta\\) 来近似真实的收益\n\\(E(X)\\)；理论上也可用 \\(\\overline{X} - \\delta\\)，但是 UCB\n方法会用上界，这也是 UCB 中 U (upper) 的含义。那么现在的问题便是\n\\(\\delta\\) 该选多大了？\nUCB1 方法中将 \\(\\delta\\)\n设为如下公式，公式中的 \\(N\\)\n表示目前所有 arm 试验的总次数，\\(n\\)\n表示某个 arm 的实验次数\n\\[\\begin{align}  \\delta =\n\\sqrt{\\frac{2\\ln{N}}{n}} \\end{align}\\]\n直观地看上面定义的 \\(\\delta\\),\n分子的 \\(N\\) 对所有的 arm\n是相同的，分母的 \\(n\\) 则表示某个 arm\n目前为止试验的次数，如果这个值越小，那么 \\(\\delta\\) 便越大，相当于\nexploration；而当各个 arm 的 \\(n\\)\n相同时，实际上就是在比较各个 arm 的历史收益情况了。\nUCB1 方法的流程如下，该图摘自 Optimism\nin the Face of Uncertainty: the UCB1 Algorithm\n\n\nUCB1\n\n可以看到 UCB1 的 bound 完全是由 Hoeffding's inequality\n推导出来的，而除了 Hoeffding's inequality，其他的一些 inequality\n也能够推导出相应的 bound，Reinforcement\nLearning: Exploration vs Exploitation 中就提到了一些其他的\ninequality\n\nBernstein’s inequality\nEmpirical Bernstein’s inequality\nChernoff inequality\nAzuma’s inequality\n.......\n\nUCB2\n从名字上基本就可以猜出 UCB2 是 UCB1 的改进，改进的地方是降低了 UCB1\n的 regret 的上界，regret\n指的是每次能获得的最大的收益与实际获得的收益的差距，这部分涉及到较多的数学证明，这里略去这部分，详细可参考\nFinite-time\nAnalysis of the Multiarmed Bandit Problem。UCB2\n算法的流程如下，图片同样摘自 Finite-time\nAnalysis of the Multiarmed Bandit Problem\n\n\nUCB2\n\n从流程图上可知，UCB2 与 UCB1 相似，也是为每个 arm 计算一个\nbound，然后根据 arm 的历史收益和 bound 选出 arm ，只是对这个 arm\n不止试验一次，而是试验 \\(\\tau(r_j+1) -\n\\tau(r_j)\\) 次。上面的 \\(a_{n,\nr_j}\\) 和 \\(\\tau(r)\\)\n定义如下，由于 \\(\\tau(r)\\)\n要为整数，因此取了上界\n\\[\\begin{align} a_{n,r} =\n\\sqrt{\\frac{(1+\\alpha)\\ln(ne/\\tau(r))}{2\\tau(r)}}\n\\end{align}\\]\n\\[\\begin{align} \\tau(r) = \\lceil\n(1+\\alpha)^r\\rceil \\end{align}\\]\n上面式子中的 \\(\\alpha\\)\n是个超参数，根据上面给出的论文中的实验结果 (如下图所示)，这个值不能取得太大，论文建议值是\n0.0001\n\n\nalpha\n\nLinUCB\n上面的 UCB1 和 UCB2 算法都是解决 Bernoulli Bandit\n问题的，也就是假设每个 arm\n的优劣是服从伯努利分布，而根据历史记录计算出的 \\(\\overline\n{x}_j\\)（获得收益的试验次数和总试验次数的比值）其实就是伯努利分布的参数。\n这样基于统计的方法很简单，但是问题也比较显著，因为 arm\n的收益会跟多个因素有关（比如说某个 arm\n在早上选择时没有收益，但是晚上就有了），利用这些信息可以预估得更准确；而基于统计的方法则忽略了这一点。\n区别于 Bernoulli Bandit，这类利用了上下文信息的问题就是上面提到的\nContextual Bandit 问题，而 LinUCB 就是要解决这个问题的。 LinUCB\n中没有直接定义出一个概率分布来描述每个 arm 的历史收益状况，而是假设了\narm 的优劣和描述 arm 的特征的向量 \\(x\\)\n存在一个线性关系： \\(x^T \\theta\\)\n实际上这是一个经典的 Linear Regression (收益不在局限于 0 和 1)\n问题，\\(x\\) 是 arm\n的特征组成的向量 (需要根据具体的问题选择特征), \\(\\theta\\)\n则是模型的参数，每一次的试验就是一条样本，label 为具体的收益。\n通过历史样本可以求解出 \\(\\theta\\),\n则在每次选择选择 arm 时，LinUCB 会用 \\(x^T\n\\theta\\) 来替换掉 UCB1 或 UCB2 中的 \\(\\overline {x}_j\\)，但是这还不是 LinUCB\n的全部，作为 UCB 类方法，LinUCB 中还是有个 bound\n的，因为毕竟从历史记录只能对 arm\n进行有限次的试验，预估出来的收益情况与真实的还是存在差距的。\nUCB1 中推到出 bound 的 Hoeffding's inequality 不能直接应用到 LinUCB\n中，而关于 linear regression 的 bound 最早是在这篇论文 Exploring\ncompact reinforcement-learning representations with linear\nregression 中提出的，这里不详细展开具体的证明过程了。提出 LinUCB\n的论文 A\nContextual-Bandit Approach to Personalized News Article\nRecommendation 直接采用了这一结论，其表达形式，如下所示\n\\[\\begin{align} p(|x_j^T\\theta_j -\nE(r|x_j)| \\le \\alpha \\sqrt{x_j^T(D_j^TD_j+I)^{-1}x_j}) \\ge 1- \\delta\n\\end{align}\\]\n即对于某个 arm \\(j\\), 计算出来的\n\\(x_j^T\\theta_j\\) 与实际的期望相差小于\n$ $ 的概率要大于 \\(1- \\delta\\), 其中\n\\(D_j\\) 是 arm \\(j\\)\n前面每次被观察到的特征组成的矩阵，比如说 arm \\(j\\) 前面被观察了 \\(m\\) 次，且特征组合成的向量的维度为 \\(d\\), 则 \\(D_j\\) 的大小为 \\(m\\) X \\(d\\), \\(I\\)\n为单位向量，而 \\(\\alpha = 1 +\n\\sqrt{\\ln(2/\\delta)/2}\\) , 因此，只要根据概率选定 \\(\\delta\\)，则各个 arm 的 bound 便可通过 \\(\\alpha \\sqrt{x_j^T(D_j^TD_j+I)^{-1}x_j}\\)\n求出。\n因此 LinUCB 算法流程如下，算法同时包含了选择 arm 的方式和更新 linear\nregress 模型。\n\n\nLinUCB1\n\n上面的每个 arm 的 linear model\n的参数都是独立的，论文针对这对点设计了这些 model\n共享的一些参数，即将原来某个 arm \\(j\\)\n计算出来的 \\(x_j^T\\theta_j\\) 换成了\n\\(x_j^T\\theta_j + z_j^T \\beta\\), \\(\\beta\\) 是各个模型共享的的参数，\\(z_j^T\\)\n则是这些参数对应的特征。对应这种情况，也有了论文的第二种算法\n\n\nLinUCB2\n\nThompson sampling 方法\n前面的 UCB 方法采用的都是频率学派的思想，即认为评判某个 arm\n的优劣的指标是个定值，如果有无限次的试验，便可准确地计算出这个值，但是由于现实中只能进行有限次的试验，因此预估出来的值是有偏差的，需要通过另外计算一个\nbound 来衡量这个误差。\n而下面要介绍的 Thompson sampling\n方法采用的则是贝叶斯学派的思想，即认为评判某个 arm\n的优劣的指标不再是个定值，而是服从着某种假定的分布 (先验)，通过观察到的历史记录去更新这个分布的参数 (似然)，得到了新的分布参数 (后验),\n然后不断重复这个过程。当需要进行比较时，从分布中随机产生一个样本即可。\nBernoulli Bandit\n前面提到，Bernoulli Bandit 假设某个 arm\n的优劣服从伯努利分布，即每次是否获得收益的服从参数为 \\(\\theta\\) 的伯努利分布\n$p(reward | ) Bernoulli() $\nUCB 方法中的 UCB1 和 UCB2 都是通过简单的历史统计得到 \\(\\overline {x}_j\\) 来表示 \\(\\theta\\) 的，但是贝叶斯学派则认为 \\(\\theta\\)\n服从着一个特定的分布，根据贝叶斯公式有\n\\[\\begin{align} p(\\theta|reward) =\n\\frac{p(reward|\\theta) p{(\\theta)}}{p(reward)} \\propto p(reward|\\theta)\np{(\\theta)} ＝Bernoulli(\\theta) p(\\theta) \\end{align}\\]\n\\(p(\\theta|reward)\\)\n表示根据观察到的实验收益情况更新的后验概率，且由于似然 \\(p(reward|\\theta)\\)\n为伯努利分布，为了保持共轭便于计算；先验分布 \\(p(\\theta)\\) 选择为了 Beta 分布，即 \\(Beta(\\alpha, \\beta)\\)，而两个分布相乘 \\(Bernoulli(\\theta)*Beta(\\alpha, \\beta)\\)\n会得到一个新的 Beta 分布，简单来说，就是\n\n当 \\(Bernoulli(\\theta)\\)\n的结果为 1，则会得到 \\(Beta(\\alpha + 1,\n\\beta)\\)\n 当 \\(Bernoulli(\\theta)\\)\n的结果为 0，则会得到 \\(Beta(\\alpha, \\beta +\n1)\\)\n\n因此，Bernoulli Bandit 中的 Thompson Sampling 步骤如下，图片摘自 A Tutorial on\nThompson Sampling\n\n\nThompson Sampling for Bernoulli\nBandit\n\nContextual Bandit\n在 Bernoulli Bandit 中，我们假设 arm\n是否获得收益是服从伯努利分布的，即 \\(p(reward\n| \\theta) \\sim Bernoulli(\\theta)\\)\n而在 Contextual Bandit\n中，我们假设获得的收益和特征向量存在一个线性关系，即 $reward = x^T\n$，因此无法像前面一样直接通过似然 \\(p(reward |\n\\theta) \\sim Bernoulli(\\theta)\\) 来更新 \\(\\theta\\)\n但是根据前面解决 Bernoulli Bandit 的思路，只要定义 \\(p(reward|\\theta)\\) 和 \\(p(\\theta)\\)\n为合适的共轭分布，那么就可以使用 Thompson Sampling 来解决 Contextual\nBandit, 因为根据贝叶斯公式有如下的公式\n\\[\\begin{align} p(\\theta|reward) \\propto\np(reward|\\theta) p{(\\theta)} \\end{align}\\]\n理论上，只要 \\(p(reward|\\theta)\\) 和\n\\(p(\\theta)\\)\n为共轭即可，但是考虑到假设的分布的合理性，参考这篇论文 Thompson Sampling for\nContextual Bandits with Linear\nPayoffs，分别对这两个分布都采用正态分布的形式，\n论文给出了较多的数学证明，这里略去证明，直接给出最终结论\n对于 \\(p(reward|\\theta)\\) ，由于估计\nreward 为 \\(x^T\\theta\\)\n，因此假设真实的 reward 服从以 \\(x^T\\theta\\) 为中心、 \\(v^2\\) 为标准差的正态分布，即 \\(p(reward|\\theta) \\sim \\mathcal{N}(x^T\\theta,\nv^2)\\), \\(v\\)\n的具体含义后面会给出\n为了运算上的便利性， \\(p(\\theta)\\)\n一般会选择与 \\(p(reward|\\theta)\\)\n共轭的形式；由于 \\(p(reward|\\theta)\\)\n是正态分布，那么 \\(p(\\theta)\\)\n也应该是正态分布（正态分布的共轭还是正态分布）\n为了便于给出后验概率的形式，论文首先定义了如下的等式\n\\[\\begin{align} B(t) = I_d + X^TX\n\\end{align}\\]\n\\[\\begin{align} \\mu(t) =\nB(t)^{-1}(\\sum_{\\tau = 1}^{t-1} x_{\\tau}^Tr_{\\tau})\n\\end{align}\\]\n上面的 \\(t\\) 表示某个 arm 第 \\(t\\) 次的试验，\\(d\\) 表示特征的维度，\\(X\\) 的含义与 LinUCB 中介绍的一致，即是这个\narm 前 \\(t-1\\)\n次的特征组成的矩阵，在这个例子中其维度大小为 (\\(t-1\\)) X \\(d\\), \\(r_{\\tau}\\) 则表示前面 \\(t-1\\) 次试验中第 \\(\\tau\\) 次获得的 reward。\n则 \\(p(\\theta)\\) 在 \\(t\\) 时刻服从正态分布 \\(\\mathcal{N}(\\mu(t), v^2B(t)^{-1})\\), 而\n\\(p(\\theta)\\) 与 \\(p(reward|\\theta)\\) 相乘计算出来的后验概率\n\\(p(\\theta|reward) \\sim \\mathcal{N}(\\mu(t+1),\nv^2B(t+1)^{-1})\\)\n前面提到的 \\(v\\) 的定义为 \\(v =\nR\\sqrt{9d\\ln(\\frac{t}{\\delta})}\\)，该式子中的 \\(t\\) 和 \\(d\\) 的含义与上面的相同，而 \\(R\\) 和 \\(\\delta\\)\n是需要自定义的两个常数，均是用在两个不等式中约束 regret bound 的，其中\n\\(R \\ge 0\\) ,\\(0 \\le \\delta \\lt 1\\),\n具体的等式可参考原始论文，这里不再展开。\n当 \\(t = 1\\)\n也就是在最开始一次试验都没有的时候， \\(p(\\theta)\\) 会被初始化为 \\(\\mathcal{N}(0, v^2I_d)\\),\n而每次选择 arm 的策略跟上面的 Bernoulli Bandit 类似：每个 arm 的\n\\(p(reward|\\theta)\\) 分布分别产生一个\n\\(\\theta\\), 然后选择 \\(x^t\\theta\\) 最大的那个 arm 进行试验。\n小结\n本文主要介绍了几种针对 EE 问题的策略，除了最简单的随机方法，UCB 和\nThompson Sampling 是这个问题两种比较典型的方法，两个方法的思想均是为每个\narm 计算出一个收益值，然后根据这个值进行 ranking 然后选择值最大的\narm。\nUCB 采用的是频率学派的思想，计算的收益值是历史收益加上一个\nbound，可以认为历史收益是 Exploitation，而 bound 则是 Exploration；而\nThompson Sampling 方法中每个 arm 的收益值会从一个分布中产生，没有明显的\nExploitation 和 Exploration，但也可以认为从分布中较大概率产生的值是\nExploitation ，而较小概率产生的值是\nExploration。另外，文中重点是介绍这些方法的具体做法，更多关于这些方法的理论基础和数学推导可参考下面这些文献。\n\n参考：\n\n开栏：智能决策系列\n\n推荐系统的 EE 问题及 Bandit 算法\n\nReinforcement\nLearning: Exploration vs Exploitation\n\nMulti-Armed\nBandits and the Gittins Index\n\nBoltzmann Exploration\nDone Right\n\nOptimism\nin the Face of Uncertainty: the UCB1 Algorithm\n\nFinite-time\nAnalysis of the Multiarmed Bandit Problem\n\nExploring\ncompact reinforcement-learning representations with linear\nregression\n\nA\nTutorial on Thompson Sampling\n\nA Contextual-Ba\nndit Approach to Personalized News Article Recommendation\n\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"Dynamic Creative Optimization in Online Display Advertising","url":"/2022/02/01/Dynamic%20Creative%20Optimization%20in%20Online%20Display%20Advertising/","content":"最近在研究广告创意相关内容，\n笔者根据当前的调研，将这个领域划分为创意生成、创意优选和创意投放三大块，每一块的具体职责如下\n\n创意生成：利用素材 (标题、图片、视频、落地页等) 生成候选创意 (用户看到的广告)\n\n创意优选：从计划的候选创意 (一个计划下的候选往往有多个) 中选择 topk\n个用于投放\n\n创意投放：将优选出来的创意投放至线上\n\n严格来说，这三部分其实也并非泾渭分明，比如前两部分可以统一理解为创意生成 (从最原始的素材生成最终要投放的创意)，后两部分可以统一理解为创意投放过程 (从候选中选出来并投放至线上)。\n本文主要侧重讲述与创意投放相关的一篇 paper,\n而且偏向于上面提到第三块内容 (没有基于 E&amp;E 的优选过程)，paper\n的标题是 Dynamic\nCreative Optimization in Online Display Advertising，这篇 paper\n将素材在线投放问题建模成一个二部图匹配问题，并提供了严格求解的方法和在线的近似求解方法。更重要的是，这种建模的方法不局限于创意领域，能应用到更多投放场景下。\n\n问题建模\nonline advertising 的绝大多数方法，如 targeting、ranking、bidding\n等，本质上都是在解决在线分配问题，即让合适的广告分发给合适用户，从而使得收入最大化（同时需要考虑生态、用户体验等），因此也可以从分配的角度去构造出一个如下简单二部图的形式，左边是\nuser，右边则是具体的广告 (在这里为创意),\n连接的边表示这个用户访问了这个广告 / 创意，边的权重可根据具体的业务目标决定\n\n\ndco_graph\n\n基于上图的二部图，paper 里将问题建模成如下的最优化问题\n\n\nmodelling\n\n符号含义如下\n\n\\(u\\): user 的 index\n\n\\(i\\): item 的 index, 这里的 item\n指的是创意 (creative)\n\n\\(r_{iu}\\): 第 \\(u\\) 个 user 访问了第 \\(i\\) 个 item 的\nrevenue，比如消耗 / 点击等，即上图中的边权\n\n\\(T_u\\): 第 \\(u\\) 个 user 的总访问次数\n\n\\(x_{iu}\\): 第 \\(u\\) 个 user 访问了第 \\(i\\) 个 item 的次数\n\n\\(\\underline{k_{iu}}\\): 第 \\(u\\) 个 user 访问了第 \\(i\\) 个 item 的次数的下限\n\n\\(\\overline{k_{iu}}\\): 第 \\(u\\) 个 user 访问了第 \\(i\\) 个 item 的次数的上限\n\n\\(y_{iu}\\): 第 \\(u\\) 个 user 是否访问了第 \\(i\\) 个 item\n\n\\(l_i\\): 第 \\(i\\) 个 item\n触达的不同用户数的下限 (防止只给一个用户出广告)\n\n上面的最优化问题中，(1b) 到 (1d) 中的三个约束，对应的是如下三个约束\n\n(1b) ad fatigue constraint, which aims to help in\navoiding customers becoming fatigued by seeing a product too many times.\nThis constraint says that a particular item \\(i\\) can be shown at most \\(\\overline{k_{iu}}\\) times to user \\(u\\). To calibrate ad fatigue, this\nparameter can be set using historical data showing when customers stop\nengaging with ads.\n(1c) ad retargeting constraint, which focuses on\nshowing products to customers that have seen or searched for them\nbefore. In this constraint, we say that a particular item \\(i\\) has to be shown at least \\(\\underline{k_{iu}}\\) times to user \\(u\\). To retarget an ad, this\nparameter can be set postive for items that the user saw just before the\nad campaign is planned.\n(1d) user diversity constraint, which addresses the\nrequirement that advertisers want to increase reach and purchasing by\nshowing their products to many different customers. This constraints\nsays that at least \\(l_i\\) users have\nto see a particular item \\(i\\). These\nparameters are specifically set by the advertiser.\n\n问题求解\n这里问题的求解分为了离线和在线两部分，区别是离线时是以上帝视角看这个问题，即能够获取\n用户的访问次数 \\(T_u\\)，但是在线时则是不知道的。因此，离线可采用严格的求解方法，在线则只能采用近似的求解方法。\n离线求解\n假如能够获取 \\(T_u\\)（即用户当天的访问次数），就能够采用来求解上面的最优化问题，这类问题通常会被变换成一个网络流问题来求解，关于网络流更详细的介绍可以参考\n算法学习笔记 (28):\n网络流\n而上面建模出来的问题，可以被转变成网络流中的最小费用最大流问题 (minimum-cost\nflow problem), 这部分更详细的描述也可参考 算法学习笔记 (31):\n最小费用最大流\n因此，基于最小费用最大流的概念，可以构造出如下的网络流\n\n\ndco graph\n\n上面的网络流中有 2 个 user 和 2 个\nitem，理解以下几点就能够理解上图的含义了\n\n每条边的三个参数的含义分别是 (流量上限，流量下限，\n费用)\n\n从 source 出发到第 \\(u\\) 个 user\n的流量上限为 \\(T_u\\)(即用户当前访问的次数)\n\n每个 user 访问每个 item 的次数的上限（ad fatigue\nconstraint）和下限（ad retargeting\nconstraint）的约束体现在\nusers-&gt;item-copies 的边上\n\nitem-copies-&gt;items\n的连边是为了满足上面的 user diversity\nconstraint，即保证访问第 \\(i\\)\n个 item 的不同用户的数量不小于 \\(l_i\\)\n\nitem-&gt;sink 中，流量上限为 \\(m\\), 即访问第 \\(i\\) 个 item\n的不同用户数的上限是总用户数\n\n此外，在构图中还需要考虑如下几个问题，paper\n里没有给明确的答案，也算比较 open\n的问题，需要针对不同的业务场景设计对应的技术方案了\n(1) 边的权值 (即上面说的费用的负值) 的物理含义是什么？缺失的边的权值怎么计算？\n(2) 采用 id\n粒度过小可能导致整个图可能过大，进而导致图的求解比较困难，如果做\nclustering 要怎么做？\n(3) 新 uid/item-id 如何处理？\n在线求解\n上面的解法是离线解法，即能够拿到了用户当天的访问次数 \\(T_u\\),\n然后用严格的方法求解最优化问题，这个模式往往可以用来大致摸底方法的收益天花板（通过回放历史精排日志），但是在实际投放中，并不知道用户当天的访问次数\n\\(T_u\\)(笔者附：如果用历史平均来近似，误差会有多大？)，因此在线需要提供一种近似最优的求解方法\ndeterministic algorithm\npaper 指出了 deterministic\nalgorithm（简单来说就是不会有 random\n操作，反例是启发式算法）对于在线问题表现效果很差，至于有多差，paper\n给了一些证明，这里略过过程，只给出其中一些核心概念和结论\npaper 在这个过程中使用了 competitive\nratio来衡量 deterministic\n算法的效果，这个指标主要用来描述极端情况下，online algorithm\n的效果相比于 offline 的有多差，引用这个讲义里的定义如下\n\nThe competitive ratio of an online algorithm for an optimization\nproblem is simply the approximation ratio achieved by the algorithm,\nthat is, the worst-case ratio between the cost of the solution found by\nthe algorithm and the cost of an optimal solution.\n\n有了 competitive ratio 的概念，paper 针对 deterministic algorithm\n给出了如下结论（证明的详细过程可以参考 paper 中的 4.1 小节)，即\ndeterministic algorithm 的效果很差，哪怕给 reward 加了 bound\n\nProposition 1. The competitive ratio of any deterministic algorithm\nfor the\nonline DCO problem is arbitrarily small\nProposition 2. Suppose that the revenue associated to all item and\nuser pairs are bounded, that is, there exists an \\(r\\) such that \\(\\underline{r} ≤ r_{iu} ≤ 1\\) for all items\n\\(i\\) and users \\(u\\). In this case, the competitive ratio of\nany deterministic algorithm for the online DCO problem is \\(\\underline{r}\\)\n\ndeterministic algorithm\nwith assumption\n上面给出的结果中，没有任何的假设，因此在这个方法中加入了一个\nassumption：即用户 \\(u\\) 每天至少有\n\\(c_u\\) 次访问，paper 中引用了 IAB (Interactive\nAdvertising Bureau\n) 的一个结论，认为这个数据是比较容易能得到的，paper 描述如下，\n\nUsing historical data, advertisers can make accurate estimates about\nthe minimum number of times they expect a user to visit the platform\nduring the advertising period (Interactive Advertising Bureau 2020).\n\n但是 paper 中给出了这一结论的参考资料中似乎只是一些统计数据的罗列，并未提及上面这个结论的推导过程。。。而从逻辑上讲，笔者猜测虽然用户\n\\(u\\) 准确的访问次数 \\(T_u\\) 比较难预测，但是这个访问次数的 lower\nbound \\(c_u\\)\n的难度也许会比较容易获得\n在 \\(c_u\\) 比较容易获得的 assumption\n下，paper 里给出了第一种在线算方法，总体算法流程如下\n\n\nalgo1\n\n其实跟 offline 的区别不大，就是用了 \\(c_u\\) 替换了 \\(T_u\\),\n在线分配时优先满足约束，而满足了约束后，就可以在 ad fatigue 的约束下去拿\nhighest revenue 的边\npaper 里绕了一大圈才给出了上面的算法流程，给出了各种符号、\nlemma、proposition，但是跟最终的结论的关系又不是非常大，有点凑字数的感觉；而且比较核心的部分即\n\\(c_u\\)\n是如何获取的则是基本没怎么提。。。\npaper 里还给出了这个算法的 competitive ratio 的 lower bound\n是（证明过程可参考 paper，这里略过）\n\\[\\begin{align} 1-\\frac{(\\overline{r} -\n\\underline{r})n_1}{\\underline{r}(n_1+n_2)} \\end{align}\\]\n各个符号含义如下\n\n\\(n_1=\\sum_{u=1}^{m} c_u\\):\n即所有用户的最少访问次数之和\n\n\\(n_2= T- n_1\\): \\(T\\) 是所有用户的实际访问次数之和\n\n\\(\\underline{r}=\\min_{i=1 \\dots n, u=1\n\\dots m}r_{iu}\\): 即所有 pair 的 revenue 的最小值\n\n\\(\\overline{r}=\\max_{i=1 \\dots n, u=1\n\\dots m}r_{iu}\\): 即所有 pair 的 revenue 的最大值\n\ngreedy algorithm\npaper 里还提到到了另一种贪心的算法，其 assumption 是每个 item/user\n都有一个 reserved revenue, 第 \\(i\\) 个\nitem 的 reserved revenue 记为 $\n_i$, 第 \\(u\\) 个 user 的 reserved\nrevenue 记为 \\(\\beta_u\\), 则有\n\\[\\begin{align} r_{iu} = \\alpha_i +\n\\beta_u \\end{align}\\]\n基于这个假设，paper 里给了如下的 online algorithm，并且证明了这里的\ncompetitive ratio 能跟最优的一致\n\n\nalgo2\n\n但是笔者觉得这里的 assumption 有很大的问题，即假设了每个 item 对所有\nuser 的 revenue 是一样的，这样显然不合理，因为正是每个 user\n的兴趣不同，才需要进行个性化的推荐，也是当前推荐 / 广告里各种算法和策略有效的原因，所以这里的算法笔者觉得可以看一下数学的原理，但是在实际应用并不现实\n小结\n总的来说，这篇 paper\n将在线广告的分配建模成一个经典的二部图问题，提供了离线和在线的解法，并给出了一些理论证明，笔者觉得可以学习一下其中的思想，但是如果照搬\npaper 的方法在实际应用的话比较难\n第一个 online algorithm 的问题在于 \\(c_u\\) 的获取并没有给出具体的方法，第二个\nonline algorithm 的问题在于本来的 assumption 就不合理\n笔者认为利用用户 \\(u\\)\n过去一段时间的访问次数来近似（或者通过预估的方式）作为其未来的访问次数\n\\(T_u\\)\n是一种可行的方法，但是这个方法还需要解决如下的问题\n（1）如果求解出来某个 item 需要分配给某个 user，paper\n里说了是直接展示，但是在实际的系统中往往是 “召回 + 精排” 的结构，需要如何保证这个\nitem 一定能被 user\n看到？或者说，实际中的系统往往已经有一套召回和精排的策略，怎么让这套新策略融入现有的系统中\n（2）user-item\n边的权值的物理含义是什么？缺失的边的权值怎么计算？\n（3）采用 id\n粒度过小可能导致整个图可能过大，进而导致图的求解比较困难，可以怎么做，如果做\nclustering 怎么保证两次聚类的结果差异性不大？\n（4）新 uid/item-id 如何处理？\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"Effective Go 摘记","url":"/2019/02/18/Effective%20Go%20%E6%91%98%E8%AE%B0/","content":"本文是 Effective\nGo 中的一些摘记，主要涉及 golang\n中的语法、技巧、风格等。为了尽可能保持原文意思，会通过英文记录相关的知识点。\n\nFormatting\n\nThe gofmt program (also available as go fmt, which\noperates at the package level rather than source file level) reads a Go\nprogram and emits the source in a standard style of indentation and\nvertical alignment, retaining and if necessary reformatting\ncomments.\n\nCommentary\n\nEvery package should have a package comment, a block\ncomment preceding the package clause. For multi-file packages,\nthe package comment only needs to be present in one file, and any one\nwill do. The package comment should introduce the package and provide\ninformation relevant to the package as a whole, for example\n/*Package regexp implements a simple library for regular expressions.The syntax of the regular expressions accepted is:    .......*/package regexp\nDoc comments work best as complete sentences, which allow a wide\nvariety of automated presentations. The first sentence should be\na one-sentence summary that starts with the name being\ndeclared. // Compile parses a regular expression and returns, if successful,// a Regexp that can be used to match against text.func Compile(str string) (*Regexp, error) {\nGrouping variables can indicate relationships\nbetween items, such as the fact that a set of variables is protected by\na mutex. var (    countLock   sync.Mutex    inputCount  uint32    outputCount uint32    errorCount  uint32)\n\nNames\n\nBy convention, packages are given lower case, single-word\nnames, no need for underscores or mixedCaps\nAnother convention is that the package name is the base\nname of its source directory; the package in\n\"src/encoding/base64\" is imported as \"encoding/base64\" but has name\nbase64, not encoding_base64 and not encodingBase64\nBy convention, one-method interfaces are named by the\nmethod name plus an -er suffix or similar modification to\nconstruct an agent noun: Reader, Writer, Formatter,\nCloseNotifier\nthe convention in Go is to use MixedCaps or\nmixedCaps rather than underscores to write multiword\nnames\n\nControl structures\n\nif and switch accept an\noptional initialization statement like that of\nfor if err := file.Chmod(0664); err != nil {    log.Print(err)    return err}\nIn a := declaration a variable v may appear even if\nit has already been declared, providing that there is at least\none other variable in the declaration that is being declared\nanew, otherwise an error\nno new variables on left side of := will occur\n// legal for errf, err := os.Open(name)d, err := f.Stat()// not legal for aa := 1a := 2// not legal for a and ba, b := 1, 1a, b := 2, 2\nFor strings, the range breaks out individual\nUnicode code points by parsing the UTF-8. Erroneous encodings\nconsume one byte and produce the replacement rune U+FFFD, rune\nis Go terminology for a single Unicode code point, similar to\nchar in other languages for pos, char := range \"日本\\x80語\" { // \\x80 is an illegal UTF-8 encoding    fmt.Printf(\"character %#U starts at byte position %d\\n\", char, pos)}/* outputcharacter U+65E5 '日' starts at byte position 0character U+672C '本' starts at byte position 3character U+FFFD '�' starts at byte position 6character U+8A9E '語' starts at byte position 7*/\nif the switchhas no expression it switches on true.\nIt's therefore possible—and idiomatic—to write an\nif-else-if-else chain as a switch. func unhex(c byte) byte {    switch {    case '0' &lt;= c &amp;&amp; c &lt;= '9':        return c - '0'    case 'a' &lt;= c &amp;&amp; c &lt;= 'f':        return c - 'a' + 10    case 'A' &lt;= c &amp;&amp; c &lt;= 'F':        return c - 'A' + 10    }    return 0}\n\nFunctions\n\nDeferred functions are executed in LIFO order(imagine it\nlike a stack), so the following code will cause 4 3 2 1 0 to be\nprinted when the function returns. for i := 0; i &lt; 5; i++ {    defer fmt.Printf(\"%d \", i)}\nThe arguments to deferred functions are evaluated when\nthe defer executes, not when the function executes\nfunc trace(s string) string {    fmt.Println(\"entering:\", s)    return s}func un(s string) {    fmt.Println(\"leaving:\", s)}func a() {    defer un(trace(\"a\"))    fmt.Println(\"in a\")}func b() {    defer un(trace(\"b\"))    fmt.Println(\"in b\")    a()}func main() {    b()}/*outputentering: bin bentering: ain aleaving: aleaving: b*/\n\nData\nnew v.s make\n\nGo has two allocation primitives, the built-in functions\nnew and make\nNew does not initialize the memory,\nnew(T) allocates zeroed storage for a new item of\ntype T and returns its address, that is a pointer to a newly\nallocated zero value of type T, it's helpful to arrange when designing\nyour data structures that the zero value of each type can be used\nwithout further initialization\nSometimes the zero value isn't good enough and an\ninitializing constructor is necessary, as in this example\nderived from package os func NewFile(fd int, name string) *File {    if fd &lt; 0 {        return nil    }    f := new(File)    f.fd = fd    f.name = name    f.dirinfo = nil    f.nepipe = 0    return f}\nWe can simplify it using a composite literal,\nwhich is an expression that creates a new instance each time it is\nevaluated(File{fd, name, nil, 0} in the following code). If\na composite literal contains no fields at all, it creates a zero value\nfor the type. The expressions new(File) and\n&amp;File{} are equivalent.\nfunc NewFile(fd int, name string) *File {    if fd &lt; 0 {        return nil    }    f := File{fd, name, nil, 0}    return &amp;f}\nNote that unlike in C, it's perfectly OK to return the\naddress of a local variable, the storage associated with the\nvariable survives after the function returns\nmake(T, args) serves a purpose different from\nnew(T), it **creates slices, maps, and channels only, and it returns an\ninitialized (not zeroed) value of type T (not *T)**\nThe reason for the distinction is that these three types(slices,\nmaps, and channels) represent, under the covers, references to\ndata structures that must be initialized before\nuse\n\narray v.s slice\n\nThere are major differences between the ways arrays work in Go\nand C. In Go,\n\nArrays are values. Assigning one array to another copies all\nthe elements.\nIn particular, if you pass an array to a function, it will\nreceive a copy of the array, not a pointer to it.\nThe size of an array is part of its type. The types [10]int\nand [20]int are distinct\n\nThe value property can be useful but also expensive; if\nyou want C-like behavior and efficiency, you can pass a pointer to the\narray\nA slice does not store any data, it just describes a section of\nan underlying array, so if you assign one slice to another, both\nrefer to the same array names := [4]string{\t\"John\",\t\"Paul\",\t\"George\",\t\"Ringo\",}a := names[0:2]b := names[1:3]b[0] = \"XXX\"fmt.Println(a, b)fmt.Println(names)// output// [John XXX] [XXX George]// [John XXX George Ringo]\nIf a function takes a slice argument, modification of\nelements of the slice will be visible to the caller, but append elements\nwon't, if you want to append elements to slice in function,\npass the address instead\nslices are variable-length, for a two-dimensional slice, it is\npossible to have each inner slice be a different length\ntext := LinesOfText{\t[]byte(\"Now is the time\"),\t[]byte(\"for all good gophers\"),\t[]byte(\"to bring some fun to the party.\"),}\n\nmap\n\nFor a map in golang like map[KeyType]ValueType,\nKeyType may be any type that is comparable\n,such as integers, floating point and complex numbers, strings,\npointers, interfaces (as long as the dynamic type supports\nequality).Slices cannot be used as map keys, because equality is not\ndefined on them, and ValueType may be any type at all, including\nanother map! hits := make(map[string]map[string]int)n := hits[\"/doc/\"][\"au\"]\nLike slices, maps hold references to an underlying data\nstructure. If you pass a map to a function that changes the\ncontents of the map, the changes will be visible in the\ncaller.\nAn attempt to fetch a map value with a key that is not\npresent in the map will return the zero value for the type of the\nentries in the map.The zero value is:\n\n0 for numeric types,\nfalse for the boolean type\n\"\" (the empty string) for strings.\n\nIf you need to judge whether a key in map, you can do this\nif val, ok := dict[\"foo\"]; ok {    //do something here}\n\nMethods\n\nMethods can be defined for any named type (except a pointer or an\ninterface); the receiver does not have to be a struct.\ntype ByteSlice []bytefunc (slice ByteSlice) Append(data []byte) []byte {    // Body exactly the same as the Append function defined above.}\nThe rule about pointers vs. values for receivers is that\nvalue methods can be invoked on pointers and values, but pointer\nmethods can only be invoked on pointers.\n\nInterfaces and other types\nInterfaces\n\nAn interface is defined as a set of method\nsignatures, and a type implements an interface by implementing\nits methods. A type can implement multiple interfaces\ntype Sequence []int// Methods required by sort.Interface.func (s Sequence) Len() int {    return len(s)}func (s Sequence) Less(i, j int) bool {    return s[i] &lt; s[j]}func (s Sequence) Swap(i, j int) {    s[i], s[j] = s[j], s[i]}\nYou can define your own interface and a value of\ninterface type can hold any value that implements those\nmethods. type Abser interface {\tAbs() float64}type MyFloat float64func (f MyFloat) Abs() float64 {\tif f &lt; 0 {\t\treturn float64(-f)\t}\treturn float64(f)}func main() {\tvar a Abser\tf := MyFloat(-math.Sqrt2)\ta = f  // a MyFloat implements Abser\tfmt.Println(a.Abs())}\n\nConcurrency\nShare by communicating\n\nConcurrent programming in many environments is made difficult by\nthe subtleties required to implement correct access to shared\nvariables.\nGo encourages a different approach in which shared values\nare passed around on channels and, in fact, never actively shared by\nseparate threads of execution，only one goroutine has access to the\nvalue at any given time.\nFor example，Reference counts may be best done by putting a mutex\naround an integer variable. But as a high-level approach, using channels\nto control access makes it easier to write clear, correct\nprograms.\n\nGoroutines\n\nA goroutine has a simple model: it is a function executing\nconcurrently with other goroutines in the same address\nspace.\nPrefix a function or method call with the go keyword\nto run the call in a new goroutine. When the call completes, the\ngoroutine exits silently，don't wait for it.\n\nChannels\n\nLike maps, channels are allocated with make, and the\nresulting value acts as a reference to an underlying data\nstructure\nThere are lots of nice idioms using channels. For example, if we\nlaunched a sort in the background and do sth else while waiting for the\ngoroutine to finish. A channel allows us to do so\nc := make(chan int)  // Allocate a channel.// Start the sort in a goroutine; when it completes, signal on the channel.go func() {    list.Sort()    c &lt;- 1  // Send a signal; value does not matter.}()doSomethingForAWhile()&lt;-c   // Wait for sort to finish; discard sent value.\nThe above code works becase receivers always block until\nthere is data to receive. As for the sender, if the channel is\nunbuffered, the sender blocks until the receiver has received the value.\nIf the channel has a buffer, the sender blocks only until the value has\nbeen copied to the buffer; if the buffer is full, this means waiting\nuntil some receiver has retrieved a value.\nA buffered channel can be used like a semaphore,\nfor instance to limit throughput. In the following example, incoming\nrequests are passed to handle, which sends a value into the channel,\nprocesses the request, and then receives a value from the channel to\nready the “semaphore” for the next consumer. The capacity of the\nchannel buffer limits the number of simultaneous calls to\nprocess. var sem = make(chan int, MaxOutstanding)func handle(r *Request) {    sem &lt;- 1    // Wait for active queue to drain.    process(r)  // May take a long time.    &lt;-sem       // Done; enable next request to run.}func Serve(queue chan *Request) {    for {        req := &lt;-queue        go handle(req)  // Don't wait for handle to finish.    }}\nThe above design has a problem: Serve creates a new goroutine for\nevery incoming request, even though only MaxOutstanding of them can run\nat any moment. As a result, the program can consume unlimited\nresources if the requests come in too fast. We can address that\ndeficiency by changing Serve to gate the creation of the goroutines.\nfunc Serve(queue chan *Request) {    for req := range queue {        sem &lt;- 1        go func() {            process(req) // Buggy; see explanation below.            &lt;-sem        }()    }}\nThe bug in the above code is that in a Go\nfor loop, the loop variable is reused for each iteration,\nso the req variable is shared across all goroutines. But we\nneed to make sure that req is unique for each goroutine. Here's one way\nto do that, passing the value of req as an argument to the\nclosure in the goroutine: func Serve(queue chan *Request) {    for req := range queue {        sem &lt;- 1        go func(req *Request) {            process(req)            &lt;-sem        }(req)    }}\nAnother solution is just to create a new variable with the same\nname, like the following code, req := req may seem odd, but\nit's legal and idiomatic in Go to do this. You get a fresh version of\nthe variable with the same name func Serve(queue chan *Request) {    for req := range queue {        req := req // Create new instance of req for the goroutine.        sem &lt;- 1        go func() {            process(req)            &lt;-sem        }()    }}\nAnother approach that manages resources well is to start\na fixed number of handle goroutines all reading from the request\nchannel. The number of goroutines limits the number of\nsimultaneous calls to process. func handle(queue chan *Request) {    for r := range queue {        process(r)    }}func Serve(clientRequests chan *Request, quit chan bool) {    // Start handlers    for i := 0; i &lt; MaxOutstanding; i++ {        go handle(clientRequests)    }    &lt;-quit  // Wait to be told to exit.}\n\nChannels of channels\n\nIn the example in the previous section, handle was an idealized\nhandler for a request but we didn't define the type it was handling.\nIf that type includes a channel on which to reply, each client\ncan provide its own path for the answer.\ntype Request struct {    args        []int    f           func([]int) int    resultChan  chan int}\nThe client provides a function and its arguments, as well as a\nchannel inside the request object on which to receive the answer.\nfunc sum(a []int) (s int) {    for _, v := range a {        s += v    }    return}request := &amp;Request{[]int{3, 4, 5}, sum, make(chan int)}// Send requestclientRequests &lt;- request\nOn the server side, the handler function is the only thing that\nchanges. func handle(queue chan *Request) {    for req := range queue {        req.resultChan &lt;- req.f(req.args)    }}\n\nErrors\n\nLibrary routines must often return some sort of error indication\nto the caller, it is easy to return a detailed error description\nalongside the normal return value with Go's multivalue return\nfeature\nType error is a simple built-in interface, and\nlibrary writer is free to implement this interface with a richer model\nunder the covers, making it possible not only to see the error but also\nto provide some context // built-in error interfacetype error interface {    Error() string}// custom error// PathError records an error and the operation and// file path that caused it.type PathError struct {    Op string    // \"open\", \"unlink\", etc.    Path string  // The associated file.    Err error    // Returned by the system call.}func (e *PathError) Error() string {    return e.Op + \" \" + e.Path + \": \" + e.Err.Error()}// PathError's Error will generate a string like this:// open /etc/passwx: no such file or directory\nThe usual way to report an error to a caller is to return an\nerror as an extra return value, but sometimes the error is\nunrecoverable, the program simply cannot continue. We can use\nthe built-in function panic in this case\npanic function takes a single argument of\narbitrary type—often a string—to be printed as the program\ndies. It's also a way to indicate that something impossible has\nhappened, for example,it is reasonable to use panic with\nthe failure of initialization, var user = os.Getenv(\"USER\")func init() {    if user == \"\" {        panic(\"no value for $USER\")    }}\nWhen panic is called, including implicitly for\nrun-time errors such as indexing a slice out of bounds or failing a type\nassertion, it immediately stops execution of the current\nfunction and begins unwinding the stack of the goroutine, running any\ndeferred functions along the way. If that unwinding reaches the\ntop of the goroutine's stack, the program dies. But it is possible to\nuse the built-in function recover to regain control\nof the goroutine and resume normal execution.\nA call to recover stops the unwinding and returns\nthe argument passed to panic. Because the only code that\nruns while unwinding is inside deferred functions,\nrecover is only useful inside deferred\nfunctions.\nOne application of recover is to shut down a\nfailing goroutine inside a server without killing the other executing\ngoroutines. In this example, if do(work) panics,\nthe result will be logged and the goroutine will exit cleanly without\ndisturbing the others func server(workChan &lt;-chan *Work) {    for work := range workChan {        go safelyDo(work)    }}func safelyDo(work *Work) {    defer func() {        if err := recover(); err != nil {            log.Println(\"work failed:\", err)        }    }()    do(work)}\n\nSome Syntax\n\nint(math.Pow(float64(x), float64(count)))\nAtoi (string to int) and Itoa (int to\nstring). i, err := strconv.Atoi(\"-42\")s := strconv.Itoa(-42)\nconcate string s1 and s2\nbuffer := bytes.NewBufferString(s1)buffer.WriteString(s2)buffer.String()\nappend function // append multiple elementsx := []int{1,2,3}x = append(x, 4, 5, 6)// append a slice x := []int{1,2,3}y := []int{4,5,6}x = append(x, y...)\n\n","categories":["go"],"tags":["go"]},{"title":"Exposure Bias In Machine Learning","url":"/2021/04/03/Exposure%20Bias%20In%20Machine%20Learning/","content":"机器学习本质上是在学习数据的分布，其有效性的假设是模型 training 和\nserving 时的数据是独立同分布 (Independent and Identically Distributed,\nIID) 的，但是在实际应用中，由于采样有偏、具体场景等约束， training\n的样本与 serving 时的样本并不是 IID 的。在广告场景下，最典型的就是训练\ncvr 模型时，训练样本都是 post clicked 的，但是 serving 时，cvr\n模型面临的是所有被召回的样本；这类问题也被称为 exposure bias 或 sample\nselection bias，除了 exposure bias，position bias 等也是常见的\nbias。\n本文首先会简单介绍一些机器学习中的常见 bias，并着重介绍上面提到的\nexposure bias (也叫 sample selection bias) 的在当前的一些解决思路，\n笔者将其总结为 Data Augmentation、IPS 和 Domain Adaption\n三大类方法。\n\nBias In Recommender System\n除了本文要重点介绍的 exposure bias，这篇综述 Bias and Debias in Recommender\nSystem: A Survey and Future Directions\n描述了当前的推荐系统中存在的若干种 bias，paper 将当前的推荐系统划分为\nUser、Data、Model 三个大模块，并将各个模块的 iteraction 导致的 7 种 bias\n归纳成下图\n\n\nBias in ML\n\n\nUser-&gt;Data: 产生训练数据的过程，也是 Bias 的最主要来源\n\nPosition Bias (implicit)：用户更倾向于和位置靠前的物品进行交互\n\nExposure/Observation\nBias (implicit)：带标签的数据都是曝光过的，未曝光的数据无法确定其标签\n\nSelection\nBias (explicit)：用户倾向于给自己喜欢或者不喜欢的物品进行打分 (导致\nexposure bias 的一个重要原因，不少 paper 将这个 bias 也当做 exposure\nbias)\n\nConformity\nBias (explicit)：用户打分的分数倾向于和群体观点保持一致\n\n Data-&gt;Model: 利用数据训练出模型的过程\n\nInductive\nBias: 指的是模型为了泛化性而做出的一些 assumption，如 Occam's\nrazor，SVM 假设线性可分的 assumption 等，这个 bias\n没有带来显示的缺陷\n\n Model-&gt;User Interaction: 指的是模型预估的过程\n\nPopularity\nBias：指的是长尾效应，热门物品会得到更高的曝光概率，因为模型会更倾向于推荐这些物品（unbalanced\ntraining data 引起）\n\nUnfairness: 一些预估结果带有性别歧视、种族歧视等（unbalanced\ntraining data 引起\n\n\n上面的各种 bias 的起因、影响以及一些解决思路可归纳为下表\n\n\nBias details Table\n\n可以看到，上面这些 bias 并不是孤立的，而是相互影响和恶化的，其中\nposition bias 和 exposure bias\n又是最为常见且相关研究较多的一个领域，下面主要详细描述一些应对 exposure\nbias 的方法，值得注意的是，这里的 selection bias 跟 exposure bias\n面临的问题是一致的，因此这里的方法也会一并归纳； bias\n相应的方法可以参考上面的 paper。\nSolutions to Exposure Bias\nexposure bias 也被称为 Sample Selection Bias (SSB), 本质上是一个\ntraining 和 serving\n不一致的问题。这个问题往往是由于具体业务场景的限制，导致\ntraining data 中的样本只是其 serving\n时的很小一部分，因为其他的样本没被曝光 / 点击，导致了无法得到其\nlabel。\n如文章开头提到的 cvr\n模型，对于那些不被点击的样本是无法得知其是否被转化的；同样地，在 ctr\n模型中，那些没有曝光机会的样本是无法得知其是否被点击的了；但是在 serving\n阶段，ctr/cvr\n模型面对的是所有的样本，而其中有很多是从未曝光过的，因此便导致了一个\ntraining 与 serving 不一致的问题。\n针对这个问题，当前有以下几种解决思路 (不限于上面综述的 paper)\n\nData\nAugmentation：这个是最朴素的想法，就是尽可能将那些没进入训练数据集的样本利用上，因此不少研究也是给\nunobserved/unclicked 的样本打上一个相对准确的\nlabel；而这里面也主要分为下面三类方法\n\n\n最粗暴的是把所有未曝光的样本当做 negative\nsample，然后基于不同策略给这些样本不同的权重\n\n训练一个 imputation model，然后通过 imputation model\n来预估未曝光样本的 label\n\n通过 multitask 的方式建模，训练使用前一转化目标的全量样本 (ESMM)\n\n\nIPS(Inverse Propensity Score):\n这个方法的假设是样本被曝光或点击服一个伯努利分布，然后从概率论推导出：只要给每个曝光的样本加权 (权重即为\ninverse propensity\nscore)，最终在曝光的样本上求得的期望等于在全量样本上的期望；实际上，这个方法的思想就是\nimportance\nsampling\nDomain Adaption: 类似 transfer learning\n的思想，将曝光 / 点击的样本视为 source domain，全部样本视为 target\ndomain；通过 domain adaption 的一些方法去进行 debias\n\nData Augmentation\n上面提到了，最朴素的想法是将那些未被观测到的样本利用上，而这里面又可分为三大类方法：all\nnagative with confidence、imputation model 和 multitask learning\nall nagative with confidence\n第一类方法是将所有未被观测到的样本都当做负样本，而这里的核心是如何给每个样本一个合理的\nconfidence，其实就是样本的加权，上面提到的综述中介绍了三种方法，分别是\nHeuristic、Sampling、Exposure-based model\n\nHeuristic 尝试将 user activity level、user preference、item\npopularity、user-item feature similarity 等作为 (user, item) pair 的\nconfidence,\n其思想都是认为用户活跃度越高、商品越热门、用户 - 商品匹配度越高，该样本的可信度 (confidence)\n越高；但是这种方法往往可行性不够高，因为这个 confidence\n实际的值是比较难获取的，其量纲以及需要对庞大的数据集都生成这个\nconfidence，难度是很大的。\nSampling：TODO\nExposure-based mode：TODO\n\nimputation model\n第二种方法也很直观，就是训练一个 imputation model\n为那些未曝光 / 点击的样本打上标签，然后基于这些 imputed label\n来训练模型；而 imputation model 可单独训练，也与目标的模型做 joint\ntraining；这个方法的弊端也很明显，就是生成的 imputed label 缺少绝对的\nground truth 来衡量其效果，实际上，这个是所有直接为样本生成 label\n的方法的弊端。\nmultitask learning\n第三种方法是基于 multitask\n的方法，主要思想就是同时建模当前的任务及其更浅一层的任务，进而间接利用那些没被曝光 / 点击的数据；其中为人熟知的是阿里在\n2018 年发表的 ESMM, Entire Space Multi-Task\nModel: An Eﬀective Approach for Estimating Post-Click Conversion\nRate，这篇 paper 主要针对的是 cvr 模型中缺少未点击的样本带来的\nbias，增加了两个 auxiliary task (CTR 和 CTCVR)\n来缓解这个问题，总体的模型结构如下图所示\n\n\nESMM\n\n训练的 loss function 如下所示 (\\(y\\)\n表示点击事件，\\(z\\) 表示转化事件)\n\\[\\begin{align}\nL(\\theta_{ctr},\\theta_{cvr}) &amp;=\n\\sum_{i=1}^{N}l(y_i,f(x_i;\\theta_{ctr})) \\notag \\\\\\  \n&amp;+\\sum_{i=1}^{N}l(y_i\\&amp;\nz_i,f(x_i;\\theta_{ctr})×f(x_i;\\theta_{cvr})) \\notag  \n\\end{align}\\]\n可以将模型利用的样本视为 (show, click, convert) pair 对，则相比传统\ncvr 模型，ESMM\n能够利用那些曝光未点击的样本 (即下表中最后一列，0/1 表示对应的中间链路事件是否发生)\n\n\n\nshow\nclick\nconvert\n\n\n\n\n1\n1\n1\n\n\n1\n1\n0\n\n\n1\n0\n0\n\n\n\n在预估时，通过如下条件概率公式，能够从统计意义上保证 cvr\n的值是无偏的\n\\[\\begin{align} p(z=1|y=1,x) =\n\\frac{p(y=1,z=1|x)}{p(y=1|x)} \\end{align}\\]\n实验效果如下，各种方法的具体含义如下\n\n\nESMM 效果对比\n\n\n BASE: 普通建模 CVR 模型\n\nAMAN: 对负样本做 negative sampling\n\nOVERSAMPLING: 对正样本做 over sampling\n\nUNBIAS: 参考 这篇\npaper 将 pCTR 当做 rejection probability\n\nDIVISION: 分别建模 pCTR 和 pCTCVR，然后利用上面的公式计算 pCVR\n\nESMM-NS: 不 share bottom 的 ESMM\n\nESMM: share bottom 的 ESMM\n\nIPS(Inverse Propensity Score)\nInverse Propensity Score 的做法是为每个有 label 的样本预估一个\npropensity score (倾向性得分)，其含义直观来说就是样本进入训练集 (被标记\nlabel) 的概率，如对于 CTR 模型，propensity 就是曝光的概率，对于 CVR\n模型，propensity 就是点击的概率\nIPS 实际上借鉴了 Importance Sampling\n的思想，通过给每个样本一个概率值作为权重，从统计学上证明了基于观测到的数据求得的期望与全量数据的期望是一致，其推导过程可简单描述为如下方式\n记 \\(L\\) 是观测到 label\n的样本数，\\(L'\\)\n则是同时包含了那些没被观测到的样本 (\\(L'\n\\gg L\\)); 则无偏的优化目标应该是 (\\(y_i\\)、\\(p_i\\) 分别表示 label 与预估值)\n\\[\\begin{align} \\min \\sum_{i=1}^{L'}\nl(y_i, p_i) \\tag{1} \\end{align}\\]\n现在令第 \\(i\\)\n个样本的是否被观测到记为 \\(O_i \\in \\lbrace 0,\n1 \\rbrace\\)，则可假设 \\(O_i\\)\n服从一个伯努利分布即 \\(O_i \\sim\nBern(z_i)\\), 这里的 \\(z_i\\)\n是样本 \\(i\\)\n被观测到的概率，则上面的优化问题可写成如下形式\n\\[\\begin{align}  \nLoss &amp;= \\sum_{i=1}^{L'} l(y_i, p_i)\\notag \\\\\\  \n&amp;= \\sum_{i=1}^{L'} \\frac{l(y_i, p_i)}{z_i} E_{O}(O_i)\\notag\n\\\\\\  \n&amp;= E_{O}(\\sum_{i=1}^{L'} \\frac{l(y_i, p_i)}{z_i} O_i)\\notag\n\\\\\\  \n&amp;= \\sum_{i=1}^{L} \\frac{l(y_i, p_i)}{z_i}\\notag  \n\\end{align}\\]\n则上面问题 (1) 可被写成如下形式，\n即可通过观测到的数据进行模型的训练，而 \\(z_i\\) 可利用 label\n为是否被观测到的数据进行训练，训练方式可以是单独训练或与这个 task 做\njoint training。\n\\[\\begin{align} \\min \\sum_{i=1}^{L}\n\\frac{l(y_i, p_i)}{z_i} \\tag{2} \\end{align}\\]\n而如果套用 importance\nsampling 的方法，其实也能得到上面问题 (2) 的形式，在观测到的样本中，\n样本 \\(i\\) 被采样的概率是 \\(z_i\\),\n而在全部样本中，由于每个样本都会被采样到，因此其采样概率是\n1，即加权的系数是 \\(\\frac{1}{z_i}\\)\n此外，也有方法同时融合了第一类方法中的 imputation model 和这里的 IPS\n方法，被称为 Doubly Robust\nMethod，则其损失函数可写成如下形式（\\(\\sigma_i\\) 是通过 imputation model 得到的\nlabel）\n\\[\\begin{align} \\min \\sum_{i=1}^{L}\n(\\frac{l(y_i, p_i)}{z_i} - l(\\sigma_i, p_i)) + \\sum_{i=1}^{L'}\nl(\\sigma_i, p_i) \\end{align}\\]\n上述方法在 Improving\nAd Click Prediction by Considering Non-displayed Events\n有较为详细的描述，可以参考一下\nDomain Adaption\nDomain\nAdatption 可以认为是 transfer learning 的一个子领域，根据 wiki\n的说法其目标是\n\naim at learning from a source data distribution a well performing\nmodel on a different (but related) target data distribution.\n\n这个说法比较宽泛，实际中用到的方法可分为下面四大类 (from wiki)，实际上跟我们上面提到的方法的思想都比较相似\n\nReweighting algorithms: reweight the source labeled\nsample such that it \"looks like\" the target sample (in terms of the\nerror measure considered).\n\nIterative algorithms：iteratively \"auto-labeling\"\nthe target examples\n\nSearch of a common representation space：construct\na common representation space for the two domains\n\nHierarchical Bayesian Model：build a factorization\nmodel to derive domain-dependent latent representations allowing both\ndomain-specific and globally shared latent factors\n\n而在 Exposure Bias 场景下，观测到的数据被当做 source\ndomain，全量数据被当做 target domain，利用 domain adaption 解决 exposure\nbias 比较有代表性的 paper 是阿里在 2020 发表的 ESAM，ESAM: Discriminative gDomain\nAdaptation with Non-Displayed Items to Improve Long-Tail\nPerformance\nESAM (Entire Space Adaption Modelling) 跟上面提到的 ESMM (Entire Space\nMulti-Task Model) 名字很相似，要解决的问题也很相似，但是前者是召回场景，\n后者是 cvr 场景；\nESAM\n总体如下所示，各符号含义如下（基本上就是向量化召回的涉及到的概念）\n\n\\(q\\): query\n\n\\(d^s\\): 有曝光的 item\n\n\\(d^t\\): 没有曝光的 item\n\n\\(f_q\\), \\(f_d\\): 将 query 和 item 映射成 embedding\n的函数\n\n\\(v_{q}\\), \\(v_{d}\\): query 和 item 被 \\(f_q\\), \\(f_d\\) 映射出来的 embedding\n\n\\(f_s\\): 计算 \\(v_{q}\\), \\(v_{d}\\) 相似性的函数，常见的是內积\n\n\\(L_s\\): 基于 \\(f_s\\) 吐出的预估值 \\(S_{c_{q,d^s}}\\) 计算的损失函数，常见的有\npoint-wise、pair-wise、list-wise 三大类\n\n\\(L_{DA}\\)、\\(L_{DC}^{c}\\)、\\(L_{DC}^{p}\\): paper 中提出的缓解 exposure\nbias 的三种途径，以 loss 形式叠加在原始的 \\(L_s\\) 上，也是下文要重点展开描述的部分\n\n\n\n ESAMOverview\n\nESAM 的核心在于叠加在原始损失函数 \\(L_s\\) 上的三项：\\(L_{DA}\\)、\\(L_{DC}^{c}\\)、\\(L_{DC}^{p}\\)，下面会分别描述这三项的含义和计算方式\n\\(L_{DA}\\)：Domain Adaptation with Attribute\nCorrelation Alignment\n这个 loss 项的出发点是两个特征在 source domain 和 target domain\n中的相关性是保持一致的，如下图所示，左边的图认为 price 跟 brand\n是具有强相关的、brand 跟 material 中度相关、price 跟 material 弱相关，而\n\\(L_{DA}\\)\n这个损失项是基于下面两个假设产生的\n\n相关性在 source domain 和 target domain 都是一致的\n\n原始的特征 (即 price、brand、material) 会被映射到 embedding\n中某一维或几维\n\n\n\n L_DA\n\n因此，可以设计 loss 项令两个特征的相关性在 source domain 和\ntarget domain 尽可能保持一致 , 而这里的相关性可采用协方差\n因此，\\(L_DA\\) 项计算方式如下，令\n\\(D^{s} = \\lbrack v_{d_1^s};v_{d_2^s};\\dots\nv_{d_n^s} \\rbrack \\in \\mathbb{R}^{n×L}\\) 为 n 个 source domain\n样本映射出来的 embedding matrix，而 \\(D^{s} =\n\\lbrack v_{d_1^t};v_{d_2^t};\\dots v_{d_n^t} \\rbrack \\in\n\\mathbb{R}^{n×L}\\) 则是 n 个 target domain 样本映射出来的\nembedding matrix, 每个 \\(v\\)\n是一个长度为 \\(L\\) 的向量\n如果将每个 embedding 相同的维度提取出来作为一个 vector \\(h\\)，则 \\(D^{s}\\) 和 \\(D^{t}\\) 也可写成如下形式，每个 \\(h\\) 是一个长度为 \\(n\\) 的向量\n\\(D^{s}=\\lbrack h_{1}^{s},h_{2}^{s};\\dots\nh_{L}^{s} \\rbrack \\in \\mathbb{R}^{n×L}\\)\n\\(D^{t}=\\lbrack h_{1}^{t},h_{2}^{t};\\dots\nh_{L}^{t} \\rbrack \\in \\mathbb{R}^{n×L}\\)\n则 \\(L_{DA}\\) 计算方式如下\n\\[\\begin{align}\nL_{DA} &amp;= \\frac{1}{L^2}\\sum_{(j,k)}({h_j^s}^T{h_k^s} -\n{h_j^t}^T{h_k^t})^2 \\notag  \\\\\\  \n&amp;= \\frac{1}{L^2} ||Cov(D^s) - Cov(D^t)||_F^2 \\notag  \n\\end{align}\\]\n上式中的 \\(Cov(D^s) \\in\n\\mathbb{R}^{L×L}\\) 和 \\(Cov(D^t) \\in\n\\mathbb{R}^{L×L}\\) 分别代表 source domain 和 target domain 的\ncovariance matrix，此外，笔者认为上面的公式其实表达不完全准确，因为 covariance\n计算还需要减去均值的，上面并没有这么做\n加上改了这一项后，可以认为 source domain 和 target domain\n在向量空间中的分布变化如下\n\n\nESAM_L1\n\n\\(L_{DC}^{c}\\)：Center-Wise Clustering for\nSource Clustering.\n第二项 loss \\(L_{DC}^{c}\\) \\(L_{DC}^{c}\\) 跟人脸识别中最早提出的 center\nloss\n很相似，就是让相同类型的样本在向量空间中尽可能接近，在广告的场景下这个类型可以是\nclick、non-click、purchase 等；此外，还会令不同类型的样本在向量空间中的距离尽可能远；这个思想比较好理解\n而为了定义出距离，会为每一个类型定义出一个聚类中心，聚类中心会被参数化；具体计算方式如下\n\\[\\begin{align}\nL_{DC}^{c} &amp;= \\sum_{j=1}^{n} \\max(0,\n||\\frac{v_{d_j^s}}{||v_{d_j^s}||} - c_{q}^{y_j^s}||_{2}^{2} -\nm_1)  \\notag \\\\\\  \n&amp;+ \\sum_{k=1}^{n_y} \\sum_{u=k+1}^{n_y} \\max(0, m2 - ||c_{q}^{k} -\nc_{q}^{u}||_{2}^{2})\\notag  \n\\end{align}\\]\n上面的公式中 \\(m_1\\) 和 \\(m_2\\) 分别是两个\nmargin，表示样本距离其类簇距离小于 \\(m_1\\) 和 类簇之间的距离大于 \\(m_2\\) 的情况下无需优化；同时假设所有样本有\n\\(n_y\\) 种类型，第 \\(k\\) 种类型 (即样本 label 为 \\(Y_k\\)) 样本的 center 是 \\(c_{q}^{k}\\)，其定义为当前训练样本中类型为\n\\(k\\)\n的样本的向量的中心，表示如下 (\\(\\delta(cond)\\) 函数的值为 1 (cond=true) 或\n0 (cond=false))\n\\[\\begin{align} c_q^k =\n\\frac{\\sum_{j=1}^{n} \\delta(y_j^s =\nY_k)\\frac{v_{d_j^s}}{||v_{d_j^s}||}}{\\sum_{j=1}^{n} \\delta(y_j^s = Y_k)}\n\\end{align}\\]\n则加入 \\(L_{DC}^{c}\\)\n这一项后，source domain 和 target domain\n在向量空间中的分布变化如下，可以看到，虽然 target domain\n中的样本具有高内聚性，但是其聚类的簇可能是错误，其原因是对于 target\ndomain 中的样本，目前为止都没有加入 label 信息，而这便是下一项\nloss \\(L_{DC}^{p}\\) 要解决的问题\n\n\nESAM_Loss2\n\n\\(L_{DC}^{p}\\)：Self-Training for Target\nClustering.\n从这项 loss 的描述中的 self\ntraining，可以猜测其做法是为 target domain 中 unlabeled\n的样本打上标签用于训练模型，这是 semi supervised learning 中常见做法，而\npaper 中并没有直接为 unlabeled 的样本打上标签，而是通过 loss\n\\(L_{DC}^{p}\\)\n较为巧妙地实现了这一点，其表达如下\n\\[\\begin{align} L_{DC}^{p} =\n-\\frac{\\sum_{j=1}^{n} \\delta(S_{c_{q,d^t}} &lt; p_1 | S_{c_{q,d^t}} &gt;\np_2)S_{c_{q,d^t}} \\log S_{c_{q,d^t}}} {\\sum_{j=1}^{n}\n\\delta(S_{c_{q,d^t}} &lt; p_1 | S_{c_{q,d^t}} &gt; p_2)}\n\\end{align}\\]\n其中 \\(p_1\\) 和 \\(p_2\\)\n是两个阈值，表示含义是样本的置信度达到一定程度才认为其是负样本 / 正样本，则这个\nloss 的目标会让预估值小于 \\(p_1\\)\n的样本预估值更接近 0，预估值大于 \\(p_2\\) 的样本的预估值更接近\n1，原因见可以看下面画出的 \\(-p(x)\n\\log p(x)\\) 的函数，直接令这一项最小即可达到 self-training\n的目标\n\n\nEntropy regularization\n\n加入 \\(L_{DC}^{p}\\) 这一项后，source\ndomain 和 target domain 在向量空间中的分布变化如下，也是 ESAM\n的最终形态\n\n\nESAM_Loss3\n\n则最终的 loss 为下式所示，其中 \\(\\lambda_1\\), \\(\\lambda_2\\), \\(\\lambda_3\\) 是三个超参，通过 gradient\ndescent 即可求解\n\\[\\begin{align} L_{all} = L_s + \\lambda_1\nL_{DA} + \\lambda_2 L_{DC}^{c} + \\lambda_3 L_{DC}^{p}\n\\end{align}\\]\nSummary\n综上，本文主要针对 exposure bias 介绍了三大类方法，分别是 Data\nAugmentation、IPS 和 Domain Adaption，三类方法主要思想如下\n\nData Augmentation: 即利用那些 unlabeled\n的样本，方法较多，如将所有样本都当做负样本、训练一个 imputation model\n来给 unlabeled 的样本打上标签、通过 multitask 方式利用等\n\nIPS：只利用曝光的样本，从概率论推导出给曝光样本进行合适的加权后，基于曝光的样本求的期望是无偏的\n\nDomain Adaption：利用了 unlabeled 的样本，主要分析了 ESAM 这篇\npaper, 同时通过在 loss 上添加了三项，能够令曝光和未曝光的 item\n训练得到的向量空间尽可能保持一致，这三项的 loss\n背后的思想也值得参考\n\n此外，上面的一些方法虽然从理论上看起来比较\nfancy，但是根据笔者当前的工作经验，实际中应用这些方法，还需要考虑到模型的迭代效率、、理论的假设是否符合实际等等；比如说\ndata augmentation/ESAM\n方法会导致数据量增加不止一个量级，而这会势必会导致训练时长增加，即使这个方法有效果，也要考虑带来的收益以及牺牲的迭代效率和机器资源等兑换是否划算，\n或者考虑如何改进采样策略尽可能打平样本量。\n","categories":["机器学习"],"tags":["计算广告","机器学习"]},{"title":"Google C++ Style Guide 阅读笔记","url":"/2018/10/02/Google%20C++%20Style%20Guide%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"统一规范的代码风格在团队协作中非常重要，在若干的风格标准中，Google\nC++ Style 又是较为被认可的，本文是阅读了 Google\nC++ Style Guide\n中第六 (命名约定)、七 (注释)、八 (格式) 章后的一些笔记，主要涉及代码的一些基本规范。需要注意的是，各种规范之间并没有绝对的好坏之分，只要团队保持一致即可。\n\n命名约定\n一般性命名规则\n\n给出描述性的名称 (某些为人熟知的词可用缩写如 DNS)\n\n// 使用int num_errors; // Good.int num_completed_connections; // Good.// 不使用int nerr; // Bad - ambiguous abbreviation.int n_comp_conns; // Bad - ambiguous abbreviation.\n\n变量一般是名词 (如上）， 函数名一般是指令性的（如\nopen_file()、 set_num_errors())\n\n文件命名\n\n文件名要全部小写，可以 ** 包含下划线（_）** ,\nC++ 文件以 .cc 结尾，头文件以 .h 结尾\n尽量让文件名更加明确， 如 http_server_logs.h 比\nlogs.h 要好\n定义类时文件名一般成对出现，如 foo_bar.h 和\nfoo_bar.cc，对应类 FooBar\n\n类型命名\n所有类型命名，包括类、结构体、类型定义（typedef）、枚举，均使用相同约定：每个单词以大写字母开头，不包含下划线，其实就是驼峰法，\n如下\n// classes and structsclass UrlTable { ...class UrlTableTester { ...struct UrlTableProperties { ...// typedefstypedef hash_map&lt;UrlTableProperties *, string&gt; PropertiesMap;// enumsenum UrlTableErrors { ...\n变量命名\n\n变量名一律小写，单词间以下划线相连，类的成员变量以下划线结尾，结构体的数据成员可以和普通变量一样，不用像类那样接下划线，如 \n\nstring table_name; // OK - uses underscore.string tableName; // Bad - mixed caseclass UrlTableProperties {  string name_;  int num_entries_;}struct UrlTableProperties {  string name;  int num_entries;}\n\n对全局变量没有特别要求，少用就好，可以以 g_\n或其他易与局部变量区分的标志为前缀\n\n常量命名\n\n在名称前加 k, 且 k 后接大写字母开头的单词，如\nkDaysInAWeek\n\n函数命名\n函数这里做了两种分类：普通函数 (regular functions) 和存取函数 (accessors\nand mutators)\n\n普通函数: 每个单词首字母大写，没有下划线 , 如\nMyExcitingMethod()\n存取函数: 与变量名匹配：如\nset_my_exciting_member_variable()\n\nclass MyClass {  public:    ...    int num_entries() const { return num_entries_; }    void set_num_entries(int num_entries) { num_entries_ = num_entries; }  private:   int num_entries_;}\n枚举命名\n\n枚举值应全部大写，单词间以下划线相连 (宏的命名也与其保持一致)：\nMY_EXCITING_ENUM_VALUE\n枚举名称属于类型，因此大小写混合，即\nUrlTableErrors。\n\nenum UrlTableErrors {    OK = 0,    ERROR_OUT_OF_MEMORY,    ERROR_MALFORMED_INPUT,};\n小结\n\n总体规则：不要随意缩写，如果说 ChangeLocalValue\n写作 ChgLocVal 还有情可原的话，把 ModifyPlayerName 写作 MdfPlyNm\n就太过分了，除函数名可适当为动词外，其他命名尽量使用清晰易懂的名词\n宏、枚举等使用全部大写 + 下划线；\n变量（含类、结构体成员变量）、文件、命名空间、存取函数等使用全部小写 + 下划线\n，类成员变量以下划线结尾，全局变量以 g_开头；\n普通函数、类型（含类与结构体、枚举类型）、常量等使用大小写混合，不含下划线；\n\n注释\n注释对保证代码可读性至为重要，当然也要记住，注释的确很重要，但最好的代码本身就是文档（\nself-documenting），类型和变量命名意义明确要比通过注释解释模糊的命名好得多。\n下面的规则描述了应该注释什么、注释在哪儿。比通过注释解释模糊的命名好得多\n文件注释\n在每一个文件开头加入版权公告，然后是文件内容描述。\n1. 版权公告\n每一文件包含以下项，依次是： 1) 版权（copyright\nstatement） ：如 Copyright 2008 Google Inc.； 2)\n许可版本（license boilerplate）\n：为项目选择合适的许可证版本，如 Apache 2.0、BSD、 LGPL、 GPL； 3)\n作者（author line） ：标识文件的原始作者\n如果你对其他人创建的文件做了重大修改，将你的信息添加到作者信息里，这样当其他人对该文件有疑问时可以知道该联系谁。\n2. 文件内容\n每一个文件版权许可及作者信息后，都要对文件内容进行注释说明。\n1）.h\n文件要对所声明的类的功能和用法作简单说明 2)\n.cc\n文件包含了更多的实现细节或算法讨论，如果你感觉这些实现细节或算法讨论对于阅读有帮助，可以把\n.cc 中的注释放到 .h 中，并在 .cc\n中指出文档在 .h 中。\n不要单纯在 .h 和 .cc\n间复制注释，复制的注释偏离了实际意义。\n类注释\n每个类的定义要附着描述类的功能和用法的注释，如\n// Iterates over the contents of a GargantuanTable. Sample usage:// GargantuanTable_Iterator* iter = table-&gt;NewIterator();// for (iter-&gt;Seek(\"foo\"); !iter-&gt;done(); iter-&gt;Next()) {// process(iter-&gt;key(), iter-&gt;value());// }// delete iter;class GargantuanTableIterator {  ...};\n函数注释\n注释于声明之前，描述函数功能及用法，注释使用描述式（\"opens\nthe file\"）而非指令式（\"open the\nfile\"）；注释只是为了描述函数而不是告诉函数做什么。\n// Returns an iterator for this table. It is the client's// responsibility to delete the iterator when it is done with it,// and it must not use the iterator once the GargantuanTable object// on which the iterator was created has been deleted.//// The iterator is initially positioned at the beginning of the table.//// This method is equivalent to:// Iterator* iter = table-&gt;NewIterator();// iter-&gt;Seek(\"\");// return iter;// If you are going to immediately seek to another place in the// returned iterator, it will be faster to use NewIterator()// and avoid the extra seek.Iterator* GetIterator() const;\n函数定义处注释的内容:\n每个函数定义时要以注释说明函数功能和实现要点，如使用的漂亮代码、实现的简要步骤、如此实现的理由、为什么前半部分要加锁而后半部分不需要。\n简要说明函数功能是可以的，重点要放在如何实现上。\n变量注释\n通常变量名本身足以很好说明变量用途，特定情况下，需要额外注释说明\n类数据成员\n每个类数据成员（也叫实例变量或成员变量）应注释说明用途，如果变量可以接受\nNULL 或 - 1 等警戒值（ sentinel values） ，须说明之，如：\nprivate:  // Keeps track of the total number of entries in the table.  // Used to ensure we do not go over the limit. -1 means  // that we don't yet know how many entries the table has.  int num_total_entries_;\n全局变量（常量）\n和数据成员相似，所有全局变量（常量）也应注释说明含义及用途，如：\n// The total number of tests cases that we run through in this regression test.const int kNumTestCases = 6\nTODO 注释\n对那些临时的、短期的解决方案，或已经够好但并不完美的代码使用\nTODO 注释。\n这样的注释要使用全大写的字符串 TODO，后面括号里加上你的大名、邮\n件地址等，还可以加上冒号目的是可以根据统一的 TODO 格式进行查找：\n// TODO(kl@gmail.com): Use a \"*\" here for concatenation operator.// TODO(Zeke) change this to use relations.\n如果加上是为了在 “将来某一天做某事”，可以加上一个特定的时间（\"Fix by\nNovember 2005\"）或事件（\"Remove this code when all clients can handle\nXML responses.\"）。\n格式\n代码风格和格式确实比较随意，但一个项目中所有人遵循同一风格是非常容易的，作为个人未必同意下述格式规则的每一处，但整个项目服从统一的编程风格是很重要的，这样做才能让所有人在阅读和理解代码时更加容易。\n空格还是制表位\n只使用空格，每次缩进 2 个空格。\n使用空格进行缩进，不要在代码中使用 tabs，设定编辑器将 tab\n转为空格。\n函数声明与定义\n返回类型和函数名在同一行，合适的话，参数也放在同一行。\n函数看上去像这样：\nReturnType ClassName::FunctionName(Type par_name1, Type par_name2) {  DoSomething();  ...}\n如果同一行文本较多，容不下所有参数：\nReturnType ClassName::ReallyLongFunctionName(Type par_name1,                                             Type par_name2,                                             Type par_name3) {  DoSomething();  ...}\n甚至连第一个参数都放不下\nReturnType LongClassName::ReallyReallyReallyLongFunctionName(    Type par_name1, // 4 space indent    Type par_name2,    Type par_name3) {  DoSomething(); // 2 space indent...}\n注意以下几点 1) 返回值总是和函数名在同一行； 2) 左圆括号（ open\nparenthesis） 总是和函数名在同一行； 3)\n函数名和左圆括号间没有空格； 4)\n圆括号与参数间没有空格； 5)\n左大括号总在最后一个参数同一行的末尾处； 6)\n右大括号总是单独位于函数最后一行； 7)\n右圆括号和左大括号间总是有一个空格； 8)\n函数声明和实现处的所有形参名称必须保持一致； 9)\n所有形参应尽可能对齐； 10) 缺省缩进为 2 个空格； 11)\n独立封装的参数保持 4 个空格的缩进。\n如果函数为 const 的，关键字 const 应与最后一个参数位于同一行。\n// Everything in this function signature fits on a single lineReturnType FunctionName(Type par) const {  ...}// This function signature requires multiple lines, but// the const keyword is on the line with the last parameter.ReturnType ReallyLongFunctionName(Type par1,                                  Type par2) const {...}\n函数调用 的风格跟定义一样\n条件语句\n有些条件语句写在同一行以增强可读性，只有当语句简单并且没有使用 else\n子句时使用：\nif (x == kFoo) return new Foo();if (x == kBar) return new Bar();\n如果语句有 else 分支是不允许的： // Not allowed - IF statement on one line when there is an ELSE clauseif (x) DoThis();else DoThat();\n通常，单行语句不需要使用大括号，如果你喜欢也无可厚非，也有人要求 if\n必须使用大括 号： if (condition)  DoSomething(); // 2 space indent.if (condition) {  DoSomething(); // 2 space indent.}\n如果语句中哪一分支使用了大括号的话，其他部分也必须使用\n// Not allowed - curly on IF but not ELSEif (condition) {  foo;} else  bar;// Not allowed - curly on ELSE but not IFif (condition)  foo;else {  bar;}// Curly braces around both IF and ELSE required because// one of the clauses used braces.if (condition) {  foo;} else {  bar;}\n循环和开关选择语句\nswitch 语句可以使用大括号分块；空循环体应使用 {} 或\ncontinue\nswitch 语句中的 case\n块可以使用大括号也可以不用，取决于你的喜好，使用时要依下文所述。\n如果有不满足 case 枚举条件的值，要总是包含一个\ndefault（如果有输入值没有 case 去处理，编译器将报警）。如果 default\n永不会执行，可以简单的使用 assert：\n// switch 语句switch (var) {  case 0: { // 2 space indent      ... // 4 space indent      break;  }  case 1: {      ...      break;  }  default: {      assert(false);  }}// 空循环体for (int i = 0; i &lt; kSomeNumber; ++i) {} // Good - empty body.while (condition) continue; // Good - continue indicates no logic.while (condition); // Bad - looks like part of do/while loop.\n指针和引用表达式\n句点（.）或箭头（-&gt;）前后不要有空格，指针 / 地址操作符（*、&amp;）后不要有空格\n下面是指针和引用表达式的正确范例：\nx = *p;p = &amp;x;x = r.y;x = r-&gt;y;\n在声明指针变量或参数时，星号与类型或变量名紧挨都可以：\n// These are fine, space preceding.char *c;const string &amp;str;// These are fine, space following.char* c; // but remember to do \"char* c, *d, *e, ...;\"!const string&amp; str;char * c; // Bad - spaces on both sides of *const string &amp; str; // Bad - spaces on both sides of &amp;\n布尔表达式\n如果一个布尔表达式超过标准行宽（80 字符），如果断行要统一一下。\n下例中，逻辑与（&amp;&amp;）操作符总位于行尾 (可也考虑放在行首)：\nif (this_one_thing &gt; this_other_thing &amp;&amp;    a_third_thing == a_fourth_thing &amp;&amp;    yet_another &amp; last_one) {  ...}\n变量及数组初始化\n选择 = 或 (), 下面的形式都是正确的：\nint x = 3;int x(3);string name(\"Some Name\");string name = \"Some Name\";\n预处理指令\n预处理指令不要缩进，从行首开始\n即使预处理指令位于缩进代码块中，指令也应从行首开始。\n// Good - directives at beginning of line  if (lopsided_score) {#if DISASTER_PENDING // Correct -- Starts at beginning of line    DropEverything();#endifBackToNormal();  }// Bad - indented directives  if (lopsided_score) {    #if DISASTER_PENDING // Wrong! The \"#if\" should be at beginning of line      DropEverything();    #endif // Wrong! Do not indent \"#endif\"      BackToNormal();}\n类格式\n声明属性依次序是 public:、 protected:、 private:,\n每次缩进 1 个空格；除第一个关键词（一般是\npublic）外，其他关键词前空一行，关键词后不要空行；\n每一块中，声明次序一般如下：\n1) typedefs 和 enums； 2) 常量； 3) 构造函数； 4) 析构函数；\n5) 成员函数，含静态成员函数； 6) 数据成员，含静态数据成员。\nclass MyClass : public OtherClass { public: // Note the 1 space indent!   MyClass(); // Regular 2 space indent.   explicit MyClass(int var);   ~MyClass() {}   void SomeFunction();   void SomeFunctionThatDoesNothing() {   }   void set_some_var(int var) { some_var_ = var; }   int some_var() const { return some_var_; } private:   bool SomeInternalFunction();   int some_var_;   int some_other_var_;   DISALLOW_COPY_AND_ASSIGN(MyClass); };\n命名空间格式化\n命名空间不添加额外缩进层次，例如：\nnamespace {void foo() { // Correct. No extra indentation within namespace....} } // namespacenamespace {  // Wrong. Indented when  it should not be.  void foo() {    ...  }} // namespace\n水平留白\n添加冗余的留白会给其他人编辑时造成额外负担，因此，不要加入多余的空格。如果确定一\n行代码已经修改完毕，将多余的空格去掉；或者在专门清理空格时去掉（确信没有其他人在\n使用）\n一般情况\nvoid f(bool b) { // 大括号前需要有空格...int i = 0; // 分号前一般没空格int x[] = {0}; // 初始化数组的分号的空格可选int x[] = { 0 }; // 要使用的话，两边都加上\n循环和条件语句\nif (b) { // Space after the keyword in conditions and loops.} else { // Spaces around else.}while (test) {} // There is usually no space inside parentheses.switch (i) {for (int i = 0; i &lt; 5; ++i) {\n操作符\n赋值符号和二元操作符一般左右都带上空格，一元操作符则不用，如下\nx = 0; // Assignment operators always have spaces around them.v = w*x + y/z; // but it's okay to remove spaces around factors.v = w * (x + z); // Parentheses should have no spaces inside themx = -5; // No spaces separating unary operators and their++x; // arguments.\n垂直留白\n垂直留白越少越好。这不仅仅是规则而是原则问题了：不是非常有必要的话就不要使用空行。尤其是：\n1）不要在两个函数定义之间空超过 2 行\n2）函数体头、尾不要有空行，函数体中也不要随意添加空行。\n3）基本原则是：同一屏可以显示越多的代码，程序的控制流就越容易理解。当然，过于密集的\n代码块和过于疏松的代码块同样难看，取决于你的判断，但通常是越少越好。\n函数头、尾不要有空行，如：\nvoid Function() {  // Unnecessary blank lines before and after}\n代码块头、尾不要有空行，如：\nwhile (condition) {  // Unnecessary blank line after}if (condition) {  // Unnecessary blank line before}\n最后，有人根据这些规则总结了一张图，比较直观归纳了上面提到的各种规则风格，图片出自这里\n\n\nstyle image\n\n","categories":["编程"],"tags":["C++","编程"]},{"title":"Google 图片爬虫","url":"/2017/09/23/Google%20%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB/","content":"这里的 Google 图片爬虫指的是爬取在 Google\n上通过关键词搜索得到的图片，由于最近需要一些特定领域的图片，而且现有的数据库满足不了要求，因此就想通过\nGoogle\n搜索筛选出这些特定领域的图片，然后下载下来后再进行人工筛选。这里采用了两种方法，区别在于是否需要解析网页端的\nJS 代码。该项目的代码已经放到了 Github 上，详细代码参见这里。\n\n整体的思路就是先获取 Google 搜索结果页面的 html\n代码，然后从中提取出图片的真实 URL，因为实际上 Google\n也是通过爬虫来爬取这些图片的地址以及根据描述归类，所以图片并不存储在\nGoogle 的服务器中。\n这个过程中的一个关键点就是通过关键词 search_query\n搜索得到的页面可直接通过下面这个 url 访问\nhttps://www.google.com/search?q=search_query&amp;source=lnms&amp;tbm=isch\n将 search_query 换成需要搜索的关键词即可。\n这样便可以直接获取这个页面的源码，然后通过正则表达式解析出所有的图片的链接，这个方法只需要\npython 的 urllib 库即可，对应的源码文件为 download_with_urllib.py\n上面是第一种方法，这种方法比较简单，当时有一个问题就是每个关键词最多只能下载\n100 张图片，原因是 访问上面的链接返回的 HTTP Response\n中限制了最多只有前 100 张图片的 URL，如果需要显示更多，则需要在解析页面中相应的\nJS 代码，这就是接下来要介绍的第二种方法。\n所谓解析 JS\n代码，其实就是浏览器的滚动条向下滚动时，浏览器引擎解析了页面的 JS\n代码，从而向 Google\n的服务器发出新的请求，从而返回更多的图片，当向下滚动时到底时，会出现一个显示为\nShow more results 需要点击的按钮，只有点击后才能加载更多的图片，这些操作要通过浏览器完成，而\npython 提供了一个 selenium\n库，这个库通过相应的浏览器驱动来启动浏览器（不同的浏览器对应于不同的驱动），并在代码中制定具体的浏览器操作。\n这里采用的是 FireFox 浏览器，对应的驱动为 geckodriver，具体的安装步骤就是先安装\nFireFox 浏览器，然后在这里下载与浏览器版本相应的\ngeckodriver，并且在环境变量 PATH 中添加 geckodriver\n的路径。\n第二种方法对应的源码文件为 download_with_selenium.py\n第二种方法的源码中主要有两个方法:\nget_image_links 和，，第一个方法是获取所有图片链接并写入到文件中，第二个方法则是下载第一个方法获取的文件中的图片；在测试时，第二个方法常常会卡住，原因可能是网络的不稳定，也可能是图片服务器那边的反爬虫机制，代码中没有捕获到\nException,\n因此后面对第二个方法进行了改进，并将新的方法写到文件 download_images_with_time_limit.py\n新的方法主要是限制 HTTP\n请求的最大耗时，超过这个时间就会抛出异常，实现是通过系统的 signal\n来中断进程的，并且用了 SIGALRM 这个信号，而这个信号只在\nunix-like 系统中，因此 Windows 无法运行这个脚本。\n而 SIGALRM 在 python 中的用法如下：\nimport signal, osdef handler(signum, frame):    print 'Signal handler called with signal', signum    raise IOError(\"Couldn't open device!\")try:    # Set the signal handler and a 5-second alarm    signal.signal(signal.SIGALRM, handler)    signal.alarm(5)exception IOERROR:    # This open() may hang indefinitely    fd = os.open('/dev/ttyS0', os.O_RDWR)finally:    signal.alarm(0)          # Disable the alarm\n","categories":["python","爬虫"],"tags":["python","爬虫"]},{"title":"Hadoop 中 MapReduce 快速入门","url":"/2015/12/14/Hadoop%E4%B8%ADMapReduce%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","content":"因为研究生的方向是数据挖掘，所以免不了要接触到 Hadoop,Hadoop 是一个用 Java 语言实现开源软件框架，通过大量计算机组成的集群对海量数据进行分布式计算。\n\nHadoop 中两个重要组成部分为 HDFS 和 MapReduce。其中 HDFS 用于存储海量的数据，MapRudece 则负责处理这些数据，从中获取所需的信息。\nHDFS 简单介绍\nHDFS（Hadoop Distributed File System）翻译过来就是 \"Hadoop\n分布式文件系统 \"，用于存储海量的数据。从 “分布式文件系统” 的名字可以知道这个文件系统运行在集群上。对于一个文件，Hadoop 会将其先分成若干个 block（每个 block 的大小默认为 64M, 当然也可以自己指定 block 的大小），然后再将 block 存储到集群上。为了保证数据的冗余性，HDFS 会为每个 block 创建 2 个副本，然后将这三个相同的 block 分别存储在不同的机器上。\n例如下图就是将 data1 分成了 1、2、3 共三个 block，为每个 block 创建副本后再存储在不同的机器上；同理将 data2 分成了 4、5 共两个 block\n\nMapReduce 介绍\n有了数据就可以对其进行处理，从中提取出我们所需的信息。在 Hadoop 中是通过 MapReduce 来实现的。\nMapReduce 任务过程被分为两个阶段：Map 阶段和 Reduce 阶段。每个阶段都用 key/value 作为输入和输出；每个阶段都需要定义函数，也就是 map 函数和 reduce 函数；可以简单认为 map 函数是对原始数据提出出有用的部分，而 reduce 函数则是对提取出来的数据进行处理。\n所以实际编写程序时需要编写三个函数：Map 函数，Reduce 函数和调用他们执行任务的主函数，在编写程序时必须要有这个整体的概念。\n下面会以 Hadoop 官方文档中的 WordCount 任务为例阐述 MapReduce，WordCount 的任务很简单，就是计算出一个文本中每个单词出现了多少次。下面分别来分析这几个函数：\n需要注意的而是在编写这三个函数时均需要用到 Hadoop 本身提供的 jar 包\n下面的实例是 Hadoop 1.2.1 版本提供的 jar 包\nMap 函数\n在本例中 map 函数的主要作用就是以 k-v 形式记录所有出现过的词，代码如下\n/*  *WordCount的map程序  */package com.lc.hadoop;import java.io.IOException;  import java.util.StringTokenizer;//引入Hadoop本身提供的jar包  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Mapper;/*继承Mapper类，&lt;Object,Text,Text,IntWritable&gt;表示输入输出的key-value 类型*/  public class TokenizerMapper extends Mapper&lt;Object,Text,Text,IntWritable&gt; {  \tIntWritable one=new IntWritable(1);  \tText text=new Text();\tpublic void map(Object key,Text value,Context context)throws IOException,InterruptedException{  /*key为输入的key，value为输入的value，因为用不上输入的key的类型，所以直接定义为Object类型，而Context是定义在Mapper类内部的，用于存储key-value键值对*/  \t\tStringTokenizer tokenizer=new StringTokenizer(value.toString());  \t\twhile(tokenizer.hasMoreTokens()){  \t\t\ttext.set(tokenizer.nextToken());  \t\t\tcontext.write(text,one);  \t\t}  \t}  }  ```  关于程序的几点解释：- StringTokenizer类的作用是根据某一分隔符将String分隔开，默认是采用空格。  - IntWritable 类表示的是一个整数，是一个以类表示的**可序列化的整数**  - Text 类代表的是**可序列化的String类型**  - Mapper 类将输入键值对映射到输出键值对，也就是 MapReduce 里的 Map 过程经过map过程后，文章被分割成大量的k-v对，k为实际的单词，v均为1，下一步就是要将相同的单词合并在一起。### Reduce函数  Reduce函数的作用就是将相同的单词出现的次数合并在一起，代码如下：```java  /*  *WordCount的reduce程序  */package com.lc.hadoop;import java.io.IOException;import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Reducer;public class CountReducer extends Reducer&lt;Text ,IntWritable,Text,IntWritable&gt;{\tIntWritable result=new IntWritable();  \tpublic void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context)throws IOException,InterruptedException{  \t\tint sum=0;  \t\tfor(IntWritable iw : values){  \t\t\tsum+=iw.get();  \t\t}  \t\tresult.set(sum);  \t\tcontext.write(key,result);  \t}  }  ```  Reduce与Map函数有很多地方比较相似，均是继承了hadoop提供的jar包中的类，只是map函数继承了Mapper类，而reduce函数继承了Reducer类，输入输出的类型均是k-v键值对。而且reduce函数的输入就是map函数的输出。### 主函数  主函数的任务就是要创建一个任务，并且把map和reduce类都引进来，代码如下：```java  /*  *WordCount的主程序  */package com.lc.hadoop;import java.io.IOException;import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.Path;  import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  import org.apache.hadoop.mapreduce.Job;  import org.apache.hadoop.util.GenericOptionsParser;public class WordCount{\tpublic static void main(String[] args) throws Exception{  \t\tConfiguration conf=new Configuration();//从hadoop配置文件中读取参数  \t\t//从命令行读取参数  \t\tString[] otherArgs=new GenericOptionsParser(conf,args).getRemainingArgs();\t\tif(otherArgs.length!=2){  \t\t\tSystem.out.println(\"Usage:wordcount &lt;in&gt; &lt;out&gt;\");  \t\t\tSystem.exit(2);  \t\t}\t\tJob job=new Job(conf,\"WordCount\");  \t\tjob.setJarByClass(WordCount.class);  \t\tjob.setOutputKeyClass(Text.class);  \t\tjob.setOutputValueClass(IntWritable.class);  \t\tjob.setMapperClass(TokenizerMapper.class);  \t\tjob.setReducerClass(CountReducer.class);  \t\tFileInputFormat.addInputPath(job,new Path(otherArgs[0]));  \t\tFileOutputFormat.setOutputPath(job,new Path(otherArgs[1]));  \t\tSystem.exit( (job.waitForCompletion(true)?0:1));\t}  }  \n关于程序有几点需要注意的地方：\n\nConfiguration 类用于读写和保存各种配置资源\n\nPath 类保存文件或者目录的路径字符串\n\nJob 类：在 hadoop 中每个需要执行的任务是一个\nJob，这个 Job 负责很多事情，包括参数配置，设置\nMapReduce 细节，提交到 Hadoop\n集群，执行控制，查询执行状态，等等\n FileInputFormat 和 FileOutputFormat 用于处理文件的输入和输入（针对 MapReduce 而言）\n\nGenericOptionsParser 类负责解析 hadoop 的命令行参数\n\n执行任务\n编写好源程序后，需要在 hadoop 上执行我们在源程序中写好的代码，大致的过程如下：编译-&gt;打包-&gt;执行，下面分别介绍。为了程序的规范性，首先建立一个 wordcount 的文件夹，下面再建两个子文件夹 src 和 classes，分别放置源程序文件和编译好后的 class 文件。且默认是在 Linux 上执行这些操作的。\n编译\n首先将上面写好的三个源文件放到 wordcount 的 src 目录下，同时拷贝安装 hadoop 后提供的两个 jar 包 hadoop-core-1.2.1.jar 和 commons-cli-1.2.jar。进入 wordcount 目录，采用下面命令进行编译\njavac -classpath hadoop-core-1.2.1.jar:commons-cli-1.2.jar -d ./classes  ./src/*\n这条命令的作用是将 src 目录下的所有文件进行编译，生成的 class 文件放到 classes 目录下，编译过程中需要引入的 hadoop-core-1.2.1.jar 和 commons-cli-1.2.jar 两个包，里面包含了上面源文件中导入的 hadoop 的类。\n编译完成后，可以在 classes 目录下发现以下子目录的结构 classes-&gt;com-&gt;lc-&gt;hadoop, 最后在目录 hadoop 下会有三个 class 文件，分别对应上面的的三个源文件。\n打包\n打包需要用到 jar 命令，jar 命令是 JDK 的打包命令行工具，跟 tar\n非常像。\n先切换到 WordCount 目录，再执行下面的命令：\njar -cvf  WordCount.jar -C ./classes/*  .\n在命令里，-C 是指需要打包的 class 文件的路径，。打包结果是\nwordcount.jar 文件，放在当前目录下。\n执行\n执行 hadoop 任务需要在 HDFS 上进行，所以文件的输入输出路径也就是在 HDFS 上的路径\n首先需要将待处理的文件放入到 HDFS 中，可以按顺序输入以下命令：\nhadoop fs -mkdir in // 在 HDFS 中创建一个名为 in 的文件夹\nhadoop fs -put Readme.txt readme.txt\n// 将 Linux 当前目录下的 Readme.txt 文件放置到 HDFS 中的 in 目录\nhadoop jar WordCount.jar com.lc.hadoop WordCount in/readme.txt out// 执行 Linux 当前目录下的 WordCount\njar 包里面的 WordCount 类，输入文件是 HDFS 中 in 目录下的 readme.txt 文件，输出文件放到 HDFS 中的 out 目录\nhadoop fs -cat out/part-r-0000 // 查看得到的结果\n需要注意的是 HDFS 中的文件路径不能够在 Linux 下直接通过 cd 或 ls 进行切换或查看，而必须要通过 hadoop fs 进行操作。\n以上就是 Hadoop 中 MapReduce 的流程，针对不同的应用会有不同的变化，但是总体上的流程是一致的，就是先编写好三个函数（map 函数，reduce 函数和主函数），然后要经历编译-&gt;打包-&gt;执行的流程。再查看得到的结果即可。\n参考资料：最短路径系列之一从零开始学习 Hadoop\n","categories":["Hadoop"],"tags":["Hadoop","分布式","Java"]},{"title":"HTTP 协议简介","url":"/2016/06/10/HTTP%E5%8D%8F%E8%AE%AE%E7%AE%80%E4%BB%8B/","content":"HTTP 协议采用了非常简单的请求 - 响应模式。由浏览器向网站发出请求（称为 http\nrequest），网站根据请求将相关的资源返回给浏览器（称为 http\nresponse）。这样周而复始就形成了网络通信。\n\nHTTP 协议是无状态的，也就是说同一个客户端的这次请求和上次请求是独立的，对 http 服务器来说，它并不知道这两个请求来自同一个客户端。为了解决这个问题，在 HTTP 中通过 Session 和 Cookie 机制来维护状态。\nHTTP 请求（http request）\n消息的结构\nHTTP 消息可分为三部分，第一部分叫 request line（请求行），\n第二部分叫 http header,\n第三部分是 body。下面通过一个例子解释\n# 第一部分：request lineGET http://www.google.com/ HTTP/1.1  # 第二部分：http headerHost: www.google.comProxy-Connection: keep-aliveAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36...# 第三部分：body（由于请求方法为get，所以body为空）\n从上面的例子可以看到： request line\n可分为三部分，分别是请求方法 请求资源 使用的协议，上面的例子中，请求的方法是 GET，请求的资源是 http://www.google.com/，使用的 HTTP 协议是 1.1 版本的，也有 1.0 版本的 HTTP 协议，主要区别在于 1.1 版本允许多个 HTTP 请求复用一个 TCP 连接，以加快传输速度。\nhttp header\n是格式为 key:value 的一系列 kv 对，这些 kv 对主要作用是告知服务器关于浏览器的一些基本信息，如请求的 host、使用的终端类型、页面缓存情况等。具体作用要根据具体的 kv 对分析\nbody 是发送给服务器的 query 信息 当使用的是 \"GET\"\n方法的时候，body 是为空的（GET 只能读取服务器上的信息，post 能写入)\n上面提到的三部分通过换行符 \\r\\n，其中 request line 永远都是占第一行，接下来每个 header 一行一个，换行符是 \\r\\n，当遇到连续两个\\r\\n 时，Header 部分结束，后面的数据全部是 Body\n请求的方法\n上面提到的 request\nline 的最后一个部分为请求的方法，在 HTTP 中请求的方法最基本的有 4 种，分别是 GET,POST,PUT,DELETE。\n一个 URL 地址用于描述一个网络上的资源，而 HTTP 中的 GET, POST, PUT,\nDELETE 就对应着对这个资源的查，改，增，删 4 个操作。\n我们最常见的就是 GET 和 POST 了。GET 一般用于获取 / 查询资源信息，而 POST 一般用于更新资源信息.\nGET 方法和 POST 方法的区别如下：\n\nGET\n提交的数据会放在 URL 之后，以 ? 分割 URL 和传输数据，参数之间以 &amp; 相连，如 EditPosts.aspx?name=test1&amp;id=123456。POST\n方法是把提交的数据放在 HTTP 包的 Body 中。\nGET\n提交的数据大小有限制（因为浏览器对 URL 的长度有限制），而 POST 方法提交的数据没有限制.\nGET\n方式提交数据，会带来安全问题，比如一个登录页面，通过 GET 方式提交数据时，用户名和密码将出现在 URL 上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码.\n\nHTTP 响应（http response）\n消息的结构\nHTTP 也分为三部分，第一部分叫 response line,\n第二部分叫 response header，第三部分是 body。下面是对应着上面的请求的一个响应\n# 第一部分：response lineHTTP/1.1 302 Found # 第二部分：response headerLocation: http://www.google.com.hk/url?sa=p&amp;hl=zh-CN&amp;pref=hkredirect&amp;pval=yes&amp;q=http://www.google.com.hk/%3Fgws_rd%3Dcr&amp;ust=1465567219496491&amp;usg=AFQjCNGmfj-b-0AJ0D2coSy_40k76XajIwCache-Control: privateContent-Type: text/html; charset=UTF-8Date: Fri, 10 Jun 2016 13:59:49 GMTServer: gwsContent-Length: 390Proxy-Connection: keep-alive# 第三部分：body....\n从上面的例子可以看到： response\nline 可以分为两部分，分别是使用的 HTTP 协议，状态码及其含义。在上面的例子中使用的协议版本为 1.1，状态码为 302，表示重定向，也就是会向 response 中的 location 表示的 url 发出新的请求。\nresponse\nheader 也是格式为 key:value 的一系列 kv 对，表示服务器的状态和返回的内容的一些信息：如 Content-Type 表明返回的内容的类型，该类型也决定了 body 的内容，如果是网页，Body 就是文本，如果是图片，Body 就是图片的二进制数据；Content-Length 表明返回的内容的类型；Content-Encoding 表示，Body 数据是被压缩的，最常见的压缩方式是 gzip，如 content-Encoding: gzip，压缩的目的在于减少 Body 的大小，加快网络传输。\nbody\n则是返回给浏览器的实际内容，由 content 决定其类型。如上面的例子中返回的类型是 text\\html, 则 body 的内容是该网页的 html 代码。如果该 html 代码中还有其他的资源如图片等，浏览器会发送一个新的 http 请求来获取这个资源。\n状态码\nresponse\nline 中的状态码表示对浏览器的请求的回应状况，状态码由三位数字组成，第一个数字定义了响应的类别其中\n\n1XX 提示信息 - 表示请求已被成功接收，继续处理\n 2XX 成功 - 表示请求已被成功接收，理解，接受\n 3XX 重定向 - 要完成请求必须进行更进一步的处理\n 4XX 客户端错误 - 请求有语法错误或请求无法实现\n 5XX 服务器端错误 - 服务器未能实现合法的请求\n\n一些常见的状态码及其含义如下所示：\n\n200 OK 请求被成功地完成，所请求的资源发送回客户端\n 302 Found\n重定向，新的 URL 会在 response 中的 Location 中返回，浏览器将会使用新的 URL 发出新的 Request\n304 Not Modified 文档已经被缓存，直接从缓存调用\n 400 Bad Request 客户端请求与语法错误，不能被服务器所理解\n 403 Forbidden 服务器收到请求，但是拒绝提供服务\n 404 Not Found 请求资源不存在\n 500 Internal Server Error 服务器发生了不可预期的错误\n 503 Server Unavailable\n服务器当前不能处理客户端的请求，一段时间后可能恢复正常\n\n更多状态码详细解释见 http://tool.oschina.net/commons?type=5\n","categories":["杂"],"tags":["web","计算机网络"]},{"title":"Hadoop2.6.0 安装注意事项","url":"/2016/02/20/Hadoop2-6-0%E5%AE%89%E8%A3%85%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","content":"本文为在 centos 上安装 hadoop 2.6.0\n的一些需要注意的地方。过程不会很详细，如需详细配置过程，可看后面的参考链接。\n\n基本注意事项\n改主机名\n修改 /etc/hosts 文件。每台机器都要，目的是为了每台机器都能够通过主机名访问其他机器\njava 环境\n每台机器都要。** 为了方便可直接安装 open-jdk (1.7.0 及以上)，要安装下面两个包。\njava-1.8.0-openjdk  \njava-1.8.0-openjdk-devel  # 运行 jps 命令需要的\n同时添加 JAVA_HOME\n环境变量，如果是 yum 直接安装，JAVA_HOME\n的值应该是 /usr/lib/jvm/java-1.8.0\nssh 环境\n要求无需密码登录。如果是伪分布式，要求能够无密码登录本机。如果是完全分布式，要求 master 能够无密码登录所有的 slaves。\n下载 hadoop 编译好的文件，解压并做软链接到 /usr/local/hadoop\n添加 hadoop 相关的环境变量\nexport HADOOP_HOME=/usr/local/hadoop  export HADOOP_INSTALL=$HADOOP_HOME  export HADOOP_MAPRED_HOME=$HADOOP_HOME  export HADOOP_COMMON_HOME=$HADOOP_HOME  export HADOOP_HDFS_HOME=$HADOOP_HOME  export YARN_HOME=$HADOOP_HOME  export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native  export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin  \n伪分布式安装的配置\n需要修改 core-site.xml 和 hdfs-site.xml 两个文件，core-site.xml 中的 localhost 可改为本机主机名，但是 hosts 文件要有对应关系\n#core-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;          &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;      &lt;/property&gt;  &lt;/configuration&gt;#hdfs-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;dfs.replication&lt;/name&gt;          &lt;value&gt;1&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;      &lt;/property&gt;  &lt;/configuration&gt;  \n初次启动需要将 namenode 格式化。\nhadf namenode -format\n启动 map-reduce\nstart-dfs.sh\n注意：若出现提示 “WARN util.NativeCodeLoader: Unable\nto load native-hadoop library for your platform… using builtin-java\nclasses where # applicable”，该 WARN 提示可以忽略，不会影响 Hadoop\n正常运行\n原因:http://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-warning\n此时可访问 http://localhost:50070，查看 NameNode 和\nDatanode 信息，还可以在线查看 HDFS 中的文件。\n启动 YARN（伪分布式下可选）。Yarn 是从 MapReduce\n中分离出来的，负责资源管理与任务调度。YARN 运行于\nMapReduce 之上，提供了高可用性、高扩展性，\n需要修改配置文件 mapred-site.xml 和\nyarn-site.xml 。修改配置文件 mapred-site.xml，需要将\nmapred-site.xml.template 重命名为 mapred-site.xml，不用 yarn 时做相反操作，因为\nmapred-site.xml 存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying\nconnect to server: 0.0.0.0/0.0.0.0:8032” 的错误，\n#mapred-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;mapreduce.framework.name&lt;/name&gt;          &lt;value&gt;yarn&lt;/value&gt;      &lt;/property&gt;  &lt;/configuration&gt;#yarn-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;          &lt;/property&gt;  &lt;/configuration&gt;  ```  **启动yarn,先要start-dfs.sh,然后执行下面命令**    ./sbin/start-yarn.sh      # 启动YARN      ./sbin/mr-jobhistory-daemon.sh start historyserver  # 开启历史服务器，才能在Web中查看任务运行情况访问localhoost:8088便可以看到启动yarn后开启的界面。**YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。**## 完全分布式配置  master和所有的slave的时间要同步，可通过ntp实现    yum -y install ntp &amp;&amp; ntpdate time.nist.gov可选的时间服务器：  ```  time.nist.gov  time.nuri.net  0.asia.pool.ntp.org  1.asia.pool.ntp.org  2.asia.pool.ntp.org  3.asia.pool.ntp.org  \nmaster 上修改五个配置文件:salves、core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml\n#slaves #datanodes的主机名，一行一个#core-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;          &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://master主机名:9000&lt;/value&gt;      &lt;/property&gt;  &lt;/configuration&gt;#hdfs-site.xml  &lt;configuration&gt;      &lt;property&gt;          &lt;name&gt;dfs.replication&lt;/name&gt;          &lt;value&gt;1&lt;/value&gt;  #datanode的数量      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;          &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;          &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;      &lt;/property&gt;      &lt;property&gt;           &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;           &lt;value&gt;Master:50090&lt;/value&gt;      &lt;/property&gt;  &lt;/configuration&gt;#mapred.xml  &lt;configuration&gt;          &lt;property&gt;                  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                  &lt;value&gt;yarn&lt;/value&gt;          &lt;/property&gt;          &lt;property&gt;                  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                  &lt;value&gt;master主机名:10020&lt;/value&gt;          &lt;/property&gt;          &lt;property&gt;                  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                  &lt;value&gt;master主机名:19888&lt;/value&gt;          &lt;/property&gt;  &lt;/configuration&gt;#yarn-site.xml  &lt;configuration&gt;          &lt;property&gt;                  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                  &lt;value&gt;Master&lt;/value&gt;          &lt;/property&gt;          &lt;property&gt;                  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;          &lt;/property&gt;  &lt;/configuration&gt;  \n将 master 的 hadoop 文件夹复制到 salve（也是 /usr/local/hadoop 目录），然后在 master 启动即可：\nstart-dfs.sh  \nstart-yarn.sh  \nmr-jobhistory-daemon.sh start historyserver\n","categories":["Hadoop","安装配置"],"tags":["Hadoop","分布式"]},{"title":"Highlight Detection In Video","url":"/2022/08/27/Highlight%20Detection%20In%20Video/","content":"Highlight\nDetection，直译过来就是高光检测，一般应用在图像或视频里，本文主要关注视频场景，其任务就是从一段长视频里找到某个 “高光” 的片段。这里的 “高光” 是一个非常宽泛的定义，不像\nctr/cvr\n有明确的含义，不同场景下对 “高光” 的定义不一样：比如说对于带货直播，高光片段也许是\ngmv\n最高的时间段；对于非带货直播，高光的片段也许是观看人数或者刷礼物最多的时间段。\nHighlight Detection\n在实际的应用场景较为广泛：一些视频网站 (如爱奇艺、哔哩哔哩) 里鼠标停留在视频上时会自动播放一些片段，这些片段可认为是高光片段；主流的直播平台基本都提供了直播回放工具，其中往往也会提供高光片段的候选，除了提供给用户侧，广告主 / 商家侧也会提供类似产品，如巨量千川、磁力金牛等平台的产品\nHighlight\nDetection\n在学术界也是一个研究方向，但是学界基本研究局限在几个人工标注的数据集上，一般无法直接应用到实际的生产环境中，原因就是上面说的，不同场景下对高光的定义不一样，需要的数据集也不一样。Highlight\nDetection 相关 paper 不少，本文主要讲 2 篇更贴近业界的\npaper，可以重点关注高光的监督信号的定义，损失函数的设计以及数据集的获取\n\nTaoHighlight\n这个方法来自 TaoHighlight:\nCommodity-Aware Multi-Modal Video Highlight Detection in\nE-Commerce\n这是淘宝在 2021\n提出的一个方法，总体的模型结构图如下图所示，模型不复杂，左边是抽取多模态特征部分，右边则是基于抽取出来的特征和\nscore，通过 GCN 做 finetune，损失函数由两部分组成，即\nLoss_reg 和 Loss_ag\n\n特征工程部分，visual information 通过 I3D+BiGRU 提取，比较常规；text\ninformation 提出了一个 QFGA (Query-Focus Graph Aggregation), 一个基于\ngraph 抽取特征的模块\nCo-Attention\nModule，这个模块主要作用是融合多模态特征（即 visual information\n和 text information），基本的原理是参考了 trasformer 的 self-attention\n机制，对于下面左边的 block，\\(v\\)\n相当于 query, \\(s\\) 相当于 key 和\nvalue\n\n其计算逻辑如下图所示\n\nGraph-based Fine-tuning:\n这部分主要是为了减少抽取出来的特征里的 noise；paper 里是这么说的:\nDue to the presence of visual and text noises in multi-modal video highlight detection, we propose a graph based fine-tuning module to improve the accuracy of our model.，但是也没进一步说明原因\n具体的做法就是给每帧打一个分，然后选取按 score 排序 topk 的 frame\n构造一个 graph，基于 graph 做 GCN 的计算，关于 GCN\n的详细解释可参考这篇文章：Understanding\nConvolutions on Graphs，\n而最终的损失函数由 2 部分组成，\\(L_{reg}\\) 和 \\(L_{ag}\\), 两部分的含义如下\n\\(L_{reg}\\)\n，计算预估的开始 / 结束时间和真正的开始 / 结束时间的的差异，计算方式如下\n\\[\\begin{align} L_{reg} = \\frac{1}{N}\n\\sum_{i=1}^{N}[R(\\hat{s_i}, s_i) + R(\\hat{e_i}, e_i)]\n\\end{align}\\]\n各符号含义如下\n\n\\(s_i\\), \\(e_i\\):\n预估的高光片段的开始和结束时间点\n\n\\(\\hat{s_i}\\), \\(\\hat{e_i}\\): 高光开始和结束时间的 ground\ntruth\n\n\\(R\\): L1 函数\n\n\\(L_{ag}\\) 计算方式如下，\n主要用来计算两段视频的相关性，\\(k\\)\n表示将每段视频切成 \\(k\\) 段\nclips，主要由三项组成\n\\[\\begin{align} L_{ag} = -\n\\sum_{i=1}^{k}e(v_i, \\hat{v}) \\end{align}\\]\n\\[\\begin{align} e(v_i, v_j) = \\theta_{r}\n\\cdot r(v_i, v_j)+\\theta_{d} \\cdot d(v_i, v_j)+\\theta_{s} \\cdot\n\\cos(v_i, v_j) \\end{align}\\]\n各符号含义如下\n\n\\(v_i\\), \\(\\hat{v_i}\\): 预估的高光片段和 ground\ntruth\n\n\\(r(v_i, v_j) = \\frac{I(v_i, v_j)}{U(v_i,\nv_j)}\\), 就是 IoU\n指标，表示重合面积占比\n\n\\(d(v_i, v_j) = \\frac{ |c_i - c_j|}{U(v_i,\nv_j)}\\), \\(c_i\\) 和 \\(c_j\\) 表示两个 video 的中心位置\n\n\\(cos\\): 两个片段的 cos 相似性\n\n预估时实际是一个多分类模型 (softmax)，会对最后构造的 graph\n做一个预估，并选择概率最大的一帧作为起始帧，然后取其后的 128\nframe 作为固定的高光片段\n实验评估的效果指标就是看各种 IoU 的占比，数据集是 taobao 提供的包含 5\n个大分类的数据，整个 dataset 的信息如下\n\npaper 做了消融实验，结果如下图所示\n\n各符号含义如下，可以看到文本特征，Graph-based Fine-tuning\n以及损失函数中的 \\(L_{ag}\\)\n项作用还是不小的\n\nw/o.ti. 去掉文本特征\n\nw/o.ci. 文本特征只包含 video title（去掉了商品 titile\n和商品属性）\n\nw/o.st. 去掉了 Graph-based Fine-tuning\n\nw/o.ag. 去掉了损失函数中的 \\(L_{ag}\\) 项\n\n“unsupervised” solution\n上面的 TaoHighlight\n方法使用的是人工标注的数据集，其缺陷是比较明显的，即人工标注导致了成本较高，可维护性太差；一是高光的定义因人而异，标注时主观性会比较强，二是成本和标注的难度决定了数据更新频率不会很高，这在业界基本是无法接受的\n那很自然就会想到，能否利用一些无须标注的信号来规避掉需要人工打标这个环节呢？这篇\npaper 就提供了一个思路 Less\nis More: Learning Highlight Detection from Video Duration\npeper 认为 Less is More,\n即越短的视频的信息量就越高，所以切分出来的片段都可以认为是高光片段，反之越长的视频的信息量约低，切出来的都不是高光片段，因此，paper\n将训练样本 \\(D\\)\n分为分为三部分，即 \\(D= \\lbrace D_S, D_L, D_R\n\\rbrace\\)，\\(D_S\\)\n表示短视频的集合，\\(D_L\\)\n表示长视频的集合，paper 将短于 15s 定义为短视频，长于 45s\n的定义为长视频\n每个视频都会被切成等成的 segment，记为 \\(s\\), \\(v(s)\\) 表示 segment 对应的视频\npaper 采用了 pair-wise 的方法来构造样本，即从 \\(D_S\\) 和 \\(D_L\\) 切好的 segment\n中分别取出一个，来构成一对 pair \\((s_i,\ns_j)\\)，然后基于下面的 ranking\nloss 计算两部分的差异，这里的 ranking loss\n其实是一类损失函数，常见的 triplet loss、magrin loss、hinge loss\n其实都可以算做 ranking loss\n损失函数的表达如下\n\\[\\begin{align} L(D) = \\sum_{(s_i, s_j)\n\\in \\mathcal{P}} \\max(0, 1 - f(x_i) + f(x_j)) \\end{align}\\]\n但这种认为短视频切出来的都是高光，长视频切出来都不是高光的方法显然是比较武断的，或者说存在\nnoise，所以需要计算每对 pair 的置信度，因此引入了一个 binary latent\nvariable \\(w_{ij}\\)，表示每对 pair\n的置信度，因此上面的损失函数变成了如下形式\n\\[\\begin{align}  \n&amp;L(D) = \\sum_{(s_i, s_j) \\in \\mathcal{P}} w_{ij} \\max(0, 1 - f(x_i)\n+ f(x_j)) \\\\\\  \n&amp;\\begin{array} \\\\\\  \ns.t.&amp; \\sum_{(s_i, s_j) \\in \\mathcal{P}} w_{ij} = p|\\mathcal{P}|,\nw_{ij} \\in [0,1] \\\\\\  \n&amp;w_{ij} = h(x_i, x_j)  \n\\end{array}  \n\\end{align}\\]\n上面的 \\(p\\) 表示训练样本里有效的\npair 的比例，\\(h\\) 则是计算 \\(w_{ij}\\) 这个 variable\n的网络，在训练时会跟原来的网络做 joint training，实际实现时，会通过分\nbatch + softmax 生效\n虽然这里通过 \\(w_{ij}\\)\n做到了在统计意义上只有部分样本有效，但是未必就能把\nnoise 完全干掉，因为缺少人工先验的信息，可能最终训练出来，在真正有效的\npair 上，\\(w_{ij}\\) 可能会更小\n总体的模型和流程如下图所示，\\(\\mathcal{P_1}\\) 到 \\(\\mathcal{P_t}\\) 可认为是 \\(t\\) 个 batch，每个 batch 有 n 个 pair\n\n上面也提到，\\(w_{ij}\\) 实际的生效是通过分 batch +\nsoftmax，即上面的损失函数最终会改成如下形式，\\(\\sigma\\) 是个 softmax 函数，生效在 \\(\\mathcal{P_g}\\) 中，相当于 \\(p=\\frac{1}{n}\\)\n\\[\\begin{align}  \n&amp;L(D) = \\sum_{g=1}^{m} \\sum_{(s_i, s_j) \\in \\mathcal{P_g}} w_{ij}\n\\max(0, 1 - f(x_i) + f(x_j)) \\\\\\  \n&amp;\\begin{array}\\\\\\  \ns.t.&amp; \\sum_{(s_i, s_j) \\in \\mathcal{P_g}} w_{ij} = \\sum_{(s_i, s_j)\n\\in \\mathcal{P_g}} \\sigma(h(x_i, x_j)) = 1 \\\\\\  \n&amp;w_{ij} \\in [0,1]  \n\\end{array}  \n\\end{align}\\]\n实验采用的指标是 mAP (mean average precision)，在 object detection\n中比较常见的指标，可以简单理解为多个类别物体检测中，每一个类别都可以根据 recall 和 precision 绘制一条曲线，AP\n就是该曲线下的面积，mAP 是多个类别 AP 的平均值\nmAP 的定义跟 AUC 有点像，只是这里采用了 PR 曲线，AUC 采用的是 ROC\n曲线，两者的区别可参考 ROC\n曲线与 PR 曲线\n实验主要在两个公开数据集上做，数据集为 YouTube Highlights 和\nTVSum，数据集里对视频做了分类 (domain)，因此也尝试了总体建模（下图的\nOurs-A）和分 domain 建模（下图的\nOurs-S），效果还是挺不错的，也超过了一些 supervised 的方法\n\n\n文章也做了消融实验，主要是 2 部分\n（1）针对上面的 binary latent variable \\(w_{ij}\\)，对比了去掉 \\(w_{ij}\\) （下图中的 Ranking-D）和通过 EM\n来更新 \\(w_{ij}\\) （下图中的\nRanking-EM）的效果，效果是 joint training &gt; EM &gt; 去掉 \\(w_{ij}\\)\n\n（2）对比了数据集大小的影响，随着数据集增大，准确率逐渐上升并减缓，比较常规的结论\n\n小结\n关于 highlight detection 的 paper\n不少，这里主要挑选了两篇有针对性的，两篇 paper 的一些核心点如下\n第一篇 paper，TaoHighlight: Commodity-Aware Multi-Modal Video\nHighlight Detection in E-Commerce\n\n特征工程: video 和 text 特征的提取，通过 co-attention\n机制融合这两部分特征\n\n损失函数的设计，\\(L_{reg}\\) + \\(L_{ag}\\)\n\n减少 noise: graph-based fine-tunning 模块，对一些 topk 的候选做\nfine-tuning\n\n第二篇 paper，Less is More: Learning Highlight Detection from Video\nDuration\n\n数据集，根据 video\n的长短来判断视频的是否属于高光，无需人工打标\n\n损失函数的设计，pair-wise 的 ranking loss\n\n减少 noise: 通过一个可训练的 binary latent variable\n来标识样本的置信度\n\n第二篇的 paper\n的模式感觉是更适合实际的生产环境的，主要是人工标注的可维护性和持续性都不好，而在第二篇基础上，真正落地时可能还有几个问题需要思考\n\nvideo duration\n是一个比较粗糙的信号，实际的业务中，会有很多的指标 (比如说\nctr、cvr、roi 等)，这些指标作为高光的定义也许是一个更好的选择，同时需要权衡选择的信号的深度和数据稀疏的\ntrade-off\n\n除了以上的 ranking loss，LTR (pair-wise, list-wise)\n建模也是一个不错的选择，实际业务中需要考虑 pair 或 list\n怎么构建\n\n如果是直播的场景下，需要实时做高光的检测，无法拿到整个视频，需要考虑一种流式的检测方法\n\n","categories":["机器学习"],"tags":["计算广告","机器学习","深度学习"]},{"title":"adload 约束下的混排价值最大化","url":"/2025/01/19/Hierarchically%20Constrained%20Adaptive%20Ad%20Exposure%20in%20Feeds/","content":"之前写的混排文章，主要介绍了《Ads\nAllocation in Feed via Constrained\nOptimization》里的基本做法，同时拓展讨论了混排中的一些开放性问题，这篇 paper\n解决的问题是 request 维度的插入规则，但是没有考虑到一些比较实际的约束如\nadload，即出广告的比例是有限的\n而如果考虑到 adload 的约束，就不能只考虑 request\n内的价值比较了，而是要考虑到 request 之间或者说 session\n维度的价值最大化了，如在某些高价值请求上多出，低价值请求上少出或不出，以此达到\nadload 约束下收入最大化。这篇 paper《Hierarchically Constrained\nAdaptive Ad Exposure in Feeds》为这个问题提供了一个解决思路；paper\n整体做法还是 beam search 和 generator+evaluator\n的混排范式，但是在这个过程中会把整体约束考虑到这个在线的求解过程中；相较于常规做法把控\nadload 独立在混排之外，是一个比较好的思路，值得一读～\n\n问题建模\n定义\npaper 把整体问题定义如下\n\n其中有两点值得注意\n\n价值度量\n\n价值度量或者说量纲对齐，是混排阶段无法绕开的问题； paper\n里推荐候选的用户价值即 \\(U^{rec}(s|\\pi_s)\\) 和广告的商业价值即 \\(U^{ad}(s|\\pi_s)\\)\n量纲的对齐，还是用一个货币化超参 \\(\\alpha\\) 来做的\n这也是业界常见做法，究其原因，应该还是商业价值（ecpm）有比较明确的物理含义，且不容易突变（因为 bid 基本稳定、ctr、cvr\n预估也有准确性要求），像推荐往往只关注序不关注高低估，这可能导致的问题是推荐的分一旦抖动时，容易导致出现积压，所以往往也会对推荐分做一些稳定性的保障，如\ncap 掉异常值，整体做归一化等\n\napplication level 建模\n\n如背景提到的，前一篇 paper 主要在于建模的是 request-level\n价值的最大化，但没有考虑到有 adload 约束（即下图的 \\(m^*\\)）的时的情况，而这里建模时会把一个\nsession 的 request 共同考虑\n建模\n上面的问题虽然定义起来比较简单，但是如果要直接求解 \\(\\pi\\)\n还是比较难的，所以还需要对问题做一些转化，转换成数学上可求解的形式，具体的方法是把整体的策略拆解成两部分，分别是\napplication-level 的策略 \\(x\\) 和\nrequest-level 的策略 \\(\\pi\\), 如下所示\n\\(x\\) 解决否在这个请求出广告，而 \\(\\pi\\)\n解决在这条请求出多少广告；除了这两个符号，paper 使用的符号定义如下\n\n在 reqeust 维度定义了出广告的价值 \\(v\\) （\\(\\pi_0\\) 表示不出广告）和广告数量 \\(w\\) ，如下所示；这里有个点 paper\n里没有说得很清楚，就是 \\(v\\)\n的详细计算方式，乍一看下面的公式（4），\\(v\\)\n就是广告的价值之和，最直观的方式就是纯广告候选的 \\(\\sum\necpm\\)，但这种方式是考虑不全的，一是只考虑了商业价值，\n二是广告插入后对推荐内容的影响没被考虑在内\n猜测实际的计算方式应该是：同时出推荐内容和广告的 list 价值 -\n只有推荐内容的价值 , 这也意味着在 beam search 中每个 list\n的价值时，需要同时计算两个，即有广告的 \\(U(s|\\pi_s)\\) 和无广告的 \\(U(s|\\pi^0)\\)；另外，paper\n这里也没给出具体的 \\(U(s|\\pi_s)\\) 和\n\\(U(s|\\pi^0)\\)\n计算方式，实际的计算应该还要考虑位置的衰减，即需要乘上\nposition_discount\n\n在 adload 约束下的价值最大化，其实就是一个典型的 0-1 背包问题，adload\n就是背包的容量，需要最大化出广告的请求的价值，则基于上面定义的符号，可以写出下面的最优化稳定的定义，\\(x_s\\) 就是某条请求是否要出广告\n\n但与传统背包算法不同的点是，物品的价值是会动态变化的，即由\nrequest-level 的策略 \\(\\pi_s\\)\n决定的，因此 paper 设计了 two-level 的算法来优化这个问题，一个是\napplication level，一个是 request level\n问题求解\napplication level\n这里是通过贪心算法求解 application-level\n的问题的，即决定当前请求是否出广告（ps，贪心算法求解背包问题非最优，但足够轻量级），具体做法就是基于上面定义的\n\\(v\\) 和 \\(w\\) 计算 \\(\\rho =\nv/w\\) （可以理解为广告的边际价值），然后基于 \\(\\rho\\) 排序，然后在 adload 的约束下求一个\n\\(\\rho\\) 的门槛，线上 serving\n时只有门槛大于这个的请求才能出广告，如下图所示\n\nrequest level\nrequest level 要决定的就是具体出广告的策略 \\(\\pi_s\\)\n假设出广告策略从 \\(\\pi_1\\) 变到了\n\\(\\pi_2\\)，相应的这个请求的 \\(v\\) 和 \\(w\\) 都会发生变化（记为 \\(\\Delta v^{+}\\) 和 \\(\\Delta w_s\\)），而由于 \\(w\\)\n发生了变化，会导致其他请求会多出或少出广告，进而影响 application-level\n的价值\n如下图所示，在这个请求内增加的价值的为 \\(\\Delta v^+\\)，但是其他请求减少的价值为\n\\(\\Delta\nv^-\\)（近似值，用上面求出来的请求门槛近似减少的广告的价值，极端情况下不一定成立），而只有这两者加和大于\n0 时，这个策略才是有效的\n\n而如果对 \\(\\Delta v^{+} + \\Delta\nv^{-} &gt; 0\\) 做展开，可以得到下图的公式 (10)，而根据公式\n(10) 可以得到得到最优的 \\(\\pi_s\\)\n的思路是要找到一个 \\(\\pi_s\\)，使得\n\\(v(s|\\pi_s) - \\rho_{thres} *\nw(s|\\pi_s)\\) 最大，即下图（11）\n这是非常关键的一步，能够把一个跨请求的优化转为当前请求就能解决的优化的问题，因为\n\\(v(s|\\pi_s)\\) 和 \\(\\rho_{thres} * w(s|\\pi_s)\\)\n都是当前请求就能得到的值\n\n保序问题\n混排中另一个常讨论的问题是，是否要对输入的队列保序；即输入的推荐队列和广告队列在混排后，内部是否保序，如广告队列\n[A1,A2,A3]\n在混排后，内部保序意味着整体队列会长这个样子 [...A1...A2...A3...]\n直观来看，保序有两点比较重要的意义\n（1）不影响各链路精排的迭代，即精排 auc\n的指标的迭代是有效，而如果在混排改了精排的序，是有可能导致精排 auc\n的不可读 （2）对于广告扣费，需要保证广告排序时按照 sorted_ecpm\n降序排列的，否则扣费会有问题\npaper 则从拍卖机制出发，从 Incentive Compatibility (IC) 和 Individual\nRationality (IR) 强调了保序的重要性\nIC\n就是常说的激励兼容，具体来说，一个机制是激励兼容的，意味着当且仅当每个参与者在这个过程中真实表达自己的偏好时，能获得的效用比边际要更高，即这个机制就是鼓励参与者表达真实的价值预期，放在广告里就是\nbids truthfully。IR\n直译过来就是 “个体理性”，指的是每个参与者在参与拍卖机制时，其效用不低于不参与拍卖时的效用，即参与拍卖不会使参与者变得更糟，这两者的联系，用\npaper 的话是这么说的\n\nFormally, if every participant advertiser bids truthfully (i.e., bids\nthe maximum willing-to-pay price), IR will guarantee their non-negative\nprofits, and IC can further ensure that they earn the best outcomes\n\n那为什么这个跟保序有关？其实跟上面说的扣费原因是一样的，因为一旦破坏了序，意味着扣的钱就会发生变化，而这会破坏上面提到的\nIC 和 IR，更严格的理论可参考 Myerson–Satterthwaite\ntheorem， 这里不再详细展开\n但是混排也不是完全不能改精排的序？因为混排有很多精排感知不到的\ncontext 信息（如上下文的推荐内容、位置等），因此在混排基于 context model\n重新预估 ctr、cvr（相当于让 ctr、cvr\n预估更准确），这种情况也是有可能改序的，但这种改序的同时也改了\necpm，也保证了 ecpm\n是递减的，也不违背上面说的问题，这种方式也是笔者认为在混排改序唯一合理的方式（但这种方式就存在死循环的问题，即如果改了序，context\n也会变，ctr、cvr 也要重新预估了）\n混排算法\n整体的算法还是基于 beam search 的框架来做，如下图所示，下文的\nexposure template，均指的是基于 beam search 搜出来的候选 list\n\n整体的算法如下图所示，第 6 行和第 9 行的公式 14 ，其实就是上图的公式\n12，即找到 \\(v(s|\\pi_s) - \\rho_{thres} *\nw(s|\\pi_s)\\) 最大的候选 list；12\n是对当前候选做剪枝，即把候选中突破了红线约束（如\nmingap）的候选排除，减少计算复杂度；13-14 行的含义是选择当前价值最大\n\\(B\\) 个候选 list 进入下一步的\nsearch，比较标准的一个 beam-search 算法\n\n而在 beam search\n最终选择出来的候选中，最终决定是否要插入广告，会基于上面提到的\napplication-level 求解出来的价值门槛 \\(\\rho_{thres}\\)，如下图所示\n\n一个值得注意的问题是 \\(\\rho_{thres}\\)\n的求解频率，如果做天级更新，即根据过去 n 天数据计算一个 \\(\\rho_{thres}\\)\n然后在当天保持不变，容易出现的问题是当前流量如果有波动，容易导致 adload\n超或打不满；一种解决方法是提高求解 \\(\\rho_{thres}\\)\n的频率，同时使用更实时的数据来求解（eg 每小时求解一次，用过去 n\n个小时的数据），另一种解决方法就是在输出的 \\(\\rho_{thres}\\) 基础上再叠加一个\npacer，调控实时的 adload，即当 adload\n超了就提门槛，反之降门槛（跟常见的调控 sorted_ecpm 门槛控制 adload\n一个道理）\n小结\npaper 在当前业界广泛使用的 “list 价值评估 + beam-search”\n的混排框架下，考虑了在 adload\n约束下，整体价值（自然 + 广告）最大化的问题；相较于更常见的仅考虑广告侧信息控\nadload 的方法（如调整 ecpm 门槛），有创新的地方，而除了 adload\n约束，其他的约束也可以考虑利用这套建模和求解方法，来融入到整体的混排算法中\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记","url":"/2017/05/15/ImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"ImageNet\nClassification with Deep Convolutional Neural Networks\n这篇论文可以说是多层 CNN 用在图像领域的首次尝试（此前的 LeNet 也将 CNN 用在手写数字的识别上，但是没有用到连续多层 CNN）。文中提出的网络模型\nAlexNet (设计者的名字叫 Alex) 在 ImageNet 2010、2012\n年的比赛上取得的效果远远地优于传统方法，这篇文献最重要的工作是设计并验证了这样一个有多层卷积层的网络的有效性，对学术界和工业界的影响都很大。\n本文主要介绍这篇文章中提出的网络模型\nAlexNet，以及其他涉及到的一些知识，主要的介绍的内容有：CNN 的基础知识，AlexNet\n的设计，训练和效果，以及对网络泛化（generalization）性能的一些探讨。\n\nCNN 简介\nCNN\n全称是 Convolutional Neural\nNetwork，翻译做卷积神经网络，类似于我们常见的神经网络，CNN\n也是一层一层连接起来的。顺便一提，这种 layer by layer 的网络叫做\nfeed-forward network，特点是无环，以区别于贝叶斯网络这种有环的网络\n但是与传统的多层神经网络不同点在于，组成 CNN\n的各个网络层不是常见的神经网络的里的网络层，而是由以下四种特殊的网络层组成\n\n卷积层 (convolutional layer)\n 池化层 (pooling layer)\nReLU 层 (ReLU layer)\n 全连接层 (fully connected layer)\n\n下面分别介绍各个层的具体结构与作用\n卷积层 (Convolutional layer)\n卷积层是 CNN 中最重要的层，也是 CNN\n名称的来源，卷积层里面有几个重要概念：核（kernel/filter)、卷积（convolution）、输出（activation\nmap）。对照下图可以比较清晰的理解\n\n\nconvolutional layer\n\n核（kernel/filter): 移动的橙色正方形 卷积（convolution）：kernel 和\nimage 相应区域的点积 输出（activation\nmap）：卷积的输出，图中的粉色区域（convolved feature）\n卷积层的作用可以理解为特征抽取，这种设计来源于人脑里面的机制，但是我们也能够直观地理解这类操作的意义，假设说我们现在要观察一幅图像，往往是从左到右，从上到下来观察的，而相邻的区域往往具有相似性，对于小面积区域（也就是\nkernel 覆盖的区域）可以用更少的数据来概括这部分的特征。\n上面的图像只用了一个\nkernel，我们将其看做是一个人观察这张图片得到的信息，那假如有更多的人观察这张图片，并将所有人观察到的信息综合起来，得到的信息是否会更加完备呢？答案是肯定的，这就涉及到了多个 kernel 的情况，具体如下图所示\n\n\nmultiple_kernel.jpg-16.2kB\n\n除了多个 kernel，图片也会有多个 channel，上图显示的 input image\n只有一个 channel，但是实际中用于分类的图片往往是 RGB\n图片，有 3 个 channel，分别是 red， blue，green，如下图所示\n\n\nRGB channels\n\n因此当输入的图片有多个 channel，并且用多个 kernel\n去进行卷积的时候，过程会如下图所示（来源）\n\n\nconvolutional network\n\n上图的左边是 input image 的三个 channel，中间是两个\nkernel，右边是输出的两个 activation map。对于有多个 channel 的情况，每个\nkernel 就不再是二维的，而是三维，除去表示 kernel\n大小的两个维度，剩下的一个维度也叫深度，大小就是输入的 channel\n的大小。将从各个 channel 得到的值加起来，就得到了对应的 activation map\n相应位置的输出。这里需要注意的的是，无论输入有多少个 channel，每个\nkernel 最终只产生一个 activation map。\n池化层 (pooling layer)\n池化类似于一种下采样（down sampling),\n目的是要较少参数数量和计算量，如下所示是一个 max polling\n的例子，其步长（stride）为 2，kernel 为 2 X 2。\n\n\nmax pooling\n\nmax pooling 指的是每次取 kernel 中的最大值最为输出，除了 max\npooling，还有 average pooling\n等其他方式。除了上面提到的减少参数数量和计算量，池化还可以避免过拟合。\n当上面的步长改为 1 后，kernel 移动过的区域会有重叠，我们称之为\noverlapping pooling，如下图所示\n\n\noverlapping pooling\n\nReLU 层 (ReLU layer)\nReLU 的全称是 Rectified Linear Units，从严格意义上来讲，ReLU\n只是一个激活函数，而不能称之为一个层。其函数表达式为 \\(f(x) = max(0, x)\\)，其图像如下所示\n\n\nReLU\n\nReLU 的主要作用是提升整个网络的非线性判别能力。关于选择 ReLU\n作为激活函数而不是 sigmoid 或 tanh，后面会有详细说明。\n全连接层 (fully connected\nlayer)\n全连接层就是我们常见的神经网络中的网络层，每个神经元都与前面或后面的各个神经元有连接，如下图所示\n\n\nfully connected layer\n\n由于全连接层的参数过多，在 CNN\n中全连接层往往是作为最后几层用于输出。\n网络的设计\n上面介绍的四类 layer 是构成这篇论文中的 CNN 网络四种\nlayer，下面介绍论文中的 CNN 网络的结构及其特点。\n网络结构总览\n文中提出的 CNN 网络结构如下\n\n\nAlexnet\n\n上图有以下几个特点\n\n由 5 层卷积层 + 3 层全连接层构成，并且整个网络在两个 GPU 上训练\n在第 1、2、5 层卷积层后添加了最大池化的操作\n在每层卷积层和全连接层后都有 ReLU 激活函数\n\n上图中网络就是\nAlexNet，网络结构可以这样理解，首先上下两部分表示网络在两个 GPU 上训练，前五层表示 5 层卷积层，后三层表示 3 层全连接层；而立方体（最左边的是输入的图像，这里不算入）表示每层卷积层的输出，立方体里面的小立方体表示 kernel 的大小。\n第一层卷积层采用了 48+48 共 96 个 kernel，输入的图像有三个\nchannel，但是前面提到无论有多少个 channel，一个 kernel 只会产生一个\nactivation map，所以图中的第一个立方体 48 表示输出的 48 个 activation\nmap，而这 48 个 activation map 作为第二层卷积层的输入又成为了 48 个 input\nchannels，依次类推，第二层卷积层采用了 128+128 共 256 个 kernel。\n网络的特点\n这个网络有三个特点并没有在上图中并不是非常显式地展示出来：分别是 ReLU\nNonlinearity、Local Response Normalization 和 Overlapping Pooling。\nReLU Nonlinearity\nReLU 在前面已经简单地进行了介绍，这里要讨论的是为什么采用了 ReLu\n作为激活函数而不是 其他的如 sigmoid 或 tanh。主要原因是 ReLU\n能够更快地收敛，因为其能够在一定程度上避免梯度消失（vanishing gradient\n）的现象。\n要解释这个原因首先需要看看这三个函数的图像（ReLU\n的图像上面已经给出）\n\n\ntanh 和 sigmoid\n\n这两个激活函数的图像非常相似，均是两边平，中间陡。当通过反向传播（backpropgation）训练时，需要通过链式法则求出总的梯度，而当激活值很大或很小的时候，也就是对应到上面图像两边平缓的地方是，对这两个激活函数的求导结果几乎为 0，\n从而导致相乘得到的总的梯度也几乎为 0，错误不能有效地传播到前面的层，修正前面层的参数。这种现象就称为梯度消失。\n而对于 ReLU\n函数，当激活值小于 0 的时候，也存在着相同问题，而且这时候导数完全是 0；但是大于 0 的时候\nReLU\n的导数总是 1，因此大于 0 的时候不存在梯度消失的现象。也有人说当激活值小于 0 的时候会带来稀疏性的好处。\nLocal Response Normalization\nLocal Response Normalization 指的是对网络中经过 ReLU\n层输出的结果进行正规化， 其正规化的公式如下所示：\n\\[\\begin{align} b_{x,y}^i = a_{x,y}^i/(k +\n\\alpha \\sum_{j=\\max(0, i-n/2)}^{\\min(N-1, i+n/2)}(a_{x,y}^j)^2)^{\\beta}\n\\end{align}\\]\n上式中的 \\(a_{x,y}^i\\) 表示第 \\(i\\) 个 kernel 在位置 \\((x,y)\\) 的原始输出，而 \\(b_{x,y}^i\\) 表示正规化后的输出，\\(N\\) 表示所有 kernel 的数目。上式表明对某个\nkernel 在某个位置的输出的正规化利用了与这个 kernel 相邻的 \\(n\\) 个 kernel 在相同位置的值进行。\n而其他参数 \\(k, n, \\alpha, \\beta\\)\n则是通过 cross-validataion 获得的参数，Local Response Normalization\n分别被应用到第一层和第二层卷积层，文章里说这种方法分别将 top-1 error 和\ntop-5 error 降低了 1.4% 和 1.2%。\nOverlapping Pooling\n这个机制我们在前面谈到池化层的时候已经提到，文章里说这种方法分别将\ntop-1 error 和 top-5 error 降低了 0.4% 和 0.3%。\n网络的训练\n目标函数\n文章中的问题是一个图像多分类的问题，多分类问题有若干种方法，在神经网络中最常用的就是\nsoftmax\n单纯从数学的角度来讲，softmax\n只是一种向量变换方式，假设现在有一个长度为 \\(k\\) 的向量 \\(z =\n(z_1,...z_k)\\), 对其进行 softmax 变换后得到向量 \\(\\sigma(z)\\), 其变换公式如下\n\\[\\begin{align} \\sigma(z)_j =\n\\frac{e^{z_j}}{\\sum_{l=1}^k e^{z_l}}~~~(j=1,...k)\n\\end{align}\\]\n变换后的向量 \\(\\sigma(z)\\)\n有一个重要特征，就是所有元素之和加起来为\n1；从概率论的角度来考虑，很自然地可以将这个向量作为属于各个分类的一个概率分布，选择值最大的那个项对应的分类作为其分类。\n这种 “自然” 也是有数学支撑的，实际上，softmax 的这个特性可以从 Generalized\nLinear Model 中推导出来。这里就不详细展开论述了。\n有了概率分布，很自然地一个想法就是做极大似然估计，如下是一个 \\(k\\) 分类问题中，最大化 \\(m\\) 个 sample 的联合概率分布，其中 \\(1 \\lbrace . \\rbrace\\) 的含义为 \\(1 \\lbrace True \\rbrace = 1, 1 \\lbrace False\n\\rbrace = 0\\)，如 \\(1 \\lbrace 2=2\n\\rbrace = 1, 1 \\lbrace 2=3 \\rbrace = 0\\)\n\\[\\begin{align} \\max \\sum_{i=1}^{m}\n\\sum_{j=1}^{k} 1 \\lbrace y^{(i)} = j \\rbrace \\log\n\\frac{e^{z_j}}{\\sum_{l=1}^k e^{z_l}} \\end{align}\\]\n在其前面添加一个负号和一个常数 \\(\\frac{1}{m}\\)\n可以将其转为如下的极小化问题\n\\[\\begin{align} \\min -\\frac{1}{m}\n\\sum_{i=1}^{m} \\sum_{j=1}^{k} 1 \\lbrace y^{(i)} = j \\rbrace \\log\n\\frac{e^{z_j}}{\\sum_{l=1}^k e^{z_l}} \\end{align}\\]\n实际上，上面要最小化的目标函数是交叉熵损失（cross-entropy\nerror），这个目标函数也可以通过交叉熵的定义推导出来。\n训练算法\n上面得到的最后是一个无约束的最优化问题，对于这类最优化问题有多种方法可用，其中最常用的是随机梯度下降（Stochastic\nGradient Descent），但是这里没有用原始的 SGD， 而是采用了带有 momentum,\nweight decay 和 mini-batch 的 SGD。\nmomentum, weight decay 和 mini-batch\n是三个非常重要的概念，这里简单说明他们的作用\nmomentum\n指的是每次更新参数的梯度除了用当前迭代得到的梯度，还要加上前一次迭代得到的梯度，起作用是为了加快收敛，避免局部最优（如果问题非凸的话）\nweight\ndecay 实际上是 L2 regularization\n微分后得到的项，其目的是为了防止过拟合。\nmini-batch\n则是指每个更新时不仅采用一个样本，而是采用多个样本，这种方法介于 BGD 和\nSGD 之间。\n其更新的规则如下所示，参数 ，\\(v_i\\)\n被称作 momentum variable， $- 0.0005 w_i $ 是 weight decay 项，\\(&lt;\\frac{\\partial L}{\\partial\nw}|_{w_i}&gt;_{D_i}\\) 则是从 mini-batch 为 \\(D_i\\) 中得到的 gradient， \\(\\epsilon\\) 为步长。\n\\[\\begin{align}\n&amp;v_{i+1} = 0.9v_i - 0.0005 \\epsilon w_i - \\epsilon\n&lt;\\frac{\\partial L}{\\partial w}|_{w_i}&gt;_{D_i}\\\\\\\n&amp;w_{i+1} = w_i + v_{i+1}\n\\end{align}\\]\n多 GPU 训练\n前面已经提到了整个网络在两个 GPU 上训练， 原因是 GPU\n能够并行的处理数据，训练速度较快，而同时一个 GPU\n限制了模型的大小，因此用到了两个，两个 GPU\n是并行的训练网络的，除了在第三层的卷积层两个 GPU 交换了数据用于 cross\nvalidation。与一个 GPU 训练的模型相比，两个 GPU 训练的模型分别将 top-1 error\n和 top-5 error 降低了 1.7% 和 1.2%。\n防止过拟合\n为了防止过拟合，文章采用了两种方法，data augmentation 和\ndropout。\ndata augmentation\ndata augmentation\n指的是如何从提供的数据集中得到更多的数据，文中也采用了两种途径，其中一种是从原始图像（大小为\n256 × 256）中抽出多个大小为 224 × 224\n的块作为图像，因此一幅原始图像能够生成多个图像；另外一种途径就是在原始图像的像素上加上通过 PCA 从图像中抽取出来的信息，从而生成新的图像。这两种方法将\ntop-1 error 降低了 1%。\ndropout\ndroupout 指的是每个神经元每次传递值时只有 50%\n的概率工作，如下图所示，灰色的神经元指的是该神经元并没有工作。\n\n\ndropout\n\n这种方法的好处是降低了神经元间的依赖性，使得每个神经元更加\nrobust。droupout 被添加在第一和第二层全连接层中。\n网络的效果\n文中采用的数据集是 ImageNet，这是一个有着约 14 million\n张 labeled image 的图片集，每一年通过这个数据集会举办一次名为 ImageNet\nLarge-Scale Visual Recognition Challenge (ILSVRC)\n的比赛，就是一个图片多分类比赛，文中展示了上面提到的 cnn 网络在 2010\n年和 2012 年比赛中的表现结果，结果如下所示\n2010 年\n\n\n2010\n\n2012 年 \n其中 2012 年的表格中 5 CNNs 表示用了 5 个 CNN 做投票 ensemble\n后的效果，CNN*\n表示在原来的 5 层卷积层的基础上再增加一层卷积层。可以看到 CNN 所得到的结果要远远优于第二名的，而这也是当年这篇文章震惊了学术界和工业界的原因。\n网络泛化能力的探讨\n从上面的论述中可知，我们将多个卷积层和全连接层连在一起，然后加上 pooling，dropout 等操作，就构建了一个取得非常好效果的网络，很自然我们会问，这个网络为什么能够取得这么好的效果？或者说这个网络的泛化能力为什么会这么好，是不是有什么保证了其泛化误差不会过大？\n在统计机器学习中，有一个叫 VC dimension\n的概念，用于描述模型的复杂度，这个概念中的 VC bound\n为泛化误差约束了一个 bound，但是这个概念需要很复杂的数学推导，这里我们略去这些推导。只说一个\nVC dimension\n给我们揭示的一个很直观的概念：要取得较好的泛化能力，用于训练模型的样本数目应该至少是参数数目的 10 倍。\n这个理论在统计机器学习的 svm 等模型中都得到了较好的验证，但是文中提出的网络有\n60 million 的参数和 1.25 million\n的样本，因此这个条件远远没得到满足。但是网路却取得了很好的效果，这样看来，VC\ndimension 这个理论并适用于这个网络，实际上，不仅仅是这个网络，VC\ndimension 在深度学习中多个网络中也不适用。\n而这一点，也被 2017\nICLR 的最佳论文 Understanding deep learning\nrequires rethinking generalization指出，下图是从这篇论文的\npresentation 中抽取的一张图片。\n\n\n4 networks\n\n图中四个宠物小精灵代表了四个著名的网络，随着 p/n 值越来越大，也就是\n“样本 / 参数” 的比值越来越小，越不满足 VC dimension\n提出的条件，但是泛化的误差却越来越小。\n这篇最佳论文还做了很多其他实验，这里就不详细展开，但是从这篇文章并没有从理论上说明了这个网络泛化误差小的理论依据，也就是没有提出在深度学习领域适用的\n“VC\ndimension”，而这一工作将会是未来深度学习发展中非常重要和有意义的工作。\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"Java 中的修饰符","url":"/2016/06/20/Java%20%E4%B8%AD%E7%9A%84%E4%BF%AE%E9%A5%B0%E7%AC%A6/","content":"Java 的修饰符主要分为两种\n\n访问修饰符\n非访问修饰符\n\n\n访问修饰符\n访问修饰符用于控制对类、方法、变量的访问权限，使用中有 private、default、protected、public 四个修饰符，其访问权限从小到大。其中 default 不是一个保留字，而是当任何修饰符都不加的时候的默认权限。\n四种修饰符的区别如下所示\n\n\n\n修饰符\n属性\n\n\n\n\n private\n 仅对同一个类（class）内部可见\n\n\n default\n 对同一个包（package）内的所有类可见\n\n\n protected\n 对同一个包内的所有类及其子类可见（即使子类在另外一个包）\n\n\npublic\n 对所有包的所有类均可见\n\n\n\n访问权限的继承的一个原则就是子类从父类继承过来的方法和变量的访问权限只能变大，private 除外，因此就有了以下几条原则\n\n父类中声明为 public 的方法在子类中也必须为 public。\n父类中声明为 protected 的方法在子类中要么声明为 protected，要么声明为 public。不能声明为 private。\n父类中声明为 private 的方法，不能够被继承。\n\n非访问修饰符\n非访问修饰符指的是一些实现其他功能的修饰符，为了与控制访问权限的访问修饰符区别，就命名为了非访问修饰符。\n非访问修饰符主要包含 static，final， abstract，\nsynchronized 和 volatile。其主要区别如下：\n\n\n\n修饰符\n属性\n\n\n\n\n static\n 声明方法或变量属于整个类而不是变量\n\n\n final\n 声明方法或变量不可被修改\n\n\n abstract\n 仅仅声明了类或方法的名称而让子类对抽象类进行拓展和修改\n\n\n synchronized 和 volatile\n 用于线程的同步\n\n\n\n关于上面的几个修饰符有以下几点需要注意：\nstatic\n\n静态（static）方法不能使用类的非静态变量\n静态变量或方法可通过类直接调用：如 class.staticVariable 或 class.staticMethod\n\nfinal\n\nfinal 变量需要在声明的时候显式初始化并且只能初始化一次\n final 方法可以被子类继承，但是不能被子类修改，或者说重写 (override)\nfinal 类不能被继承，没有类能够继承 final 类的任何特性\n final 修饰符通常和 static 修饰符一起使用来创建类常量\n\nabstract\n\n抽象（abstract）方法是一种没有任何实现的方法（没有花括号），该方法的的具体实现由子类提供\n抽象方法的声明以分号结尾，且例如：public abstract sample();\n任何继承抽象类的子类必须实现父类的所有抽象方法，除非该子类也是抽象类。\n如果一个类包含抽象方法，那么该类一定要声明为抽象类；但是抽象类可以不包含抽象方法，也可以同时包含抽象方法和非抽象方法\n\n一个抽象类的定义 abstract class Caravan{   private double price;   public abstract void goFast(); //抽象方法   public abstract void changeColor()   {    // 抽象类中可以包含实现了的方法   }}\n抽象方法: public abstract class SuperClass{    abstract void m(); //抽象方法}class SubClass extends SuperClass{     //实现抽象方法      void m(){          .........      }}\n这里顺便穿插抽象类和接口（interface）的一些差异：\n接口（interface）就是给出一些没有内容的方法，封装到一起，到某个类需要使用的时候，再根据具体情况将这些方法写出。\n与抽象类不同的是抽象类中的方法可以有主体（也就是实现了该方法）；而接口中的所有方法都不可以有主体（也就是所有方法都不可以实现）\n除此之外，一个类可以实现多个接口，实现了类似于多继承的特性；但是一个类不能继承多个类，并且一个类在实现一个接口的同时也可以继承一个类，如：class son extends father implements interface{}\nsynchronized 和 volatile\n\nsynchronized 关键字声明的方法同一时间只能被一个线程访问，通过 synchronized 修饰符可以实现线程锁的功能\n volatile\n修饰的成员变量在每次被线程访问时，都强制从共享内存中重新读取该成员变量的值。而且，当成员变量发生变化时，会强制线程将变化值回写到共享内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值\n\n关于 violate 的一个列子如下所示 public class MyRunnable implements Runnable{    private volatile boolean active;    public void run()    {        active = true;        while (active) // 第一行        {            // 代码        }    }    public void stop()    {        active = false; // 第二行    }}\n通常情况下，在一个线程调用 run () 方法（在 Runnable\n开启的线程），在另一个线程调用 stop () 方法。 如果 第一行\n中缓冲区的 active 值被使用，那么在 第二行 的 active 值为 false\n时循环不会停止。但是以上代码中我们使用了 volatile 修饰\nactive，所以该循环会停止。\n在对变量和方法使用修饰符时，访问修饰符和非访问修饰符可以混用，且对访问修饰符只能选择其中一个，而非访问修饰符则没有这个限制。因此以下修饰符都是合法的\npublic static void main() // 执行类的入口--main 方法public static final pi = 3.14 //不可修改的变量πpublic final class Test // 不能被继承的类.............\n\nReferer\n\nhttp://www.runoob.com/java/java-modifier-types.html\n\n","categories":["Java","语法"],"tags":["Java"]},{"title":"Java 面向对象的几个概念","url":"/2016/07/01/Java%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A6%82%E5%BF%B5/","content":"本文主要记录 Java 面向对象中几个容易混淆的概念。主要包括重写 (override) 与重载 (overload)，多态，抽象类与接口。\n\n继承\n在 Java 中，类的继承是单一继承，一个子类只能拥有一个父类。通过 extends 关键字实现类的继承。所有 Java 的类均是由 java.lang.Object 类继承而来的，所以 Object 是所有类的祖先类，而除了 Object 外，所有类必须有一个父类。\n通过 instanceof 关键字可以判断一个对象是不是一个类的实例。见下面的例子：\n//A.javaclass A{}//B.javaclass B extends A{  public static void main(String[] args){  A a = new A();  B b = new B();  System.out.println(a instanceof A);  System.out.println(b instanceof B);  System.out.println(a instanceof B);  System.out.println(b instanceof A);  }}\n上述代码的输出为 truetruefalsetrue\n也就是说子类的对象也是父类的一个实例。\n重写 (override) 与重载 (overload)\n重写 (override)\n重写 (override) 是子类对父类的允许访问的方法的实现过程进行重新编写，返回值和形参都不能改变。\n重写有以下几点规则\n\n参数列表和返回类型必须完全与被重写方法相同；\n访问权限不能比父类中被重写的方法的访问权限更低。例如：如果父类的一个方法被声明为 public，那么在子类中重写该方法就不能声明为 protected。\n子类只能重写有访问权限的父类方法，在此基础上声明为 final 的方法不能被重写。\n重写的方法能够抛出任何非强制异常，无论被重写的方法是否抛出异常。但是，重写的方法不能抛出新的强制性异常，或者比被重写方法声明的更广泛的强制性异常，反之则可以\n构造方法不能被重写。 如果不能继承一个方法，则不能重写这个方法\n\n详见下面的例子： class Animal{   public void move(){      System.out.println(\"动物可以移动\");   }}class Dog extends Animal{   public void move(){      System.out.println(\"狗可以跑和走\");   }}public class TestDog{   public static void main(String args[]){      Animal a = new Animal(); // Animal 对象      Animal b = new Dog(); // Dog 对象      a.move();// 执行 Animal 类的方法      b.move();//执行 Dog 类的方法   }}\n在上面的例子中可以看到，尽管b属于Animal类型，但是它运行的是Dog类的move方法。\n这是由于在编译阶段，只是检查参数的引用类型。\n然而在运行时，Java 虚拟机 (JVM) 指定对象的类型并且运行该对象的方法。\n因此在上面的例子中，之所以能编译成功，是因为 Animal 类中存在 move 方法，然而运行时，运行的是特定对象的方法。而这就是一个典型的多态例子。\n再看看下面的例子能更好地理解上面的话 class Animal{   public void move(){      System.out.println(\"动物可以移动\");   }}class Dog extends Animal{   public void move(){      System.out.println(\"狗可以跑和走\");   }   public void bark(){      System.out.println(\"狗可以吠叫\");   }}public class TestDog{   public static void main(String args[]){      Animal a = new Animal(); // Animal 对象      Animal b = new Dog(); // Dog 对象      a.move();// 执行 Animal 类的方法      b.move();//执行 Dog 类的方法      b.bark();   }}\n以上实例编译运行结果如下： TestDog.java:30: cannot find symbolsymbol  : method bark()location: class Animal                b.bark();                 ^\n该程序将抛出一个编译错误，因为 b 的引用类型 Animal 没有 bark 方法。\n当需要在子类中调用父类的被重写方法时，要使用 super 关键字。\nclass Animal{   public void move(){      System.out.println(\"动物可以移动\");   }}class Dog extends Animal{   public void move(){      super.move(); // 应用super类的方法      System.out.println(\"狗可以跑和走\");   }}public class TestDog{   public static void main(String args[]){      Animal b = new Dog(); // Dog 对象      b.move(); //执行 Dog类的方法   }}\n以上实例编译运行结果如下： 动物可以移动狗可以跑和走\n重载 (overload)\n重载 (overloading)\n是在一个类里面，方法名字相同，而参数不同，返回类型可以相同也可以不同。\n重载有以下几点规则\n\n被重载的方法必须改变参数列表；\n被重载的方法可以改变返回类型；\n被重载的方法可以改变访问修饰符，没有限制权限只能变大或变小的限制\n方法能够在同一个类中或者在一个子类中被重载。\n\n见下面的例子\npublic class Overloading {\tpublic int test(){\t\tSystem.out.println(\"test1\");\t\treturn 1;\t}\tpublic void test(int a){\t\tSystem.out.println(\"test2\");\t}\t\t//以下两个参数类型顺序不同\tpublic String test(int a,String s){\t\tSystem.out.println(\"test3\");\t\treturn \"returntest3\";\t}\t\tpublic String test(String s,int a){\t\tSystem.out.println(\"test4\");\t\treturn \"returntest4\";\t}\t\tpublic static void main(String[] args){\t\tOverloading o = new Overloading();\t\tSystem.out.println(o.test());\t\to.test(1);\t\tSystem.out.println(o.test(1,\"test3\"));\t\tSystem.out.println(o.test(\"test4\",1));\t}}\n重写 (override) 与重载 (overload) 的区别\n\n\n\n区别点\n重写 (override)\n 重载 (overload)\n\n\n\n\n 参数列表\n不能改变\n必须改变\n\n\n返回类型\n不能改变\n可以改变\n\n\n范围\n只能在子类中重写\n可以在当前类或子类中重载\n\n\n权限\n重写的方法的访问权限只能变大\n重载方法的访问权限无变化限制\n\n\n异常\n可以减少或删除，不能抛出新的或者更广的异常\n无添加减少的限制\n\n\n\n多态\n从字面上的意思解释，多态是同一个行为具有多个不同表现形态的能力。反映在 Java 面向对象中指的是同一方法（参数列表和返回类型都相同）有具有多种实现方式。\n因此，结合上面说到的内容，多态存在有以下三个必要条件:\n\n继承\n重写\n父类引用指向子类对象\n\n当使用多态方式调用方法时，首先检查父类中是否有该方法，如果没有，则编译错误；如果有，再去调用子类的同名方法。\n上面的重写所提到的例子就是一个典型的多态例子。\n抽象类与接口\n抽象类\n抽象类不能实例化对象，抽象类的用途在于声明了一系列需要被继承并实现的抽象方法，然后被其他类继承并实现。也是因为这个原因，通常在设计阶段决定要不要设计抽象类。\n抽象类通过 abstract class 来定义，同时需要注意如果一个类包含抽象方法，那么该类一定要声明为抽象类；但是抽象类可以不包含抽象方法，也可以同时包含抽象方法和非抽象方法。\n见下面的例子 // Employee.javapublic abstract class Employee{   private String name;   private String address;   private int number;   public abstract double computePay();}// seller.javapublic  class seller{    public double computePay     {     }}\n继承抽象类后需要注意下面两点：\n\n如果一个类包含抽象方法，那么该类必须是抽象类。\n任何子类必须重写父类所有的抽象方法，否则需要声明自身为抽象类。\n\n接口（interface）\n接口（英文：Interface），在 JAVA 中是抽象方法的集合，接口通常以\ninterface\n来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。\n接口并不是类，编写接口的方式和类很相似，但是它们属于不同的概念。类描述对象的属性和方法。接口则包含类要实现的方法。\n接口有以下特性：\n\n接口是隐式抽象的，当声明一个接口的时候，不必使用 abstract 关键字。\n接口中每一个方法也是隐式抽象的，声明时同样不需要 abstract 关键子。\n接口中的方法都是公有的。\n\n如下面就声明了一个接口： interface Animal {   public void eat();   public void travel();}\n当类实现接口的时候，类要实现接口中所有的方法。否则，类必须声明为抽象的类。\n类使用 implements 关键字实现接口，且一个类可以实现多个接口。\n下面是实现上面的接口的一个例子： public class MammalInt implements Animal{   public void eat(){      System.out.println(\"Mammal eats\");   }   public void travel(){      System.out.println(\"Mammal travels\");   }    public int noOfLegs(){      return 0;   }   public static void main(String args[]){      MammalInt m = new MammalInt();      m.eat();      m.travel();   }} \n一个接口能继承另一个接口，和类之间的继承方式比较相似。接口的继承使用 extends 关键字，见下面的例子\n// Sports.javapublic interface Sports{   public void setHomeTeam(String name);   public void setVisitingTeam(String name);}// Football.javapublic interface Football extends Sports{   public void homeTeamScored(int points);   public void visitingTeamScored(int points);   public void endOfQuarter(int quarter);}\n除此之外，接口还允许多继承，但是 Java\n中是不允许类的多继承的。如下面的接口就继承了上面的两个接口\npublic interface Socer extends Sports, Football{}\n接口与抽象类非常相似，两者的区别入下：\n\n\n\n区别\n接口\n抽象类\n\n\n\n\n继承 (实现) 的个数\n一个类可实现多个接口\n一个类仅能继承一个抽象类\n\n\n内部是否可以含有实现的方法\n没有实现的方法\n可以有实现的方法\n\n\n\n\n参考：http://www.runoob.com/java\n","categories":["Java","语法"],"tags":["Java"]},{"title":"Java 中 Iterator 和 Enumeration 的区别","url":"/2016/04/03/Java%E4%B8%ADIterator%E5%92%8CEnumeration%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"Java 中的 java.util.Iterator 和 java.util.Enumeration 均可用来遍历 Java 中的集合框架（list,map,set 等）。\n\n但是两者也有一些区别，主要表现为:\n\n并非所有的 collection 都支持 Enumeration 的遍历，但是都支持 Iterator 的遍历。如 Hashtable 支持但是 HashMap 不支持。但是两者都支持 Iterator 的遍历。\nIterator\n提供了 remove() 方法可以在遍历的同时删除集合中的元素，但是 Enumeration 没有这个方法，对集合中的元素只读\n两者的方法名不同，具体见下面的代码\n\n\n/**  \n * compare Iterator with Enumeration  \n */\n\nimport java.util.*;  \npublic class HashTableAndHashMap  \n{  \n    public static void main(String[] args)  \n    {  \n        //init the map  \n        Hashtable&lt;String,String&gt;  ht = new Hashtable&lt;String,String&gt;();  \n        Map&lt;String,String&gt;  hm = new HashMap&lt;String,String&gt;();\n\n        for(int i=0;i&lt;5;i++)  \n        {  \n            ht.put(Integer.toString(i), Integer.toString(i));  \n            hm.put(Integer.toString(i), Integer.toString(i));  \n        }\n\n        //different ways of iterating the values  \n        Iterator&lt;Map.Entry&lt;String, String&gt;&gt; it = hm.entrySet().iterator();  \n        System.out.println(\"traverse hashmap\");  \n        while (it.hasNext())  \n        {  \n            String key = it.next().getKey();  \n            String value = hm.get(key);  \n            if (key.equals(\"2\"))  \n            {  \n                it.remove();  \n                System.out.println(key+\" is removed\");  \n                continue;  \n            }  \n            System.out.println(key+\":\"+value);  \n        }  \n        System.out.println(\"hashmap final size:\"+hm.size()+\"\\n\");\n\n\n        System.out.println(\"traverse hashtable\");  \n        Enumeration&lt;String&gt; em = ht.keys();  \n        while(em.hasMoreElements())  \n        {  \n            String key = em.nextElement();  \n            String value = ht.get(key);  \n            if (key.equals(\"2\"))  \n            {  \n                //em不提供remove的方法，但是可以通过ht.remove(key)删除  \n                System.out.println(key+\" is removed\");  \n                continue;  \n            }  \n            System.out.println(key+\":\"+value);  \n        }  \n    }  \n}  \n从上面的代码可以看到，两者遍历的方法名不同：\n\n\n\n名称\n是否还有元素\n找到下一元素\n\n\n\n\n Iterator\nhasNext()\nnext()\n\n\nEnumeration\nhasMoreElements()\nnextElement()\n\n\n\n而且虽然 em 遍历的时候不可以通过自己删除某个元素，但是可以通过 collection 自身删除，见上面的 ht.remove(), 而通过 Iterator 遍历的时候不可以通过 collection 自身删除，如上面假如用了 hm.remove() 会抛出 ConcurrentModificationException。\n","categories":["Java","语法"],"tags":["Java"]},{"title":"Java 中 Hashtable 与 HashMap 的区别","url":"/2016/04/03/Java%E4%B8%ADHashtable%E4%B8%8EHashMap%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"据说这是面试中被问频率非常高的一个问题，下面做简单的记录：\n\n相同点\n\n父类都是 Map 类\n\n都是用来存储 kv 对的\n\n存取 kv 的方法名一样（前提是均使用 iterator 来遍历）\n\n不同点\n线程安全\nHashtable 是线程安全的，HashMap 不是线程安全的。什么是线程安全？简单就是在多线程的情况下变量的值能否正确地被每个线程修改。因为多线程同时修改的时候有可能发生冲突，如同时修改一个变量等操作。\n所以在多线程的情况下，要使用 Hashtable。\n效率\nHashtable 的效率比 HashMap 的效率要低。表现为相同的数据下 HashMap 比 Hashtable 使用的内存更多而且更慢。这个可以算是线程安全所付出的代价，因为要保证线程间的同步，需要额外的维护变量不能同时被修改。\n所以单线程的时候，建议使用 HashMap，效率较高\n遍历的方法\n两者均可通过 java.util.Iterator 来遍历，除此之外 Hashtable 还可以通过 java.util.Enumeration 来遍历。关于两者的区别可以看这篇文章\n空键值\nHashtable 不允许 null 的键值，HashMap 则允许一个 null\n键和若干 null 的 value。\n使用建议\n单线程的情况下使用 HashMap，效率较高，多线程的情况下使用 Hashtable 保证线程间的同步。\n此外，Vector 和 ArrayList 的一个重要区别也是上面提到的线程安全问题，其中 Vector 是线程安全的，而 ArrayList 不是线程安全的。\n","categories":["Java","语法"],"tags":["Java"]},{"title":"Java 中 String 的比较方式（== 和 equals）","url":"/2016/05/18/Java%E4%B8%ADString%E7%9A%84%E6%AF%94%E8%BE%83%E6%96%B9%E5%BC%8F%EF%BC%88==%20%E5%92%8C%20equals%EF%BC%89/","content":"基本概念\n在 Java 中，String 既可以作为一个对象来使用，又可以作为一个基本类型来使用。\n\n这里指的作为一个基本类型来使用只是指使用方法上的，比如 String s = \"hello\"，它的使用方法如同基本类型 int 一样，比如 int\ni =\n1;，而作为一个对象来使用，则是指通过 new 关键字来创建一个新对象，比如 String s = new String(\"Hello\")\nJava 中 String 比较的方法有两种：\n1）用 \"==\" 来比较。这种比较是比较两个 String 类型变量的引用是否相同 (即是否指相同的内存地址)\n2）用 Object 对象的 equals() 方法来比较。String 对象继承自 Object，并且对 equals () 方法进行了重写。两个 String 对象通过 equals () 方法来进行比较时，也就是对 String 对象的实际内容进行比较。\n实际例子\nString 作为对象时的比较\nString s1 = new String(\"Hello\");String s2 = new String(\"Hello\");System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/falsetrue\n两个 String 对象都是通过 new 创建出来的，而 new 关键字为创建的每个对象分配一块新的、独立的内存堆，因此当通过 \"==\" 来比较它们的引用是否相同时，将返回 false；而通过 equals() 方法来比较时，则返回 true，因为这两个对象所封装的字符串内容是完全相同的。\nString 作为基本类型时的比较\nString s1 = \"Hello\";String s2 = \"Hello\";System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/truetrue\n由于这两个 String 对象都是作为一个基本类型来使用的，而不是通过 new 关键字来创建的，因此虚拟机不会为这两个 String 对象分配新的内存堆，而是到 String 缓冲池中来寻找。\n什么是 String 缓冲池？在 Java 中，由于 String（final）是不可改变的，为了提高效率，不重复创建新的字符创，Java 引用了 String 缓冲池的概念。\n首先为 s1 寻找 String 缓冲池内是否有与 \"Hello\" 相同值的 String 对象存在，此时 String 缓冲没有相同值的 String 对象存在，所以虚拟机会在 String 缓冲池内创建此 String 对象，其动作就是 new\nString (\"Hello\");。然后把此 String 对象的引用赋值给 s1。\n接着为 s2 寻找 String 缓冲池内是否有与 \"Hello\" 相同值的 String 对象存在，此时虚拟机找到了一个与其相同值的 String 对象，这个 String 对象其实就是为 s1 所创建的 String 对象。既然找到了一个相同值的对象，那么虚拟机就不在为此创建一个新的 String 对象，而是直接把存在的 String 对象的引用赋值给 s2。\n这里既然 s1 和 s2 所引用的是同一个 String 对象，即自己等于自己，所以以上两种比较方法都返回 ture。\n。\n对象与基本类型的比较\nString s1 = \"Hello\";String s2 = new String(\"Hello\");System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/falsetrue\n由于new 关键字会申请新的内存空间，创建新的对象，因此不会去查找缓存池，即使缓存池中有 \"Hello\"，因此两者的内存地址不是一样的，所以第一个输出为 false，而两者的内容是一样的，输出为 true。\n将上面的代码稍作修改\nString s1 = \"Hello\";String s2 = new String(\"Hello\");s2 = s2.intern();System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/truetrue\n上面的代码增加了一行 s2 = s2.intern();其作用是从 String 缓冲池内取出一个与其值相同的 String 对象的引用赋值给 s（假如有的话）。\n这样做的原因是如果频繁地创建相同内容的对象，虚拟机分配许多新的内存堆，虽然它们的内容是完全相同的。由于 String 是 final 类，因此 String 对象在创建后不能改变。所以为了节省内存，可以使用 String 缓冲池，因为 String 缓冲池内不会存在相同内容的 String 对象。而 intern() 方法就是使用这种机制的途径。\n在一个已实例化的 String 对象 s 上调用 intern() 方法后，虚拟机会在 String 缓冲池内寻找与此 s 对象存储内容相同的 String 对象，如果能找到，则返回对象在缓冲池中的地址，如果找不到，那么虚拟机在缓冲池中以 s 的内容新建一个对象并返回这个对象的地址。注意需要将 s 指向返回的缓冲池对象的地址，这样才能通过垃圾回收器回将原先那个通过 new 关键字所创建出的 String 对象回收。\n因此可以解释上面的 s1==s2 返回结果为什么是 true 了，因为此时，两者的引用相同，均指向缓冲池的对象。\n拼接字符串后的比较\n上面提到由于 String 是 final 类，因此 String 对象在创建后不能改变，那么像拼接字符串的操作如 String s1=s2+s3;（s2、s3 是已赋值的 String）应该会产生一个新的对象，用新的地址空间存储。但是这句话也不完全对，详见下面的例子\nString s1 = \"a\";  String s2 = \"b\";  String s3 = \"ab\";  String s4 = \"a\"+\"b\";  System.out.println(\"s3==s4? \"+ (s3==s4));  String s5 = s1+s2;  System.out.println(\"s3==s5? \"+ (s3==s5));  final String s6 = \"a\" ;   final String s7 = \"b\" ;  String s8 = s6 + s7;  System.out.println(\"s3==s8? \"+ (s3==s8));  \n输出如下： s3==s4? trues3==s5? falses3==s8? true\ns4 由 \"a\"、\"b\" 两个常量拼接而成，本来按照上面的说法应该会生成新的内存空间，但是因为 \"a\"、\"b\" 为两个为常量，不可变，在编译期间编译器会把 s5=\"a\"+\"b\" 优化为 s5=\"ab\"。\ns5 由 s1 和 s2 拼接而成，由于两个变量的相加所以编译器无法优化，\n在运行时，会有新的 String 地址空间的分配，而不是指向缓冲池中的 “ab”。所以结果 false。\ns6 虽让也是由两个变量拼接而成，但是这两个变量已经声明为 final 不可变的了，所以类似于 s4，在编译期间编译器也进行了优化确定了 s8 的值。\n\n参考： http://blog.csdn.net/wangdong20/article/details/8566217\nhttp://www.itxxz.com/a/tea/2014/0814/208.html\nhttp://renxiangzyq.iteye.com/blog/549554\n","categories":["Java","语法"],"tags":["Java"]},{"title":"Java 内置的数据类型","url":"/2016/05/15/Java%E5%86%85%E7%BD%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","content":"Java 可分为两大数据类型:\n\n内置数据类型（Primitive）\n引用数据类型（Reference） \n两者的分类及区别如下所示：\n\n\n从上图可知，内置数据类型有八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。上图直接将字符类型 char 归入到整数类型中，原因是两者可以进行相互的转换。\n整数类型\nbyte\n\nbyte 数据类型是 8 位、有符号的，以二进制补码表示的整数；\n最小值是 - 128（-2^7）；\n最大值是 127（2^7-1）；\n默认值是 0；\nbyte 类型用在大型数组中节约空间，主要代替整数，因为 byte 变量占用的空间只有 int 类型的四分之一；\n例子：byte a = 100，byte b = -50。\n\nshort\n\nshort 数据类型是 16 位、有符号的以二进制补码表示的整数\n最小值是 - 32768（-2^15）；\n最大值是 32767（2^15 - 1）；\nshort 数据类型也可以像 byte 那样节省空间。一个 short 变量是 int 型变量所占空间的二分之一；\n默认值是 0；\n例子：short s = 1000，short r = -20000。\n\nint\n\nint 数据类型是 32 位、有符号的以二进制补码表示的整数；\n最小值是 - 2,147,483,648（-2^31）；\n最大值是 2,147,485,647（2^31 - 1）；\n一般地整型变量默认为 int 类型；\n默认值是 0；\n例子：int a = 100000, int b = -200000。\n\nlong\n\nlong 数据类型是 64 位、有符号的以二进制补码表示的整数；\n最小值是 - 9,223,372,036,854,775,808（-2^63）；\n最大值是 9,223,372,036,854,775,807（2^63 -1）；\n这种类型主要使用在需要比较大整数的系统上；\n默认值是 0L；\n例子： long a = 100000L，int b = -200000L。\n\n小结\n可见整数类型的最大区别在于其表示范围和占用的内存大小不同。在使用时根据实际的使用场景选择合适的数据类型。\n可将表示范围小的数值复制给表示范围大的数，但是反过来不行，因为大数的值可能会超出小的数的表示范围。如下所示\nint a = 100;short b = 3;byte c = a; // 错误long d = b; // 正确\n浮点数\nfloat\n\nfloat 数据类型是单精度（single-precision）、32 位（4 字节）、符合 IEEE\n754 标准的浮点数；\nfloat 在储存大型浮点数组的时候可节省内存空间；\n默认值是 0.0f；\n浮点数不能用来表示精确的值，如货币；\n例子：float f1 = 234.5f。\n\ndouble\n\ndouble 数据类型是双精度（double-precision）、64 位（8 字节）、符合 IEEE\n754 标准的浮点数；\n浮点数的默认类型为 double 类型，见下面的例子\n/*下面的表达式会导致编译器会报错，因为默认0.1是double类型，而a是一个float类型，这样赋值会导致精度的损失*/float a=0.1; //有两种方法，一是将float改为double，二是将0.1改成0.1f\ndouble 类型同样不能表示精确的值，如货币；\n默认值是 0.0f；\n例子：double d1 = 123.4。\n\n小结\nfloat 是单精度数字，而 double 是双精度数字。两者的主要区别如下：\n\n存储\n\nfloat 由 32 bit 存储，其中 1 位符号位，8 位指数位，23 位尾数位。 double 由 64\nbit 存储，其中 1 位符号位，11 位指数位，52 位尾数位。\n\n数值范围\n\nfloat 的指数范围为 - 128+127，而 double 的指数范围为 - 1024+1023，并且指数位是按补码的形式来划分的。\n其中负指数决定了浮点数所能表达的绝对值最小的非零数；而正指数决定了浮点数所能表达的绝对值最大的数，也即决定了浮点数的取值范围。\nfloat 的范围为 - 2^128 ~ +2^127，也即 - 3.40E+38 ~\n+3.40E+38；double 的范围为 - 2^1024 ~ +2^1023，也即 - 1.79E+308 ~\n+1.79E+308。\n\n精度\n\nfloat 和 double 的精度是由尾数的位数来决定的。浮点数在内存中是按科学计数法来存储的，其整数部分始终是一个隐含着的 “1”，由于它是不变的，故不能对精度造成影响。\nfloat：2^23 =\n8388608，一共七位，由于最左为 1 的一位省略了，这意味着最多能表示 8 位数：\n2*8388608 =\n16777216。有 8 位有效数字，但绝对能保证的为 7 位，也即 float 的精度为 7~8 位有效数字\ndouble：2^52 =\n4503599627370496，一共 16 位，同理，double 的精度为 16~17 位。\n之所以不能用 f1==f2 来判断两个数相等，是因为虽然 f1 和 f2 在可能是两个不同的数字，但是受到浮点数表示精度的限制，有可能会错误的判断两个数相等！\nboolean\n\nboolean 数据类型表示一位的信息，大小不确定\n只有两个取值：true 和 false；\n这种类型只作为一种标志来记录 true/false 情况；\n默认值是 false；\n例子：boolean one = true。\n\nchar\n\nchar 类型是一个单一的 16 位（2 字节）Unicode 字符；\n最小值是’000’（即为 0）；\n最大值是’’（即为 65,535）；\nchar 数据类型可以储存任何字符，包括汉字；\n例子：char letter =\n'A'。(只能用单引号，不能用双引号)\n\n参考：\nhttps://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html\nhttp://www.runoob.com/java/java-basic-datatypes.html\nhttp://blog.csdn.net/zq602316498/article/details/41148063\nhttp://blog.csdn.net/abing37/article/details/5332798\n","categories":["Java","语法"],"tags":["Java"]},{"title":"LeetCode 解题报告 (19)-- 从后往前删除链表第 n 个元素","url":"/2016/03/10/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(19)--%E4%BB%8E%E5%90%8E%E5%BE%80%E5%89%8D%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%AC%ACn%E4%B8%AA%E5%85%83%E7%B4%A0/","content":"原题如下：\n&gt;Given a linked list, remove the nth node from the end of list and\nreturn its head.\n\nFor example,\n\nGiven linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.  \nAfter removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5.  \n\nNote:\nGiven n will always be valid.\nTry to do this in one pass.\n\n\n题目比较容易理解，就是删除从后往前数的第 n 个节点。一开始想到的方法就是先找出链表元素总数 num，再删除第 num-n+1 个元素即可。实现方法如下：\nclass Solution(object):      def removeNthFromEnd(self, head, n):          \"\"\"          :type head: ListNode          :type n: int          :rtype: ListNode          \"\"\"          nextNode = head.next          num = 1          while not nextNode == None:              num +=1              nextNode = nextNode.next        delete = num-n+1          if delete == 1:              return head.next        else:              currNode = head              i = 1              while i&lt;delete-1:                  currNode = currNode.next                  i+=1              currNode.next = currNode.next.next          return head  \n这种方法虽然能 AC，但是题目提示了 Try to do this in one pass, 也就是这道题目存在只需遍历一遍的解法。在网上查找后发现可以先让一个指针 p1 从 head 向后移动 n 个 node，然后另外一个指针 p2 此时从 head 出发和 p1 一起移动，当 p1 移动到最后一个节点时，p2 后面的一个元素即为需要删除的元素。实现的代码如下：\nclass Solution(object):      def removeNthFromEnd(self, head, n):          \"\"\"          :type head: ListNode          :type n: int          :rtype: ListNode          \"\"\"        p1=head          i = 1          while i&lt;=n:              p1 = p1.next              i += 1          p2 = head          while not p1.next==None:              p1 = p1.next              p2 = p2.next          p2.next =p2.next.next          return head  \n但是在提交时出错，排查后发现上述代码删除的 node 刚好是 head 时会出错，因为是从 1 开始往后 n 步，当删除的 node 是 head 是，n 是所有节点的数目，而此时会导致 p1 为 None，然后 p1.next 自然会报错。\n解决方法是增加一个无意义的 node 指向 head，从这个 node 开始遍历即可。\n实现代码如下：\nclass Solution:      def removeNthFromEnd(self, head, n):          \"\"\"          :type head: ListNode          :type n: int          :rtype: ListNode          \"\"\"          dummy=ListNode(0)          dummy.next=head          p1=p2=dummy          for i in range(n):              p1=p1.next          while p1.next:              p1=p1.next              p2=p2.next          p2.next=p2.next.next          return dummy.next  \n除了上面的解决方法，一开始还想过通过将链表反转再删除第 n 个 node 即可，但是题目意思是删除这个 node 不能改变其他 node 原来的排序。虽然不符合题意，但是这里一并附上代码记录链表反转的一种方法：\nclass Solution(object):      def removeNthFromEnd(self, head, n):          \"\"\"          :type head: ListNode          :type n: int          :rtype: ListNode          \"\"\"        preNode  = head          nextNode = head.next          tmp = None          while not nextNode == None:              preNode.next = tmp              tmp = preNode              preNode = nextNode              nextNode = nextNode.next          preNode.next = tmp          head = preNode          if n==1:              return head.next          else:              i = 1              while i &lt; n-1:                  nextNode = nextNode.next                  i += 1              nextNode.next = nextNode.next.next              return head  \n","categories":["算法"],"tags":["python","双指针","链表"]},{"title":"LeetCode 解题报告 (207,210)-- 拓扑排序 (Topological Sort)","url":"/2016/07/27/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(207,210)--%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/","content":"拓扑排序是有向图中一种较常用的排序方法，本文主要以 LeetCode 上的两道题目\n207. Course\nSchedule 和 210. Course\nSchedule II 讲述如何拓扑排序的概念与使用方法。\n\nLeetCode 上的两道题目是关于拓扑排序（Topological\nSort）比较经典的两道题目，大意就是有 0~(n-1) 的 n 门课程需要修，但是有部分课程需要先修了别的课程才能修，问是否能修完所有课程。从题意可知，将这些课程及依赖关系表示成一个有向图，那么当图中存在闭环时是不可能修完所有课程的，因为形成闭环的几门课程相互依赖，无法选择第一门开始修的课程。\n因此题目就是要求判断一个有向图中是否存在闭环。\n最直观的思路就是通过深度优先搜索以每个点为起点遍历，判断每次的遍历中是否存在闭环，若存在则无法修完，反之则可以。\n针对 207.\nCourse Schedule，利用上面的思路实现的代码如下所示 class Solution(object):    def canFinish(self, numCourses, prerequisites):        \"\"\"        :type numCourses: int        :type prerequisites: List[List[int]]        :rtype: bool        \"\"\"        course = [[0 for i in xrange(numCourses)] for j in xrange(numCourses)]        for s in prerequisites:            course[s[0]][s[1]] = 1        visited = [0 for i in xrange(numCourses)]        for i in xrange(numCourses):            if self.dfs(course, i, visited)==False:                return False        return True    def dfs(self, course, num, visited):        for i in xrange(len(course[num])):            if course[num][i] == 1:                visited[num] = 1                if visited[i] == 1:                    return False                else:                    if self.dfs(course, i, visited) == False:                        return False                    visited[num] = 0\n这个方法的时间复杂度是 \\(O(n^2)\\),n 是图中所有点的数目。在提交时会提示 TLE。\n其实解决这种问题还有更好的方法，就是下面要介绍的拓扑排序，拓扑排序实际上返回的结果是有向图中所有点依据有向边指向顺序的排列而成的点。下面是拓扑排序的两个例子，第一个是合法的拓扑排序的结果，第二个是非法的拓扑排序结果。\n\n\n那么该如何获得一个有向图的拓扑排序的结果？\n最直观的方法步骤如下 （1）\n找到图中入度（indegree）为 0 的点，然后记录这个点并删除这个点的所有出度（outdegree），也就是连接了这个点的其他点的入度\n（2） 检查图中是否有入度为 0 的点，如果有重复步骤（1）\n重复步骤（1）-（2）直至无法找到入度为 0 的点，此时如果记录的点的数目与所有点的数目相同，那么说明图中不存在闭合的环，反之则存在。\n上面这种方法虽然比较直观，但是每次找到入度为 0 的点的时间复杂度是 \\(O(n)\\), 因此总的时间复杂度是 \\(O(n^2)\\)，n 为图中所有点的数目。下面介绍一种利用队列将时间复杂度降为 \\(O(n+e)\\),e 为图中所有边的数目。\n这种方法特殊的地方在于利用了一个队列存储当前入度为 0 的点，每次队首元素出列时，将队首元素出度相连的所有点的入度减去 1。因此还需要一个额外的空间存储每个点出度连接的所有点。整体的数据结构如下：\n\n图中的 In-Degree array 就是存储每个点的入度数的队列，而 Adjacency list 则是存储每个点通过出度连接的所有点的数据结构。\n这个算法的具体流程如下：\n(1) 遍历有向图中所有的边可以构造出上面提到的两个数据结构，时间复杂度为 \\(O(e)\\)\n(2) 从 In-Degree array 中找所有点中入度数为 0 的点，构成初始队列，时间复杂度 \\(O(n)\\)\n(3) 通过队首出列，调整与队首元素出度相连的所有点的入度数，并检查这些点的入度数是否为 0，若是则入队列\n(4) 重复步骤 (3) 直至队列为空，此时若从队列中出列的所有点的数目为原来有向图中所有点的数目，则图中不存在闭合的环，且出列的顺序是一种可行的遍历方法。步骤 (3)(4) 的时间复杂度为 \\(O(n+e)\\)\n因此这个方法的时间复杂度为 \\(O(n+e)\\), 除了能够判断有向图中是否有闭合的环，也能够给出一种合理的遍历方法，因此能解决上面提到的 207. Course\nSchedule 和 210. Course\nSchedule II两个问题。\n下面是 207.\nCourse Schedule 的解决方法 class Solution(object):    def canFinish(self, numCourses, prerequisites):        \"\"\"        :type numCourses: int        :type prerequisites: List[List[int]]        :rtype: bool        \"\"\"        indegree = [0 for i in xrange(numCourses)]         connection = {i:[] for i in xrange(numCourses)}        for link in prerequisites:            connection[link[1]].append(link[0])            indegree[link[0]] += 1        zero_indegree = []        for i in xrange(numCourses):            if indegree[i] == 0:                zero_indegree.append(i)        i = 0        while i&lt;len(zero_indegree):            for node in connection[zero_indegree[i]]:                indegree[node] -= 1                if  indegree[node] == 0:                    zero_indegree.append(node)            i += 1        if len(zero_indegree) == numCourses:            return True        else:            return False 210. Course\nSchedule\nII的解决方法跟上面一样，只需要将 return True 改为 return zero_indegree 即可。\n","categories":["算法"],"tags":["python","图"]},{"title":"Java 通过 socket 获取 mysql 中的记录遇到的问题","url":"/2016/01/12/Java%E9%80%9A%E8%BF%87socket%E8%8E%B7%E5%8F%96mysql%E4%B8%AD%E7%9A%84%E8%AE%B0%E5%BD%95%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","content":"在实际应用中，经常会遇到这种需求，客户端发送关键字到服务器端，服务器端根据关键字查找数据库中的内容并发送给客户端。本文主要阐述根据传输过来的关键字在查找数据库时查找不到（实际上是数据库中是有数据的）的问题，以及解决方法。\n\n先看 Java 与 mysql 交互的代码:\n String driver = \"com.mysql.jdbc.Driver\"; // 驱动程序名   String url = \"mysql的地址\"; // URL指向要访问的数据库   String user = \"用户名\";  // MySQL配置时的用户名   String password = \"你的密码\"; // MySQL配置时的密码Class.forName(driver);// 加载驱动程序  Connection conn = DriverManager.getConnection(url, user, password);// 连接数据库  if(!conn.isClosed())      System.out.println(\"成功连接到数据库!\");PreparedStatement prepstmt = null; //用预编译对象执行sql语句并返回结果  String sql = \"select admoment,url1,url2  from captiontest where filmname=? order by admoment\";//要执行的SQL语句,按时间查找广告出现的数据记录  prepstmt=conn.prepareStatement(sql);  prepstmt.setString(1,name); // 由于filmnam是String类所以这里是setString，其他同理为setxxx（xxx为类型）  ResultSet rs=prepstmt.executeQuery(); //结果集  \n其中程序中的 name 为 String\n类型，也就是客户端传过来的关键字，但是测试时的问题是即使数据库中存在相关的关键字信息，也不会输出相关信息。。\n这个时候首先要做的就是将执行的 SQL 语句打印出来，因为很多时候得不到预期的结果都是因为是实际执行的 SQL 语句跟自己预期会执行的 SQL 语句不同。通过下面语句可以得出执行的 SQL 语句:\nSystem.out.println(prepstmt.toString());。\n结果如下\nselect admoment,url1,url2  from captiontest where filmname='爱情公寓4\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\' order by admoment  \n怪不得找不到，sql 语句都变成这个模样了，那中间的 \\0\n是从哪里来的？在网上查了后得出的原因如下：在 socket\n传输自己流时，默认会有一段固定长度的 bufferstream\n来接受传输的字节流，这段固定长度的默认值就是 \\0 , 而那些\n\\0 就是 bufferstream\n中没被传输过来的字节流覆盖的单元，解决方法也很简单，就是在服务器接受到客户端传输过来的字节流时，根据字节流有效长度来创建 String\n原来我的 name 是这样产生的，is 是 socket 的输入流：\nint n = is.read(b);  String name=new String(b,\"UTF-8\");  \n没有指定长度，改为下面的产生方式就好了\nint n = is.read(b);  String name=new String(b,0,n);//0和n是指定产生String用到的字节流的开头和结尾  \n","categories":["Java","网络编程"],"tags":["Java","计算机网络"]},{"title":"LeetCode 解题报告 (167, 1, 15, 16, 18)-- 双指针解决 ksum 问题","url":"/2016/03/08/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(167,%201,%2015,%2016,%2018)--%E5%8F%8C%E6%8C%87%E9%92%88%E8%A7%A3%E5%86%B3ksum%E9%97%AE%E9%A2%98/","content":"ksum 问题是一类问题，\n要求从给定的数字中找到 k 个数，使得这 k 个数的和等于特定值。题目不难，直观的方法的时间复杂度为\n\\(O(n^k)\\), \\(n\\) 为给定的数字的个数，\n关键在于时间复杂度的控制，\n本文主要讲述通过双指针将这类问题的时间复杂度降为 \\(O(n^{k-1})\\)。\n\n下面将以 LeetCode 上的几道题目为例讲述这类问题： 167.\nTwo Sum II - Input array is sorted 1. Two Sum 15.\n3Sum 16. 3Sum\nClosest 18.\n4Sum\n之所以从 167.\nTwo Sum II - Input array is sorted\n开始讲述，是因为解决这个题目的思路就是解决这类问题的核心思想。\n167.\nTwo Sum II - Input array is sorted 这道题目的解决思路如下：\n1）初始化双指针，left 指针在最左端，right 指针在最右端\n2）计算 left 指针和 right 指针指向的两个数的和，如果大于目标和，右指针往左移动一步；如果小于目标和，左指针往右移动一步；如果等于目标和则返回\n3) 重复 2）的操作直到两个指针相遇\n上面的方法需要的前提条件是数字列表是有序的。\n实现的 python 代码如下所示：\nclass Solution(object):    def twoSum(self, numbers, target):        \"\"\"        :type numbers: List[int]        :type target: int        :rtype: List[int]        \"\"\"        left, right = 0, len(numbers) - 1        while left &lt; right:            tmp = numbers[left] + numbers[right]            if tmp &gt; target:                right -= 1            elif tmp &lt; target:                left += 1            else:                return [left+1, right+1]\n通过上面的方法的思想可以解决这一类问题。\n对于题目 1. Two\nSum\n，因为原来的数组数无序的，因此需要对数组先排序，而且由于要求返回的是下标而不是数字，因此也需要一个\nHashTable 来存储原来数字的下标。时间复杂度为 \\((nlgn)\\)\nAC 的 python 代码如下： # O(nlgn) time, O(n) space, ACclass Solution(object):    def twoSum(self, nums, target):        \"\"\"        :type nums: List[int]        :type target: int        :rtype: List[int]        \"\"\"        inx = {}        for i in xrange(len(nums)):            inx.setdefault(nums[i], [])            inx[nums[i]].append(i)        nums.sort()        left, right = 0, len(nums) - 1        while left &lt; right:            tmp = nums[left] + nums[right]            if tmp &gt; target:                right -= 1            elif tmp &lt; target:                left += 1            else:                return sorted([inx[nums[left]].pop(), inx[nums[right]].pop()])\n对于题目 15.\n3Sum\n, 也是需要先要对无序的数组进行排序，然后固定第一个数的下标，再用上面的双指针方法找到另外两个数；由于需要返回的是数字而不是下标，因此不需要一个\nHashTable 来存储这些数字的下标。时间复杂度为 \\(O(n^2+nlgn)\\), 也就是 \\(O(n^2)\\)。\nAC 的 python 代码如下所示：\nclass Solution(object):    def threeSum(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        i = 0        result = []        nums.sort()        while i&lt;len(nums)-2:            if nums[i]&gt;0:                break            else:                j = i+1                k = len(nums)-1                while j &lt; k:                    if nums[i]+nums[j]+nums[k]&gt;0:                        k-=1                    elif nums[i]+nums[j]+nums[k]&lt;0:                        j+=1                    else:                        tmp = [nums[i],nums[j],nums[k]]                        if tmp not in result:                            result.append(tmp)                        j+=1                        k-=1                i+=1        return result\n题目 16. 3Sum\nClosest 与 15.\n3Sum 思路一致，只是要找到与目标和最接近的和。时间复杂度也是 \\(O(n^2)\\)\nAC 的 python 代码如下所示 class Solution(object):    def threeSumClosest(self, nums, target):        \"\"\"        :type nums: List[int]        :type target: int        :rtype: int        \"\"\"        if len(nums) == 0:            return 0        nums.sort()        min = None        for i in range(len(nums)-2):            j = i+1            k = len(nums)-1            while j&lt;k:                sum = nums[i]+nums[j]+nums[k]                if sum &gt; target:                    gap = abs(sum -target)                    if min == None or min &gt; gap:                        min = gap                        result = sum                    k-=1                elif sum &lt; target:                    gap = abs(target - sum)                    if min==None or min &gt; gap:                        min = gap                        result = sum                    j+=1                else:                    result = sum                    return result        return result\n题目 18. 4Sum\n的思路与上面的 15.\n3Sum，\n只是一开始需要固定前两个数，然后通过双指针找到另外两个数，时间复杂度是 \\(O(n^3)\\)。推广到 ksum\n问题就是开始需要固定 k-2\n两个数，然后通过双指针找到剩下的两个数。时间复杂度是 \\(O(n^{k-1})\\)\nAC 的 python 代码如下：\nclass Solution(object):    def fourSum(self, nums, target):        \"\"\"        :type nums: List[int]        :type target: int        :rtype: List[List[int]]        \"\"\"        result =[]        if len(nums) == 0:            return result        nums.sort()        for i in range(len(nums)-3):            for j in range(i+1,len(nums)-2):                m = j +1                n = len(nums)-1                while m &lt; n:                    sum = nums[i] + nums[j] + nums[m] + nums[n]                    if  sum&gt;target:                        n-=1                    elif sum&lt;target:                        m+=1                    else:                        tmp = [nums[i],nums[j],nums[m],nums[n]]                        if tmp not in result:                            result.append(tmp)                        m+=1                        n-=1        return result\n","categories":["算法"],"tags":["python","双指针"]},{"title":"LeetCode 解题报告 (343, 377)- 动态规划求解数字的组合方案","url":"/2017/06/28/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(343,%20377)-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%B1%82%E8%A7%A3%E6%95%B0%E5%AD%97%E7%9A%84%E7%BB%84%E5%90%88%E6%96%B9%E6%A1%88/","content":"LeetCode 上的这两道题 343.\nInteger Break 和 377.\nCombination Sum IV\n从名字上来看没有什么联系，但是实际上两个题目都是通过动态规划来降低了求解时间复杂度，并且面对这种题目一开始往往难以往动态规划方向去想，特此记录。\n\n对我而言，之所以不会一开始就想到动态规划，原因是这两个题目要求解的问题均是以某种方式构造某个数字时，总共的方案有几种或者最优的方案是哪种。一开始往往就是往数学或者\ndfs 方向去想，而不会想到通过动态规划建立一个从 1\n到目标数字的数组来记录各个状态的答案。\n对于问题 343.\nInteger Break,\n该问题属于求解组成某个数字的最优方案的问题，首先需要注意的是只把数字分为两部分，这样能够简化问题，然后对于这两部分都取最优时，则这两部分的乘积就是这种分法的最优解。那对于这两部分，可以很自然地想到均通过递归去求解最优，但是这种方法或导致很多重复的计算，比如说将\n8 分为 6 和 2 时，需要计算 6 和 2 的最优解，但是进一步将 6 分为 4 和 2\n时，还要计算 2 的最优解，这样显然会导致 多次计算 2 的最优解。\n解决这个问题就是通过动态规划建立的数组记录数字 2\n的最优解，需要求解时直接从数组中取即可，这时候动态规划的数组就有点类似\ncache 的作用，实现的 java 代码如下，其中 dp[i] 表示数字 i\n的最优解（即分解的数字的乘积最大）\npublic class Solution {    public int integerBreak(int n)     {        int[] dp = new int[n+1];        dp[1] = 1;        for (int i = 2; i &lt;= n; i++)        {            int max = 0;            for(int j = 1; j &lt;= (i &gt;&gt; 1); j++)            {                max = Math.max( Math.max(dp[j], j) * Math.max(i - j, dp[i-j]), max);            }            dp[i] = max;        }        return dp[n];    }}\n针对 LeetCode\n的评测方法（建立一个对象，输入多组评测数据），上面的代码还能该继续优化，就是为这个对象建立一个全局的数组记录各个数字的最优解，供多组评测数据使用，当评测数字在数组中已经有答案了就不必再求解，反之递增数组。实现的\nJava 代码如下，代码中的 cache list 与上面方法中的\ndp 数组的作用相同\npublic class Solution {    private List&lt;Integer&gt; cache = new ArrayList&lt;Integer&gt;();    public int integerBreak(int n)     {        for (int i = cache.size(); i &lt;= n; i++)        {            int max = 0;            for(int j = 1; j &lt;= (i &gt;&gt; 1); j++)            {                max = Math.max( Math.max(cache.get(j), j) * Math.max(i - j, cache.get(i-j)), max);            }            cache.add(max);        }        return cache.get(n);    }}\n针对问题 377.\nCombination Sum IV，一开始想的是通过 dfs\n来求解，但是题目中允许重复的数字，并且不同的顺序被认为是不同的组合方案，这样就使得如果通过\ndfs 求解，时间复杂度会非常大。\n通过动态规划求解，通过 dp 数组中的 dp[i]\n表示数字 i\n的组合方式的数目，可以同时解决上面提到的重复数字和数字的顺序问题。\n则 dp[i+1] 可以考虑所有数字与已有的\ndp[j] (j=0，1，...i) 的组合，实现的 Java 代码如下\npublic class Solution {    public int combinationSum4(int[] nums, int target)     {        int[] dp = new int[target+1];        dp[0] = 1;        Arrays.sort(nums);        int count;        for (int i = 1; i &lt;= target; i++)        {            count = 0;            for (int j = 0; j &lt; nums.length; j++)            {                if (nums[j] &gt; i) break;                count += dp[i - nums[j]];            }            dp[i] = count;        }        return dp[target];    }}\n往往会听到说动态规划的难点在构造递推关系，这没错，但是还有一个难点就是想到题目可以通过动态规划去求解，其实只要能往动态规划方向去想，递推关系往往也不难得到。因此，本文的主要作用还是要提醒面对这种求解目标数字的最优构造方案或者构造方案数目的时候，可以往动态规划方向去想，建立一个从 1 到目标数字的数组记录各个数字的结果，而不是仅仅拘泥于目标数字。\n","categories":["算法"],"tags":["Java","动态规划"]},{"title":"Java 解释 XML 的四种方法","url":"/2015/12/21/Java%E8%A7%A3%E9%87%8AXML%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95/","content":"最近毕设需要用 Java 解释 XML 文件，在网上搜到了 Java 解释 XML 的一篇比较详细的文章，原文链接, 下面为原文\n众所周知，现在解析 XML 的方法越来越多，但主流的方法也就四种，即：DOM、SAX、JDOM 和 DOM4J\n\n下面首先给出这四种方法的 jar 包下载地址\nDOM：在现在的 Java JDK 里都自带了，在 xml-apis.jar 包里\nSAX：http://sourceforge.net/projects/sax/\nJDOM：http://jdom.org/downloads/index.html\nDOM4J：http://sourceforge.net/projects/dom4j/\n介绍及优缺点分析\nDOM（Document Object Model)\nDOM 是用与平台和语言无关的方式表示 XML 文档的官方 W3C 标准。DOM 是以层次结构组织的节点或信息片断的集合。这个层次结构允许开发人员在树中寻找特定信息。分析该结构通常需要加载整个文档和构造层次结构，然后才能做任何工作。由于它是基于信息层次的，因而 DOM 被认为是基于树或基于对象的。\n【优点】\n①允许应用程序对数据和结构做出更改。\n②访问是双向的，可以在任何时候在树中上下导航，获取和操作任意部分的数据。\n【缺点】\n①通常需要加载整个 XML 文档来构造层次结构，消耗资源大。\nSAX（Simple API for XML)\nSAX 处理的优点非常类似于流媒体的优点。分析能够立即开始，而不是等待所有的数据被处理。而且，由于应用程序只是在读取数据时检查数据，因此不需要将数据存储在内存中。这对于大型文档来说是个巨大的优点。事实上，应用程序甚至不必解析整个文档；它可以在某个条件得到满足时停止解析。一般来说，SAX 还比它的替代者 DOM 快许多。\n选择 DOM 还是选择 SAX？\n对于需要自己编写代码来处理 XML 文档的开发人员来说，\n选择 DOM 还是 SAX 解析模型是一个非常重要的设计决策。\nDOM 采用建立树形结构的方式访问 XML 文档，而 SAX 采用的是事件模型。\nDOM 解析器把 XML 文档转化为一个包含其内容的树，并可以对树进行遍历。用 DOM 解析模型的优点是编程容易，开发人员只需要调用建树的指令，然后利用 navigation\nAPIs 访问所需的树节点来完成任务。可以很容易的添加和修改树中的元素。然而由于使用 DOM 解析器的时候需要处理整个 XML 文档，所以对性能和内存的要求比较高，尤其是遇到很大的 XML 文件的时候。由于它的遍历能力，DOM 解析器常用于 XML 文档需要频繁的改变的服务中。\nSAX 解析器采用了基于事件的模型，它在解析 XML 文档的时候可以触发一系列的事件，当发现给定的 tag 的时候，它可以激活一个回调方法，告诉该方法制定的标签已经找到。SAX 对内存的要求通常会比较低，因为它让开发人员自己来决定所要处理的 tag. 特别是当开发人员只需要处理文档中所包含的部分数据时，SAX 这种扩展能力得到了更好的体现。但用 SAX 解析器的时候编码工作会比较困难，而且很难同时访问同一个文档中的多处不同数据。\n【优势】\n①不需要等待所有数据都被处理，分析就能立即开始。\n②只在读取数据时检查数据，不需要保存在内存中。\n③可以在某个条件得到满足时停止解析，不必解析整个文档。\n④效率和性能较高，能解析大于系统内存的文档。\n【缺点】\n①需要应用程序自己负责 TAG 的处理逻辑（例如维护父 / 子关系等），文档越复杂程序就越复杂。\n②单向导航，无法定位文档层次，很难同时访问同一文档的不同部分数据，不支持 XPath。\nJDOM(Java-based Document\nObject Model)\nJDOM 的目的是成为 Java 特定文档模型，它简化与 XML 的交互并且比使用 DOM 实现更快。由于是第一个 Java 特定模型，JDOM 一直得到大力推广和促进。正在考虑通过 “Java 规范请求 JSR-102” 将它最终用作 “Java 标准扩展”。从 2000 年初就已经开始了 JDOM 开发。\nJDOM 与 DOM 主要有两方面不同。首先，JDOM 仅使用具体类而不使用接口。这在某些方面简化了 API，但是也限制了灵活性。第二，API 大量使用了 Collections 类，简化了那些已经熟悉这些类的 Java 开发者的使用。\nJDOM 文档声明其目的是 “使用 20%（或更少）的精力解决 80%（或更多）Java/XML 问题”（根据学习曲线假定为 20%）。JDOM 对于大多数 Java/XML 应用程序来说当然是有用的，并且大多数开发者发现 API 比 DOM 容易理解得多。JDOM 还包括对程序行为的相当广泛检查以防止用户做任何在 XML 中无意义的事。然而，它仍需要您充分理解 XML 以便做一些超出基本的工作（或者甚至理解某些情况下的错误）。这也许是比学习 DOM 或 JDOM 接口都更有意义的工作。\nJDOM 自身不包含解析器。它通常使用 SAX2 解析器来解析和验证输入 XML 文档（尽管它还可以将以前构造的 DOM 表示作为输入）。它包含一些转换器以将 JDOM 表示输出成 SAX2 事件流、DOM 模型或 XML 文本文档。JDOM 是在 Apache 许可证变体下发布的开放源码。\n【优点】\n①使用具体类而不是接口，简化了 DOM 的 API。\n②大量使用了 Java 集合类，方便了 Java 开发人员。\n【缺点】\n①没有较好的灵活性。\n②性能较差。\nDOM4J(Document Object Model\nfor Java)\n虽然 DOM4J 代表了完全独立的开发结果，但最初，它是 JDOM 的一种智能分支。它合并了许多超出基本 XML 文档表示的功能，包括集成的 XPath 支持、XML\nSchema 支持以及用于大文档或流化文档的基于事件的处理。它还提供了构建文档表示的选项，它通过 DOM4J\nAPI 和标准 DOM 接口具有并行访问功能。从 2000 下半年开始，它就一直处于开发之中。\n为支持所有这些功能，DOM4J 使用接口和抽象基本类方法。DOM4J 大量使用了 API 中的 Collections 类，但是在许多情况下，它还提供一些替代方法以允许更好的性能或更直接的编码方法。直接好处是，虽然 DOM4J 付出了更复杂的 API 的代价，但是它提供了比 JDOM 大得多的灵活性。\n在添加灵活性、XPath 集成和对大文档处理的目标时，DOM4J 的目标与 JDOM 是一样的：针对 Java 开发者的易用性和直观操作。它还致力于成为比 JDOM 更完整的解决方案，实现在本质上处理所有 Java/XML 问题的目标。在完成该目标时，它比 JDOM 更少强调防止不正确的应用程序行为。\nDOM4J 是一个非常非常优秀的 Java XML\nAPI，具有性能优异、功能强大和极端易用使用的特点，同时它也是一个开放源代码的软件。如今你可以看到越来越多的 Java 软件都在使用 DOM4J 来读写 XML，特别值得一提的是连 Sun 的 JAXM 也在用 DOM4J.\n【优点】\n①大量使用了 Java 集合类，方便 Java 开发人员，同时提供一些提高性能的替代方法。\n②支持 XPath。\n③有很好的性能。\n【缺点】\n①大量使用了接口，API 较为复杂。\n比较\n\nDOM4J 性能最好，连 Sun 的 JAXM 也在用 DOM4J。目前许多开源项目中大量采用 DOM4J，例如大名鼎鼎的 Hibernate 也用 DOM4J 来读取 XML 配置文件。如果不考虑可移植性，那就采用 DOM4J.\nJDOM 和 DOM 在性能测试时表现不佳，在测试 10M 文档时内存溢出，但可移植。在小文档情况下还值得考虑使用 DOM 和 JDOM. 虽然 JDOM 的开发者已经说明他们期望在正式发行版前专注性能问题，但是从性能观点来看，它确实没有值得推荐之处。另外，DOM 仍是一个非常好的选择。DOM 实现广泛应用于多种编程语言。它还是许多其它与 XML 相关的标准的基础，因为它正式获得 W3C 推荐（与基于非标准的 Java 模型相对），所以在某些类型的项目中可能也需要它（如在 JavaScript 中使用 DOM）。\nSAX 表现较好，这要依赖于它特定的解析方式－事件驱动。一个 SAX 检测即将到来的 XML 流，但并没有载入到内存（当然当 XML 流被读入时，会有部分文档暂时隐藏在内存中）。\n\n我的看法：如果 XML 文档较大且不考虑移植性问题建议采用 DOM4J；如果 XML 文档较小则建议采用 JDOM；如果需要及时处理而不需要保存数据则考虑 SAX。但无论如何，还是那句话：适合自己的才是最好的，如果时间允许，建议大家讲这四种方法都尝试一遍然后选择一种适合自己的即可。\n示例\n为了节约篇幅，这里暂时不给出这四种建立 XML 文档的方法与差异，仅给出解析 XML 文档的代码，如果需要完整工程（建立 XML 文档 + 解析 XML + 测试比较），可去我的 CSDN 下载。\n这里以下面的 XML 内容为例进行解析：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  &lt;users&gt;      &lt;user id=\"0\"&gt;          &lt;name&gt;Alexia&lt;/name&gt;          &lt;age&gt;23&lt;/age&gt;          &lt;sex&gt;Female&lt;/sex&gt;      &lt;/user&gt;      &lt;user id=\"1\"&gt;          &lt;name&gt;Edward&lt;/name&gt;          &lt;age&gt;24&lt;/age&gt;          &lt;sex&gt;Male&lt;/sex&gt;      &lt;/user&gt;      &lt;user id=\"2\"&gt;          &lt;name&gt;wjm&lt;/name&gt;          &lt;age&gt;23&lt;/age&gt;          &lt;sex&gt;Female&lt;/sex&gt;      &lt;/user&gt;      &lt;user id=\"3\"&gt;          &lt;name&gt;wh&lt;/name&gt;          &lt;age&gt;24&lt;/age&gt;          &lt;sex&gt;Male&lt;/sex&gt;      &lt;/user&gt;  &lt;/users&gt;\n首先定义 XML 文档解析的接口：\n/**   * @author Alexia   *   * 定义XML文档解析的接口   */  public interface XmlDocument {    /**       * 解析XML文档       *       * @param fileName       *            文件全路径名称       */      public void parserXml(String fileName);  }  \nDOM 示例\npackage com.xml;import java.io.FileNotFoundException;  import java.io.FileOutputStream;  import java.io.IOException;  import java.io.PrintWriter;  import javax.xml.parsers.DocumentBuilder;  import javax.xml.parsers.DocumentBuilderFactory;  import javax.xml.parsers.ParserConfigurationException;  import javax.xml.transform.OutputKeys;  import javax.xml.transform.Transformer;  import javax.xml.transform.TransformerConfigurationException;  import javax.xml.transform.TransformerException;  import javax.xml.transform.TransformerFactory;  import javax.xml.transform.dom.DOMSource;  import javax.xml.transform.stream.StreamResult;  import org.w3c.dom.Document;  import org.w3c.dom.Element;  import org.w3c.dom.Node;  import org.w3c.dom.NodeList;  import org.xml.sax.SAXException;/**   * @author Alexia   *   * DOM 解析XML文档   */  public class DomDemo implements XmlDocument {      private Document document;    public void parserXml(String fileName) {          try {              DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();              DocumentBuilder db = dbf.newDocumentBuilder();              Document document = db.parse(fileName);              NodeList users = document.getChildNodes();            for (int i = 0; i &lt; users.getLength(); i++) {                  Node user = users.item(i);                  NodeList userInfo = user.getChildNodes();                for (int j = 0; j &lt; userInfo.getLength(); j++) {                      Node node = userInfo.item(j);                      NodeList userMeta = node.getChildNodes();                    for (int k = 0; k &lt; userMeta.getLength(); k++) {                          if(userMeta.item(k).getNodeName() != \"#text\")                              System.out.println(userMeta.item(k).getNodeName()                                      + \":\" + userMeta.item(k).getTextContent());                      }                    System.out.println();                  }              }        } catch (FileNotFoundException e) {              e.printStackTrace();          } catch (ParserConfigurationException e) {              e.printStackTrace();          } catch (SAXException e) {              e.printStackTrace();          } catch (IOException e) {              e.printStackTrace();          }      }  }  \nSAX 示例\npackage com.xml;import java.io.FileInputStream;  import java.io.FileNotFoundException;  import java.io.FileOutputStream;  import java.io.IOException;  import java.io.InputStream;  import java.io.OutputStream;  import java.io.StringWriter;import javax.xml.parsers.ParserConfigurationException;  import javax.xml.parsers.SAXParser;  import javax.xml.parsers.SAXParserFactory;  import javax.xml.transform.OutputKeys;  import javax.xml.transform.Result;  import javax.xml.transform.Transformer;  import javax.xml.transform.TransformerConfigurationException;  import javax.xml.transform.sax.SAXTransformerFactory;  import javax.xml.transform.sax.TransformerHandler;  import javax.xml.transform.stream.StreamResult;import org.xml.sax.Attributes;  import org.xml.sax.SAXException;  import org.xml.sax.helpers.AttributesImpl;  import org.xml.sax.helpers.DefaultHandler;/**   * @author Alexia   *   * SAX 解析XML文档   */  public class SaxDemo implements XmlDocument {    public void parserXml(String fileName) {          SAXParserFactory saxfac = SAXParserFactory.newInstance();        try {              SAXParser saxparser = saxfac.newSAXParser();              InputStream is = new FileInputStream(fileName);              saxparser.parse(is, new MySAXHandler());          } catch (ParserConfigurationException e) {              e.printStackTrace();          } catch (SAXException e) {              e.printStackTrace();          } catch (FileNotFoundException e) {              e.printStackTrace();          } catch (IOException e) {              e.printStackTrace();          }      }  }class MySAXHandler extends DefaultHandler {      boolean hasAttribute = false;      Attributes attributes = null;    public void startDocument() throws SAXException {          // System.out.println(\"文档开始打印了\");      }    public void endDocument() throws SAXException {          // System.out.println(\"文档打印结束了\");      }    public void startElement(String uri, String localName, String qName,              Attributes attributes) throws SAXException {          if (qName.equals(\"users\")) {              return;          }          if (qName.equals(\"user\")) {              return;          }          if (attributes.getLength() &gt; 0) {              this.attributes = attributes;              this.hasAttribute = true;          }      }    public void endElement(String uri, String localName, String qName)              throws SAXException {          if (hasAttribute &amp;&amp; (attributes != null)) {              for (int i = 0; i &lt; attributes.getLength(); i++) {                  System.out.print(attributes.getQName(0) + \":\"                          + attributes.getValue(0));              }          }      }    public void characters(char[] ch, int start, int length)              throws SAXException {          System.out.print(new String(ch, start, length));      }  }  \nJDOM 示例\npackage com.xml;import java.io.FileNotFoundException;  import java.io.FileOutputStream;  import java.io.IOException;  import java.util.List;import org.jdom2.Document;  import org.jdom2.Element;  import org.jdom2.JDOMException;  import org.jdom2.input.SAXBuilder;  import org.jdom2.output.XMLOutputter;/**   * @author Alexia   *   * JDOM 解析XML文档   *   */  public class JDomDemo implements XmlDocument {    public void parserXml(String fileName) {          SAXBuilder builder = new SAXBuilder();        try {              Document document = builder.build(fileName);              Element users = document.getRootElement();              List userList = users.getChildren(\"user\");            for (int i = 0; i &lt; userList.size(); i++) {                  Element user = (Element) userList.get(i);                  List userInfo = user.getChildren();                for (int j = 0; j &lt; userInfo.size(); j++) {                      System.out.println(((Element) userInfo.get(j)).getName()                              + \":\" + ((Element) userInfo.get(j)).getValue());                }                  System.out.println();              }          } catch (JDOMException e) {              e.printStackTrace();          } catch (IOException e) {              e.printStackTrace();          }    }  }  \nDOM4J 示例\npackage com.xml;import java.io.File;  import java.io.FileWriter;  import java.io.IOException;  import java.io.Writer;  import java.util.Iterator;import org.dom4j.Document;  import org.dom4j.DocumentException;  import org.dom4j.DocumentHelper;  import org.dom4j.Element;  import org.dom4j.io.SAXReader;  import org.dom4j.io.XMLWriter;/**   * @author Alexia   *   * Dom4j 解析XML文档   */  public class Dom4jDemo implements XmlDocument {    public void parserXml(String fileName) {          File inputXml = new File(fileName);          SAXReader saxReader = new SAXReader();        try {              Document document = saxReader.read(inputXml);              Element users = document.getRootElement();              for (Iterator i = users.elementIterator(); i.hasNext();) {                  Element user = (Element) i.next();                  for (Iterator j = user.elementIterator(); j.hasNext();) {                      Element node = (Element) j.next();                      System.out.println(node.getName() + \":\" + node.getText());                  }                  System.out.println();              }          } catch (DocumentException e) {              System.out.println(e.getMessage());          }      }}  \n","categories":["Java"],"tags":["Java"]},{"title":"LeetCode 解题报告 (22)-- 生成所有合法的嵌套括号","url":"/2016/03/29/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(22)--%E7%94%9F%E6%88%90%E6%89%80%E6%9C%89%E5%90%88%E6%B3%95%E7%9A%84%E5%B5%8C%E5%A5%97%E6%8B%AC%E5%8F%B7/","content":"原题如下：\n&gt;Given n pairs of parentheses, write a function to generate all\ncombinations of well-formed parentheses.\n\n&gt;For example, given n = 3, a solution set is:\n\"((()))\", \"(()())\", \"(())()\", \"()(())\", \"()()()\"\n\n题目要求生成给定数量的所有合法嵌套括号，最直观的想法是通过穷举得到所有的组合结果，再判断每个是否合法，但是这样的时间复杂度太大，会导致 TLE。\n既然上面的方法会导致 TLE,\n那么能不能在构造的过程中判断目前所构造的嵌套括号是否合法？\n答案是可以的，只需要遵循以下的两条规则即可:\n1. 只要 \"(\" 的数量没有超过 n，都可以插入 \"(\"。\n2. 而可以插入 \")\" 的前提则是当前的 \"(\" 数量必须要多余当前的 \")\" 数量。\n通过回溯法能够实现这个判断并构造出所有合法的嵌套括号，实现代码如下：\nclass Solution(object):      def generateParenthesis(self, n):          \"\"\"          :type n: int          :rtype: List[str]          \"\"\"          res = []          self.helper(n,n,'',res)          return res    def  helper(self,left,right,item,res):          if left==0 and right==0:              res.append(item)              return          if left&gt;0:              self.helper(left-1,right,item+'(',res)          if right&gt;left:              self.helper(left,right-1,item+')',res)  \n","categories":["算法"],"tags":["python","回溯法"]},{"title":"Java 中 String 和 byte [] 的转换","url":"/2016/01/13/Java%E4%B8%ADString%E5%92%8Cbyte%5B%5D%E7%9A%84%E8%BD%AC%E6%8D%A2/","content":"在 Java 的网络编程中，通过 socket 发送和接收到的数据是 byte [\n] 类型的，而我们希望发送的消息一般是用 String 类型表示的，所以在 Java 的网络编程中，发送端需要将 String 类型转换为 byte [\n] 类型，接收端需要将 byte [ ] 类型转换为 String 类型。\n\nString 类型转换为 byte [ ] 类型\n代码如下：\nString sendString=\"hello\";  byte[] sendByte=sendString.getBytes(\"UTF8\");  ```  除了UTF8编码方式外，还可以采用\"GBK\",\"IOS-8859-1\"等编码方式，这几种编码方式的差别在于表示同一个字符采用的字节数不同，而且为了中文显示不乱码，一般采用UTF8的编码方式。## byte[ ]类型转换为String类型  代码如下：```java  String receiveString=new String(receiveBytes,\"UTF8\");  \nreceiveBytes 为需要转为 String 的字节数组，\"UTF8\" 的作用也是为了指定转换采用的编码方式，省略时会采用系统默认的编码方式。\n","categories":["Java"],"tags":["Java"]},{"title":"LeetCode 解题报告 (402,316,321)-- 删除字符串 k 个字符使剩余字符串取最值","url":"/2016/09/25/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(402,316,321)--%E5%88%A0%E9%99%A4%E5%AD%97%E7%AC%A6%E4%B8%B2k%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%BD%BF%E5%89%A9%E4%BD%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8F%96%E6%9C%80%E5%80%BC/","content":"本文主要讲述如何解决这一类问题：给定一个含有数字或英文字母的字符串，从中删除 k 个字符，使得剩下的字符取得最小值或最大值。数字的大小的比较容易理解，而字母的大小则是按照其 ASCII 码来排列，如 'abc'&gt;'abd'。\n\n下面以 LeetCode 上的几道题目为例进行讲解： 402. Remove K\nDigits 316.\nRemove Duplicate Letters 321. Create\nMaximum Number\n解决这类题目的关键点是借助栈这种数据结构，遍历给出的字符串，将当前元素与栈顶元素比较大小，从而决定是否要将当前元素出栈，最后栈内剩余元素就是所需结果。这只是大致的过程，具体的步骤需要根据题目的具体要求。下面以上面的题目为例讲解\n402. Remove\nK Digits 这个题目要求去掉给定字符串（全是数字）中的 k\n个字符，使得剩余的字符串表示的数字最小。根据我们上面说到的大致流程，这道题目的解决步骤如下：\n1. 创建一个栈，以及记录当前已经删除的字符数量的计数器\n2. 对于字符串中的每个字符，记为当前字符，先将其与栈顶元素比较（假如栈不为空），若当前字符小于栈顶元素，则将栈顶元素出栈，将计数器加一，重复该操作直到栈为空或栈顶元素小于当前元素或计数器等于 k，然后将当前元素入栈\n具体的 python 代码如下所示： class Solution(object):    def removeKdigits(self, num, k):        \"\"\"        :type num: str        :type k: int        :rtype: str        \"\"\"        stack = []        remain = len(num) - k        for dig in num:            while k and stack and stack[-1] &gt; dig:                stack.pop()                k -= 1            stack.append(dig)        return ''.join(stack[:remain]).lstrip('0') or '0'\n最后 return 语句之所以要选取前 len(num) - k 个字符是因为\nresult\n中可能会有多余这个数目的字符，如对于从小到大排列的字符串就会出现这种情况，另外还需要处理掉前缀 0 以及当结果为空的时候返回\n'0'\n316.\nRemove Duplicate Letters 的要求跟 402. Remove K\nDigits\n类似，只是这次要求删除的是字母，而且每个字符要求出现一次且只能出现一次。\n解决的思路跟上面的一样，只是因为要求每个字符出现且只出现一次，在入栈和出栈的时候需要特殊的限制条件。具体步骤入下\n1. 创建一个栈，记录每个字符在字符串中出现的次数的 table\n2. 对于字符串中的每个字符，先判断其是否已在栈内，若已在栈内，将 table 中对应该字符的计数减去 1，然后跳到字符串的下一字符；若不在栈内，栈顶元素大于当前字符且 table 内剩余的栈顶元素的个数大于 1 时，将栈顶元素出栈，重复该操作直到栈为空或栈顶元素小于当前元素，然后将当前字符入栈\n实现的 python 代码如下所示： class Solution(object):    def removeDuplicateLetters(self, s):        \"\"\"        :type s: str        :rtype: str        \"\"\"        result, stored, count = [], set(), {}        for char in s:             count.setdefault(char, 0)             count[char] += 1        for char in s:            if char in stored:                count[char] -= 1                continue            else:                while result and result[-1] &gt; char and count[result[-1]] &gt; 1:                    count[result[-1]] -= 1                    stored.remove(result.pop())                stored.add(char)                result.append(char)        return ''.join(result)\n321.\nCreate Maximum Number\n题目要求与上面两题相比不是相似性不高，但是也是利用这种思想的，题目要求从两个数组\nnums1 和 nums2\n中共选出 k 个数字，从而使得这 k 个数字组成的数最大。\n解决方法就是先从 nums1 中选出 i 个数（0 &lt;= i &lt;=\nk）使得这 i 个数组成的数字最大，然后从 nums2 中选出 k-i\n个数，同样使得这 k-i\n个数组成的数字最大，最后将从两个数组中抽出的最大数字合并起来。\n实现的代码如下所示： class Solution(object):    def maxNumber(self, nums1, nums2, k):        \"\"\"        :type nums1: List[int]        :type nums2: List[int]        :type k: int        :rtype: List[int]        \"\"\"        return max(self.merge(self.single_max(nums1, i), self.single_max(nums2, k-i)) for i in xrange(k+1) if i &lt;= len(nums1) and (k-i) &lt;= len(nums2))    def single_max(self, nums, k):        drop = len(nums) - k        stack = []        for digit in nums:            while drop and stack and stack[-1] &lt; digit:                stack.pop()                drop -= 1            stack.append(digit)        return stack[:k]    def merge(self, nums1, nums2):        return [max(nums1,nums2).pop(0) for _ in xrange(len(nums1)+len(nums2))]\n上面的代码中 max(num1, num2) 中的 nums1 和\nnums2 是两个数组，比较的时候回比较两个数组的第一个元素，然后返回第一个元素较大的数组，若第一个元素相等，则比较第二个元素，依此类推；pop (0) 则是删除并返回下标为 0 的元素，也就是第一个元素。通过这些语法可以简化代码\n","categories":["算法"],"tags":["python","栈"]},{"title":"LeetCode 解题报告 (51)-- 回溯法解决 N 皇后问题","url":"/2016/05/16/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(51)--%E5%9B%9E%E6%BA%AF%E6%B3%95%E8%A7%A3%E5%86%B3N%E7%9A%87%E5%90%8E%E9%97%AE%E9%A2%98/","content":"原题如下：\n&gt;The n-queens puzzle is the problem of placing n queens on an n×n\nchessboard such that no two queens attack each other.  \n\nGiven an integer n, return all distinct solutions to the n-queens\npuzzle.\n\n\nEach solution contains a distinct board configuration of the\nn-queens' placement, where 'Q' and '.' both indicate a queen and an\nempty space respectively.\n\n\nFor example, There exist two distinct solutions to the 4-queens\npuzzle: [ [\".Q..\",  // Solution 1  \"...Q\",  \"Q...\",  \"..Q.\"], [\"..Q.\",  // Solution 2  \"Q...\",  \"...Q\",  \".Q..\"]]\n\n经典的 n 皇后问题，通过回溯法解决。\n关键的地方有以下几点：\n1）由于题目本身的特点，每一行只能放置一个皇后，因此每行只要找到第一个合适的位置便可跳到下一行找下一个皇后的合适位置。\n2）由于遍历的顺序是从坐到又从上到下的，因此判断当前位置能否放置皇后时只需要判断当前位置上方的列、左上斜边、右上斜边是否有皇后即可。\n实现的代码如下所示： class Solution(object):    def solveNQueens(self, n):        \"\"\"        :type n: int        :rtype: List[List[str]]        \"\"\"        solutions = []        board = [['.' for i in range(n)] for j in range(n)] #生成n*n的原始矩阵        self.helper(board, solutions,0 )        return solutions    def helper(self,board,solutions, row):        n = len(board)        if row==n:            tmp=[]                      for i in board:                tmp.append(reduce(lambda x,y:x+y,i)) # reduce函数的作用是将['Q','.','.','.']转为'Q...'            solutions.append(tmp)            return        for i in range(n):            if self.is_valid(board,row,i):                board[row][i] = 'Q'                self.helper(board,solutions,row+1)                board[row][i] = '.'    def is_valid(self,board,i,j):        n = len(board)        # check column        for k in xrange(n):            if board[k][j] == 'Q':                return False        # check diagonal，只需要检查上面的两条边        ri = i        rj = j        while 0&lt;=i&lt;=n-1 and 0&lt;=j&lt;=n-1:            if board[i][j]=='Q':                return False            i-=1            j-=1        i = ri        j = rj        while 0&lt;=i&lt;=n-1 and 0&lt;=j&lt;=n-1:            if board[i][j]=='Q':                return False            i-=1            j+=1        return True\n上面的代码虽然结果正确，但是判断当前位置能否放置皇后的方法效率较低，判断的一次的平均时间复杂度为O(n)，n为棋盘的边的大小，下面主要介绍一种判断当前位置能否放置皇后的时间复杂度为O(1)的方法。\n通过观察可发现，在同一条左斜边上的每个点的行值加上列值的结果相等，而在同一条右斜边上的每个点的行值减去列值的结果相等。如下图所示就是左斜线每个点的行值加上列值的情况：\n\n右斜线每个点的行值减去列值的情况同理。\n由于这些结果大小是连续的，因此我们可以使用数组的下标代表某一斜边，用该下标的对应的数组值（0 或 1）表示该斜边上是否有皇后。设棋盘的大小为 n*n。则左斜边的范围为 [0,2n-2], 长度为 2n+1；右斜边的范围为 [-n+1,n-1], 由于数组的下标不能为负，所以每个下标加上 n-1，将范围变为 [0,2n-2], 长度同样为 2n-1。\n实现的代码如下所示 class Solution(object):    def solveNQueens(self, n):        \"\"\"        :type n: int        :rtype: List[List[str]]        \"\"\"        solutions = []        board = [['.' for i in range(n)] for j in range(n)] #生成n*n的原始矩阵        col = [0 for i in range(n)]        left_dia =  [0 for i in range(2*n-1)]        right_dia = [0 for i in range(2*n-1)]        self.helper(board, solutions,0, col,left_dia,right_dia)        return solutions    def helper(self,board,solutions,row,col,left_dia,right_dia):        n = len(board)        if row==n:            tmp=[]                      for i in board:                tmp.append(reduce(lambda x,y:x+y,i)) # reduce函数的作用是将['Q','.','.','.']转为'Q...'            solutions.append(tmp)            return        for i in range(n):            if col[i]==0 and left_dia[row+i]==0 and right_dia[i-row+n-1]==0:                board[row][i] = 'Q'                col[i],left_dia[row+i],right_dia[i-row+n-1]=1,1,1                self.helper(board,solutions,row+1,col,left_dia,right_dia)                board[row][i] = '.'                col[i],left_dia[row+i],right_dia[i-row+n-1]=0,0,0\n","categories":["算法"],"tags":["python","回溯法"]},{"title":"LeetCode 解题报告 (739,901,907)- 线性时间寻找数组中各个元素作为最值的最大范围","url":"/2018/12/28/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(739,901,907)-%E7%BA%BF%E6%80%A7%E6%97%B6%E9%97%B4%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E5%90%84%E4%B8%AA%E5%85%83%E7%B4%A0%E4%BD%9C%E4%B8%BA%E6%9C%80%E5%80%BC%E7%9A%84%E6%9C%80%E5%A4%A7%E8%8C%83%E5%9B%B4/","content":"题目有点拗口，其实就是给定一个数组，要求给出某个元素作为最小值或最小值的那些\ncontinous subarrays 中最长的长度，如对于数组 [1, 2, 5, 6],\n元素 5 作为最大值的 continous subarrays 有三个：\n[5], [2, 5], [1, 2, 5]，长度最长的是\n3。遍历的解法找出一个元素要 \\(O(n)\\)\n的时间复杂度，找出所有元素则需要 \\(O(n^2)\\)\n的时间复杂度，而通过栈能够在 \\(O(n)\\)\n的时间复杂度内解决这个问题。\n\n下面这两个题目都直接用到了这种解法\n739.\nDaily Temperatures 901.\nOnline Stock Span\n以 901. Online Stock Span\n为例，如下所示，其实题目就要找出某个元素作为最大值时 subarray\n的最大长度，且这个 subarray 是有方向的，即只能从当前元素往左延伸\n\nWrite a class StockSpanner which collects daily price quotes for some\nstock, and returns the span of that stock's price for the current\nday.\nThe span of the stock's price today is defined as the maximum number\nof consecutive days (starting from today and going backwards) for which\nthe price of the stock was less than or equal to today's price.\nFor example, if the price of a stock over the next 7 days were [100,\n80, 60, 70, 60, 75, 85], then the stock spans would be [1, 1, 1, 2, 1,\n4, 6].\n\n暴力的遍历需要 \\(O(n^2)\\)\n的时间复杂度，那通过栈如何在 \\(O(n)\\)\n的时间复杂度解决这个问题呢？\n首先创建一个空栈用于存储每个元素的下标，从左到右遍历数组中的元素，对于当前元素，假如栈为空或栈顶元素大于当前元素，则将当前元素入栈，否则一直出栈直到栈为空或栈顶元素大于当前元素，这样做将左边比当前元素小的元素都出栈，最后栈顶元素（如果有）和当前元素之间的距离就是要求的距离，如果栈为空，则当前元素是当前遍历的所有元素中最大的，其\n下标 + 1 便是要求的距离。\n由于每个元素最多会被入栈一次和出栈一次，因此其时间复杂度便是 \\(O(n)\\),\n其减小时间复杂度的原理其实就是通过出栈减少了元素比较的次数，比如说对于当前元素\ne，出栈了 k 个元素，那后面如果有个元素比 e 大，肯定也比出栈的 k\n个元素要大，因此无需进行比较，而这 k 个元素已经出栈了，因此减少了这 k\n次的比较。\n另外，需要注意的是，由于要求的是距离，因此栈存储的是元素的下标，比较元素大小是通过原数组加下标即可。\n因此 901 题目的 python 代码如下，也有 c++ 版本\n和 go 版本\nclass StockSpanner(object):    def __init__(self):        self.nums = []        self.stack = []        self.idx = 1    def next(self, price):        \"\"\"        :type price: int        :rtype: int        \"\"\"        while self.stack and self.nums[self.stack[-1]-1] &lt;= price:            self.stack.pop()        self.nums.append(price)        span = self.idx-self.stack[-1] if self.stack else self.idx        self.stack.append(self.idx)        self.idx += 1        return span\n739.Daily Temperatures\n的题目原理是一样的，只是要求的是最小元素往右的最长\nsubarray，解法同上，只是此时需要从后往前遍历数组了，具体的代码可见: python 版本，c++ 版本，go 版本\n拓展\n上面两个题目是比较明确要求出各个元素作为最小值或最大值时的最长\nsubarray 的长度，但是有些问题不会直接要求这么求解。比如说题目 907.\nSum of Subarray Minimums, 题目要求的是求出所有 subarray\n中最小元素的和，下面是一个简单的例子\nInput: [3,1,2,4]Output: 17Explanation: Subarrays are [3], [1], [2], [4], [3,1], [1,2], [2,4], [3,1,2], [1,2,4], [3,1,2,4]. Minimums are 3, 1, 2, 4, 1, 1, 2, 1, 1, 1.  Sum is 17.\n通过穷举法求解的时间复杂度显然太高了，但是我们可以换一个角度来求解这个问题，就是只要求出某个元素作为最小值的\nsubarray 个数，那么该元素乘上 subarray\n个数便是这个元素对最终的结果的贡献值，比如说比如说对于某个长度为\nn 的数组 A, 其各个元素作为最小值的 subarray 个数分别是\nf[0], f[1].....f[n-1], 则最终结果为\n\\[\\begin{align} \\sum_{i=0}^{n-1} A[i]f[i]\n\\end{align}\\]\n那么现在的问题就是要求出各个元素作为最小值的 subarray\n个数，这就要用到了我们前面提到的通过栈求解的方法了，而且要分别往左和往右求出\nsubarray 的长度。\n对于当前元素 A[i],\n将当前值作为最小值，分别往左和往右求出的 subarray 长度记为 left 和\nright，则包含 A[i] 作为最小值的 subarray 个数为：\nf[i] = left * right\n实现的 python 代码如下，也可参考 c++ 版本\n和 go 版本\nclass Solution(object):    def sumSubarrayMins(self, A):        \"\"\"        :type A: List[int]        :rtype: int        \"\"\"        n = len(A)        s, left = [], []        for i in xrange(n):            while s and A[s[-1]] &gt;= A[i]:                s.pop()            left.append(i - s[-1] if s else i + 1)            s.append(i)        s, right = [], []        for i in xrange(n - 1, -1, -1):            while s and A[s[-1]] &gt; A[i]:                s.pop()            right.append(s[-1] - i if s else n - i)            s.append(i)    return sum(A[i] * left[i] * right[n-i-1] for i in xrange(n)) % (10**9 + 7) \n上面的思路就是分别从左到右和从右到左获取 left 和 right 这两个表示\nsubarray 数量的数组，需要注意的是，获取 left 数组是用的比较条件是\nA[s[-1]] &gt;= A[i], 但是获取 right 数组时用的是\nA[s[-1]] &gt; A[i]; 这里以一个例子简介会比较方便，假如说对于数组\n[71,55,82,55], 如果两个比较条件都采用\n&gt;=，那么 [55,82,55] 这个 subarray\n会被重复计算两次， 如果都采用 &gt;, [55,82,55]\n则不会被计算，因此一定要有一个采用 &gt;=, 而另一个采用\n&gt;。\n这里其实也引出另外一个非常重要的思想，就是分别求出每个元素对最终结果的贡献，然后累加起来便是最终的结果，在上面的问题即是求出某个元素作为最小值的\nsubarray 个数，那么该元素乘上 subarray\n个数便是这个元素对最终的结果的贡献值。且这一类问题一般都跟\nsubarray、 subsequence 相关，比如说 828.\nUnique Letter String 和 891.\nSum of Subsequence Widths 都是通过这种思想来解决的\n首先看问题 828. Unique Letter String，题目要求出所有的 subarray 中的\nunique characters 的数量，遍历的时间复杂度是 \\(O(n^3)\\),\n但是采用上面提到的思想，可以分别求出每个元素作为 unique character 时的\nsubarray 的数量，然后累加起来即可，这样的时间复杂度变为了 \\(O(n^2)\\), AC 的 python 代码如下，\n另外也可参考 c++ 版本\n和 go 版本\nclass Solution(object):    def uniqueLetterString(self, S):        \"\"\"        :type S: str        :rtype: int        \"\"\"        result = 0        MOD = 1000000007        for i in xrange(len(S)):            left, right = i - 1, i + 1            while left &gt;=0 and S[left] != S[i]:                left -= 1            while right &lt; len(S) and S[right] != S[i]:                right += 1            result += ((i - left) * (right - i)) % MOD            result %= MOD        return result\n问题 891. Sum of Subsequence Widths 与前面的不同，前面的都是连续的\nsubarray， 而这里是可以不连续的\nsubsequence，题目要求出每个 subsequence\n中最大值和最小值的差，然后求和得到最终的结果。同样采用上面的思想，求出某个元素\nA[i] 作为最小值的 subsequence 的数量 n1, 作为最大值的\nsubsequence 的数量 n2, 则 A[i] 对最终结果的贡献是\nn2 * A[i] - n1 * A[i]。但是 n1、n2\n不能像之前一样分别往左往右延伸获取了，这里有一个很重要但是很容易被忽略的事实：数组的顺序不影响最终的结果。因此可以将数组进行排序，n1\n就是当前元素左边元素的一个组合数量了，n2 同理。AC 的 python 代码如下，\n在实现中计算 \\(2^i\\) 使用位移操作即\n1&lt;&lt;i 而不是直接计算，否则会导致超时\nclass Solution(object):    def sumSubseqWidths(self, A):        \"\"\"        :type A: List[int]        :rtype: int        \"\"\"        MOD = 1000000007        n = len(A)        A.sort()        return sum((((1 &lt;&lt; i) - (1 &lt;&lt; (n-i-1))) * A[i]) % MOD for i in xrange(n)) % MOD\n小结\n本文主要介绍了两个重要的思想，其中一个是通过栈在线性时间内求解数组中某个元素作为最小值（或最大值）的最长\nsubarray，代表性的题目有 739.\nDaily Temperatures、901.\nOnline Stock Span 和 907.\nSum of Subarray\nMinimums；另外一个重要的思想是分别求出每个元素对最终结果的贡献，然后累加起来便是最终的结果，代表性的题目有\n828.\nUnique Letter String、891.\nSum of Subsequence Widths 和 907.\nSum of Subarray Minimums。\n","categories":["算法"],"tags":["栈"]},{"title":"LeetCode 解题报告 (684,685,721)- 并查集介绍及应用","url":"/2017/10/12/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(684,685,721)-%E5%B9%B6%E6%9F%A5%E9%9B%86%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%BA%94%E7%94%A8/","content":"本文主要以 LeetCode 上的几道题目： 684.\nRedundant Connection 、 685.\nRedundant Connection II 和 721.\nAccounts Merge 为例讲解并查集（merge–find\nset）这种数据结构的应用。\n\n说到并查集，不得不提的是最小生成树，因为并查集的最经典的应用地方便是解决最小生成树的\nKruskal 算法。\n最小生成树\n有两个经典的算法可用来解决 最小生成树\n问题： Kruskal\n算法 和 Prim\n算法。其中 Kruskal\n算法中便应用了并查集这种数据结构，该算法的步骤如下\n\n新建图 G，G 中拥有原图中相同的节点，但没有边\n将原图中所有的边按权值从小到大排序\n从权值最小的边开始，如果这条边连接的两个节点于图 G 中不在同一个连通分量中，则添加这条边到图 G 中\n重复 3，直至图 G 中所有的节点都在同一个连通分量中\n\n该算法的动图显示如下 (摘自维基百科)\n\n\nKruskalDemo.gif-415.5kB\n\nKruskal 算法很简单，实际上 Kruskal\n算法是一种贪心算法，并且已被证明最终能够收敛到最好结果。而在实现 Kruskal\n算法时，则需要用到并查集这种数据结构来减小算法的时间复杂度。下面将详细介绍这种数据结构。\n在介绍并查集前，顺便介绍一下 Prime 算法，Prime\n算法也是一种贪心算法，而且也被证明了最终能够得到最好的结果，只是两者的侧重点不同，\nKruskal 算法维护的是一个边的集合，而 Prime\n算法则同时维护了一个边的集合和一个点的集合，Prim 算法的过程如下\n\n输入：一个加权连通图，其中顶点集合为 V，边集合为 E；\n初始化：Vnew = {x}，其中 x 为集合 V 中的任一节点（起始点），Enew =\n{}；\n重复下列操作，直到 Vnew = V：\n\n在集合 E 中选取权值最小的边（u, v），其中 u 为集合 Vnew\n中的元素，而 v 则是 V 中没有加入 Vnew 的顶点（如果存在有多条满足前述条件即具有相同权值的边，则可任意选取其中之一）；\n将 v 加入集合 Vnew 中，将（u, v）加入集合 Enew 中；\n\n输出：使用集合 Vnew 和 Enew 来描述所得到的最小生成树。\n\n其动图描述如下 (摘自维基百科)\n\n\nPrimAlgDemo.gif-51.1kB\n\n并查集\n在上面描述的 Kruskal 算法中，第三步是\n\n\n从权值最小的边开始，如果这条边连接的两个节点于图 G 中不在同一个连通分量中，则添加这条边到图 G 中\n\n\n而判断这条边连接的两个节点是否在同一个连通分量中，\n实际上就是判断加入了这条边后，是否会与原来已经添加的边形成环路，并查集正是高效的实现了这个功能。\n并查集主要有三种操作：MakeSet，Find 和 Union。\n\nMakeSet 是初始化操作，即为每个 node\n创建一个连通分量，且这个 node\n为这个连通分量的代表，这里连通分量的代表指的是当连通分量中有多个点时，需要从这些点中选出一个点来代表这个连通分量，而这个点也往往被称为这个连通分量的\nparent（意思即指这个点是其他点的 parent）\nFind 是指找到这个点所属的连通分量的 parent\nUnion\n是指将两个连通分量合并成一个连通分量，并选出代表这个连通分量的新的\nparent\n\n那么怎么通过上面这几种操作判断某条边是否会与原来的边形成环路呢？具体操作如下\n\n给定一条边，为这条边的两个顶点执行 Find 操作，假如两个顶点的 parent\n一样，那么说明这两个点已经在同一个连通分量中，再添加就会导致闭环\n当两个点的 parent 不同，即两个点在不同的连通分量时，需要通过 Union\n操作将这两个连通分量连起来\n重复 1、2 步操作直到所有边遍历完\n\n在具体实现，往往并不需要集合这种数据结构，而是仅仅通过数组即可，比如说有\nn 个点，那么就创建一个长度为 n\n的数组，每个下标代表一个点，而下标对应的值则代表这个点的 parent。\n并查集还有两个重要的概念 path compression 和 union by\nrank，目的均是降低时间复杂度，下面会详细说明。\n现在通过具体的题目来讲解上面提到若干概念\nRedundant Connection\n684.\nRedundant Connection\n这道题目实际上就是要找到一个无向图中形成环路的最后那条边 (输入保证了所有边会形成回路)。首先，看一种最简单的解决方法\nclass Solution:    def findRedundantConnection(self, edges):        parents = range(1001)        for edge in edges:            v1, v2 = edge[0], edge[1]            if parents[v1] == parents[v2]:                return edge            tmp = parents[v2]            for i in xrange(len(parents)):                if parents[i] == tmp:                    parents[i] = parents[v1]        return None\n这种方法中每次 Find 的时间复杂度为 \\(O(1)\\)（即 parents [v1] 操作）, 每次 Union\n则需要遍历所有的点，时间复杂度是 \\(O(n)\\)，总体时间复杂度是 \\(O(mn)\\), \\(m\\) 为边的数目，而 \\(n\\) 为点的数目。\n而我们也可以改变思路，就是进行 Union\n操作时不再将某个连通分量中所有点的 parent 改为另一个连通分量的\nparent，而是只改变那个连通分量的代表；这样进行 Find\n操作的时候只需要递归的查找即可，下面为这种思路对应的代码\nclass UnionFindSet(object):    def __init__(self):        self.parents = range(1001)    def find(self, val):        if self.parents[val] != val:            return self.find(self.parents[val])        else:            return self.parents[val]    def union(self, v1, v2):        p1, p2 = self.find(v1), self.find(v2)        if p1 == p2:            return True        else:            self.parents[p1] = p2            return Falseclass Solution(object):    def findRedundantConnection(self, edges):        \"\"\"        :type edges: List[List[int]]        :rtype: List[int]        \"\"\"        ufs = UnionFindSet()        for edge in edges:            if ufs.union(edge[0], edge[1]):                return edge\n这个方法每次 Union 的时间复杂度为 \\(O(1)\\), 但是每次 Find 的时间复杂度是 \\(O(n)\\)，所以总体时间复杂度还是 \\(O(mn)\\),\n那么有没有一种改进总体时间复杂度的方法呢？\n答案就是上面提到的 path compression 和 union by rank。\npath compression 指的是在上面的递归的 Find\n操作中，将最终得到的结果赋给递归过程中经过的所有点，从而降低连通分量的高度，实际上可以将一个连通分量当做一颗树，树的每个节点都连着其\nparent，而 path compression\n则相当于将搜寻路径中的所有点直接连到最终的那个 parent\n上，因此能够降低树的高度。\n降低树的高度有什么好处？那就是能够降低查找的时间复杂度，从 \\(O(n)\\) 降为了 \\(O(logn)\\),\n因为原来的递归搜索实际上是在一颗每个节点只有一个子节点的树上进行搜索，树的高度即为点的个数，而通过\npath compression 则能够有效降低树的高度。\n另外一个问题就是进行 Union\n操作时，需要将高度低的树连接到高度较高的树上，目的是为了减少 Union\n后的整棵树的高度，这就是 union by rank, rank 代表的就是树的高度。\n采用 path compression 和 union by rank 后，Find 的时间复杂度变为了\n\\(O(logn)\\), Union 的时间复杂度为 \\(O(1)\\), 因此总体时间复杂度是 \\(O(mlogn)\\), \\(m\\) 为边的数目，而 \\(n\\) 为点的数目。改进后的代码如下\nclass UnionFindSet(object):    def __init__(self):        self.parents = range(1001)        self.rank = [0] * 1001    def find(self, val):        \"\"\"find with path compression\"\"\"        if self.parents[val] != val:            self.parents[val] = self.find(self.parents[val])        return self.parents[val]    def union(self, v1, v2):        \"\"\"union by rank, check whether union two vertics will lead to a cycle\"\"\"        p1, p2 = self.find(v1), self.find(v2)        if p1 == p2:            return True        elif self.rank[p1] &gt; self.rank[p2]:            self.parents[p2] = p1        elif self.rank[p1] &lt; self.rank[p2]:            self.parents[p1] = p2        else:            self.rank[p2] += 1            self.parents[p1] = p2        return Falseclass Solution(object):    def findRedundantConnection(self, edges):        \"\"\"        :type edges: List[List[int]]        :rtype: List[int]        \"\"\"        ufs = UnionFindSet()        for edge in edges:            if ufs.union(edge[0], edge[1]):                return edge\nRedundant Connection II\n685.\nRedundant Connection II\n从前面的无向图升级到了有向图，对应的要求从原来的仅要求不形成环路升级到在不形成环路的基础上，拓扑必须要是一棵合法树，也就是每个点只能有一个父节点，例如\n[[2,1],[3,1]] 这两条边虽然没有形成环路，但是 1\n有两个父亲节点（2 和 3），因此不是一棵合法的树。\n由于题目说明了输入只有一条不合法的边，因此首先可以统计一下这些边中是否存在某个点有两个父亲节点，假如有，则需要移除的边必定为连着这个点的两条边中的一条，通过上面\nUnion-find\n的方法，可以判断出假如移除掉连着这个点的第一条边时，是否会形成回路。如果会，则说明需要移除第二条边，否则直接移除第一条边。\n如果统计的结果中没有点含有两个父亲节点，那么可以直接通过第一题的方法直接找到形成回路的最后那条边。AC 的代码如下\nclass UnionFindSet(object):    def __init__(self):        self.parents = range(1001)        self.rank = [0] * 1001    def find(self, val):        \"\"\"find with path compression\"\"\"        if self.parents[val] != val:            self.parents[val] = self.find(self.parents[val])        return self.parents[val]    def union(self, v1, v2):        \"\"\"union by rank, check whether union two vertics will lead to a cycle\"\"\"        p1, p2 = self.find(v1), self.find(v2)        if p1 == p2:            return True        elif self.rank[p1] &gt; self.rank[p2]:            self.parents[p2] = p1        elif self.rank[p1] &lt; self.rank[p2]:            self.parents[p1] = p2        else:            self.rank[p2] += 1            self.parents[p1] = p2        return Falseclass Solution(object):    def findRedundantDirectedConnection(self, edges):        \"\"\"        :type edges: List[List[int]]        :rtype: List[int]        \"\"\"        redundant_edges = None        count = {}        for e in edges:            if e[1] not in count:                count[e[1]] = []            count[e[1]].append(e)            if len(count[e[1]]) == 2:                redundant_edges = count[e[1]]                break        if redundant_edges:            ufs = UnionFindSet()            for edge in edges:                if edge == redundant_edges[1]:                    continue                if ufs.union(edge[0], edge[1]):                    return redundant_edges[0]            return redundant_edges[1]        else:            ufs = UnionFindSet()            for edge in edges:                if ufs.union(edge[0], edge[1]):                    return edge\nAccounts Merge\n这道题目虽然也用到了并查集的数据结构，但是与前面的两道题目又有点不同，主要体现在两个方面\n\n节点不再以数字标识，因此标识 parents 的数据结构要从 array 变为\nmap\n 不需要判断是否形成闭环，而要返回最终各个集合内的元素；在这个操作中需要注意的是不能直接利用存储各个节点的\nparent 的 map 直接为每个节点找到其 parent， 因为并非各个节点都进行了\npath compression。对应有两种方法 (1) 借助 find 方法找到各个节点的 parent\n(2) 对存储各个节点的 parent 的 map 再进行一次 path compression,\n然后直接在 map 中找到各个节点的 parent 对应的方法入下\n\n方法 (1)\nclass Solution(object):    def accountsMerge(self, accounts):        \"\"\"        :type accounts: List[List[str]]        :rtype: List[List[str]]        \"\"\"        owners, parents = {}, {}        for account in accounts:            owners[account[1]] = account[0]            for i in xrange(1, len(account)):                parents[account[i]] = account[i]        for account in accounts:            p = self.find(account[1], parents)            for i in xrange(1, len(account)):                parents[self.find(account[i], parents)] = p        unions = {}        for account in accounts:            for i in xrange(1, len(account)):                p = self.find(account[i], parents)                unions.setdefault(p, set())                unions[p].add(account[i])        result = []        for k, v in unions.items():            result.append([owners[k]] + sorted(v))        return result    def find(self, email, parents):        if parents[email] !=  email:            parents[email] = self.find(parents[email], parents)        return parents[email]\n方法 (2)\nclass Solution(object):    def accountsMerge(self, accounts):        \"\"\"        :type accounts: List[List[str]]        :rtype: List[List[str]]        \"\"\"        owners, parents = {}, {}        for account in accounts:            owners[account[1]] = account[0]            for i in xrange(1, len(account)):                parents[account[i]] = account[i]        for account in accounts:            p = self.find(account[1], parents)            for i in xrange(1, len(account)):                parents[self.find(account[i], parents)] = p        # not all paths are compressed currently        for k, v in parents.items():            if k!=v:                parents[k] = self.find(parents[v], parents)        unions = {}        for k, v in parents.items():            if v  not in unions:                unions[v] = set()            unions[v].add(k)        result = []        for k, v in unions.items():            result.append([owners[k]] + sorted(v))        return result    def find(self, email, parents):        if parents[email] !=  email:            parents[email] = self.find(parents[email], parents)        return parents[email]\n","categories":["算法"],"tags":["python","树","图"]},{"title":"LeetCode 解题报告 (10, 44)-- 正则表达式的匹配与通配符的匹配","url":"/2016/10/23/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(10,%2044)--%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E5%8C%B9%E9%85%8D%E4%B8%8E%E9%80%9A%E9%85%8D%E7%AC%A6%E7%9A%84%E5%8C%B9%E9%85%8D/","content":"正则表达式和通配符均是用来匹配字符串的，但是两者使用的范围不一样，通配符一般用在\nLinux\n命令行 shell 中，而正则表达式使用则更加广泛，在各种编程语言和工具中均有支持。下面主要讲述如何实现正则表达式和通配符的简单匹配。\n\n题目选自 LeetCode 的 10.\nRegular Expression Matching 和 44. Wildcard\nMatching，之所以说是简单匹配，是因为两者的实现的只是两个符号的功能。正则匹配要求实现\n* 和\n. 的匹配功能，而通配符匹配要求实现 ? 和\n* 的匹配。\n解决这种两个字符串的比较问题一般可考虑动态规划。以两个字符串的长度分别作为长和宽建立一个二维矩阵，通过二维动态规划解决。\n正则表达式\n在正则表达式中，. 表示任意一个单一字符，* 表示 0 个或若干个前一个字符（表示为 0 个的时候前一字符也失效，也就是\na* 可以匹配出空字符串）。\n首先我们建立了一个 m*n 的 dp 矩阵，其中 m 表示匹配模式字符串 p\n的长度，n 表示待匹配字符串 s 的长度。则 dp[i][j]\n表示子字符串 p[:i] 和 s[:j]\n(均包含 i 和 j) 是否匹配 (true/false)。假设目前已知 dp[i][j-1]\n及其前面的所有情况的匹配关系，那么要求 dp[i][j] 通过动态规划的递推关系如下：\n1. 假如 p[i] == '.'，则dp[i][j] = dp[i-1][j-1]2. 假如 p[i] == letter(a-zA-Z)，则dp[i][j] = dp[i-1][j-1] &amp;&amp; (p[i]==s[j])3. 假如 p[i] == '*',则 dp[i][j] = dp[i-2][j] ||                                   dp[i-1][j] ||                                   (dp[i][j-1] &amp;&amp; (p[i-1] == s[j]))\n上面的 1,2 均比较好理解，关键是出现 *\n时要分三种情况讨论，分别是 * 匹配了 0 个，1\n个，和若干个前一字符。假如匹配了 0 个前一字符，那么当前位置的匹配结果与 dp[i-2][j] 相同；匹配了 1 个前一字符，则当前位置的匹配结果与\ndp[i-1][j]\n相同；关键是假如匹配了多个前一字符，该如何判断，此时我们无法知道到底匹配了多少个前一字符，但是换个角度去想这个问题，假如匹配了多个前一字符，那么前一字符要与当前的\ns[j] 匹配（p [i-1]==s [j] 或\np [i-1]='.'），此时要想匹配成功 (dp[i][j] 为 true)，则当前的匹配串 (p [:i]) 必须能够匹配 s[:j-1], 也就是 dp[i][j-1] 为 true。对于这三种情况出现任意一种均可认为匹配，因此取或操作。\n在具体实现中还要注意数组越界的问题，可以看到上面出现了\ni-1，j-1，i-2 的下标，那么在实现的时候要在原二维矩阵中各增加一行和一列，表示第 0 个字符也就是空字符从而避免了 i-1 的越界；同时只有在遇到 * 时才会出现 i-2 的下标，且这种情况下只有当 * 出现在匹配串第一个的时候才会越界，而当 * 出现在匹配串的第一个字符的时候表示为空字符串，除了待匹配字符串为空时一律为 false。具体实现的 Java 代码如下所示：\npublic class Solution {    public boolean isMatch(String s, String p)     {        int m = p.length()+1, n = s.length()+1;        boolean[][] dp = new boolean[m][n];        for (int i=0; i&lt;m; i++)        {            for (int j=0;j&lt;n; j++)            {                if(i==0)                {                    if(j==0) dp[i][j] = true;                    else dp[i][j] = false;                }                else if(j==0)                {                    if(p.charAt(i-1)!='*') dp[i][j] = false;                    else dp[i][j] = dp[i-1][j] || dp[i-2][j];                }                else                {                    if (p.charAt(i-1)=='.') dp[i][j] = dp[i-1][j-1];                    else if (p.charAt(i-1) == '*')                     {                        if (i==1) dp[i][j] = false;                        else dp[i][j] = dp[i-2][j] ||                                         dp[i-1][j] ||                                         ((p.charAt(i-2)== '.' || p.charAt(i-2)==s.charAt(j-1)) &amp;&amp; dp[i][j-1]);                    }                    else dp[i][j] = (s.charAt(j-1)==p.charAt(i-1)) &amp;&amp; dp[i-1][j-1];                }            }        }        return dp[m-1][n-1];    }}\n通配符\n在通配符中 ？ 表示一个字符，而 *\n与正则表示式中的含义不一样，在这里 *\n表示任意字符串（包括空字符串）。\n采用上面的符号 (dp,s,p) 的定义，在求\ndp[i][j], 有以下的递推关系\n1. 假如 p[i] == '?'，则dp[i][j] = dp[i-1][j-1]2. 假如 p[i] == letter(a-zA-Z)，则dp[i][j] = dp[i-1][j-1] &amp;&amp; (p[i]==s[j])3. 假如 p[i] == '*',则 dp[i][j] = dp[i-1][j] || dp[i][j-1]\n前面的两种情况都比较好理解，这里的关键点是当遇到 *\n时，需要讨论两种情况，第一种是 * 表示空字符，这时候匹配结果与 dp[i-1][j] 相同；第二种是 * 表示任意字符串，这时候假如\ndp[i-1][0] 到 dp[i-1][j-1]\n有一个为真，则 dp[i][j] 为真，但是这样遍历的话遇到\n*\n时会导致时间复杂度变得很大，这时候用到了一个技巧，就是 dp[i-1][0]\n到 dp[i-1][j-1] 的结果已经包含在 dp[i-1][j]\n中了（根据递推式可知道），所以此时只需要或上 dp[i-1][j] 即可。\n实现时也需要注意越界问题，解决方法同上面提到的添加空字符，具体实现的 Java 代码如下所示：\npublic class Solution {    public boolean isMatch(String s, String p)     {        int m = p.length()+1, n = s.length()+1;        boolean[][] dp = new boolean[m][n];        for (int i=0; i&lt;m; i++)        {            for (int j=0; j&lt;n; j++)            {                if(i==0)                {                    if(j==0) dp[i][j] = true;                    else dp[i][j] = false;                }                else if(j==0)                {                    if(p.charAt(i-1)=='*') dp[i][j] = dp[i-1][j];                    else dp[i][j] = false;                }                else                {                    if (p.charAt(i-1)=='?') dp[i][j] = dp[i-1][j-1];                    else if(p.charAt(i-1)=='*') dp[i][j] = dp[i-1][j] || dp[i][j-1];                    else dp[i][j] = (s.charAt(j-1)==p.charAt(i-1)) &amp;&amp; dp[i-1][j-1];                }            }        }        return dp[m-1][n-1];    }}\n","categories":["算法"],"tags":["Java","动态规划","字符串"]},{"title":"LeetCode 解题报告 (105. 106)-- 树的重构","url":"/2016/07/20/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(105.%20106)--%E6%A0%91%E7%9A%84%E9%87%8D%E6%9E%84/","content":"本文主要从 LeetCode 两道题出发讲解如何从树的周游结果来重构树。这两道题分别是 105.\nConstruct Binary Tree from Preorder and Inorder Traversal和 106.\nConstruct Binary Tree from Inorder and Postorder Traversal\n\n105.\nConstruct Binary Tree from Preorder and Inorder\nTraversal要求从树的前序周游和中序周游的结果重构树。由两种周游的遍历顺序可知，两种周游方式得到的结果的结构如下\n\n其中 left_tree 是 root 的左子树，\nright_tree 是 root 的右子树。\n因此通过树的根节点 root 以及左子树的最后一个节点，可以从两个周游结果中分别找到左子树的两种周游结果和右子树的两种周游结果。然后将问题转化为原问题的子问题，通过递归解决即可。\npython 实现的代码如下所示，需要注意的是下面的代码通过下标指出子树的范围，而不是通过 slice，也就是 preorder [i:j] 的方式来将子树范围切出来，因为 slice 会在内存开辟新的空间，递归时会开辟大量空间而导致 MLE 错误：\nclass Solution(object):    def buildTree(self, preorder, inorder):        \"\"\"        :type preorder: List[int]        :type inorder: List[int]        :rtype: TreeNode        \"\"\"        return self.helper(preorder, 0, len(preorder)-1, inorder, 0, len(inorder)-1)    def helper(self, preorder, pleft, pright, inorder, ileft, iright):        if pleft &gt; pright:            return None        if pleft == pright:            return TreeNode(preorder[pleft])        inx = inorder.index(preorder[pleft])        left_len = inx - ileft        root = TreeNode(preorder[pleft])        root.left = self.helper(preorder, pleft + 1, pleft + left_len, inorder, ileft, inx-1)        root.right = self.helper(preorder, pleft + left_len + 1, pright, inorder, inx+1, right)        return root\n106.\nConstruct Binary Tree from Inorder and Postorder\nTraversal的结题思路与上面一致。两种周游结果的结构如下：\n\n同样可以通过树的根节点 root 以及左子树的最后一个节点，分别找到两棵子树的两种周游结果，进而将问题转化为原来问题的子问题，通过递归解决。\npython 实现代码如下所示 class Solution(object):    def buildTree(self, inorder, postorder):        \"\"\"        :type inorder: List[int]        :type postorder: List[int]        :rtype: TreeNode        \"\"\"        return self.helper(inorder, 0, len(inorder)-1, postorder, 0, len(postorder)-1)    def helper(self, inorder, ileft, iright, postorder, pleft, pright):        if ileft &gt; iright: return None        if ileft == iright: return TreeNode(inorder[ileft])        inx = inorder.index(postorder[pright])        left_len = inx - ileft        root = TreeNode(postorder[pright])        root.left = self.helper(inorder, ileft, inx-1, postorder, pleft, pleft+left_len-1)         root.right = self.helper(inorder, inx+1, iright, postorder, pleft+left_len, pright-1)        return root\n","categories":["算法"],"tags":["python","树","分治法"]},{"title":"LeetCode 解题报告 (200,130)-- 图的孤立子图","url":"/2016/07/17/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(200,130)--%E5%9B%BE%E7%9A%84%E5%AD%A4%E7%AB%8B%E5%AD%90%E5%9B%BE/","content":"200.\nNumber of Islands题目描述如下： &gt;Given a 2d grid map of '1's\n(land) and '0's (water), count the number of islands. An island is\nsurrounded by water and is formed by connecting adjacent lands\nhorizontally or vertically. You may assume all four edges of the grid\nare all surrounded by water.  &gt;Example 1:\n\n11110 11010 11000 00000\n\n\nAnswer: 1\n\n上面的问题是一个经典的图论问题，需要求孤立的岛屿的个数，而岛屿仅可通过上下左右来连接，这种问题可以通过深度优先搜索 (DFS) 或广度优先搜索 (BFS) 来解决，每次遇到一个没访问过的岛屿就对当前岛屿进行 DFS 或 DFS，并记录这个过程中访问过的岛屿，直至最终所有岛屿都被访问\n采用 DFS 的的 python 代码如下所示：\nclass Solution(object):    def numIslands(self, grid):        \"\"\"        :type grid: List[List[str]]        :rtype: int        \"\"\"        if len(grid) == 0: return 0        m, n, result = len(grid), len(grid[0]), 0        visited = [[0 for j in xrange(n)] for i in xrange(m)]        for i in xrange(m):            for j in xrange(n):                if grid[i][j] == '1' and visited[i][j] == 0:                    self.dfs(grid, visited, i, j)                    result += 1        return result    def dfs(self, grid, visited, i, j):        visited[i][j] = 1        m, n = len(grid), len(grid[0])        if 0&lt;= i-1 &lt;m and grid[i-1][j] == '1' and visited[i-1][j] == 0:            self.dfs(grid, visited, i-1, j)        if 0&lt;= i+1 &lt;m and grid[i+1][j] == '1' and visited[i+1][j] == 0:            self.dfs(grid, visited, i+1, j)        if 0&lt;= j-1 &lt;n and grid[i][j-1] == '1' and visited[i][j-1] == 0:            self.dfs(grid, visited, i, j-1)        if 0&lt;= j+1 &lt;n and grid[i][j+1] == '1' and visited[i][j+1] == 0:            self.dfs(grid, visited, i, j+1)\n假如将上面的题目改为一个岛屿邻接的岛屿除了上下左右，还有左上、左下、右上、右下，也就是有八个可能连接的地方，那么答案该如何修改？\n实际上方法也很简单，对上面代码中的 dfs 方法增加检查左上、左下、右上、右下这四个点的代码即可。\n上面的 DFS 中利用了 visited 数组检查某个点是否已经被访问过，空间复杂度为 O(m*n)。实际上在原来的 grid 可修改的情况下，可以通过修改 grid 中的值为 '1','0' 以外的值，从而使得空间复杂度为 O (1)。下面的 BFS 方法利用了这点\nfrom collections import dequeclass Solution(object):    def numIslands(self, grid):        \"\"\"        :type grid: List[List[str]]        :rtype: int        \"\"\"        m = len(grid)        if m == 0:            return 0        n = len(grid[0])        count = 0        for i in xrange(m):            for j in xrange(n):                if grid[i][j] == '1':                    self.bfs(i, j, grid)                    count += 1        return count    def bfs(self, row, col, grid):        m, n = len(grid), len(grid[0])        que = deque()        que.append((row, col))        grid[row][col] = '2'        while que:            row, col = que.popleft()            if row&gt;0 and grid[row-1][col]=='1':                que.append((row-1, col))                grid[row-1][col] = '2'            if row&lt;m-1 and grid[row+1][col] == '1':                que.append((row+1, col))                grid[row+1][col] = '2'            if col&gt;0 and grid[row][col-1]=='1':                que.append((row, col-1))                grid[row][col-1] = '2'            if col&lt;n-1 and grid[row][col+1] == '1':                que.append((row, col+1))                grid[row][col+1] = '2'\n此外，130. Surrounded\nRegions跟上面的题目也类似，只是需要对边上的点进行 BFS 或 DFS，具体代码见这里\n","categories":["算法"],"tags":["python","图","深度优先搜索","广度优先搜索"]},{"title":"LeetCode 解题报告 (17)-- 电话数字组合成的不同字符串","url":"/2016/03/06/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(17)--%E7%94%B5%E8%AF%9D%E6%95%B0%E5%AD%97%E7%BB%84%E5%90%88%E6%88%90%E7%9A%84%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E4%B8%B2/","content":"原题如下:\n&gt;Given a digit string, return all possible letter combinations that\nthe number could represent.\n\n&gt;A mapping of digit to letters (just like on the telephone buttons)\nis given below.\n\nInput:Digit string \"23\"\nOutput: [\"ad\", \"ae\", \"af\", \"bd\", \"be\", \"bf\", \"cd\", \"ce\", \"cf\"].\n题目很好理解，就是组合出所有可能结果即可。每个数字写一个 for 循环即可，但是在程序运行前我们无法知道到底输入有几个数字，下面给出两个方法解决这个问题。\n方法一是回溯法，每次选择当前数字的一个字符，然后将下一个数字作为当前数字，直到所有的数字都遍历完即可。实现的代码如下所示：\nclass Solution(object):      numDict ={'2':['a','b','c'],'3':['d','e','f'],'4':['g','h','i'],'5':['j','k','l'],'6':['m','n','o'],'7':['p','q','r','s'],'8':['t','u','v'],'9':['w','x','y','z']}      def letterCombinations(self, digits):          \"\"\"          :type digits: str          :rtype: List[str]          \"\"\"          result = []          self.helper(0, digits, '', result)          return result    def helper(self, index, digits, tmp, result):          if index == len(digits):              if tmp: # empty string                  result.append(tmp)              return          for char in Solution.numDict[digits[index]]:              self.helper(index+1, digits, tmp+char, result)  \n除了回溯法，还可以通过顺序的合并，考虑每次对两个 string\nlist 进行合并，合并后作为一个 string list，再和后面的一个数字表示的 string\nlist 合并，就这样一直循环下去直到最后一个数字\n实现代码如下：\n#encoding:utf-8  class Solution(object):      def letterCombinations(self, digits):          \"\"\"          :type digits: str          :rtype: List[str]          \"\"\"          numDict ={'2':['a','b','c'],'3':['d','e','f'],'4':['g','h','i'],'5':['j','k','l'],'6':['m','n','o'],'7':['p','q','r','s'],'8':['t','u','v'],'9':['w','x','y','z']}        result = []          if len(digits)==0:              return result          # 先将第一位对应的字符复制给result，防止调用函数combine2StrList时一开始result为空          result = numDict[digits[0]]          for i in range(1,len(digits)):              result = self.combine2StrList(result,numDict[digits[i]])          return result    def combine2StrList(self,s1,s2):          mix =[]          for i in range(len(s1)):              for j in range(len(s2)):                 mix.append(s1[i]+s2[j])          return mix  \n如有更好的解法，欢迎交流\n","categories":["算法"],"tags":["python","回溯法"]},{"title":"LeetCode 解题报告 (11)-- 双指针找最大储水容器","url":"/2016/02/05/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(11)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%9C%80%E5%A4%A7%E5%82%A8%E6%B0%B4%E5%AE%B9%E5%99%A8/","content":"原题如下：\n&gt;Given n non-negative integers a1, a2, ..., an, where each represents\na point at coordinate (i, ai). n vertical lines are drawn such that the\ntwo endpoints of line i is at (i, ai) and (i, 0). Find two lines, which\ntogether with x-axis forms a container, such that the container contains\nthe most water.\n\n这道题的不难理解，但是对时间复杂度有要求。简单地通过两个 for 循环寻找最大的蓄水量的时间复杂度为 O (n^2), 提交时提示超时。\n蓄水量由 container\n的底（也就是两个下标之差）乘上其两端的边的最小值。假如 left\n为第一条竖直边的下标，right\n为最后一条竖直边的下标，i,j 为其中的两条边（设 i &lt; j），那么：\n要使任何 S (i&gt;=left, j&lt;=right) &gt;=\nS (left,right)，由于 j-i &lt;=\nright-left，必然要有 min (ai,aj)&gt;=min (a (left),a (right)) 才行\n因此可以从两边同时开始往中间逼近，每次只移动一步（左边的边或右边的边），而且两条边中最短的边进行移动。时间复杂度为 O (n)，实现代码如下：\n# encoding:utf-8class Solution(object):      def maxArea(self, height):          \"\"\"          :type height: List[int]          :rtype: int          \"\"\"          left = 0          right = len(height) - 1          max = 0          while left &lt; right:              current = (right - left)* min(height[left],height[right])              if current &gt;max:                  max = current              if height[left] &gt; height[right]:                  right-=1              else:                  left+=1          return max  \n","categories":["算法"],"tags":["python","双指针"]},{"title":"LeetCode 解题报告 (29)-- 通过加法完成除法","url":"/2016/04/01/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(29)--%E9%80%9A%E8%BF%87%E5%8A%A0%E6%B3%95%E5%AE%8C%E6%88%90%E9%99%A4%E6%B3%95/","content":"原题如下：\n&gt;Divide two integers without using multiplication, division and mod\noperator.\n\nIf it is overflow, return MAX_INT.\n\n\n题目要求返回一个数除以另外一个数的结果，并且要求不能用乘除或取模，意思就是只能用加减，用加减也不难，先初始化 sum=0，每次 sum 加上除数的大小并与被除数比较，直到 sum 大于等于被除数即可。但是这样的时间复杂度是 \\(\\Theta(n)\\)，提交时会超时。\n解决思路是采用二分搜索，或者说变形的二分搜索。思路类似于上面所说的，但是 sum 每次加上自己的大小，也相当于 sum=sum*2, 这样的时间复杂度是 \\(\\Theta(log_2n)\\), 提交时能够 AC\n实现的代码如下：\nclass Solution(object):      def divide(self, dividend, divisor):          \"\"\"          :type dividend: int          :type divisor: int          :rtype: int          \"\"\"          MAX_INT = pow(2,31)-1          if divisor == 0:              return MAX_INT        flag = 1          if (dividend &gt; 0 and divisor&lt;0 ) or (dividend&lt;0 and divisor &gt;0):              flag = -1          a = abs(dividend)          b = abs(divisor)          res = 0          while a &gt;= b :              sum = b              count =1              while a &gt;= sum+sum:                  sum += sum                  count += count              a-=sum              res+=count          if res*flag &gt; MAX_INT:              return MAX_INT          else:              return res*flag  \n","categories":["算法"],"tags":["python"]},{"title":"LeetCode 解题报告 (23)-- 合并 k 个有序数组","url":"/2016/03/29/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(23)--%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84/","content":"原题如下：\n&gt;Merge k sorted linked lists and return it as one sorted list.\nAnalyze and describe its complexity.\n\n题目不难理解，就是将 k 个排好序的链表合并成一个，可以说是 merge two\nsorted\nlists 的升级版。一开始想的方法超时，后来参考了网上的两种方法并通过 python 实现后能够 AC，下面分别讲述这三种方法。\n方法一：线性合并（TLE）\n一开始想到的方法就是基于 merge two sorted lists 逐个合并 list，就是先将两个 list 合成一个，然后将这个合并好的 list 再和一个未合并的 list 进行 merge 操作，这样总共会合并 k-1 次，时间复杂度为 \\(O(2n+3n+....+kn) =\nO(nk^2)\\)（设 n 为链表的平均长度）。\n实现代码如下：\nclass Solution(object):      def mergeKLists(self, lists):          \"\"\"          :type lists: List[ListNode]          :rtype: ListNode          \"\"\"          k = len(lists)          if k == 0:              return None        mergedList = lists[0]          for i in range(1,k):              mergedList = self.mergeTwoLists(mergedList,lists[i])          return mergedList    def mergeTwoLists(self, l1, l2):          \"\"\"          :type l1: ListNode          :type l2: ListNode          :rtype: ListNode          \"\"\"          if l1 == None:              return l2          if l2 == None:              return l1        dummy = ListNode(0)          nextNode = dummy          while l1 and l2:              if l1.val &gt; l2.val:                  nextNode.next = l2                  nextNode = nextNode.next                  l2 = l2.next              else:                  nextNode.next = l1                  nextNode = nextNode.next                  l1 = l1.next          if l1:              nextNode.next = l1          if l2:              nextNode.next = l2          return dummy.next  \n方法二：归并合并（AC）\n这种方法类似于归并排序，先将当前需要排序的 list 对半分，重复这个步骤直到对半分出的 list 的数量为 1，在进行 merge，这时实际进行的是 merge\ntwo sorted list。\n这种方法采用的思想跟第一种一样，都是分治法，先处理好局部，再合并成一个整体。但是与第一种方法在于这种方法在给出的 lists 的数量很大时需要进行 merge 的操作小于方法一。\n这里有两个需要注意是当 lists 的数量很大（也就是 k 很大）是 merge 的操作才会比方法一要少。方法一无论 k 的大小 merge 的次数为 k-1，而方法二 merge 的次数为\n\\[\\begin{align} \\sum_{i=0}^m 2^i\n(m=log_2k-1) \\end{align}\\]\n可以通过下面的程序验证当 k 很大时，这两种方法 merge 的次数不同\nimport mathk = 100000  m = int(math.log(k,2))  sum = 0  for i in range(m):      sum+=math.pow(2,i)  print 'merge times for method 1 when k=%s: %s'%(k,k-1)  print 'merge times for method 2 when k=%s: %s'%(k,sum)  \n根据主定理分析，这种方法的时间复杂度是 O(nklog(nk)), 下面是方法二实现的具体代码\nclass Solution(object):      def mergeKLists(self, lists):          \"\"\"          :type lists: List[ListNode]          :rtype: ListNode          \"\"\"          k = len(lists)          if k == 0:              return None          return self.helper(lists,0,len(lists)-1)    def helper(self,lists,l,r):          if l&lt;r:              m = (r-l)/2              return self.mergeTwoLists(self.helper(lists,l,l+m),self.helper(lists,l+m+1,r))          else:              return lists[l]    def mergeTwoLists(self,l1,l2):          if l1 == None:              return l2          if l2 == None:              return l1        dummy = ListNode(0)          curr = dummy          while l1 and l2:              if l1.val &lt; l2.val:                  curr.next = l1                  curr = curr.next                  l1 = l1.next              else:                  curr.next = l2                  curr = curr.next                  l2 = l2.next          if l1:              curr.next = l1          if l2:              curr.next = l2          return dummy.next  \n方法三：基于堆排序的归并（AC）\n第三种方法非常巧妙，先建立一个大小为 k 的堆（k 就是链表数量），\n堆中的一个元素代表一个链表当前的最小元素，每次取堆顶的最小元素放到结果中，然后读取该元素的下一个元素放入堆中，重新维护好。\n因为每个链表是有序的，每次又是去当前 k 个元素中最小的，所以当所有链表都读完时结束，这个时候所有元素按从小到大放在结果链表中。\n时间复杂度是 O (nklogk)。\n实现代码如下，建堆的方法有两种，下面一并给出：\nclass Solution(object):      def mergeKLists(self, lists):          \"\"\"          :type lists: List[ListNode]          :rtype: ListNode          \"\"\"          k = len(lists)          if k == 0:              return None        listsHeap = []          listsHeap.append(0) # 使堆中的元素从1开始        for i in range(k):              if lists[i] == None: # avoid empty list                  continue              listsHeap.append(lists[i])        dummy = ListNode(0)          curr = dummy        # 初始化堆有两种方法          '''方法一          j = len(listsHeap) - 1          while j &gt; 1:              if listsHeap[j].val&lt;listsHeap[j/2].val:                  listsHeap[j],listsHeap[j/2] = listsHeap[j/2],listsHeap[j]                  self.siftDown(listsHeap,j)  #必须，否则初始建的堆会有问题              j-=1          '''          # 方法二          leafParent = (len(listsHeap)-1)/2          for i in range(leafParent,0,-1):              siftDown(listsHeap,i)        # 取堆顶元素并调整堆          while len(listsHeap) &gt; 1:              curr.next = listsHeap[1]              curr = curr.next              if listsHeap[1].next == None:  # 将空的列表移到最后并删除                  listsHeap[1] = listsHeap[len(listsHeap)-1]                  del(listsHeap[len(listsHeap)-1])              else:                  listsHeap[1] = listsHeap[1].next              self.siftDown(listsHeap,1)          return dummy.next    def siftDown(self,listsHeap,i):          while i*2+1 &lt;= len(listsHeap):              if i*2+1 &lt; len(listsHeap):                  if listsHeap[i].val &gt; min(listsHeap[i*2].val,listsHeap[i*2+1].val):                      if listsHeap[i*2].val &lt; listsHeap[i*2+1].val:                          listsHeap[i],listsHeap[i*2] = listsHeap[i*2],listsHeap[i]                          i = i*2                      else:                          listsHeap[i],listsHeap[i*2+1] = listsHeap[i*2+1],listsHeap[i]                          i = i*2+1                  else:                      return              elif i*2+1 == len(listsHeap):                  if listsHeap[i*2].val &lt; listsHeap[i].val:                      listsHeap[i],listsHeap[i*2] = listsHeap[i*2],listsHeap[i]                  i = i*2          return  \n\n参考：http://blog.csdn.net/linhuanmars/article/details/19899259\n","categories":["算法"],"tags":["python","堆"]},{"title":"LeetCode 解题报告 (27)-- 双指针找数组所有特定元素","url":"/2016/03/31/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(27)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%95%B0%E7%BB%84%E6%89%80%E6%9C%89%E7%89%B9%E5%AE%9A%E5%85%83%E7%B4%A0/","content":"原题如下：\n&gt;Given an array and a value, remove all instances of that value in\nplace and return the new length.\n\nDo not allocate extra space for another array, you must do this in\nplace with constant memory.\n\nThe order of elements can be changed. It doesn't matter what you leave\nbeyond the new length.\n\n\nExample:\nGiven input array nums = [3,2,2,3], val = 3\n\n\nYour function should return length = 2, with the first two elements\nof nums being 2.\n\n实现思路：双指针遍历，指针 1 从前往后遍历，指针 2 从后往前遍历。指针 2 先往前找到与 val 值不同的元素，然后停止；接着指针 1 开始往后找到与 val 相同的元素，然后指针 1 和指针 2 的元素交换，这样便把与 val 值相同的元素全部扔到了数组末尾。\n需要注意的是，虽然该数组提供了删除元素的操作，但是删除一个元素的平均时间复杂度是 \\(O(n)\\), 总的时间复杂度为 \\(O(n^2)\\)\n还需要注意的是一个溢出的问题，对于一般的编译型语言如 Java、C++ 等 int 是 4 个字节的，所以 int 的范围是 \\(-2^{31}\\) ~ \\(2^{31}-1\\), 但是在 python 中 int 在 32 位系统上占四个字节，在 64 为系统上占 8 个字节，可通过 sys.maxint 查看，而在本题中默认就是 4 个字节，如果溢出时输出 sys.maxint 会报错。\n实现代码如下:\nclass Solution(object):      def removeElement(self, nums, val):          \"\"\"          :type nums: List[int]          :type val: int          :rtype: int          \"\"\"          if len(nums) == 0:              return 0          l = 0          r = len(nums)-1          while l&lt;r:              if nums[r] == val:                  r-=1                  continue              if nums[l] == val:                  nums[l],nums[r] = nums[r],nums[l]                  l += 1                  r -= 1              else:                  l += 1        if l == r: # 有可能l&gt;r              if not nums[l] == val:                  l+=1          return l  \n","categories":["算法"],"tags":["python","双指针"]},{"title":"LeetCode 解题报告 (26)-- 消除有序数组中重复值 (常数空间)","url":"/2016/03/31/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(26)--%E6%B6%88%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E5%80%BC(%E5%B8%B8%E6%95%B0%E7%A9%BA%E9%97%B4)/","content":"原题如下：\n&gt;Given a sorted array, remove the duplicates in place such that each\nelement appear only once and return the new length.\nDo not allocate extra space for another array, you must do this in place\nwith constant memory.\n\n&gt;For example,\nGiven input array nums = [1,1,2],\n\nYour function should return length = 2, with the first two elements\nof nums being 1 and 2 respectively. It doesn't matter what you leave\nbeyond the new length.\n\n题目不难理解，但是要注意的是除了返回 length 以外，对原来数组的元素也要处理，因为 LeetCode 的评测是根据你给出的 length 去读取原来数组的前 length 个元素。\n注意题目要求的空间复杂度是常数，这意味着不能新建一个数组来存放不重复元素。\n实现思路是双指针，就是先用一个整数 count 来维护已经选出的不重复的元素的个数，同时 count 也作为数组下标。然后另外一个指针遍历数组，找到与当前数组下标为 count 的元素不重复的元素，交换两者即可。\n因为数组已经排列了，所以后面的元素肯定会比前面的要大。所以假如给出的数组是无序的同时要实现这个功能，那么可以先排序再使用上面的方法。\n实现代码如下：\nclass Solution(object):      def removeDuplicates(self, nums):          \"\"\"          :type nums: List[int]          :rtype: int          \"\"\"          if len(nums) &lt;= 1:              return len(nums)        count = 0          for i in range(1,len(nums)):              if not nums[i] == nums[count]:                  count+=1                  nums[count]=nums[i]          return count+1  \n","categories":["算法"],"tags":["python","双指针"]},{"title":"LeetCode 解题报告 (30)-- 双指针找拼接子字符串","url":"/2016/04/04/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(30)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%8B%BC%E6%8E%A5%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","content":"原题如下：\n&gt;You are given a string, s, and a list of words, words, that are all\nof the same length. Find all starting indices of substring(s) in s that\nis a concatenation of each word in words exactly once and without any\nintervening characters.\n\n&gt;For example, given:\ns: \"barfoothefoobarman\"\nwords: [\"foo\", \"bar\"]\n\nYou should return the indices: [0,9].\n(order does not matter).\n\n题目不难理解，实现也不难，关键是时间复杂度的控制。下面会讲述两种方法，第一种方法时间复杂度 \\(O((n-k)*k)\\), 其中 n 为 s 的长度，k 为所有的 words 拼接起来后的长度，这种方法提交时在 TLE 和 AC 间徘徊（即偶尔 AC，偶尔 TLE）。第二种方法时间复杂度为 O (n)，提交的时候能够 AC。\n方法一\n思路\n设 wordLen 为给出的 words 列表中每个词的长度，wordNum 为给出的 words 列表的长度，n 为给出的 s 的长度。\n那么可以从头开始遍历 s 直到 n-wordLen*wordNum+1，每次遍历用一个字典存储遍历中遇到的属于 words 的词语，并检查两者对应的词语的数量关系。遇到不属于 words 的词语或者字典中的某一词语的数量大于 words 中该词语的数量则跳出执行下一次的遍历\n这种方法的时间复杂度 \\(O((n-k)*k)\\)，n 为 s 的长度。提交时偶尔 TLE，偶尔 AC，AC 时显示的时间是 876ms。\n### 实现代码：\nclass Solution(object):      def findSubstring(self, s, words):          \"\"\"          :type s: str          :type words: List[str]          :rtype: List[int]          \"\"\"          # 注意words中的词可能会重复          wordDict = {}          for word in words:              if word in wordDict:                  wordDict[word] += 1              else:                  wordDict[word] = 1        wordLen = len(words[0])          result = []        for i in range(len(s)-wordLen*wordNum+1):              tmp = s[i:i+wordLen]              tmpDict = {} # 存储words的临时list              if tmp in words:                  head = i                  while tmp in words and i &lt; len(s)-wordLen+1:                      if tmp in tmpDict:                          tmpDict[tmp] += 1                      else:                          tmpDict[tmp] = 1                    if tmpDict[tmp] &gt; wordDict[tmp]:                          tmpDict[tmp] -= 1                          break                    i += wordLen                      tmp = s[i:i+wordLen]                  if tmpDict == wordDict:                      result.append(head)              else:                  continue          return result  \n方法二\n思路\n这种方法参考了这里, 主要思想是利用了双指针围着当前符合 words（包括数量和种类）中的连续子字符串，同时计算围着的子字符串中 word 的数量。\n巧妙的地方在于双指针当前包围的子字符串中如果含有某个 word 的数量大于 words 中这个 word 的数量时，移动左指针直到把这个 word 的数量减少。\n这种方法的时间复杂度为 O (n), 提交时 AC 的时间约 100ms\n实现代码\nclass Solution(object):      def findSubstring(self, s, words):          \"\"\"          :type s: str          :type words: List[str]          :rtype: List[int]          \"\"\"          # 注意words中的词可能会重复          wordDict = {}          for word in words:              if word in wordDict:                  wordDict[word] += 1              else:                  wordDict[word] = 1        wordLen = len(words[0])          wordNum = len(words)          wordSet = set(words)          sLen = len(s)          result = []        for i in range(wordLen):              left = i              tmpDict = {}              count = 0              j=i              while j &lt; sLen-wordLen+1:                  tmp = s[j:j+wordLen]                  j += wordLen                  if tmp in wordSet:                      if tmp in tmpDict:                          tmpDict[tmp]+=1                      else:                          tmpDict[tmp]=1                    if tmpDict[tmp]&lt;=wordDict[tmp]:                          count+=1                      else:                          # 某个词的数量比wordDict中规定的要多了，往右移动左指针                          while tmpDict[tmp] &gt; wordDict[tmp]:                              t = s[left:left+wordLen]                              left += wordLen                              tmpDict[t] -= 1                              if tmpDict[t] &lt; wordDict[t]:                                  count -= 1                      # 仅去掉最左边的一个word，再往右找                      if count == wordNum:                          result.append(left)                          t = s[left:left+wordLen]                          tmpDict[t] -= 1                          left += wordLen                          count -= 1                  # 当前的词不在wordDict                  else:                      tmpDict = {}                      count = 0                      left = j          return result  \n","categories":["算法"],"tags":["python","双指针"]},{"title":"LeetCode 解题报告 (31)-- 数字排列的下一项","url":"/2016/04/04/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(31)--%E6%95%B0%E5%AD%97%E6%8E%92%E5%88%97%E7%9A%84%E4%B8%8B%E4%B8%80%E9%A1%B9/","content":"原题如下：\n&gt;Implement next permutation, which rearranges numbers into the\nlexicographically next greater permutation of numbers.\n\n&gt;If such arrangement is not possible, it must rearrange it as the\nlowest possible order (ie, sorted in ascending order).\n\nThe replacement must be in-place, do not allocate extra memory.\n\n\nHere are some examples. Inputs are in the left-hand column and its\ncorresponding outputs are in the right-hand column.\n1,2,3 → 1,3,2\n3,2,1 → 1,2,3\n1,1,5 → 1,5,1\n\n题目的意思就是对给出的数字 list 进行排列组合并从小到大排序，找出给出的 list 组成的数字的下一个数。\n实现的时候当然不会弄得像上面说的这么复杂，只需要对给出的 list 从后往前遍历，找到第一个当前数字 (记为 i) 比前一数字 (记为 j) 大的那个数字，然后从这个位置往后的数字中找到一个 &gt;j&amp;&amp;&lt;=i 的最小的数字，交换两者后再对 i 后面的数字排序即可，说起来比较拗口。下面举个简单的例子。\n假如给出的 list 是 [1,2,5,4,3], 找到的 i=5，j=2, 然后将 i 后面大于 j 且小于等于 i 的最小数字与 j 交换，这里是 3，也就是交换后的 list 是 [1,3,5,4,2], 然后对 i 后面的数字进行排序，排序后为 [1,3,2,4,5]，也是我们得到的结果。\n实现代码如下：\n#encoding:utf-8  class Solution(object):      def nextPermutation(self, nums):          \"\"\"          :type nums: List[int]          :rtype: void Do not return anything, modify nums in-place instead.          \"\"\"          i = len(nums)-1          flag = 0          while i &gt; 0:              if nums[i] &gt; nums[i-1]:                  # 往后找大于nums[i-1]且小于nums[i]的数与nums[i-1]交换                  j = len(nums)-1                  while j&gt;i:                      if nums[i-1]&lt;nums[j]&lt;nums[i]:                          nums[j],nums[i-1] = nums[i-1],nums[j]                          flag = 1                          break                      j-=1                # 找不到这样的数                  if flag == 0:                      nums[i],nums[i-1] = nums[i-1],nums[i]                # 交换后对i后面的元素进行排序                  nums[i:] = sorted(nums[i:])                  return              else:                  i-=1          nums.sort()  # 重新从小到大排序  \n","categories":["算法"],"tags":["python"]},{"title":"LeetCode 解题报告 (3)-- 双指针找最长不重复子串","url":"/2016/01/08/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(3)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/","content":"原题如下\n\nGiven a string, find the length of the longest substring without\nrepeating characters. For example, the longest substring without\nrepeating letters for \"abcabcbb\" is \"abc\", which the length is 3. For\n\"bbbbb\" the longest substring is \"b\", with the length of 1.\n\n\n这道题是典型的找最长子字符串问题，就是要找出没有重复的最长子字符串。本文讲述了两种方法，一种时间复杂度为 \\(O(n^2)\\)，另一种的时间复杂度为 \\(O(n)\\)\n最容易想到的方法就是遍历整个字符串，找出以每个字符开头的最长子字符串，再从中找出最长的子字符串。这样的时间复杂度是 \\(O(n^2)\\)，实现代码如下\nclass Solution(object):      def lengthOfLongestSubstring(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          max=0          for  i in range(len(s)):             tmp=[]             tmp.append(s[i])             for j in range(i+1,len(s)):                 if s[j] not in tmp:                     tmp.append(s[j])                 else:                     if len(tmp)&gt;max:                         max=len(tmp)                         break          return max  \n这种方法虽然易于理解，但是由于时间复杂度问题，运行超时。\n现在再分析一下题目，是否有必要计算以每个字符开头的最长子字符串？\n答案是不必要的。比如说对于字符串 abcdedfghi, 从 a 开始往后进行判断，则遇到第二个 d 的时候会发现与前面的 d 重复了，判断停止，那么以 a 开头的最长子字符串为 abcde，然后以一个新的字母开头找最长的子字符串。按照上面的方法，接下来会以 b 为开头找其最长子字符串。但是你会发现找到的最长子字符串肯定比以 a 开头的最长子字符串要短，原因是后面重复的元素位置没有变，而从 b 或 c 开始相当于把不重复的部分截断了。所以如果要找到可能更长的子字符串，必须要从与当前字符重复的字符往后的一个字符位置开始找，在本例中就是要从 e 开始找，才有可能找得到比以 a 开头的最长子字符串更长的子字符串。\n这样，便可以通过找与当前字符重复的前一字符的位置来避免第二层的 for 循环，将时间复杂度缩短为 \\(O(n)\\)\n通过双指针能够实现上面的功能，左右指针共同维护一个没有重复字符的子字符串，每当没有重复字符时，右指针一直往右移动；有重复字符时左指针往右移动直到左右指针之间没有重复字符。\nclass Solution(object):      def lengthOfLongestSubstring(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          left, right, n = 0, 0, len(s)          visited, result = set(), 0          while right &lt; n:              if s[right] in visited:                  while s[right] in visited:                      visited.remove(s[left])                      left += 1                  visited.add(s[right])              else:                  visited.add(s[right])                  result = max(right-left+1, result)              right += 1          return result  \n上面的方法虽然能够 AC, 但是每次遇到重复元素时左指针往右一步一步地移动并删除元素的操作会比较慢，因此可以\n通过 HashTable\n存储每个元素的最大下标，遇到重复元素的时候判断前一重复元素位置的下标是否大于当前左指针位置，若是则更新左指针，否则不更新；同时更新 HashTable\n当前重复元素的下标。下面是实现的代码\nclass Solution(object):      def lengthOfLongestSubstring(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          left, right,result= 0, 0, 0          indices = {}          while right &lt; len(s):              if s[right] in indices and indices[s[right]] &gt;= left:                  result = max(result, right - left)                  left = indices[s[right]] + 1              indices[s[right]] = right              right += 1          return max(result, right - left)  \n","categories":["算法"],"tags":["python","双指针","字符串"]},{"title":"LeetCode 解题报告 (37)-- 回溯法解决数独问题","url":"/2016/04/15/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(37)--%E5%9B%9E%E6%BA%AF%E6%B3%95%E8%A7%A3%E5%86%B3%E6%95%B0%E7%8B%AC/","content":"原题如下：\n&gt;Write a program to solve a Sudoku puzzle by filling the empty\ncells.\n\nEmpty cells are indicated by the character '.'.  You may\nassume that there will be only one unique solution.\n\n\n\n\n\nA sudoku puzzle...\n\n\n\n\n题目要求解决一个给定的数独，且每个数独的答案唯一。\n解题思路如下：采用回溯法，每插入一个数之前检查插入这个数之后是否有效。假如有效就插入并且保留当前插入了这个数的状态。以保证后面插入的数无效的时候能够回溯到当前这个状态，修改当前状态插入的数。实现这种状态的保存与回溯可以通过递归实现。\n实现代码如下： class Solution(object):    def solveSudoku(self, board):        \"\"\"        :type board: List[List[str]]        :rtype: void Do not return anything, modify board in-place instead.        \"\"\"        self.helper(board)    def helper(self, board):        num = len(board)        for i in range(num):            for j in range(num):                if board[i][j] == '.':                    for t in range(1,10):                        c = str(t)                        if self.is_valid(board, c, i, j):                            board[i][j] = c                            if self.helper(board):                                return True                            else:                                board[i][j] = '.'                     return False  # 当前插入1~9均无效，回溯到前一状态        return True  # 插入最后一个数成功时需要返回True,从而将前面的状态确定下来    def is_valid(self, board, c, row , col):        num = len(board)        # check row         for m in range(num):            if board[row][m] == c:                return False        # check column         for n in range(num):            if board[n][col] == c:                return False        # check block         row_start = (row/3)*3        column_start = (col/3)*3        for i in range(row_start,row_start+3):            for j in range(column_start,column_start+3):                if board[i][j] == c:                    return False        return True\n","categories":["算法"],"tags":["python","回溯法"]},{"title":"LeetCode 解题报告 (33,81,153,154)-- 二分搜索找旋转数组特定值","url":"/2016/04/08/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(33)--%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%89%BE%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%89%B9%E5%AE%9A%E5%80%BC/","content":"本文主要讲述如何在一个 Rotated Sorted Array 中找到特定的值，Rotated Sorted Array 指旋转了的数组，如 4 5 6 7 0 1 2 就是 0 1 2 4 5 6 7 的一个旋转数组。正常情况下遍历一遍即可，但是这样的时间复杂度为 \\(O(n)\\), 但是本文主要讲述通过二分查找将时间复杂度降到 \\(O(log_2n)\\)。\n\n本文主要以 LeetCode 上的四道题目为例讲解，分别是\n33.\nSearch in Rotated Sorted Array\n81.\nSearch in Rotated Sorted Array II\n153.\nFind Minimum in Rotated Sorted Array\n154.\nFind Minimum in Rotated Sorted Array II\n其中 33、81 是在 Rotated Sorted Array 找出给定的值，不同的地方在于 33 中没有重复元素，而 81 中有重复的元素；153、154 则是在 Rotated Sorted Array 中找到最小值，区别也是一个有重复元素而另一个没有。\n33.\nSearch in Rotated Sorted Array\n对于一个 Rotated Sorted Array，每次的 binary\nsearch 查找的中间元素都会将原来的 list 分为两个 lists, 其中的一个 list 肯定是有序的，可以判断目标元素是否在这个有序的 list 中，如果在这个 list 则在这个 list 内进行二分查找，否则在另外一个 list 内找。\n这种方法时间复杂度为 \\(O(log_2n)\\)。实现代码如下：\nclass Solution(object):      def search(self, nums, target):          \"\"\"          :type nums: List[int]          :type target: int          :rtype: int          \"\"\"          if len(nums) == 0:              return -1          left = 0          right = len(nums) - 1        while left &lt; right:              mid = (left + right)/2              if nums[mid] == target:                  return mid            if nums[left] &lt;= nums[mid]:                  if nums[left]&lt;=target&lt;=nums[mid]:                      right = mid - 1   # mid要减去1，否则两个元素的时候会导致死循环                  else:                      left = mid+1            if nums[mid] &lt;= nums[right]:                  if nums[mid]&lt;=target&lt;=nums[right]:                      left = mid + 1                  else:                      right = mid - 1        if nums[left] == target:              return left          else:              return -1  \n81.\nSearch in Rotated Sorted Array II\n这个问题的解决思路同上面的类似，但是因为有可能存在重复的元素，所以每次\nbinary search 后分成的两个 lists 中不一定存在一个有序的 list，\n这时候就需要两个 list 都遍历了，所以这种方法最差情况下的时间复杂度为 O(n)。\n实现代码如下：\nclass Solution(object):      def search(self, nums, target):          \"\"\"          :type nums: List[int]          :type target: int          :rtype: bool          \"\"\"          if len(nums) == 0:              return False          n = len(nums)          if self.helper(nums, 0, n-1,target):              return True          else:              return False    def helper(self,nums,left,right,target):          if left &gt; right:              return False          if left == right:              return nums[left] == target          else:              mid = (left+right)/2              if nums[mid] == target:                  return True              elif  nums[left]&lt;=target&lt;nums[mid]:                  return self.helper(nums,left,mid-1,target)              elif nums[mid]&lt;target&lt;=nums[right]:                  return self.helper(nums,mid+1,right,target)              else:                  return self.helper(nums,mid+1,right,target) or self.helper(nums,left,mid-1,target)  \n153.\nFind Minimum in Rotated Sorted Array\n本题需要求\nRotated Sorted Array 中最小的元素，且 Rotated Sorted Array 中没有重复元素。关键点在于每次将 nums[(left+right)/2] 与 nums[right] 比较，假如 nums[(left+right)/2]&gt;nums[right], 则最小值在 nums[(left+right)/2:right] 中，反之在另一半元素中。\n实现代码如下：\nclass Solution(object):      def findMin(self, nums):          \"\"\"          :type nums: List[int]          :rtype: int          \"\"\"          if len(nums) == 0: return None          left, right = 0, len(nums)-1          while left &lt; right:              mid = (left+right)/2              if nums[right] &lt; nums[mid]:                  left = mid + 1              else:                  right = mid          return nums[left]  \n通过找到最小元素，也可以解决上面提到的问题 33.\nSearch in Rotated Sorted\nArray，就是先用二分搜索找到最小元素的下标，最小元素的下标会将原来的 list 分为两个 sorted\nlist，判断 target 在哪个 sorted list 然后对这个 sorted\nlist 进行二分查找即可。\n这种方法对 array 进行了两次二分查找\n实现代码如下：\nclass Solution(object):      def search(self, nums, target):          \"\"\"          :type nums: List[int]          :type target: int          :rtype: int          \"\"\"          if len(nums) == 0:              return -1          left = 0          right = len(nums)-1          while left&lt;right:              mid = (left+ right)/2              if nums[mid]&gt;nums[right]:                  left = mid + 1              else:                  right = mid          minIndex = left        if minIndex == 0:              if nums[minIndex]&lt;=target&lt;= nums[len(nums)-1]:                  return self.binarySearch(minIndex,len(nums)-1,nums,target)              else:                  return -1          else:              if nums[0]&lt;= target&lt;=nums[minIndex-1]:                  return self.binarySearch(0,minIndex-1,nums,target)              elif nums[minIndex]&lt;= target&lt;=nums[len(nums)-1]:                  return self.binarySearch(minIndex,len(nums)-1,nums,target)              else:                  return -1    # 对一个sorted list的经典二分查找      def binarySearch(self,left,right,nums,target):          while left &lt; right:              mid = (left + right)/2              if nums[mid] == target:                  return mid              elif nums[mid]&gt;target:                  right = mid -1              else:                  left = mid+1          if nums[left] == target:              return left          else:              return -1\n154.\nFind Minimum in Rotated Sorted Array II\n这个问题跟 153.\nFind Minimum in Rotated Sorted\nArray不同的地方是 Rotated Sorted Array 中可能存在重复的元素，虽然也可以通过二分搜索解决，但是最差的情况下的时间复杂度也是 O(n), 最差的情况下就是 nums[left]=nums[mid]=nums[right], 这是无法确定最小元素在那一边，因此两边都要搜索。下面给出两种实现方法：\n# method 1  class Solution(object):      def findMin(self, nums):          \"\"\"          :type nums: List[int]          :rtype: int          \"\"\"          if len(nums) == 0: return None          left, right = 0, len(nums)-1          while left &lt; right:              mid = (left+right)/2              if nums[right] &lt; nums[mid]:                  left = mid + 1              elif nums[right] &gt; nums[mid]:                  right = mid              else:                  if nums[mid] == nums[left]:                      if sum(nums[left:mid+1]) == nums[mid] * (mid-left+1):                          left = mid + 1                      else:                          right = mid                  else:                      right = mid          return nums[left]# method 2  class Solution(object):      def findMin(self, nums):          \"\"\"          :type nums: List[int]          :rtype: int          \"\"\"          if len(nums) == 0: return None          left, right = 0, len(nums)-1          while left &lt; right:              mid = (left+right)/2              if nums[right] &lt; nums[mid]:                  left = mid + 1              elif nums[right] &gt; nums[mid]:                  right = mid              else:                  right -= 1          return nums[left]  \n","categories":["算法"],"tags":["python","二分搜索"]},{"title":"LeetCode 解题报告 (39,40)-- 数字集合中找特定和","url":"/2016/04/16/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(39,40)--%E6%95%B0%E5%AD%97%E9%9B%86%E5%90%88%E4%B8%AD%E6%89%BE%E7%89%B9%E5%AE%9A%E5%92%8C/","content":"这两道题目 39. Combination\nSum和 40.\nCombination Sum\nII均要求从给定的一个数字集合中找出若干个数字的和等于某个给定的数。解决方法有两种，第一种是回溯法，第二种是动态规划。下面分别讲述。\n\n39. Combination Sum\n原题如下：\n\nGiven a set of candidate numbers (C) and a target number (T), find\nall unique combinations in C where the candidate numbers sums to T.\n The same repeated number may be chosen from C unlimited\nnumber of times.\n\n\nNote: All numbers (including target) will be positive integers.\nElements in a combination (a1, a2, … , ak) must be in non-descending\norder. (ie, a1 ≤ a2 ≤ … ≤ ak). The solution set must not contain\nduplicate combinations. For example, given candidate set 2,3,6,7 and\ntarget 7, A solution set is: [7] [2, 2, 3]\n\n回溯法\n回溯法的的过程如下：先对数组从小到大排序。然后从第一个数字开始往后加，每加上一个数字就保留当前的状态，再加上下一个数字的时候假如和超过了给定的数字，就回到上一状态，上一个状态就跳过这个数字继续往后加。注意当前数字可以重复使用。\n实现方法如下： class Solution(object):    def combinationSum(self, candidates, target):        res = []        candidates.sort()        self.dfs(candidates, target, 0, [], res)        return res    def dfs(self, nums, target, index, path, res):        if target == 0:            res.append(path)            return         for i in xrange(index, len(nums)):            if target-nums[i] &lt; 0:                return  # backtracking            self.dfs(nums, target-nums[i], i, path+[nums[i]], res)\n动态规划法\n动态规划利用一个 list 存储每个数字可能的组合，dp [i] 表示和为 i 的数字的可能的组合，那么下一个 dp [i+1]=dp [i]+dp [1]=dp [i-1]+dp2...... 上面的加号表示对这两个可能的数字的组合进行组合。实现代码如下：\nclass Solution(object):    def combinationSum(self, candidates, target):        dp = {}        dp[0] = []        for i in xrange(1,target+1):            dp [i] = []            if i in candidates:                dp[i].append([i])            for j in xrange(1,i/2+1):                if len(dp[j]) ==0 or len(dp[i-j])==0:                    continue                for m in dp[j]:                    for k in dp[i-j]:                        tmp = m+k                        tmp.sort()                        if tmp not in dp[i]:                            dp[i].append(tmp)        return dp[target]\n40. Combination Sum II\n原题如下：\n&gt;Given a collection of candidate numbers (C) and a target number (T),\nfind all unique combinations in C where the candidate numbers sums to T.\n &gt;Each number in C may only be used once in the\ncombination.\n\nNote: All numbers (including target) will be positive integers.\nElements in a combination (a1, a2, … , ak) must be in non-descending\norder. (ie, a1 ≤ a2 ≤ … ≤ ak). The solution set must not contain\nduplicate combinations. For example, given candidate set 10,1,2,7,6,1,5\nand target 8, A solution set is: [1, 7] [1, 2, 5] [2, 6] [1, 1, 6]\n\n本题与前一题 Combination\nSum 非常相似，只是附加了每个数字只能使用一次的要求。需要注意给出的 candidate\nnumbers 中会有重复的数字。\n解决方法与前一题 Combination\nSum 也类似，只是在这个过程中元素不能被重复使用。\n回溯法\n实现代码如下： class Solution(object):    def combinationSum2(self, candidates, target):        candidates.sort()        result = []        self.dfs(candidates,target,0 ,[],result)        return result    def dfs(self, nums, tar, index, tmp, res):        if tar == 0 :            if tmp not in res:                res.append(tmp)        else:            for i in range(index,len(nums)):                if tar-nums[i]&lt;0:                    return                self.dfs(nums, tar-nums[i], i+1, tmp+[nums[i]], res)\n动态规划法\n需要先获取原来的 candidates 中各个数字的个数，然后每次遇到和等于目标数字的 list 时都要判断这个 list 中的各个数字个数是否超过给定的 candidates 中的标准。\n实现代码如下： class Solution(object):    def combinationSum2(self, candidates, target):        can_dict = {}        for candidate in candidates:            if candidate not in can_dict:                 can_dict[candidate] = 0            can_dict[candidate] += 1        dp = {}        dp[0] = []        for i in xrange(1,target+1):            dp[i] = []            if i in candidates:                dp[i].append([i])            for j in xrange(1,i/2+1):                if len(dp[j])==0 or len(dp[i-j])==0:                    continue                for m in dp[j]:                    for n in dp[i-j]:                        tmp_dict = {}                        flag = 0                                       tmp = m+n                        for t in tmp:                            if t not in tmp_dict:                                 tmp_dict[t] = 0                            tmp_dict[t] += 1                        for te in tmp_dict:                            if tmp_dict[te] &gt; can_dict[te]:                                 flag = 1                                break                        if flag == 0:                            tmp.sort()                                                     if tmp not in dp[i]:                                dp[i].append(tmp)        return dp[target]\n","categories":["算法"],"tags":["python","动态规划","回溯法"]},{"title":"LeetCode 解题报告 (32)-- 最长合法的子括号串","url":"/2016/04/06/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(32)--%E6%9C%80%E9%95%BF%E5%90%88%E6%B3%95%E7%9A%84%E5%AD%90%E6%8B%AC%E5%8F%B7%E4%B8%B2/","content":"原题如下：\n&gt;Given a string containing just the characters '(' and ')', find the\nlength of the longest valid (well-formed) parentheses substring.\n\n&gt;For \"(()\", the longest valid parentheses substring is \"()\", which\nhas length = 2.\n\nAnother example is \")()())\", where the longest valid parentheses\nsubstring is \"()()\", which has length = 4.\n\n题目的要求从给出的包含括号的 string 中找到最长的合法 string。\n最暴力的方法就是从长到短遍历 string 中的所有可能的 subString，再判断 subString 是否合法，这种方法的时间复杂度是 \\(O(n^3)\\)，提交时 TLE。\n两种比较巧妙的方法的方法的时间复杂度都是 O (n)，一种是动态规划，一种是利用栈存储不合法的括号的位置。\n下面分别列出这三种方法\n方法一，暴力枚举，时间复杂度 \\(O(n^3)\\)\n虽然容易理解，答案也正确，但是提交超时\nclass Solution(object):      def longestValidParentheses(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          if len(s)&lt;2:             return 0          left = 0          right = 0          for i in range(len(s)):              if s[i] == '(':                  left+=1              elif s[i] == ')':                  right+=1        m = min(left,right)        cut = len(s)-2*m # 从长到短遍历          while cut&lt;=len(s)-2:              subLen = len(s) - cut              for i in range(cut+1):                  subStr = s[i:i+subLen]                  if subStr[0] == ')':                      continue                  if self.isValid(subStr):                      return subLen              cut +=2      # 判断子字符串是否有效      def isValid(self,s):          stack = []          for i in range(len(s)):              if s[i] == '(':                  stack.append('(')              elif s[i] == ')':                  if len(stack)&gt;0:                      stack.pop(len(stack)-1)                  else:                      break        if len(stack)==0 and i==len(s)-1:              return True          else:              return False  \n方法二，动态规划，时间复杂度 \\(O(n)\\)\nDP 会先建立一个 longest 列表，其中 longest [i] 代表以 s [i]\n结尾的最长合法串的长度。这样便有了以下判断条件：\n\n假如 s[i]=( ，那么 longest [i]=0\n\n假如 s[i]=) &amp;&amp; s[i-1]=( ,\n那么 longest [i]=longest [i-2]+2\n\n假如\ns[i]=) &amp;&amp; s[i-1]=) &amp;&amp; s[i-longest[i-1]-1]==(\n, 那么 longest [i]=longest [i-longest [i-1]-2]+longest [i-1]+2\n\n假如上面情况都不符合那么，longest [i] = 0\n\n实现代码如下，提交时能够 AC：\nclass Solution(object):      def longestValidParentheses(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          longest = []          longest.append(0)          for i in range(1,len(s)):              if s[i] == '(':                  longest.append(0)              else:                  if i-1&gt;=0 and s[i-1]=='(':                      longest.append(longest[i-2]+2)                  elif i-1&gt;=0 and s[i-1]==')' and i-longest[i-1]-1 &gt;=0 and s[i-longest[i-1]-1]=='(':                      tmp = longest[i-longest[i-1]-2] if i-longest[i-1]-2&gt;=0 else 0                      longest.append(longest[i-1]+2+tmp)                  else:                      longest.append(0)          return max(longest)\n方法三，通过栈存储不合法的括号位置，时间复杂度 \\(O(n)\\)\n这种方法通过栈来存储不合法的括号的位置。\n具体流程是先遍历字符串 s，遇到左括号就将位置其入栈，遇到右括号就检查栈顶是否有左括号号，有的话就弹出这个左括号，否则将右括号的位置入栈。最后得到的栈中的元素就是原来 str 中不合法的括号的位置，那么栈中相邻位置的元素的差值就是一个合法的括号串的长度，找到其中最大值即可。\n需要注意的是最后遍历栈找最长合法串时需要在最左边和最右边分别加上 -1 和 len(s)(s 就是原字符串的长度)，目的是为了当原字符串最左边或最右边的括号均为合法时能够被计算长度。\n实现代码如下：\nclass Solution(object):      def longestValidParentheses(self, s):          \"\"\"          :type s: str          :rtype: int          \"\"\"          maxLen = 0          stack=[]          for i in range(len(s)):              if s[i] == ')' and len(stack)&gt;0 and s[stack[-1]]=='(':                  stack.pop(len(stack)-1)                  maxLen = 2              else:                  stack.append(i)        # 全匹配          if len(stack) == 0:              return len(s)          # 不在不匹配的要在头尾插入元素来计算头或尾匹配的括号          stack.append(len(s))          stack.insert(0,-1)          j = len(stack)-1          while j&gt;0:              maxLen = max(maxLen,stack[j]-stack[j-1]-1)              j -= 1          return maxLen  \n参考：\n【1】https://leetcode.com/discuss/7609/my-o-n-solution-using-a-stack\n【2】https://leetcode.com/discuss/8092/my-dp-o-n-solution-without-using-stack\n","categories":["算法"],"tags":["python","动态规划","栈"]},{"title":"LeetCode 解题报告 (4)-- 二分法查找两个有序数组的中位数","url":"/2016/01/10/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(4)--%E4%BA%8C%E5%88%86%E6%B3%95%E6%9F%A5%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/","content":"原题如下：\n\nThere are two sorted arrays nums1 and nums2 of size m and n\nrespectively. Find the median of the two sorted arrays. The overall run\ntime complexity should be O(log (m+n)).\n\n\n大意就是找出两个排好序的数组中所有数的中位数。\n最直观的做法是先将两个数组按从大到小合并成一个数组，再找出中位数。这样相当于遍历了两个数组，其时间复杂度是 O (m+n)。实现代码如下：\nclass Solution(object):      def findMedianSortedArrays(self, nums1, nums2):          \"\"\"          :type nums1: List[int]          :type nums2: List[int]          :rtype: float          \"\"\"          index1=0          index2=0          mergedArr=[]          while index1&lt;len(nums1) and index2&lt;len(nums2):              if nums1[index1] &gt; nums2[index2]:                  mergedArr.append(nums2[index2])                  index2+=1              elif nums1[index1] &lt; nums2[index2]:                  mergedArr.append(nums1[index1])                  index1+=1              else: #equal                  mergedArr.append(nums1[index1])                  mergedArr.append(nums2[index2])                  index1+=1                  index2+=1        if index1!=len(nums1):              while index1&lt;len(nums1):                  mergedArr.append(nums1[index1])                  index1+=1        if index2!=len(nums2):              while index2&lt;len(nums2):                  mergedArr.append(nums2[index2])                  index2+=1        mergedLen=len(mergedArr)          if mergedLen %2 == 0: #even length              return float(mergedArr[mergedLen/2]+mergedArr[mergedLen/2-1])/2          else:              return float (mergedArr[(mergedLen-1)/2])  \n尽管上面的时间复杂度是 O (m+n)，但是实际的提交时也能 AC。\n虽然上面的方法也能够 AC，但是根据题目的提示的 O (log (m+n)) 时间复杂度，显然还有更好的解法。下面就来讨论一种时间复杂度为 O (log (m+n)) 的算法。\n提到时间复杂度为 O (log (m+n)) 的算法，很容易想到的就是二分查找，所以现在要做的就是在两个排序数组中进行二分查找。\n具体思路如下，可以将问题转化为在两个数组中找第 K 个大的数，先在两个数组中分别找出第 k/2 大的数，再比较这两个第 k/2 大的数，这里假设两个数组为 A,B。那么比较结果会有下面几种情况：\n\nA [k/2]=B [k/2], 那么第 k 大的数就是 A [k/2]\n\nA [k/2]&gt;B [k/2], 那么第 k 大的数肯定在 A [0:k/2+1] 和 B [k/2:] 中，这样就将原来的所有数的总和减少到一半了，再在这个范围里面找第 k/2 大的数即可，这样也达到了二分查找的区别了。\n\nA [k/2] &lt;\nB [k/2]，那么第 k 大的数肯定在 B [0:k/2+1] 和 A [k/2:] 中，同理在这个范围找第 k/2 大的数就可以了。\n\n上面思路的实现代码如下\nclass Solution:      def findMedianSortedArrays(self, A, B):          n = len(A) + len(B)          if n % 2 == 1:              return self.findKth(A, B, n / 2 + 1)          else:              smaller = self.findKth(A, B, n / 2)              bigger = self.findKth(A, B, n / 2 + 1)              return (smaller + bigger) / 2.0    def findKth(self, A, B, k):          if len(A) == 0:              return B[k - 1]          if len(B) == 0:              return A[k - 1]          if k == 1:              return min(A[0], B[0])        a = A[k / 2 - 1] if len(A) &gt;= k / 2 else None          b = B[k / 2 - 1] if len(B) &gt;= k / 2 else None        if b is None or (a is not None and a &lt; b):              return self.findKth(A[k / 2:], B[0:k/2+1], k - k / 2)          return self.findKth(A[0:k/2+1], B[k / 2:], k - k / 2)  \n","categories":["算法"],"tags":["python","二分搜索"]},{"title":"LeetCode 解题报告 (382, 398)-- 随机采样算法 Reservoir Sampling","url":"/2017/05/28/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(382,%20398)--%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95%20Reservoir%20Sampling/","content":"Reservoir\nsampling 是一个随机采样算法，简单来说就是从 \\(n\\) 个 items 中随机选择 \\(k\\) 个 items，并且每个 item\n被选择的概率应该都一样。这个算法的优点在于时空复杂度都不高，其中时间复杂度为\n\\(O(n)\\), 空间复杂度为 \\(O(1)\\)。下面介绍该算法的过程，并且以\nleetcode 上的两道题目为例讲解。\n\n假设现在要从 \\(n\\)\n个数里面随机选择一个数，那么通过 Reservoir sampling 选择的流程如下\n\n记最终选择的数为 result\n 遍历数组，对于数组第 i 个数，以 \\(1/i\\)\n的概率将其赋给 result（i 从 1 开始，所以第一个数肯定会赋给 result）\n遍历完数组后得到的 result 即为产生的随机数\n\n假设现在有数组 [1, 2, 3], 随机产生一个数，那么按照上面的流程有 1.\n遍历第一个数时，result = 1 2. 遍历第二个数时，result = 2 的概率为 1/2,\n即 result = 1 的概率也是 1/2 3. 遍历第三个数时，result = 3 的概率为 1/3,\nresult = 1 的概率为 (1 - 1/3) * 1/2 = 1/3, 同理 result = 2 的概率也是\n1/3\n上面的 (1 - 1/3) * 1/2 指的是这一次没有选择第三个数且之前 result\n的值为 1 的概率，通过数学归纳法可以很容易的证明遍历完整个数组后每个数被选择的概率是\n1/n (n 为数组的长度)\n而假如要从 \\(n\\) 个数里面随机选择\n\\(k\\) 个数时，Reservoir sampling\n的过程类似上面的\n\n选择前 \\(k\\) 个数作为 result\n 从第 \\(k+1\\)\n个数开始遍历数组，对于数组第 \\(k+i （i =\n1,2,.....）\\) 个数，以 \\(\\frac{k}{k+i}\\)\n的概率选择这个数加入 result 并替换掉 result 中的任意一个数\n遍历完数组后得到的 result 即为产生的 \\(k\\) 个随机样本\n\n下面通过数学归纳法证明通过上面的算法过程最终每个数被选择的概率为\n\\(k/n\\)\n\n\\(i = 1\\) 时，选择第 \\(k+1\\) 个数的概率为 \\(\\frac{k}{k+1}\\)，而在 result 中 \\(k\\) 个数里面的一个 (记为 \\(x\\)) 能够保留下来的概率为是 \\(x\\) 原来在\nresult 中且这一次没有被替换的概率，而这一次没有被替换掉又可分为两种情况，一种是根本没有选择到第\n\\(k+i\\) 个数，一种是选择了第 \\(k+i\\) 个数，但是替换 \\(k\\) 个数中的一个时没有替换掉 \\(x\\)。公式表示为\n\n\\[\\begin {align} p ( x 上一次在 result 中)\n\\* p ( x 没有被替换掉) = 1 \\*（\\frac {k}{k+1} \\* (1-\\frac {1}{k}) + (1 -\n\\frac {k}{k+1})）= \\frac {k}{k+1} \\end {align}\\]\n即每个数被选择的概率为 \\(\\frac{k}{k+1}\\)\n\n因此当 \\(i = m\\)\n时，每个数被选择的概率为 \\(k/(k+m)\\)\n 则当 \\(i = m+1\\) 时，选择第\n\\(k+m+1\\) 个数的概率为 \\(\\frac{k}{k+m+1}\\), 在 result 中 \\(k\\) 个数里面的一个 (记为 \\(x\\)) 能够保留下来的概率为:\n\n\\[\\begin {align} p ( x 上一次在 result 中)\n\\* p ( x 没有被替换掉) = \\frac {k}{k+m} \\*（\\frac {k}{k+m+1} \\* (1-\n\\frac {1}{k}) + (1 - \\frac {k}{k+m+1})）= \\frac {k}{k+m+1}\n\\end {align}\\]\n从上可知，遍历到第 \\(i\\)\n个数的时候，前 \\(k+i\\)\n每个数被选择的概率为 \\(k/(k+i)\\), 则遍历完 \\(n\\) 个数后，每个数被选择的概率为 \\(k/n\\)\nLeetCode 上的题目 382.\nLinked List Random Node 和 398.\nRandom Pick Index 均用到了 Reservoir Sampling\n的技巧，上面的依概率选择可以通过产生随机数并与概率值比较来实现，下面分别是\n解决 382. Linked List Random Node 和 398. Random Pick Index 的 Java\n代码\n// 382. Linked List Random Nodepublic class Solution {    private ListNode dummy = new ListNode(0);    public Solution(ListNode head)     {        dummy.next = head;    }    public int getRandom()    {        ListNode curr = dummy.next;        int count = 0, result = 0;        while (curr != null)        {            count ++;            if (Math.random() &lt; 1.0/count) result = curr.val;            curr = curr.next;        }        return result;    }}// 398. Random Pick Indexpublic class Solution {      private int[] numbers;    public Solution(int[] nums)     {        numbers = nums;    }    public int pick(int target)     {        int index = 0, count = 0;        for(int i = 0; i &lt; numbers.length; i++)        {            if (numbers[i] == target)            {                count++;                if(Math.random() &lt; 1.0/count) index = i;            }        }        return index;    }}\n","categories":["算法"],"tags":["python","Reservoir Sampling"]},{"title":"LeetCode 解题报告 (41)-- 第一个缺失的正整数","url":"/2016/04/20/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(41)--%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%AD%A3%E6%95%B4%E6%95%B0/","content":"原题如下：\n\nGiven an unsorted integer array, find the first missing positive\ninteger.  For example, Given [1,2,0] return 3, and [3,4,-1,1]\nreturn 2.\n\n\nYour algorithm should run in O(n) time and uses constant space.\n\n题目要求找出第一个缺失的正整数，难点在于时间复杂度要控制在 O (n)，也就是说不能对数组排序，空间复杂度要控制在 O (1)，也就是说不能开一个新的数组去存储这些数字。\n但是这道题目也有它的特点，假如给定的数组的长度为 n，那么缺失的第一个正数肯定在 [1,n+1] 内，根据这个特点，可以将范围为 [1,n] 的数字移动到与其数值相等的下标的位置，对所有的数这样操作后从头遍历一遍数组，找出的第一个\ni+1 != a[i] 的 i, 那么 i+1 就是第一个缺失的正整数。\nclass Solution(object):    def firstMissingPositive(self, nums):        \"\"\"        :type nums: List[int]        :rtype: int        \"\"\"        n = len(nums)        for i in xrange(n):            while 0&lt;nums[i]&lt;=n and (not nums[nums[i]-1]==nums[i]):                    tmp = nums[i]                    nums[i] = nums[tmp-1]                    nums[tmp-1] = tmp                    # nums[i],nums[nums[i]-1] = nums[nums[i]-1],nums[i]错误        for i in xrange(n):            if nums[i] == i+1:                continue            else:                return i+1        return n+1 # all match\n","categories":["算法"],"tags":["python"]},{"title":"LeetCode 解题报告 (42)-- 柱状图的储水量","url":"/2016/05/01/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(42)--%E6%9F%B1%E7%8A%B6%E5%9B%BE%E7%9A%84%E5%82%A8%E6%B0%B4%E9%87%8F/","content":"原题如下\n&gt;Given n non-negative integers representing an elevation map where\nthe width of each bar is 1, compute how much water it is able to trap\nafter raining.  For example, Given [0,1,0,2,1,0,1,3,2,1,2,1],\nreturn 6.  The above\nelevation map is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this\ncase, 6 units of rain water (blue section) are being trapped. Thanks\nMarcos for contributing this image!\n从题目给出的图可以比较清楚知道，题目给出的数组中的每个数字代表一根柱子的高度，将数组中的数字全部表示为特定高度的柱子后，求这些柱子组成的容器能够容纳的水量。\n解题思路：可以分别求出每根柱子的储水量，然后将每根柱子的储水量加起来即为总的储水量。而每根柱子的储水量可以通过下面方法求：找到这根柱子左边所有柱子中最高的那根，记为 A；找到这根柱子右边所有柱子中最高的那根，记为 B；min (A,B)-r 即为这根柱子的储水量，r 为这根柱子的高度。\n实现代码如下： class Solution(object):    def trap(self, height):        \"\"\"        :type height: List[int]        :rtype: int        \"\"\"        if len(height)==0:            return 0        leftMax = []        # leftMax Array        max = height[0]        for i in range(len(height)):            if height[i] &gt; max :                max = height[i]            leftMax.append(max)        sum = 0        rightMax = height[len(height)-1]        for i in reversed(range(1,len(height)-1)):            miniHeight = min(leftMax[i-1],rightMax)            if miniHeight &gt; height[i]:                sum += miniHeight - height[i]            if height[i] &gt; rightMax:                rightMax = height[i]        return sum \n","categories":["算法"],"tags":["python"]},{"title":"LeetCode 解题报告 (46，47)-- 排列","url":"/2016/05/06/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(46%EF%BC%8C47)--%E6%8E%92%E5%88%97/","content":"46. Permutations\n原题如下：\nGiven a collection of distinct numbers, return all possible\npermutations.  For example, [1,2,3] have the following\npermutations: [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], and\n[3,2,1].\n题目要求列出给定的不重复数字的所有排列结果。解题方法有两种。下面分别讲述。\n方法一\n第一种方法每次取一个数字，将数字插入现有的排列中形成新的排列，然后重复上面的过程直到所有数字都被取完。\n实现代码如下：\nclass Solution(object):    def permute(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        result = [[]]        for num in nums:            new_result = []            for seq in result:                for i in xrange(len(seq)+1):                    new_result.append(seq[:i]+[num]+seq[i:])            result = new_result        return result\n方法二\n第二种方法采用回溯法。将当前数字与后面的数字逐一交换，当与某一数字交换后，然后移动到下一个数字重复上面的操作。实现具体代码如下\nclass Solution(object):    def permute(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        result = []        self.helper(nums,0,result)        return result    def helper(self, nums, begin, result):        n = len(nums)        if begin == n:            tmp = nums[:]            result.append(tmp)            return        for i in xrange(begin,n):            nums[begin],nums[i] = nums[i], nums[begin]            self.helper(nums,begin+1,result)            nums[begin],nums[i] = nums[i], nums[begin]\n47. Permutations II\n原题如下：\nGiven a collection of numbers that might contain duplicates, return\nall possible unique permutations.\nFor example, [1,1,2] have the following unique permutations: [1,1,2],\n[1,2,1], and [2,1,1].\n这道题目的要求与上一道一样，只是在上面的基础上添加了存在重复数字的约束条件。\n上面提到回溯法在这里不适用了（即使判定交换的元素是否相等），比如说对于 [1,2,3,3] 就不正确了。\n采用方法一的直接插入法，从后往前插，遇到与当前要插入数字相同的数字时就终止当前排列的插入，进行下一个排列的插入。实现的具体代码如下：\nclass Solution(object):    def permuteUnique(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        result = [[]]        for num in nums:            new_result = []            for seq in result:                n = len(seq)                for i in xrange(n,-1,-1):                    if i &lt; n and seq[i] == num:                        break                    new_result.append(seq[:i]+[num]+seq[i:])            result = new_result        return result\n","categories":["算法"],"tags":["python","回溯法"]},{"title":"LeetCode 解题报告 (48)-- 矩阵的旋转","url":"/2016/05/12/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(48)--%E7%9F%A9%E9%98%B5%E7%9A%84%E6%97%8B%E8%BD%AC/","content":"原题如下:\nYou are given an n x n 2D matrix representing an image.\nRotate the image by 90 degrees (clockwise).\nFollow up: Could you do this in-place? \n题目要求将图片顺时针翻转 90 度。通过用一个额外的图片矩阵来存储这个图片能够轻松完成，但是题目最后说了 Could you do this in-place?，意思是能否通过 O (1) 的空间复杂度完成这项任务。\n这就需要一点技巧了，下面以一个例子说明如何不借助额外的图片矩阵完成顺时针 90 度旋转。\n1 2 3     7 8 9     7 4 14 5 6 --&gt; 4 5 6 --&gt; 8 5 27 8 9     1 2 3     9 6 3\n首先将图片的行上下对称交换，假如有奇数行，中间的行不用动；假如有偶数行，则全部都要对称交换。然后将矩阵转置即可。\n具体实现代码如下： class Solution(object):    def rotate(self, matrix):        \"\"\"        :type matrix: List[List[int]]        :rtype: void Do not return anything, modify matrix in-place instead.        \"\"\"        n = len(matrix)        for i in xrange(n/2):            matrix[i], matrix[n-1-i] = matrix[n-1-i], matrix[i]        for i in xrange(n):            for j in xrange(i+1,n):                matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j]\n参考上面的方法，同理可以推出逆时针旋转 90 度和旋转 180 度的实现方法。\n逆时针旋转 90 度\n将矩阵的列左右对称交换，然后转置。例子如下所示\n1 2 3     3 2 1     3 6 94 5 6 --&gt; 6 5 4 --&gt; 2 5 87 8 9     9 8 7     1 4 7\n旋转 180 度\n上下对称交换，然后左右对称交换。例子如下所示： 1 2 3     7 8 9     9 8 74 5 6 --&gt; 4 5 6 --&gt; 6 5 47 8 9     1 2 3     3 2 1\n","categories":["算法"],"tags":["python","图"]},{"title":"LeetCode 解题报告 (5)-- 最长回文子字符串","url":"/2016/01/24/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(5)--%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","content":"原题如下：\n\nGiven a string S, find the longest palindromic substring in S. You\nmay assume that the maximum length of S is 1000, and there exists one\nunique longest palindromic substring.\n\n就是要从给定的字符串中找出最大的回文子字符串。\n\n下面介绍两种时间复杂度为 \\(O(n^2)\\)\n的方法，第一种是以每个字符为中心向两边拓展从而得到尽可能长的回文字符串，第二种是利用动态规划。\n第一种方法的关键点是以每个字符为中心向两边拓展从而得到尽可能长的回文字符串。这里要注意的是回文字符串的长度可以是奇数也可以是偶数，所以要分两种情况讨论。\n实现代码如下：\n#encoding:utf-8  class Solution(object):      def longestPalindrome(self, s):          \"\"\"          :type s: str          :rtype: str          \"\"\"          r=s[0]          if len(s) &gt; 1:              for i in range (1,len(s)):                  # oddResult                  begin=i                  end=i                  oddResult=self.getPalindoreStr(s,begin,end)                  if oddResult != None and len(oddResult)&gt; len(r):                      r=oddResult                # enevResult                  begin=i-1                  end=i                  evenResult=self.getPalindoreStr(s,begin,end)                  if evenResult != None and len(evenResult)&gt; len(r):                      r=evenResult          return r    #从给定的begin和end位置向两边扩展，得到回文字符串      def getPalindoreStr(self,s,begin,end):          while ( begin&gt;=0 and end&lt;len(s) and s[begin]==s[end] ):                  begin-=1                  end+=1          result=s[begin+1:end] if  begin+1 &lt;= end else None          return result  \n第二种方法采用的是动态规划，dp[i][j] 表示 s [i:j]\n所形成的字符串是否为回文字符串，时间复杂度为 \\(O(n^2)\\)。代码如下\nclass Solution(object):      def longestPalindrome(self, s):          \"\"\"          :type s: str          :rtype: str          \"\"\"          n = len(s)          dp = [[False for j in xrange(n)] for i in xrange(n)]          result = ''          for i in reversed(xrange(n)):              for j in xrange(i, n):                  if i == j:                      dp[i][j] = True                  elif s[i] == s[j]:                      dp[i][j] = dp[i+1][j-1] if i+1&lt;=j-1 else True                  if dp[i][j] and j-i+1 &gt; len(result):                      result = s[i:j+1]          return result  \n","categories":["LeetCode"],"tags":["python","动态规划","字符串"]},{"title":"LeetCode 解题报告 (95,96)-- 构造二叉搜索树","url":"/2016/06/11/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(95,96)--%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","content":"两个题目均是要求利用给出的整数 [1,\nn] 构造出所有的二叉搜索树（BST），其中 95.\nUnique Binary Search Trees II要求返回所有的二叉树的根节点，96.\nUnique Binary Search Trees则仅要求返回所有二叉树的数目。\n\n两个题目均可以通过递归实现，思路如下：\n1）遍历 [1,n] 中每个整数，并将正在遍历的整数 i 设为根节点 2）将\n[1,i-1] 的整数构建出来的子树作为整数 i 构建的根节点的左子树，将 [i+1,n]\n的整数构造出来的子树作为整数 i 构建的根节点的右子树\n3）对得到的左右子树进行组合，这样便可得到以整数 i 为根节点的所有二叉树\n因此，95.\nUnique Binary Search Trees II实现代码如下:\n# Definition for a binary tree node.# class TreeNode(object):#     def __init__(self, x):#         self.val = x#         self.left = None#         self.right = Noneclass Solution(object):    def generateTrees(self, n):        \"\"\"        :type n: int        :rtype: List[TreeNode]        \"\"\"        if n==0: return []         return self.helper(1, n)    def helper(self,begin,end):        if begin &gt; end:            return [None]        if begin == end:            return [TreeNode(begin)]        result = []        for i in xrange(begin,end+1):            tmp = []            left = self.helper(begin, i-1)            right = self.helper(i+1, end)            for m in left:                for n in right:                    root = TreeNode(i)                    root.left, root.right = m, n                    result.append(root)        return result\n96.\nUnique Binary Search Trees\n的解决思路类似于上面提到的递归的方法，只是这道题目仅仅要求返回二叉树的数量。因此可以通过一个技巧缩短计算量，这个技巧就是整数\ni 作为根节点和整数 n-i\n作为根节点的时候，两者构造的二叉树的数目是一样的。因此整数 i\n作为根节点的左右子树的节点数目与整数 n-i\n作为根节点的左右子树的节点数目对称相等。\n实现的代码如下： # Definition for a binary tree node.# class TreeNode(object):#     def __init__(self, x):#         self.val = x#         self.left = None#         self.right = Noneclass Solution(object):    def numTrees(self, n):        \"\"\"        :type n: int        :rtype: int        \"\"\"        result = 0        if n&lt;=1:            return 1        for i in xrange(n/2):            left = self.numTrees(i)            right = self.numTrees(n-i-1)            result += left * right        result *= 2        if n%2 == 1:            result += pow(self.numTrees(n/2),2)        return result 要注意的是当 n\n为基数的时候，整数(n+1)/2没有这种对称关系，需要额外计算。\n从上面也可以衍生出动态规划的解法，令 dp [m] 表示 m\n个节点能够构造的二叉树的数目。那么 dp [m+1] 的计算方法如下\nfor i in xrange(0, m+1):    dp[m+1] += dp[i]*dp[m+1-i-1]\n实现的代码如下： class Solution(object):    def numTrees(self, n):        \"\"\"        :type n: int        :rtype: int        \"\"\"        dp = [0 for i in xrange(n+1)]        dp[0],dp[1] = 1, 1        for i in xrange(2,n+1):            for j in xrange(i):                dp[i] += dp[j]*dp[i-j-1]        return dp[n]\n","categories":["算法"],"tags":["python","树","动态规划","分治法"]},{"title":"Linux shell 下删除文件名乱码文件","url":"/2015/11/21/Linux%20shell%E4%B8%8B%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8D%E4%B9%B1%E7%A0%81%E6%96%87%E4%BB%B6/","content":"当文件名为乱码的时候，无法通过键盘输入文件名，所以在终端下就不能直接利用 rm，mv 等命令管理文件了。\n但是我们知道每个文件都有一个 i 节点号，我们可以考虑通过 i 节点号来管理文件。首先，我们要取得文件的\ni 节点号。这个可以通过 ls 命令的 - i 选项获得得。\n\n#ls -i  41697812 a 32983551 di 32983554 ethnet.c 32983543 hard_link  32983542 de.c 32983544 ethnet 32983541 ethnet.h 32983543 kstat\n每个文件名前面的数字就是文件的 i 节点号。有了文件的 i 节点号，我们就可以利用 find 命令的 - inum 选项配合\n常用的文件管理命令进行文件管理了。例如，如果要删除 di 文件，命令如下：\n# find . -inum 32983551 -exec rm {} \\;  \n命令中的 “{}” 表示 find 命令找到的文件，\n要重命名一个文件，命令也很简单，如下：\n$ ls -i  32983542 de.c 32983554 ethnet.c 32983543 hard_link 32983545 kstat.c  32983544 ethnet 32983541 ethnet.h 32983543 kstat 32983681 sys_link$ find . -inum 32983542 -exec mv {} di.c \\;  $ ls -i  32983542 di.c 32983554 ethnet.c 32983543 hard_link 32983545 kstat.c  32983544 ethnet 32983541 ethnet.h 32983543 kstat 32983681 sys_link\nde.c 文件被重命名为 di.c 了。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux shell 下常用快捷键","url":"/2015/11/21/Linux%20shell%E4%B8%8B%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/","content":"下述所有命令在 Linux/unix 的 shell 下有效，这里以 bash\n为主。如有出入，以你自己的服务器为准。本文所指的 Linux 主要指\nRHEL/CentOS，unix 指的是 FreeBSD，这也是服务器中用得最多的版本。\n\nCtrl + a\n切换到命令行开始\n这个操作跟 Home 实现的结果一样的，但 Home 在某些 unix\n环境下无法使用，便可以使用这个组合；在 Linux 下的\nvim，这个也是有效的；另外，在 windows\n的许多文件编辑器里，这个也是有效的。\nCtrl + e\n切换到命令行末尾\n这个操作跟 END 实现的结果一样的，但 End 键在某些 unix\n环境下无法使用，便可以使用这个组合；在 Linux 下的\nvim，这个也是有效的；另外，在 windows\n的许多文件编辑器里，这个也是有效的。\nCtrl + l\n清除屏幕内容\n效果等同于 clear\nCtrl + u\n清除剪切光标之前的内容\nCtrl + k\n剪切清除光标之后的内容\nCtrl + y\n粘贴刚才所删除的字符\n此命令比较强悍，删除的字符有可能是几个字符串，但极有可能是一行命令。\nCtrl + r\n在历史命令中查找\n输入关键字就调出以前的命令了，强烈推荐，有时 history\n比较多时，想找一个比较复杂的，直接在这里，shell\n会自动查找并调用，方便极了。\nCtrl + c\n终止命令\nCtrl + z\n转入后台运行\n不过，由 Ctrl + z\n转入后台运行的进程在当前用户退出后就会终止，所以用这个不如用 nohup\n命令 &amp;，因为 nohup\n命令的作用就是用户退出之后进程仍然继续运行，而现在许多脚本和命令都要求在\nroot 退出时仍然有效。\nCtrl + d\n退出 shell，logout\n!!\n重复执行最后一条命令\nhistory\n显示你所有执行过的编号 + 历史命令。这个可以配合！编辑来执行某某命令\n!$\n显示系统最近的一条参数。比如我先用 cat\n/etc/sysconfig/iptables，然后我想用 vim 编辑。一般的做法是先用↑\n显示最后一条命令，然后用 Home 移动到命令最前，删除 cat，然后再输入 vim\n命令。其实完全可以用 vim !$ 来代替。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux 下安装 sun/oracle 的 jdk","url":"/2015/11/26/Linux%E4%B8%8B%E5%AE%89%E8%A3%85sun-oracle%E7%9A%84jdk/","content":"Linux\n自带的 jdk 是 openjdk，但是 sun/oracle 的 jdk 更加常用一些，据说 bug 也更少。所以下面就是卸载 openjdk 安装 sun/oralce\njdk 的一个教程。\n\n检查 OpenJDK 是否已经安装\nrpm -q &lt; rpm package name&gt;\n用来查询一个包是否被安装，而 rpm -qa 则列出了所有被安装的 rpm 包\n$ rpm -qa | grep java  tzdata-java-2013b-1.el6.noarch  java-1.6.0-openjdk-1.6.0.0-1.61.1.11.11.el6_4.x86_64  java-1.7.0-openjdk-1.7.0.19-2.3.9.1.el6_4.x86_64  \n检查 OpenJDK 版本\n$ java -version  java version \"1.7.0_19\"  OpenJDK Runtime Environment (rhel-2.3.9.1.el6_4-x86_64)  OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)  \n卸载 Openjdk\n用 root 用户登录终端，rpm -e --nodeps\n表示强制卸载某个 rpm 包，因为采用 rpm -e 删除时有时会出现... is needed by ... 的依赖提示而不能卸载这个包\n$ rpm -e --nodeps java-1.7.0-openjdk-1.7.0.19-2.3.9.1.el6_4.x86_64  $ rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.61.1.11.11.el6_4.x86_64  $ rpm -e --nodeps tzdata-java-2013b-1.el6.noarch  \n下载并安装 jdk-7u17-linux-x64.rpm\n下载地址：http://pan.baidu.com/share/link?shareid=397488&amp;uk=638583574，rpm -ivh &lt;rpm package&gt; 为安装某个 rpm 包的命令，参数 ivh 各自的意义如下所示\n-i, --install   install package(s)  -v, --verbose   provide more detailed output  -h, --hash      print hash marks as package installs (good with -v)  ```  而`rpm -Uvh`则表示升级一个软件包```  $ cd /jdk1.7所在目录  $ rpm -ivh jdk-7u17-linux-x64.rpm  Preparing...                ########################################### [100%]     1:jdk                    ########################################### [100%]  Unpacking JAR files...      rt.jar...  Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/rt.pack      jsse.jar...  Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/jsse.pack      charsets.jar...  Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/charsets.pack      tools.jar...  Error: Could not open input file: /usr/java/jdk1.7.0_17/lib/tools.pack      localedata.jar...  Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/ext/localedata.pack  以上那些错误可以忽略，不影响jdk到安装和使用  \n配置环境变量\n这是很关键的一步，jdk 使用过程中绝大部分问题都跟环境变量的配置有关，需要配置的变量有 JAVA_HOME，PATH 和 CLASSPATH, 其中 JAVA_HOME 表示 Java 的安装目录，PATH 是为了让系统在任何路径下都可以识别出 java 的命令，CLASSPATH 则指定 Java 运行时查找 class 文件的路径，尤其需要注意 CLASSPATH 需要包含当前目录，也就是.，而且还要包含工具类库 tool.jar；如果需要 Swing 包，还可以添加 dt.jar。所以上面这三个变量的最简配置如下所示：\n$vi /etc/profile #在最后加入以下内容：  JAVA_HOME=/usr/java/jdk1.7.0_17  PATH=$PATH:$JAVA_HOME/bin  CLASSPATH=.:$JAVA_HOME/lib/tools.jar  export JAVA_HOME  PATH CLASSPATH  ```  使环境变量立即生效  ```  $source /etc/profile  ```  ## 测试安装是否成功  依次输入`java,java -version,javac`，看到输出信息即可,例如  ```  # java -version  java version \"1.7.0_17\"  Java(TM) SE Runtime Environment (build 1.7.0_17-b02)  Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)  \n","categories":["Linux"],"tags":["Linux","Java"]},{"title":"Linux 命令的系统调用和库函数的调用","url":"/2015/11/21/Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E5%92%8C%E5%BA%93%E5%87%BD%E6%95%B0%E7%9A%84%E8%B0%83%E7%94%A8/","content":"查看 Linux 命令的系统调用和库函数的调用可通过下面的命令。\n\nstrace -c\ncommand：判断 command 命令的系统调用的类型、次数、消耗时间（-f 则连同 command 命令 fork 出来的子进程一同统计，-e 指定列出某一具体的系统调用的参数）\n\nltrace\n用法同 strace, 但是追踪的是命令调用的库函数，strace 追踪的是系统调用\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux 传输文件的小工具 lrzsz","url":"/2015/11/28/Linux%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E7%9A%84%E5%B0%8F%E5%B7%A5%E5%85%B7lrzsz/","content":"常常有些小文件需要从本地的 Windows 传到 Linux 服务器或者从 Linux 服务器下载到本地，如果用 ftp 就显得杀鸡用牛刀了，这时候工具 lrzsz 就显得比较有用了\n\n　　首先需要安装这个工具，以 CentOS 为例，通过 yum 安装即可，即\nyum -y install lrzsz  \n可用的命令为 rz 和 sz, 可通过下面的方式来记忆\n\nsz 中的 s 意为 send（发送），告诉客户端，我（服务器）要发送文件 send to\ncilent，就等同于客户端在下载。\n\nrz 中的 r 意为 received（接收），告诉客户端，我（服务器）要接收文件\nreceived by cilent，就等同于客户端在上传。\n记住一点，不论是 send 还是 received，动作都是在服务器上发起的。\n\n　　运行命令 rz，Xshell (或 SecureCrt) 就会弹出文件选择对话框，选好文件以及传输方式（文本还是二进制）之后关闭对话框，文件就会上传到 linux 里的当前目录。\n　　运行命令 sz file\n就是发文件到 Windows 上（保存的目录是可以配置，因为 sz 利用了 ZModem 协议来传输文件，所以一般可在使用的连接工具（如 Xshell 等）中设置）；常用的参数如下所示：\n\n-a 以文本方式传输（ascii）\n\n-b 以二进制方式传输（binary）\n\n-e 对控制字符转义（escape），这可以保证文件传输正确\n\n也可将文件先压缩成一个压缩文件再传输。\n","categories":["Linux"],"tags":["Linux","工具使用"]},{"title":"Leetcode 解题报告 (496, 975, 503)-next greater/smaller element","url":"/2019/03/25/Leetcode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(496,%20975,%20503)-next%20greater-smaller%20element/","content":"本文主要介绍在 LeetCode 题目 496. Next\nGreater Element I、975. Odd Even\nJump、503. Next\nGreater Element II 中需要解决的共同问题：next greater\nelement，就是对于一个数组中的每个 element，求出下标和值都比其大的一个\nelement，根据要求不同，这个问题又可分为 nearest of next greater elements\n和 smallest of next greater elements，前者指的是 next greater elements\n中离当前 element 最近的那个，后者指的是 next greater elements\n中值最小的那个。两个问题都可通过 stack 解决，后者也可通过 treemap\n解决。最后会将原来的问题进行的拓展，将原来的数据改成头尾相接的，其解决方法是将来的数组进行\nduplicate, 然后把环解开，详细请看后文。\n\nnearest of next greater\nelements\n496.\nNext Greater Element I 要解决的就是 nearest of next greater element\n的问题，如果直接进行暴力枚举，那时间复杂度是 \\(O(n^2)\\), 而借助栈，能够让时间复杂度降为\n\\(O(n)\\),\n其过程如下所示（栈用于存储元素的下标）\n\n从后往前遍历数组\n对于当前下标，假如其对应的值大于栈顶元素对应的值，则将栈顶元素出栈\n重复步骤 2\n直至栈顶元素对应的值大于等于当前下标对应的值或栈为空，如果是前者，则当前栈顶元素便是当前元素的\nnearest of next greater elements, 如果是后者，这样的 element 不存在\n将当前下标压栈，回到步骤 1\n\n由于每个元素最多只会出栈一次和入栈一次，因此总体的时间复杂度是 \\(O(n)\\),\n这个方法减少了时间复杂度的关键点在于出栈，如对当前元素\ne1，经过出栈后小于当前元素 e1 的元素都会出栈，假如下一个元素 e2 大于\ne1，那么 e2 就不需要再跟那些已经出栈的元素进行比较了，因为那些元素小于\ne1，也必定小于 e2。这样就减少了比较次数，将时间复杂度从原来的\n\\(O(n^2)\\) 降低到了 \\(O(n)\\)。\n这里以题目 496. Next Greater Element I 为例，AC 的 pyhton\n代码如下所示，\n# traverse from right to leftclass Solution(object):    def nextGreaterElement(self, nums1, nums2):        record, stack =  {}, []        for i in xrange(len(nums2) - 1, -1, -1):            while stack and stack[-1] &lt; nums2[i]:                stack.pop()            if stack:                record[nums2[i]] = stack[-1]            else:                record[nums2[i]] = -1            stack.append(nums2[i])        return [record[num] for num in nums1]\n除了从后往前遍历，利用栈从前往后遍历也能解决这个问题，原理类似，AC\n的 python 代码\n# traverse from left to rightclass Solution(object):    def nextGreaterElement(self, nums1, nums2):        stack, record = [], {}        for num in nums2:            while stack and stack[-1] &lt; num:                record[stack.pop()] = num            stack.append(num)        for num in stack:            record[num] = -1        return [record[num] for num in nums1]\nsmallest of next greater\nelement\n975. Odd Even\nJump 要解决的是 smallest of next greater element\n这一类问题，题目不是直接求解这个问题，而是结合了动态规划和这个问题。\n动态规划比较容易想到，因为一般这类型是否能到达终点的题目都可以通过动态规划求解，如\n\n55. Jump\nGame\n70.\nClimbing Stairs\n403. Frog\nJump\n746. Min\nCost Climbing Stairs\n.....\n\n回到题目 975. Odd Even Jump 由于在每个点有两种选择，即 odd jump 或者\neven jump，则可以建立两个 dp 数组，odd_dp 和\neven_dp, odd_dp[i] 表示从 i 以 odd jump\n开始能够跳到终点，even_dp[i] 表示从 i 以 even jump\n开始能够跳到终点。则 dp 递推式就很简单了\nodd_dp[i] = even_dp[smallest of next greater elements of i]even_dp[i] = odd_dp[greatest of next smaller elements of i]\n最终统计 odd_dp 中为 true 的元素数量即可，\n则问题就变成了 smallest of next greater elements 和 greatest of next\nsmaller elements；解决的方法有两种，stack 和\ntreemap，两者的时间复杂度均为 \\(O(nlgn)\\)\nstack\n利用 stack 求解时，\n不再像前面的问题一样可以直接在原来的数组上进行遍历，而是将每个元素的值带上其下标组成一个新的\ntuple，然后根据 tuple\n中的元素的值进行从小到大的排序，则此时便将问题转化为了 nearest of next\ngreater\nelements，即只需要在当前元素后面那些元素中找到第一个下标比当前元素下标大的元素即可。排序的时间复杂素是\n\\(O(nlgn)\\),\n遍历进行动态规划的时间复杂度是 \\(O(n)\\), 因此总体的时间复杂度是 \\(O(nlgn)\\), 求解题目的 python\n代码如下所示\nfrom collections import dequeclass Solution(object):    def oddEvenJumps(self, A):        n = len(A)        sorted_A = sorted([(v, i) for i, v in enumerate(A)])        next_greater, greater_stack = [-1] * n, []        idx = n - 1        while idx &gt;= 0:            while greater_stack and greater_stack[-1] &lt; sorted_A[idx][10]:                greater_stack.pop()            if greater_stack:                next_greater[sorted_A[idx][11]] = greater_stack[-1]            greater_stack.append(sorted_A[idx][12])            idx -= 1        sorted_A = sorted([(v, -i) for i, v in enumerate(A)]) # -i if for repeated elements        next_smaller, smaller_stack = [-1] * n, []        idx = 0        while idx &lt; n:            while smaller_stack and smaller_stack[-1] &lt; -1*sorted_A[idx][13]:                smaller_stack.pop()            if smaller_stack:                next_smaller[-1*sorted_A[idx][14]] = smaller_stack[-1]            smaller_stack.append(-1*sorted_A[idx][15])            idx += 1        odd_dp, even_dp = [False] * n, [False] * n        odd_dp[-1], even_dp[-1] = True, True        result = 1        for i in xrange(n - 2, -1, -1):            if next_greater[i] &gt;= 0 and even_dp[next_greater[i]]:                odd_dp[i] = True                result += 1            if next_smaller[i] &gt;= 0 and odd_dp[next_smaller[i]]:                even_dp[i] = True        return result\ntreemap\n另一种就是利用 treemap，treemap\n是通过红黑树实现的一种数据结构，红黑树是一棵二叉搜索树，因此能够在 \\(O(lgn)\\) 时间复杂度内找到 next greater\nelement。c++ 中内置的 map 的数据结构便是 treemap，python 中没有 treemap\n这种内置的数据结构。因此，对应求解题目的 c++ 代码如下，需要注意的是 map\n的两个方法 lower_bound 和 upper_bound\n含义如下\n\nlower_bound 返回第一个大于等于当前值的 iterator\nupper_bound 返回第一个大于当前值的 iterator\n\n因此，下面的 --smaller 表示将返回的 iterator\n往后移动，找到一个小于等于当前值的数\n#include &lt;vector&gt;#include &lt;map&gt;class Solution { public:  int oddEvenJumps(std::vector&lt;int&gt;&amp; A) {    int n = A.size();    std::vector&lt;bool&gt; odd_dp(n, false), even_dp(n, false);    std::map&lt;int, int&gt; m;    odd_dp[n-1] = true;    even_dp[n-1] = true;    int result = 1;    for (int i = n - 1; i &gt;= 0; i--) {      auto greater = m.lower_bound(A[i]);      if (greater != m.end() &amp;&amp; even_dp[greater-&gt;second]) {        result++;        odd_dp[i] = true;      }      auto smaller = m.upper_bound(A[i]);      if (smaller != m.begin() &amp;&amp; odd_dp[(--smaller)-&gt;second]) even_dp[i] = true;      m[A[i]] = i;    }    return result;  }};\narray to circle\n最后是 503. Next\nGreater Element II，\n这个题目在第一题基础上将原来的数组首尾相连，并求每个元素的 next\nelement，解决方法还是利用栈。\n这种形成 circle 的问题一般都会想办法把 circle 去掉，如 213. House Robber\nII\n就是通过列举可能的两种情况来把环去掉。而这道题目是通过首先在栈按原来数组顺序存储整个数组，栈顶元素为原来数组下标为\n0\n的元素，然后按照上面的方法从后往前遍历，这样做有效的原因是每个元素最多只能比较一圈回到自己本身，\n对应的 python 代码如下所示 class Solution(object):    def nextGreaterElements(self, nums):        n = len(nums)        result = [-1] * n        stack = [nums[i] for i in xrange(n - 1, -1, -1)]        for i in xrange(n - 1, -1, -1):            while stack and stack[-1] &lt;= nums[i]:                stack.pop()            if stack:                result[i] = stack[-1]            stack.append(nums[i])        return result\nsummary\n综上，本文主要介绍了 next greater element\n问题及其衍生问题的求解方法，最原始的 nearest of next greater elements\n通过 stack 能够在 \\(O(n)\\)\n的时间复杂度内求解；对于 smallest of next greater elements 问题，可通过\nsort+stack 或 treemap 在 \\(O(nlgn)\\)\n的时间复杂度内解决；而如果将原来的 array 首尾相连，则只需要先在 stack\n内存入整个 array\n的元素（从后往前压入栈），因为每个元素最多只能比较一圈后回到自身。上面针对的是\nnext greater element 问题，但是 next smaller element\n求解的原理和方法也是类似的，这里不再赘述。\n","categories":["算法"],"tags":["树","动态规划","栈"]},{"title":"Linux 的 CPU 调度","url":"/2015/11/21/Linux%E7%9A%84CPU%E8%B0%83%E5%BA%A6/","content":"内核版本与 CPU 调度算法\n早期 Linux\n版本中的调度算法非常简单易懂：在每次进程切换时，内核扫描可运行进程的链表，计算进程的优先权，然后选择 “最佳” 进程来运行。这个算法的主要缺点是选择 “最佳” 进程所要消耗的时间与可运行的进程数量相关，因此，这个算法的开销太大，在运行数千个进程的高端系统中，要消耗太多的时间。\nLinux 2.6\n的调度算法就复杂多了。通过设计，该算法较好地解决了与可运行进程数量的比例关系，因为它在固定的时间内（时间复杂度 O (1)）选中要运行的进程。它也很好地处理了与处理器数量的比例关系，因为每个\nCPU\n都拥有自己的可运行进程队列。而且，新算法较好地解决了区分交互式进程和批处理进程的问题。因此，在高负载的系统中，用户感到在\nLinux2.6 中交互应用的响应速度比早期的 Linux 版本要快。\n\n调度方式\n内核 2.6 版本有 5 种调度方式，分别是\n\nSCHED_FIFO\n\nSCHED_RR\n\nSCHED_IDLE\n\nSCHED_BATCH\n\nSCHED_OTHER\n\n## 两种类型进程比较\n\n\n\n\n\n\n\n\n两种进程\n动态优先级进程 (非实时进程)\n 静态优先级进程 (实时进程)\n\n\n\n\n 优先级\n低\n高\n\n\n能否调整\n用户可调 nice 值，但最终由系统动态调整\n用户可通过 chrt 改，系统不会动态调整\n\n\n应用\n一般用户进程\n内核进程\n\n\n调度方式\n SCHED_IDLE(BATCH/OTHER)\nSCHED_FIFO(RR)\n\n\ntop 显示的 pr 值\n大于 0\nRT 或负值\n\n\n\n由此可知，top 命令显示的 pr 值越小，优先级越高\nchrt 工具\n通过 chrt（change real\ntime）工具可以改变实时进程优先级和所有进程的调度方式，其可调整的内容如下所示：\n\n用 top 显示出的 pr 值与进程实际优先级的关系\n\n","categories":["Linux"],"tags":["操作系统","Linux"]},{"title":"Linux 锁定用户与解锁","url":"/2015/12/06/Linux%E9%94%81%E5%AE%9A%E7%94%A8%E6%88%B7%E4%B8%8E%E8%A7%A3%E9%94%81/","content":"禁止个别用户登录\n禁止单个用户的登录有两种方法。下面以禁止 test 用户登录为例\n方法一\n直接命令禁止\npasswd -l test\n这条命令的意思是锁定 test 用户，这样该用户就不能登录了。\npasswd -u test\n对锁定的用户 test 进行解锁，用户可登录了。\n\n方法二\n修改 /etc/passwd 文件中用户登录的 shell\nvi /etc/passwd\ntest:x:500:500::/home/test:/bin/bash\n更改为：\ntest:x:500:500::/home/lynn:/sbin/nologin\n该用户就无法登录了。\n禁止所有用户登录\ntouch /etc/nologin\n除 root 以外的用户不能登录了！\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux 下的环境变量","url":"/2015/11/21/Linux%E4%B8%8B%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","content":"linux 环境变量种类\n按照生成周期看，可以分为二类\n\n永久的（需要修改配置文件，变量永久生效）\n\n临时的，使用 export 命令声明即可，变量在关闭 shell 时失效.\n\n\n设置变量三种方法\n（1）在 /etc/profile 文件中添加变量 (对所有用户生效，永久的) 例如添加 CLASSPATH 变量，\n# vi /etc/profile  \nJAVA_HOME=/usr/java/jdk1.7.0_17  \nJRE_HOME=/usr/java/jdk1.7.0_17/jre  \nPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin  \nCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/  \nexport JAVA_HOME JRE_HOME PATH CLASSPATH\n\n要想马上生效，需要 source /etc/profile\n（2）在用户目录下的.bash_profile 文件添加变量 (对单一用户生效，永久的)\n（3）直接运行 export , 对当前 shell 有效\n环境变量查看\n\n查看所有环境变量，命令 env\n\n查看单个 echo $CLASSPATH\n\nset 查看本地定义环境变量，unset 可以删除指定环境变量\n\n常用环境变量介绍\n\nPATH 指定 shell 在那个目录下寻找命令或程序\n\nHOME 当前用户登录名\n\nHISTORY 历史记录\n\nLOGNAME 当前用户登录名\n\nHOSTNAME 指定主机名称\n\nSHELL 当前 shell 类型\n\nLANGUGE 语言相关环境变量\n\nMAIL 当前邮件存放目录\n\nPSI 基本提示符，对 root 是# 普通用户 $\n\n设置 Linux 的环境变量，语法解释\n\n在修改了 PATH 值或任何环境变量后，都要用 export 将其输出，新的 PATH 值才能生效.\n\nPATH=\\$PATH:路径1:路径2:...:路径n\n意思是可执行文件的路径包括原先设定的路径 , 也包括从\n路径1 到 路径n\n的所有路径。当用户输入一个一串字符并按回车后，shell 会依次在这些路径里找对应的可执行文件并交给系统核心\n$PATH 表示原先设定的路径仍然有效，注意不要漏掉。\n\n与 DOS/Window 不同，UNIX 类系统环境变量中路径名用冒号分隔，不是分号. 另外，软件越装越多，环境变量越添越多，为了避免造成混乱，建议所有语句都添加在文件结尾，按软件的安装顺序添加 , 格式如下 ()：\n# 软件名-版本号-安装日期\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux 挂载 NTFS 文件系统","url":"/2015/11/30/Linux%E6%8C%82%E8%BD%BDNTFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","content":"最近将服务器内的数据迁移到移动硬盘上做备份时，发现 Centos\n6.5 识别不了 NTFS 文件系统的移动硬盘，google 了一下才发现原因是 Linux 内核不支持 NTFS。重新编译内核是一种方法，但是也可以采用安装一个软件来解决，本文就是讲述如何安装这个软件以及在 Linux 挂载 NTFS 文件系统的移动硬盘。\n\n这个软件就是 NTFS-3G。NTFS-3g 是一个开源软件，它支持在 Linux 下面读写 NTFS 格式的分区。更多信息可参考 NTFS-3G 官网：http://www.ntfs-3g.org\n安装\n安装方式有两种：\n### yum 源安装\n　　如果配置的 yum 源有 ntfs-3g 这个包，那么可以通过 yum install ntfs-3g 来直接安装，如果配置的 yum 源没有这个包，可以参照下一种安装方式\n### 编译安装\n下载地址为：http://www.tuxera.com/community/open-source-ntfs-3g/, 也可通过 wget 下载，安装过程如下\n# wget https://tuxera.com/opensource/ntfs-3g_ntfsprogs-2015.3.14.tgz  # tar -zxvf ntfs-3g_ntfsprogs-2015.3.14.tgz  # cd ntfs-3g_ntfsprogs-2015.3.14  # ./configure  # make  # make install  \n使用\n###　获取 NFTS 设备名称\n#fdisk -l | grep -i ntfs  \n /dev/sdb1    1     10443  83883366   7  HPFS/NTFS  \n可知设备名为 /dev/sdb1\n建立挂载点并挂载\n# mkdir /mnt/ntfs  \n# mount -t ntfs-3g /dev/sda1 /mnt/ntfs  \n这样访问 /mnt/ntfs 目录便可往硬盘进行读写了。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Long Tail Problem In Item Recommendation","url":"/2022/04/04/Long%20Tail%20Problem%20In%20Item%20Recommendation/","content":"长尾问题在推荐 / 广告系统是一个较为常见的问题 (这里主要针对 item\n的长尾)，原因可能比较多，笔者理解的主要原因是由于系统存在 feedback\nloop (即训练数据由模型产生，同时又会被模型用于训练)\n的特性，在没有外部干预的情况下，马太效应会天然导致头部效应的现象比较严重，少部分的\nitem 主导了整个系统。\n比如说推荐系统中，很多视频 / 文章并没有展示机会，在训练集中压根没出现过，高热的视频 / 文章在不同的用户中排序都比较靠前，进而得到多次被推荐的机会；在广告系统中，部分计划的消耗会特别高，而一些计划压根投不出去；这导致了用户或者广告主体验不佳，而这种现象往往也会被归为生态问题。\n既然没有干预时，系统天然的特性导致了头部效应 (或者说二八效应) 比较严重，那强行干预系统的分布能不能改变这个问题？答案是可以的，而且目前绝大部分的方法都是在做这一类事情，常用手段往往有\n2 种\n（1）策略层面，根据系统和业务特性设计规则，比如说对长尾的 item\n有特定的扶持，强行让这些 item 能触达到更多的用户\n（2）模型层面，核心思想就是让模型能更好地学习到 long tail item\n的 representation，因为这个问题的根本原因就是 long tail item\n的样本过少，进而导致模型学习的不好；而具体的手段比较多，这部分会在后面详细介绍。\n这篇文章主要介绍的几篇\npaper 都是模型层面的，因为策略层面的往往需要根据实际业务需求来拍一些规则，模型层面的一些方法更为通用。\n\nDual Transfer Learning\nFramework\n这个方法来自 google 的一篇 paper, A Model of Two Tales: Dual\nTransfer Learning Framework for Improved Long-tail Item\nRecommendation\npaper 名字中提到的 dual transfer learning，分别是 model-level 和\nitem-level 的 transfer learning，简单来说，前者让样本少的模型 (few-shot\nmodel) 的参数尽可能往样本多的模型 (many-shot\nmodel) 的参数靠拢 (这里根据样本量分为 2 个模型来建模)，后者则是让 long\ntail item 的 representation 与 head item 的尽可能接近，这里的\nrepresentation 其实就是上面提到的 few-shot model 和 many-shot model\n吐出来的 embedding，因此 paper 提出的总体框架如下如所示\n\n\nMIRec\n\n上图中的一些符号含义如下\n\n\n从上图中可以看到，根据样本量分为了 many-shot model 和 few-shot model\n两个模型来分别建模，核心是 meta-level knowledge transfer 和 curriculum\ntransfer，分别对应前面的 model-level 和 item-level, 下面分别介绍\nmodel-level\n其思想比较简单，就是要学习一个 meta learner \\(\\mathcal{F}\\), 其输入和输出都是 few-shot\nmodel 的\nparameter，监督信号是输出的 parameter 要与 many-shot model 的 parameter\n接近，最终作用方式是在 loss 上\nbase learner 的 loss 是常规的 softmax loss，\\(r(u,i)\\) 取值为 1/0 表示是否有 feedback (如\nclick 等)\n\n\nmeta learner \\(\\mathcal{F}\\)\n则是采用了 mse 的 loss，并且作为一个 regularization 项加到 few-shot\nmodel 原始的 loss 上，则最终 few-shot model 的 loss 如下公式 (5) 所示\n\n\nmeta learner \\(\\mathcal{F}\\)\n具体的结构有很多种，这里采用的是简单的 fully connected layer\nitem-level\n这部分主要通过 curriculum\nlearning 来训练模型，curriculum learning\n的基本思想是在训练时组织好样本进模型的顺序，前面的综述的链接里的话是这么说的\n\nCurriculum learning (CL) is a training strategy that trains a machine\nlearning model from easier data to harder data, which imitates the\nmeaningful learning order in human curricula. As an easy-to-use plug-in,\nthe CL strategy has demonstrated its power in improving the\ngeneralization capacity and convergence rate of various models in a wide\nrange of scenarios such as computer vision and natural language\nprocessing etc.\n\n回到 paper 里，这部分主要做的则是构造好上面提到的两个 training\ndataset：\\(\\Omega^{*}\\) 和 \\(\\Omega(k)\\)\n\n\n\\(\\Omega(k)\\)\n的构造方法如下，这部分样本包含 2 部分 item，一部分是 \\(I_{h}(k)\\) 中那些刚好有 k 个 sample 的\nitem，另一部分则是 \\(I_{t}(k)\\)\n中的全部 sample\n\n\npaper 里称这么做主要有以下 2 个原因，但是笔者觉得核心还是把 long tail\nitem 的样本单独出来，不至于被 head item dominate\n\n\ntail items are fully trained in both the many-shot model and\nfew-shot model to ensure the high quality of the learned item\nrepresentations in both many-shot and few-shot models\n(2）In the few-shot model training, the distribution of tail items\nrelatively keeps the same as the original distribution. It can alleviate\nthe bias among tail items that brings by the new distribution\n\n\ntraining &amp; serving\n因此，总体的 training 流程如下图所示，\n可以分为两个阶段，阶段一是通过常规的方法训练 many-shot\nmodel，阶段二则是通过 many-shot model 的 parameter 和 meta learner\n来训练 few-shot model\n\n\n而最终模型 serving 时，则是会对两个 model 输出的 score\n做一个常规的加权\n\n\nexperiment\npaper 里的实验用了 2 个公开的数据集，MovieLens1M 和\nBookcrossing，采用的评估指标是 Hit Ratio at top K (HR@K) 和 NDCG at top K\n(NDCG@K) ，这里的 HR@K 其实就是召回率\n实验预期的目标是在总体效果不变差的前提下，提升 long tail item\n的表现；因此上面的两个评估指标 HR@K 和 NDCG@K 分别在 all item、head item\n和 tail item 上进行了评估\npaper 里的实验着重回答了以下四个问题\n\nRQ1: How well does the dual transfer learning framework MIRec perform\ncompared to the state-of-the-art methods?\nRQ2: How do different components (meta-learning and curriculum learning)\nof MIRec perform individually? Could they complement each other?\nRQ3: How does our proposed curriculum learning strategy compare with the\nalternatives?\nRQ4: Besides downstream task performance, are we actually learning\nbetter representations for tail items? Could we see the differences\nvisually?\n\n第一个问题是跟其他的 sota 方法比较，结论是 paper\n提出的最好，具体数据这里就不贴了\n第二个问题是做了一个消融实验，结论是单独的 meta-learning 或\ncurriculum learning 都是正向的，且加起来效果最好\n第三个问题是对比不同的 curriculum learning strategy 对 training 和\nvalidation 时的 loss 的值的影响；主要对比了 head2tail 和 tail2head\n两个策略，前者先用 head item 训练，再用 tail item\n训练，后者则刚好相反，这部分效果如下图所示，也可以归纳出以下 3\n点结论\n\n\nCompared to the tail item loss in different curriculums (column 3),\nour proposed curriculum can bring a two-stage decent for both the\ntraining and validation loss\n\nWhen the model is trained based on only head/tail items, the\nvalidation performance for the other part of items decreases.\nThe different changes of head and tail loss indicate the large\nvariations between head and tail items\n\nIt is easily to get validation loss increases if the model is\ntrained purely based on head/tail items, as shown in first column of the\nfirst two rows\n\n\n\n\nCL compare\n\n第四个问题是对学习出来的 embedding 进行可视化，主要想说明通过 paper\n的方法学到的方法在可视化上后区分性也比较强，不过只做了两个 case\n分析，数据有限，这里就不贴出来了\nSelf-supervised Learning\nFramework\n这个方法来自 google 的另一篇 paper Self-supervised Learning for\nLarge-scale Item Recommendations, 也是在解决 item\n的长尾问题，提升的点也主要在 representation learning 上，如 paper\n描述所示\n\nThe framework is designed to tackle the label sparsity problem by\nlearning better latent relationship of item features. Specifically, SSL\nimproves item representation learning as well as serving as additional\nregularization to improve generalization. Furthermore, we propose a\nnovel data augmentation method that utilizes feature correlations within\nthe proposed framework.\n\nSSL Framework\npaper 提出的总体的 framework\n如下图所示，基本符号的含义都在图片下方的注释里，\n\n\n而上图中的 \\(x_i\\)、\\(x_j\\) 表示训练时一个 batch\n中的两条样本 (推荐中的样本往往有三种含义，query/item/query-item\npair，paper 里特指的是\nitem)，这里的核心思想是同一条样本经过不同的变换后的\nrepresentation 应该还是相似的，不同的样本则相反\n因此，通过 \\(h, g, \\mathcal{H},\n\\mathcal{G}\\) 得到的 representation 中，\\((z_i, z_i^{'})\\) 是正例，而 \\((z_i, z_j^{'})\\)\n是负例，因此，对于一个包含 \\(N\\)\n条样本的 batch 中，第 \\(i\\) 条样本的\nself supervised 的 loss 形式如下\n\\[\\begin{align} \\mathcal{L}_{self}(x_i) =\n-\\log \\frac{\\exp(s(z_i, z_i^{'})/\\tau)}{\\sum_{j=1}^{N}\\exp(s(z_i,\nz_j^{'})/\\tau)} \\end{align}\\]\n上面公式中的 \\(\\tau\\)\n为一个超参 (softmax temperature),\\(s(z_i,\nz_j^{'})\\) 为两个 embedding 的 cosin 相似性，即 \\(s(z_i, z_j^{'})=&lt; z_i,\nz_j^{'}&gt;/(||z_i|| \\cdot ||z_j^{'}||)\\)\n总体的 batch 内的 self supervised loss 为\n\\[\\begin{align} \\mathcal{L}_{self}(\\lbrace\nx_i \\rbrace; \\mathcal{H}, \\mathcal{G}) = - \\frac{1}{N} \\sum_{i=1}^{N}\n\\log \\frac{\\exp(s(z_i, z_i^{'})/\\tau)}{\\sum_{j=1}^{N}\\exp(s(z_i,\nz_j^{'})/\\tau)} \\end{align}\\]\n得到 embedding 的两个 emcoder： \\(\\mathcal{H}\\) 和 \\(\\mathcal{G}\\)，在 paper\n中是共享参数的 (这部分在后面讲模型结构时也会提及)\n而如果两种 data augmentation 方法 \\(h\\) 和 \\(g\\) 也是相同的话，上面的 \\(\\mathcal{L}_{self}(\\lbrace x_i \\rbrace;\n\\mathcal{H}, \\mathcal{G})\\)\n会退化成如下形式，此时损失函数的目标是不同样本的相似性 \\(s(z_i, z_j^{'})\\) 尽可能小\n\\[\\begin{align} -\\frac{1}{N}\n\\sum_{i=1}^{N} \\log \\frac{\\exp(1/\\tau)}{ \\exp(1/\\tau) + \\sum_{j \\ne\ni}\\exp(s(z_i, z_j^{'})/\\tau)} \\end{align}\\]\ntwo-stage data augmentation\n这里主要讲上面框架提到的两种 data augmentation 方法： \\(h\\) 和 \\(g\\) ，paper 称这个方法的关键在于：A\ngood transformation and data augmentation should make minimal amount of\nassumptions on the data such that it can be generally applicable to a\nlarge variety of tasks and models.\npaper 里采用的方法则是 mask, 这里借鉴了 bert 的思想，但是这里没有\nsequence 的概念，因此这里 mask 掉的是 item feature，实际使用的是一个\ntwo-stage 的方法，即 masking + dropout, 两个方法主要操作如下\n\nMasking. Apply a masking pattern on the set of item\nfeatures. We use a default embedding in the input layer\nto represent the features that are masked.\nDropout.For categorical features with multiple\nvalues,we drop out each value with a certain probability. It\nfurther reduces input information and increase the hardness of SSL\ntask.\n\n至于具体的 mask 方法，paper 里没有采用随机\nmask，而是基于特征之间的互信息 (mutual\ninformation) 来进行 mask，核心思想是 mask\n掉相关性较强的特征，paper 这样做的原因是\nthe SSL contrastive learning task may exploit the shortcut of highly correlated features between the two augmented examples, making the SSL task too easy\n具体的方法的方法是每次每次随机选择一个 feature \\(f_{seed}\\)，通过预先计算好的互信息选择与这个\nfeature 最相关的 \\(k/2\\) 个\nfeature，最终 mask 掉的是这 \\(k/2 + 1\\)\n个 feature\ntraining &amp; serving\n前面的 self supervised loss（ \\(\\mathcal{L}_{self}(\\lbrace x_i\n\\rbrace)\\)）只是一个辅助的 loss,\n而主任务的 loss 是计算 query-item 的 loss，paper 这里采用的是 batch\nsoftmax loss，形式跟 self supervised loss 其实是一样的，只是计算相似性从\nitem-item 变为了 query-item，具体形式如下\n\\[\\begin{align} \\mathcal{L}_{main} =\n-\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(s(q_i,\nx_i)/\\tau)}{\\sum_{j=1}^{N}\\exp(s(q_i, x_j)/\\tau)}\n\\end{align}\\]\n则总体的 loss 为\n\\[\\begin{align} \\mathcal{L} =\n\\mathcal{L}_{main} + \\alpha \\mathcal{L}_{self} \\end{align}\\]\n则最终的模型如下所示，注意这里的 3 个 item twoer 的参数是共享的\n\n\ntrain\n\n至于 serving，由于只是加了一个 auxiliary loss，所以按正常预估即可\nexperiment\n实验主要回答了如下 4 个问题\n\nRQ1: Does the proposed SSL Framework improve deep models for\nrecommendations?\nRQ2:SSL is designed to improve primary supervised task through\nintroduced SSL task on unlabeled examples. What is the impact of the\namount of training data on the improvement from SSL?\nRQ3: How do the SSL parameters, i.e., loss multiplier \\(\\alpha\\) and dropout rate in data\naugmentation, affect model quality?\nRQ4: How does RFM perform compared to CFM? What is the benefit of\nleveraging feature correlations in data augmentation?\n\nRQ1，通过与其他 3 个 baseline 进行了比较，采用了 2\n个公开数据，评估指标是 MAP@10/50 和 Recall@10/50, 跟前一篇 paper\n一样，做了总体 item、head item 和 tail item 各自的评估\nRQ2 主要回答数据量对 SSL\n的影响，结论是数据量越多，效果越好，感觉这个结论比较符合直觉，不知道\npaper 为什么要单独拎出来说\nRQ3 主要回答 \\(\\alpha\\)\n大小对效果的影响，paper 里对比 spread-out regularization loss 和 paper\n提出的 self supervised loss，结论是取相同的值时均是 self supervised loss\n效果更好，关于 spread-out\nregularization loss 可参考这篇 paper，也是一种 constrasive\nloss，但是没有 data augmentation\nRQ4 主要回答随机 mask 的方法 (RFM) 和基于相关性 mask\n的方法 (CFM) 哪种效果更好，结论是 CFM 在四项指标上效果均优于 RFM\n此外，paper 还补充了在线 ab 实验，结论也是比较显著的\n小结\n本文主要介绍了从模型层面缓解长尾问题的两篇 paper，两篇 paper\n的核心思想都是要更好地学习到长尾 item 的 representation\n第一篇提出了一个 dual transfer framework (model-level + item-level),\n通过 2 个模型分别建模 head item 和 tail item，在 model-level 令 tail\nitem 的模型参数往 head item 的模型学习，而在 item-level 通过 curriculum\nlearning 组织样本进模型的顺序，serving\n阶段需要用两个模型的预估分融合\n第二篇 paper 则提出了 self-supervised learning framework, 通过\nin-batch 的 data augmentation 方法 (mask + dropout)，增加一个 auxiliary\nloss，其目标是同一个 item 经过变换后的 embedding 应该是相似的，不同 item\n的则相反；离线和在线的实验都验证了有效性。\n笔者感觉实际业界中，第一种方法的成本要高于第二种，而第一种方法没有做在线的\nab 实验不知道具体在线效果，且第二种方法这种通过 in-batch\n样本来构造新的样本对的 self-supervised learning，普适性会更好\n除了推荐领域的，CV 领域也有一篇关于长尾问题的综述，Deep Long-Tailed Learning: A\nSurvey，总体的方法论会更全，思路也涵盖了上面说的两篇 paper，但是 cv\n跟 recommendation 还是有不少差异的 (比如 recommendation 的 item 数往往比\ncv 的要多得多)，具体在业务的应用也不确定，这里就不详细展开了。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"Makefile 简介","url":"/2018/12/05/Makefile%20%E7%AE%80%E4%BB%8B/","content":"C/C++ 在 linux 下可通过 gcc\n进行编译，当文件数量少，文件依赖关系简单时可通过命令进行编译，但是当文件数量庞大且关系复杂时，就要依赖于\nmake 和 Makefile 管理这些复杂关系了。MakeFile 类似于 shell\n脚本，定义了文件的依赖关系，以及编译的先后顺序。本文主要介绍 Makefile\n的基本语法，本系列文章主要参考了 跟我一起写 Makefile。\n\n基本格式\nMakefile 书写格式一般如下\n# 方式一target : prerequisites \tcommand# 方式二target : prerequisites; command\n\ntarget 可以是一个 object\nfile (目标文件)，也可以是一个执行文件，还可以是一个标签（label）\nprerequisites 是要生成那个 target 所需要的文件或是目标。\ncommand 也就是 make 需要执行的命令，如果其不与 targets :\nprerequisites 在一行，那么，必须以 Tab键开头，如果和\nprerequisites 在一行，那么可以用分号做为分隔。\n\n这是一个文件的依赖关系，也就是说，target 这一个或多个的目标文件依赖于\nprerequisites 中的文件，其生成规则定义在 command\n中。说白一点就是说，prerequisites 中如果有一个以上的文件比\ntarget 文件要新的话，command 所定义的命令就会被执行。这就是 Makefile\n的规则。也就是 Makefile 中最核心的内容。\n示例\n如果一个工程有 3 个头文件，和 8 个 c 文件，我们为了完成前面所述的那三个规则，我们的\nMakefile 应该是下面这个样子的。\n# 如果后面这些.o文件比edit可执行文件新,那么才会去执行命令edit : main.o kbd.o command.o display.o \\\t\tinsert.o search.o files.o utils.o       \tcc -o edit main.o kbd.o command.o display.o \\\t\tinsert.o search.o files.o utils.omain.o : main.c defs.h\tcc -c main.ckbd.o : kbd.c defs.h command.h\tcc -c kbd.ccommand.o : command.c defs.h command.h\tcc -c command.cdisplay.o : display.c defs.h buffer.h\tcc -c display.cinsert.o : insert.c defs.h buffer.h\tcc -c insert.csearch.o : search.c defs.h buffer.h\tcc -c search.cfiles.o : files.c defs.h buffer.h command.h\tcc -c files.cutils.o : utils.c defs.h\tcc -c utils.cclean :\trm edit main.o kbd.o command.o display.o \\\t\tinsert.o search.o files.o utils.o\n我们可以把这个内容保存在名字为 “makefile” 或 “Makefile”\n的文件中，然后在该目录下直接输入命令 “make”\n就可以生成执行文件 edit。如果要删除执行文件和所有的中间目标文件，那么，只要简单地执行一下\nmake clean 就可以了。\n在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个 tab 键作为开头。记住，make\n并不管命令是怎么工作的，他只管执行所定义的命令。make 会比较 targets\n文件和 prerequisites 文件的修改日期，如果 prerequisites\n文件的日期要比 targets\n文件的日期要新，或者 target 不存在的话，那么，make 就会执行后续定义的命令。\n输入 make 命令后，发生了如下的动作\n\nmake 会在当前目录下找名字叫 “Makefile” 或 “makefile”\n的文件。如果找到，它会找文件中的第一个目标文件（target）作为最终的目标文件，在上面的例子中即为\nedit 这个文件\n如果 edit 文件不存在，或是 edit 所依赖的后面的 .o\n文件的文件修改时间要比 edit\n这个文件新，那么，他就会执行后面所定义的命令来生成 edit 这个文件。\n如果 edit 所依赖的 .o 文件也不存在，那么 make\n会在当前文件中找目标为 .o\n文件的依赖性，如果找到则再根据那一个规则生成 .o\n文件。（这有点像一个堆栈的过程）\n\n这就是整个 make 的依赖性，make\n会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件 ,\n在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么 make\n就会直接退出并报错。\n而像 clean\n这种没有被第一个目标文件直接或间接关联，那么它后面所定义的命令将不会被自动执行，但可以通过\nmake clean 进行显式执行，以此来清除所有的目标文件，以便重编译。\n如果这个工程已被编译过，当我们修改了其中一个源文件时，比如\nfile.c，那么根据我们的依赖性，我们的目标\nfile.o 会被重编译，因此 file.o\n文件修改时间要比 edit 要新，所以 edit 也会被重新链接了\n使用变量与自动推导\n上面同一个 .o\n文件在多个地方都重复了，因此可通过变量的方式简化 Makefile，Makefile\n的变量也就是一个字符串，类似于 C 语言的宏，通过 $(变量名)\n来访问变量内容\nobjects = main.o kbd.o command.o display.o \\\t\tinsert.o search.o files.o utils.oedit : $(objects)\tcc -o edit $(objects)main.o : main.c defs.h\tcc -c main.ckbd.o : kbd.c defs.h command.h\tcc -c kbd.ccommand.o : command.c defs.h command.h\tcc -c command.cdisplay.o : display.c defs.h buffer.h\tcc -c display.cinsert.o : insert.c defs.h buffer.h\tcc -c insert.csearch.o : search.c defs.h buffer.h\tcc -c search.cfiles.o : files.c defs.h buffer.h command.h\tcc -c files.cutils.o : utils.c defs.h\tcc -c utils.cclean :\trm edit $(objects)\nmake 能够自动推导文件以及文件依赖关系后面的命令，即只要 make 看到一个\n.o 文件，它就会自动的把 .c\n文件加在依赖关系中，如果 make 找到一个 whatever.o，那么\nwhatever.c，就会是 whatever.o 的依赖文件。并且\ncc -c whatever.c 也会被推导出来，于是，我们的 Makefile\n再也不用写得这么复杂。\nobjects = main.o kbd.o command.o display.o \\\t\tinsert.o search.o files.o utils.o cc = gccedit : $(objects)\tcc -o edit $(objects)main.o : defs.hkbd.o : defs.h command.hcommand.o : defs.h command.hdisplay.o : defs.h buffer.hinsert.o : defs.h buffer.hsearch.o : defs.h buffer.hfiles.o : defs.h buffer.h command.hutils.o : defs.h.PHONY : cleanclean :\t-rm edit $(objects)\n.PHONY 意思表示 clean 是一个伪目标。而在\nrm\n命令前面加了一个小减号的意思是当某些文件出现问题时不要管，继续做后面的事。当然，clean\n的规则不要放在文件的开头，这就会变成 make\n的默认目标。不成文的规矩是 clean 从来都是放在文件的最后\n引用其他 MakeFile\n在 Makefile 使用 include 关键字可以把别的 Makefile 包含进来，这很像\nC/C++ 语言的\n#include，被包含的文件会原模原样的放在当前文件的包含位置。include 的语法是：\ninclude &lt;filename&gt;;\nfilename 可以是当前操作系统 Shell 的文件模式（可以包含路径和通配符）\n比如有这样几个 Makefile：a.mk、b.mk、c.mk，还有一个文件叫\nfoo.make，以及一个变量 $(bar)，其包含了\ne.mk 和 f.mk，那么，下面两个语句等价：\ninclude foo.make *.mk $(bar)include foo.make a.mk b.mk c.mk e.mk f.mk\nmake 命令开始时，会找寻 include 所指出的其它\nMakefile，并把其内容安置在当前的位置。类似 C/C++ 的 #include\n指令一样。如果文件都没有指定绝对路径或是相对路径的话，make 会在当前目录下首先寻找，如果当前目录下没有找到，那么，make 还会在下面的几个目录下找：\n\n如果 make 执行时，有 -I 或 --include-dir\n参数，那么 make 就会在这个参数所指定的目录下去寻找。\n如果目录\n&lt;prefix&gt;/include（一般是：/usr/local/bin 或 /usr/include）存在的话，make 也会去找。\n如果有文件没有找到的话，make 会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成 makefile 的读取，\nmake 会再重试这些没有找到，或是不能读取的文件，如果还是不行，make 才会出现一条致命信息。\n\n如果想让 make 忽略那些无法读取的文件，可以在 include 前加一个减号\n-, 即\n-include &lt;filename&gt;;\n表示无论 include\n过程中出现什么错误，都不要报错继续执行。和其它版本 make 兼容的相关命令是 sinclude，其作用和这一个是一样的。\n小结\nMakefile 里主要包含了五个东西：显式规则、隐晦规则、变量定义、文件指示和注释。\n\n显式规则：说明了如何生成一个或多个目标文件。这是由 Makefile\n的书写者明显指出，要生成的文件，文件的依赖文件，生成的命令。\n隐晦规则: make\n有自动推导的功能，所以隐晦的规则可以让我们比较简略地书写\nMakefile，这是由 make 所支持的。\n变量的定义。在 Makefile\n中我们要定义一系列的变量，变量一般都是字符串，这个有点 C 语言中的宏，当\nMakefile 被执行时，其中的变量都会被扩展到相应的引用位置上。\n文件指示。其包括了三个部分，一个是在一个 Makefile 中引用另一个\nMakefile，就像 C 语言中的 #include\n一样；另一个是指根据某些情况指定 Makefile\n中的有效部分，就像 C 语言中的预编译 #if 一样；\n还有就是定义一个多行的命令。有关这一部分的内容，我会在后续的部分中讲述。\n注释。Makefile\n中只有行注释，和 UNIX 的 Shell 脚本一样，其注释是用 #\n字符，这个就像 C/C++ 中的 // 一样。\n\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"Machine Learning with Spark 简介","url":"/2017/11/05/MachineLearning%20with%20Spark%20%E7%AE%80%E4%BB%8B/","content":"本文主要介绍 《Machine\nLearning with\nSpark》这本书各章节的主要内容，以及提供该书各章节对应的 python\n代码。\n\n这本书主要介绍了如何通过 spark\n处理大规模的数据，以及利用这些处理过的数据通过 MLlib\n进行模型的训练。全书共分为 10 章，涵盖了数据的预处理，推荐模型，分类模型，回归模型，聚类模型，数据降维，文本处理以及\nSpark 流式处理等内容。\n书中的代码大部分是 scala， 某些章节是 python，这里全部通过 python\n重写，全部代码参见 github，为了便于交互，采用了\nJupyter Notebook 的形式，且通过 HDFS\n存储数据文件，关于环境的搭建可参考这篇文章。\n下面简单介绍各个章节的内容（从第三章开始），更多细节可参考具体代码\nChapter\n3：Obtaining, Processing, and Preparing Data with Spark\n这章主要介绍了如何通过 spark 提供的 API\n从原始数据中提取特征，如一些常用的函数 map,\nfilter, reduceByKey 等，以及如何通过 spark\n进行特征的归一化和处理以下几种 feature\n\ncategory feature：one-hot 编码\n text feature：抽取文本 -&gt; 分词 -&gt;\n创建字典，为每个单词分配一个唯一的 id -&gt; 将文本转为向量\n derived\nfeature：这个主要是指从非结构化的特征中抽取出结构化的特征，如给出一个日期，可以提取出其中的小时部分，然后进行分段，分为\nmorning，noon，afternoon，evening 四个阶段，然后再进行 one-hot 编码\n\nChapter\n4：Building a Recommendataion Engine with Spark\n这一章主要介绍如何通过 spark 构建一个推荐系统，采用了电影评分数据集\nMovieLens\n100k，使用的是经典的协同过滤技术，而 spark 的 MLlib 则提供了\nalternating least squares (ALS)\n这种基于矩阵分解的方法用于求解协同过滤问题，通过\nALS 对用户 - 物品评分矩阵进行分解，能够为每个用户或物品推荐 top k\n个最相似的用户或物品。度量 ALS 算法效果的指标为 MSE，RMSE 等。\nChapter\n5：Building a Classifcation Model with Spark\n这一章主要介绍分类模型，采用了 Kaggle\n上的一个数据集，通过 MLlib 提供的\nLogisticsRegression，SVM，NaiveBayes，DecisionTree\n等分类器对其进行分类，并且比较了进行 feature standardization\n前后的效果。\nChapter\n6：Building a Regression Model with Spark\n这一章主要介绍了回归模型，采用了 bike\nsharing 数据集，主要通过 MLlib 提供的回归模型 Linear Regression 和\nDecision\nTree 进行了预测，并且对目标变量进行了 log 变换，目的是让目标变量更加接近\n正态分布，因为像 Linear Regression\n这一类模型对目标函数值的分布做了正态分布的假设。\nChapter\n7：Building a Clustering Model with Spark\n这一章主要介绍了聚类模型，采用的是前面提到的 MovieLens 100k\n数据集，通过 ALS\n进行矩阵分解，为每个用户 (物品) 提取出一个隐含属性向量作为用户 (物品) 的特征向量。然后通过\nK-Means 进行聚类。\n聚类结果的评估有两种方法：一种是 Internal\nevaluation，也就是只用数据本身进行评估，一般通过 WCSS\n(within-cluster sums of squares) 评估；第二种则是 External\nevaluation，即还通过数据的标签进行评估，评估的方法就是常见的准确率等指标。由于聚类往往是无监督方法，因此数据往往是不带标签的，\n因此第一种评估方法比较常见。\nChapter\n8：Dimensionality Reduction with Spark\n这一章主要介绍了 spark 中的降维技术，采用了 LFW（Labeled\nFaces in the Wild）\n人脸数据集，因此也介绍了图像处理的一些基本操作 (基于 python 的 opencv 库)，然后通过两种方法：PCA 和 SVD，进行了数据的降维，实际上这两种方法的关系非常密切，且可以达到相同的效果。同时介绍了\nEigenface\n的概念，Eigenface 实际上就是 PCA 提取出来的特征向量再变为人脸图像。\nChapter\n9：Advanced Text Processing with Spark\n这一章主要介绍了 Spark 中的文本处理技术，采用了 20 Newsgroups\n数据集，介绍了文本处理的基本操作\n\n分词\n过滤停止词，低频词\n构建词典\n基于词典为每篇文本构造一个向量\n\n在向量的基础上加上 TF-IDF 便可算出表示文本的 TF-IDF 向量，基于这个\nTF-IDF 向量可做文本相似性的比较，而如果文本本身是有标签的，可以将 TF-IDF\n向量作为文本特征，进而训练一个分类模型。\n最后这章还简单介绍了 Word2Vec 模型及其简单应用。\nChapter\n10：Real-time Machine Learning with Spark Streaming\n这章主要介绍了用于处理实时流的 Spark Streaming, 模拟了产生实时流的\nproducer ，通过 Spark Streaming 处理这些信息流后训练了一个 streaming\nregression 模型，并通过 online 的方式评估了这个模型的效果。\n","categories":["机器学习"],"tags":["机器学习","分布式","spark"]},{"title":"Makefile 语法详解 (1)- 文件搜索、伪目标与命令执行","url":"/2018/12/06/Makefile%20%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3(1)-%E6%96%87%E4%BB%B6%E6%90%9C%E7%B4%A2%E3%80%81%E4%BC%AA%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/","content":"本文内容是之前的文章 Makefile\n简介 的补充，详细介绍了 Makefile 中的文件搜索（即通过 VPATH 和 vpath\n进行源文件的搜索）、伪目标（定义多个生成目标）以及执行多条命令的一些做法。\n\n文件搜索\n在一些源文件较多的大工程中，通常会把源文件分类并存放在不同的目录中 (比如自定义的头文件放在\ninclude 目录，源文件放在 src 目录)，而当 make\n需要去找寻文件的依赖关系时，可以在文件前加上路径，但最好的方法是把一个路径告诉\nmake，让 make 自动去搜索。\nMakefile 文件中的特殊变量 VPATH\n就是完成这个功能的，如果没有指明这个变量，make 只会在当前的目录中去找寻依赖文件和目标文件。如果定义了这个变量，make 就会在当前目录找不到的情况下，到所指定的目录中去找寻文件。\nVPATH = src:../headers\n上面的的定义指定两个目录，src 和\n../headers，make 会按照这个顺序进行搜索。目录由\n: 分隔；然，当然，在此之前会在当前目录查找\n另一个设置文件搜索路径的方法是使用 make 的 vpath\n关键字（全小写），这不是变量，这是一个 make\n的关键字，这和上面提到的那个 VPATH\n变量很类似，但是它更为灵活。它可以指定不同的文件在不同的搜索目录中。这是一个很灵活的功能。它的使用方法有三种：\n1、vpath &lt;pattern&gt; &lt;directories&gt; 在目录\n&lt;directories&gt; 中搜索符合模式\n&lt;pattern&gt; 的文件\n2、vpath &lt;pattern&gt; 清除符合模式\n&lt;pattern&gt; 的文件的搜索路径\n3、vpath 清除所有已被设置好了的文件搜索目录。\nvpath 使用方法中的 &lt;pattern&gt; 需要包含\n% 字符。%\n的意思是匹配零或若干字符，如，%.h 表示所有以\n.h 结尾的文件。&lt;pattern&gt;\n指定了要搜索的文件集，而 &lt;directories&gt; 则指定了\n&lt;pattern&gt; 的文件集的搜索的目录。例如：\nvpath %.h ../headers 表示要 make 在\n../headers 目录下搜索所有以 .h\n结尾的文件。（如果某文件在当前目录没有找到的话）\n我们可以连续地使用 vpath\n语句，以指定不同搜索策略。如果连续的 vpath\n语句中出现了相同的 &lt;pattern&gt;，或是被重复了的\n&lt;pattern&gt;，那么，make 会按照 vpath\n语句的先后顺序来执行搜索。如：\nvpath %.c foovpath %.c blishvpath %.c bar\n其表示 “.c” 结尾的文件，先在 foo 目录，然后是\nblish ，最后是 bar 目录。\nvpath %.c foo:barvpath %.c blish\n而上面的语句则表示 .c 结尾的文件，先在 foo\n目录，然后是 bar 目录，最后才是 blis\n目录。\n伪目标\n最早先的一个例子中，我们提到过一个 clean\n的目标，这是一个 “伪目标”，因为并不生成 “clean” 这个文件\nclean:\trm *.o temp\n为了避免伪目标名称和文件重名的这种情况，可以使用一个特殊的标记\n.PHONY 来显式地指明一个目标是伪目标，如下所示\n.PHONY : clean\n只要有这个声明，不管是否有 clean 文件，要运行 clean\n这个目标，只能运行 make clean。于是整个过程可以这样写：\n.PHONY : cleanclean :\trm *.o temp\n伪目标一般没有依赖的文件，但是也可以为伪目标指定所依赖的文件。伪目标同样可以作为 “默认目标”，只要将其放在第一个。一个常用的做法就是，如果你的\nMakefile 需要一次生成若干个可执行文件，可以通过伪目标实现，如下所示\nall : prog1 prog2 prog3.PHONY : allprog1 : prog1.o utils.o\tcc -o prog1 prog1.o utils.oprog2 : prog2.o\tcc -o prog2 prog2.oprog3 : prog3.o sort.o utils.o\tcc -o prog3 prog3.o sort.o utils.o\n由于 Makefile 中的第一个目标会被作为其默认目标，上面声明的伪目标\nall 会作为默认目标，但由于 all\n又是一个伪目标，所以不会有 all 文件产生，\n但是会生成其依赖的三个文件\n从上面的例子我们可以看出，目标也可以成为依赖。所以，伪目标同样也可成为依赖。看下面的例子：\n.PHONY : cleanall cleanobj cleandiffcleanall : cleanobj cleandiff\trm programcleanobj :\trm *.ocleandiff :\trm *.diff\nmake cleanall\n将清除所有要被清除的文件。cleanobj 和\ncleandiff 这两个伪目标有点像 “子程序” 的意思。我们可以输入\nmake cleanall 和 make cleanobj 和\nmake cleandiff 命令来达到清除不同种类文件的目的。\n命令执行\n执行连续命令\n执行多条命令时可以分多行写；但是如果要让上一条命令的结果应用在下一条命令时，应该使用分号或\n&amp;&amp;\n分隔这两条命令。比如第一条命令是 cd 命令，并且希望第二条命令在 cd\n之后的基础上运行，那么就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如：\n# 示例一：exec:\tcd /usr/lib/\tpwd# 示例二：exec:\tcd /usr/lib/; pwd 或 cd /usr/lib/ &amp;&amp; pwd\n当我们执行 make exec 时，第一个例子中的 cd\n没有起到作用，pwd 会打印出当前的 Makefile\n目录，而第二个例子中，cd 就起作用了，pwd 会打印出\n/usr/lib/\n嵌套执行 make\n在一些大的工程中，往往会把不同模块或是不同功能的源文件放在不同的目录中，可以在每个目录中都书写一个该目录的\nMakefile，这有利于 Makefile\n变得更加地简洁且更容易维护，而不至于把所有的东西全部写在一个\nMakefile 中，这个技术对于我们模块编译和分段编译有着非常大的好处。\n例如，有一个子目录叫 subdir，这个目录下有个 Makefile\n文件，来指明了这个目录下文件的编译规则。那么我们总控的 Makefile 可以这样书写：\nsubsystem:        cd subdir &amp;&amp; $(MAKE)或subsystem:        $(MAKE) -C subdir\n$(MAKE) 是自定义的宏变量，不直接使用 make 命令，而是定义\n$(MAKE) 这个宏变量的原因是 make\n有时需要一些参数，所以定义成一个变量比较利于维护。\n如果要传递变量到下级 Makefile 中，那么可以使用这样的声明\nexport variable_name 如果不想让某些变量传递到下级 Makefile\n中，那么可以这样声明 unexport variable_name\n如果你要传递所有的变量，那么，只要一个 export 就行了；\n后面什么也不用跟，表示传递所有的变量。\n定义命令包\n如果 Makefile\n中出现一些相同命令序列，那么可以为这些相同的命令序列定义成一个变量。定义这种命令序列的语法以\ndefine 开始，以 endef 结束，如：\ndefine run-actionaction 1action 2action 3endeffoo.o : foo.c        $(run-action)\n这里的 run-action 是这个命令包的名字，在\ndefine 和 endef\n中的三行就是命令序列；可以看到，使用这个命令包就好像使用变量一样。\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"MLE 与 MAP 简介","url":"/2019/01/25/MLE%20%E4%B8%8E%20MAP%20%E7%AE%80%E4%BB%8B/","content":"最近看到一篇关于 MLE (Maximum Likelihood Estimation) 和 MAP（Maximum A\nPosteriori) 的文章，写的很好，非常值得一看，文章链接为 聊一聊机器学习的 MLE 和 MAP：最大似然估计和最大后验估计，本文几乎不加修改地转载了文章，侵删。\n\n概述\n有时候和别人聊天，对方会说自己有很多机器学习经验，深入一聊发现，对方竟然对 MLE 和 MAP 一知半解，至少在我看来，这位同学的机器学习基础并不扎实。难道在这个深度学习盛行的年代，不少同学都只注重调参数？\n现代机器学习的终极问题都会转化为解目标函数的优化问题，MLE 和\nMAP\n是生成这个函数的很基本的思想，因此我们对二者的认知是非常重要的。这次就和大家认真聊一聊\nMLE 和 MAP 这两种 estimator。\n两大学派的争论\n抽象一点来讲，频率学派 (Frequentist) 和贝叶斯学派 (Bayesian) 对世界的认知有本质不同：频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标就是要找到这个真值或真值所在的范围；而贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判做调整，我们的目标是要找到最优的描述这个世界的概率分布。\n在对事物建模时，用 \\(\\theta\\)\n表示模型的参数，解决问题的本质就是求 \\(\\theta\\)\n。那么频率学派和贝叶斯学派的区别在于：\n\n频率学派：存在唯一真值 \\(\\theta\\)\n。举一个简单直观的例子 -- 抛硬币，我们用 \\(P(head)\\) 来表示硬币的\nbias。抛一枚硬币 100 次，有 20 次正面朝上，要估计抛硬币正面朝上的 bias \\(P(head)=\\theta\\) 。在频率学派来看，\\(\\theta\\) = 20 / 100 =\n0.2，很直观。当数据量趋于无穷时，这种方法能给出精准的估计；然而缺乏数据时则可能产生严重的偏差。例如，对于一枚均匀硬币，即\n\\(\\theta\\) = 0.5，抛掷 5 次，出现 5 次正面\n(这种情况出现的概率是 1/2^5=3.125%)，频率学派会直接估计这枚硬币 \\(\\theta\\) = 1，出现严重错误。\n贝叶斯学派： \\(\\theta\\)\n是一个随机变量，符合一定的概率分布。在贝叶斯学派里有两大输入和一大输出，输入是先验\n(prior) 和似然 (likelihood)，输出是后验 (posterior)。先验，即 \\(P(\\theta)\\)\n，指的是在没有观测到任何数据时对\n的预先判断，例如给我一个硬币，一种可行的先验是认为这个硬币有很大的概率是均匀的，有较小的概率是是不均匀的；似然，即\n\\(P(X|\\theta)\\) ，是假设 \\(\\theta\\)\n已知后我们观察到的数据应该是什么样子的；后验，即 \\(P(\\theta|X)\\)\n，是最终的参数分布。贝叶斯估计的基础是贝叶斯公式，如下：\n\n\\[\\begin{align}\nP(\\theta|X)=\\frac{P(X|\\theta) \\times P(\\theta)}{P(X)}\n\\end{align}\\]\n同样是抛硬币的例子，对一枚均匀硬币抛 5 次得到 5 次正面，如果先验认为大概率下这个硬币是均匀的\n(例如最大值取在 0.5 处的 Beta 分布)，那么 \\(P(head)\\) ，即 \\(P(\\theta|X)\\)\n，是一个 distribution，最大值会介于 0.5~1 之间，而不是武断的 \\(\\theta\\) = 1。\n这里有两点值得注意的地方：\n\n随着数据量的增加，参数分布会越来越向数据靠拢，先验的影响力会越来越小\n如果先验是 uniform\ndistribution，则贝叶斯方法等价于频率方法。因为直观上来讲，先验是 uniform\ndistribution 本质上表示对事物没有任何预判\n\nMLE - 最大似然估计\nMaximum Likelihood Estimation, MLE 是频率学派常用的估计方法！\n假设数据 \\(x_1, x_2, ..., x_n\\) 是\ni.i.d. 的一组抽样，\\(X = (x_1, x_2, ...,\nx_n)\\) 。其中 i.i.d. 表示 Independent and identical\ndistribution，独立同分布。那么 MLE 对 \\(\\theta\\) 的估计方法可以如下推导：\n\\[\\begin{align}\n\\hat{\\theta}_\\text{MLE} &amp;= \\arg \\max P(X; \\theta) \\\\\\\n&amp;= \\arg \\max P(x_1; \\theta) P(x_2; \\theta) \\cdot\\cdot\\cdot\\cdot\nP(x_n;\\theta) \\\\\\\n&amp; = \\arg \\max\\log \\prod_{i=1}^{n} P(x_i; \\theta) \\\\\\\n&amp;= \\arg \\max \\sum_{i=1}^{n} \\log P(x_i; \\theta) \\\\\\\n&amp;= \\arg \\min - \\sum_{i=1}^{n} \\log P(x_i; \\theta)\n\\end{align}\\]\n最后这一行所优化的函数被称为 Negative Log Likelihood\n(NLL)，这个概念和上面的推导是非常重要的！\n我们经常在不经意间使用 MLE，例如\n上文中关于频率学派求硬币概率的例子，其方法其实本质是由优化 NLL 得出。\n给定一些数据，求对应的高斯分布时，我们经常会算这些数据点的均值和方差然后带入到高斯分布的公式，其理论依据是优化 NLL\n深度学习做分类任务时所用的 cross entropy loss，其本质也是 MLE\nMAP - 最大后验估计\nMaximum A Posteriori, MAP 是贝叶斯学派常用的估计方法！\n同样的，假设数据 \\(x_1, x_2, ...,\nx_n\\) 是 i.i.d. 的一组抽样，\\(X = (x_1,\nx_2, ..., x_n)\\) 。那么 MAP 对 \\(\\theta\\) 的估计方法可以如下推导：\n\\[\\begin{align}\n\\hat{\\theta}_\\text{MAP} &amp;= \\arg \\max P(\\theta | X) \\\\\\\n&amp;= \\arg \\min -\\log P(\\theta | X) \\\\\\\n&amp; = \\arg \\min -\\log P(X|\\theta) - \\log P(\\theta) + \\log P(X) \\\\\\\n&amp;= \\arg \\min -\\log P(X|\\theta ) - \\log P(\\theta)\n\\end{align}\\]\n其中，第二行到第三行使用了贝叶斯定理，第三行到第四行 \\(P(X)\\) 可以丢掉因为与 \\(\\theta\\) 无关。注意 \\(-\\log P(X|\\theta )\\)\n其实就是 NLL，所以 MLE 和 MAP 在优化时的不同就是在于先验项 \\(- \\log P(\\theta)\\)\n。好的，那现在我们来研究一下这个先验项，假定先验是一个高斯分布，即\n\\[\\begin{align} P(\\theta) =\n\\text{constant} \\times e^{-\\frac{\\theta^2}{2\\sigma^2}}\n\\end{align}\\]\n那么， \\(-\\log P(\\theta) = \\text{constant}\n+ \\frac{\\theta^2}{2\\sigma^2}\\) 。至此，一件神奇的事情发生了：\n在 MAP 中使用一个高斯分布的先验等价于在 MLE 中采用 L2 的 regularizaton！\n\n参考:\n\nBayesian\nMethods\nMLE,\nMAP, Bayes classification\n\n","categories":["机器学习"],"tags":["机器学习","数学","转载"]},{"title":"Makefile 语法详解 (2)- 变量、条件判断与函数","url":"/2018/12/07/Makefile%20%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3(2)-%E5%8F%98%E9%87%8F%E3%80%81%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E4%B8%8E%E5%87%BD%E6%95%B0/","content":"本文内容是之前的文章 Makefile\n简介 的补充，详细介绍了 Makefile\n中的变量（包括变量的定义、批量替换、局部变量等）、条件判断和函数（内置函数和自定义函数）。\n\n变量\n在 Makefile 中的定义的变量，就像是 C/C++\n语言中的宏一样，代表了一个文本字串，在 Makefile\n中执行的时候其会自动地展开在所使用的地方。其与 C/C++ 所不同的是，可以在\nMakefile 中改变其值。\n变量在声明时需要给予初值，而在使用时，需要给在变量名前加上\n$ 符号，但最好用小括号 () 或是大括号\n{} 把变量给包括起来。如果你要使用真实的 $\n字符，那么你需要用 $$ 来表示。\n变量的赋值\n定义 Makefile\n中为变量赋值可用四种操作符：=、:=、?=、+=，\n参考 StackOverflow 上的问题 What\nis the difference between the GNU Makefile variable assignments =, ?=,\n:= and +=?, 这四个符号的主要区别是\n\n= 赋值是 lazy\n的，也就是在使用的时候才会递归的获取变量的值（递归指的是可以通过一个变量为另一个变量赋值）\n:= 则是在声明的时候变量的值就确定了\n?= 表示在变量没有值的时候才给其赋值\n+= 则是在原来的值上 append\n一个其他的值（自动添加空格）\n\n其中 =\n赋值是的递归获取值比较难理解，简单来说就是右侧中的变量不一定非要是已定义好的值，也可以使用后面定义的值。如下是一个简单地例子\nfoo = $(bar)bar = $(ugh)ugh = Hahaall:    echo $(foo)\n执行 make all 时输出的值是 Haha, 而是用\n:= 赋值时就不允许这么赋值，在 :=\n右边的值必须只能是字符串或者前面定义的变量，也就是在\n:= 右边的值必须要是目前为止已经确定的值。\n# 示例 1x := fooy := $(x) barx := later# 示例 2y := $(x) barx := foo\n上面示例 1 中的 y 的值为 foo bar，示例 2 中的值为 bar。\n变量值的替换\n我们可以替换变量中的共有的部分，其格式是 $(var:a=b) 或是\n$(var: %a=%b)，其意思是，把变量 var 中所有以\na 字串结尾的那些值从 a 替换成 b;\n如下是一个简单的示例\nfoo := a.o b.o c.obar := $(foo:.o=.c) 或 bar := $(foo:%.o=%.c)# bar 的值是 a.c b.c c.c\n把变量值当做变量名\nMakefile 中如果变量 a 的值是变量 b 的名称，那么可以把变量 a\n的值直接当做变量 b 使用，如下是一些简单的例子\n# 示例一x = yy = zz = ua := $($($(x))) # u# 示例二x = $(y)y = zz = Helloa := $($(x)) # Hello\n局部变量\n前面我们所讲的在 Makefile 中定义的变量都是全局变量，\n但是也可以为某个目标设置局部变量，这种变量被称为 Target-specific\nVariable，因为它的作用范围只在这条规则以及连带规则中，所以其值也只在作用范围内有效。而不会影响规则链以外的全局变量的值，因为局部变量的名称可与全局变量的名称相同。\n其一般语法如下，首先在 target 中定义局部变量，则在 target\n及其依赖的目标中使用该变量即可 target : local_variable assignmenttarget : use variable\n如下是个简单的例子 prog : CFLAGS = -gprog : prog.o foo.o bar.o        $(CC) $(CFLAGS) prog.o foo.o bar.oprog.o : prog.c        $(CC) $(CFLAGS) prog.cfoo.o : foo.c        $(CC) $(CFLAGS) foo.cbar.o : bar.c        $(CC) $(CFLAGS) bar.c\n除了 Target-specific Variable，还有 Pattern-specific\nVariable，即不是在某个特定的 target 中定义局部变量，而是在某个特定的\npattern 中定义局部变量，其语法与上面的类似，如下所示是定义了所有以\n.o 结尾的目标中的一个局部变量\n%.o : CFLAGS = -O\n条件判断\n使用条件判断，可以让 make\n根据运行时的不同情况选择不同的执行分支，主要有以下几个关键字：\nifeq、ifneq、ifdef、ifndef、else\n和 endif;\n根据名称其实也能基本能猜出各个关键字的作用了，ifeq 和\nifneq\n是一对关键字，表示其后面跟随的两个参数是够相等，ifdef 和\nifndef\n是一对关键字，表示其后跟随的变量是否已经被定义过。\n如下是一个简单的例子，表示目标 foo 可以根据变量 $(CC)\n值来选取不同的函数库来编译 libs_for_gcc = -lgnunormal_libs =foo: $(objects)ifeq ($(CC),gcc)        $(CC) -o foo $(objects) $(libs_for_gcc)else        $(CC) -o foo $(objects) $(normal_libs)endif\n当 $(CC) 是 gcc 时，目标 foo 的规则是\nfoo: $(objects)    $(CC) -o foo $(objects) $(libs_for_gcc)\n因此，上面的写法可以写成如下更简洁且容易理解的形式\nlibs_for_gcc = -lgnunormal_libs =ifeq ($(CC), gcc)  libs=$(libs_for_gcc)else  libs=$(normal_libs)endiffoo: $(objects)        $(CC) -o foo $(objects) $(libs)\n需要注意的一点是在关键字所在的这一行上，多余的空格是被允许的，但是不能以\nTab 键做为开始，否则就被认为是命令\n使用 ifeq 可有若干种形式，如下所示的五种形式都是等价的，\nifneq 的使用方法相同\nifeq (&lt;arg1&gt;, &lt;arg2&gt;) ifeq '&lt;arg1&gt;' '&lt;arg2&gt;' ifeq \"&lt;arg1&gt;\" \"&lt;arg2&gt;\" ifeq \"&lt;arg1&gt;\" '&lt;arg2&gt;' ifeq '&lt;arg1&gt;' \"&lt;arg2&gt;\" \nifdef 和 ifndef\n的使用方法也类似，只是其后面只跟着一个变量。\n函数\nGNU make 内置了一些函数，在 Makefile\n中使用函数来处理变量，可以让我们的命令或是规则更为的灵活\n函数调用很像变量的使用，也是以 $\n来标识的，其语法如下，其中 &lt;function&gt;\n就是函数名，&lt;arguments&gt; 为函数的参数，参数间以逗号\n, 分隔，而函数名和参数之间以空格分隔\n$(&lt;function&gt; &lt;arguments&gt;)或${&lt;function&gt; &lt;arguments&gt;}\n以下是 GNU make 内置的一些函数\n字符串处理函数\n\nsubst\n用法：$(subst &lt;from&gt;,&lt;to&gt;,&lt;text&gt;)\n功能：把字串 &lt;text&gt; 中的 &lt;from&gt;\n字符串替换成 &lt;to&gt;。\n返回：函数返回被替换过后的字符串。\npatsubst\n用法：$(patsubst &lt;pattern&gt;,&lt;replacement&gt;,&lt;text&gt;)\n功能：查找 &lt;text&gt; 中的单词是否符合模式\n&lt;pattern&gt;，如果匹配的话，则以\n&lt;replacement&gt; 替换。这里，&lt;pattern&gt;\n可以包括通配符 %，表示任意长度的字串。如果\n&lt;replacement&gt; 中也包含\n%，那么，&lt;replacement&gt; 中的这个\n% 将是 &lt;pattern&gt; 中的那个 %\n所代表的字串。（可以用 \\ 来转义）\n返回：函数返回被替换过后的字符串\n示例：$(patsubst %.c,%.o,x.c.c bar.c) 返回的结果是\nx.c.o bar.o\nfindstring\n用法：$(findstring &lt;find&gt;,&lt;string&gt;)\n功能：在字串 &lt;string&gt; 中查找\n&lt;find&gt; 字串。 返回：如果找到，那么返回\n&lt;find&gt;，否则返回空字符串。\nsort 用法：$(sort &lt;list&gt;)\n功能：给字符串 &lt;list&gt; 中的单词排序（空格分隔）。\n返回：返回排序后的字符串。 示例：$(sort foo bar lose) 返回\nbar foo lose 。\nword\n用法：$(word &lt;n&gt;,&lt;text&gt;) 功能：取字符串\n&lt;text&gt; 中第 &lt;n&gt;\n个单词。（从 1 开始） 返回：返回字符串 &lt;text&gt; 中第\n&lt;n&gt; 个单词 示例：$(word 2, foo bar baz)\n返回值是 bar\n\nforeach 函数\nforeach 函数是用作循环的，其用法如下\n$(foreach  &lt;var&gt;,&lt;list&gt;,&lt;text&gt;)\n该函数的意思是，把参数 &lt;list&gt;\n中的单词逐一取出放到参数所指定的变量 &lt;var&gt;\n中，然后再执行 &lt; text&gt; 所包含的表达式。每一次\n&lt;text&gt;\n会返回一个字符串，循环过程中，&lt;text&gt;\n的所返回的每个字符串会以空格分隔，最后当整个循环结束时，&lt;text&gt; 所返回的每个字符串所组成的整个字符串（以空格分隔）将会是 foreach 函数的返回值。\n如下是一个简单的例子\nnames := a b c dfiles := $(foreach n,$(names),$(n).o)\n上面的例子中，$(name) 中的单词会被挨个取出，并存到变量 n\n中，$(n).o 每次根据 $(n)\n计算出一个值，这些值以空格分隔，最后作为 foreach\n函数的返回值，所以，$(files) 的值是\na.o b.o c.o d.o\n需要注意的是，foreach\n中的参数是一个临时的局部变量，foreach 函数执行完后，参数的变量将不在作用，其作用域只在 foreach 函数当中。\ncall 函数\ncall 函数可以用来创建自定义函数。其语法是：\n$(call &lt;expression&gt;,&lt;parm1&gt;,&lt;parm2&gt;,&lt;parm3&gt;,...)\n当 make 执行这个函数时，&lt;expression&gt; 参数中的变量:\n$(1)，$(2)，$(3) 等，会被参数\n&lt;parm1&gt;，&lt;parm2&gt;，&lt;parm3&gt; 依次取代。而\n&lt;expression&gt; 的返回值就是 call\n函数的返回值。例如：\nreverse =  $(2) $(1) foo = $(call reverse,a,b)\n经过上面的表达式得到的 foo 的值是 b a。当然，可以为\nreverse 定义更复杂的操作。\nshell 函数\n，shell 函数把执行操作系统命令后的输出作为函数返回。因此可以用操作系统命令以及字符串处理命令\nawk，sed 等等命令来生成一个变量，如：\ncontents := $(shell cat foo)files := $(shell echo *.c)\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"Markdown 图片免费上传工具","url":"/2019/04/20/Markdown%20%E5%9B%BE%E7%89%87%E5%85%8D%E8%B4%B9%E4%B8%8A%E4%BC%A0%E5%B7%A5%E5%85%B7/","content":"对于习惯用 markdown\n写作的人，日常最烦恼的问题之一应该是如何显示自己的图片，markdown\n文本存储的是图片的地址（本地路径或 url），而最为常用的是\nurl，至少我写文章是这样的。某些 markdown 编辑器也提供了 markdown\n图床服务，如有道云笔记，cmd_markdown 等，\n但是这些编辑器要么太丑（有道云笔记，说的就是你），要么就是比较小众（cmd\nmarkdown），生怕哪天停止运营了文章里的图片就没了，而且这些服务一般是要收费的。\n那么有没有一种方法能够为本地图片生成 public\nurl，同时保证数据有较高的可用性，而且最好是免费的。Github\n其实已经间接为我们提供了这样的服务，只是这个步骤较为繁琐，本文就是针对这一点开发了一个小工具来简化这个过程，代码已开源，见\nMarkdownImageUploader，本文主要介绍其基本原理和使用方法。\n\n原理\n这个方法是基于 Github 会为 repository 中的每张图片生成一个 public\nurl，假设你的 Github 某个 repository 的有某张图片，那么访问这张图片的\npublic url 为\nhttps://raw.githubusercontent.com/{user}/{repository}/{branch}/{img_path}\n其中，{user} 就是你的 Github\n用户名称，{repository}\n是你的仓库名称，{branch}\n是你的分支名称，{img_path} 则是这张图片在这个 repository\n中的路径；如对于这张图片，其\npublic url 为\nhttps://wulc.me/imgs/GFS_Architecture.png\n所以此时问题就变得很直观了，只需要将需要生成 public url 的图片 push\n到 github，然后按照上面的规则生成图片的 public url 即可，感谢\nGithub。\n使用\n但是这个过程如果重复多次也会显得重复而繁琐，每次都要\nadd &amp;&amp; commit &amp;&amp; push,\n因此，本文开发了一个简单的小工具 MarkdownImageUploader\n来让这个过程更为便捷。\nMarkdownImageUploader\n主要思想就是通过提供一个简单的界面，通过按钮来提供上面的功能，按钮按下后执行\ngit commit 等操作，因此使用前需要确保\n1. git 已经安装且在 PATH 环境变量中  2.\n上传的图片的仓库是 public 且具备修改权限\n建议在使用前首先在 github 上建立一个 public\nrepository，专门用于存储图片；并配置 MarkdownImageUploader 的\nrepo_address 为其地址，然后配置本地存储这个 repository\n的路径，这个配置只需要在第一次使用时进行配置，之后 MarkdownImageUploader\n会在 config\n目录下生成文件保存这些信息，之后的使用也可随时更新这些信息。该过程如下图所示\n\n\nconfigure\n\n在配置后，点击 commit &amp; push\n进行图片的上传，程序首先会检查配置的本地 repository\n的路径中是否已经存在了这个 repository，没有的话会先 clone\n到本地，然后填写 commit message 进行 commit 和\npush，需要注意这两者都不能为空，我一般会将文章的标题作为 commit\nmessage。该过程如下所示\n\n\nupload\n\nWindows 用户可直接从 release下载可执行文件，win10\n和 win7 的 64 位系统测试过都可用，32 位系统的不确定，但是也可通过源码和\nPyInstaller 进行构建；Linux\n和 Mac 可以下载源码，然后直接运行 python main.py\n为后台程序，但是需要安装依赖库 PySimpleGUI，而且\nPySimpleGUI\n在 Mac 上适配性不太好，不确保能顺利运行。\n代码\n代码使用的 GUI 库是 PySimpleGUI，一个小众但是比较方便的库；调用\ngit 命令使用的 python 的 subprocess 库，调用 git\n命令时会捕获异常并弹窗提醒，但是当前的异常没有细分（比如说 commit &amp;\npush 失败后，并不知道是 commit 或 push\n环节失败的）；所以直接使用时建议先在命令行跑通 git 这几条命令。\n代码很简单，所有代码不到 200 行（见 main.py), 程序的 UI\n极丑（捂脸），但是目前基本能满足我的使用需求，先使用一段时间看看，后续如果有\nbug 或新的需求再更改。\n最后， Github 生成的图片的 url 的域名是\ngithubusercontent.com, 万一哪天 Github\n放弃了这个域名或不再提供单张图片的\nurl，那就凉凉了，不过在当前看来短期发生这个事情的可能性不大。而其实通过付费使用大厂提供的\nmarkdown\n图床也是很方便的，如果有道云笔记把界面做得更好看一点，其实我还是挺愿意为其付费的。\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"MySQL 的锁机制","url":"/2016/08/16/MySQL%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/","content":"在计算机科学中，锁是在执行多线程时用于强行限制资源访问的同步机制，即用于在并发控制中保证对互斥要求的满足。\n本文主要以 MySQL 为例，讲述几个锁的概念 (行级锁、页级锁、表级锁、共享锁、排它锁等)，这些概念的范畴不限于 MySQL，在并发系统上均有应用。\n\n行级锁，页级锁，表级锁\n在 DBMS 中，可以按照锁的粒度把数据库锁分为行级锁 (INNODB 引擎)、表级锁 (MYISAM 引擎) 和页级锁 (BDB 引擎\n)。\n行级锁\n行级锁是 Mysql 中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁\n和 排他锁。\n特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。\n表级锁\n表级锁是 MySQL 中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分 MySQL 引擎支持。最常使用的 MYISAM 与 INNODB 都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。\n特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。\n页级锁\n页级锁是 MySQL 中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB 支持页级锁\n特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般\n常用存储引擎及其锁机制\n\nMyISAM 和 MEMORY 采用表级锁 (table-level locking)\nBDB 采用页面锁 (page-level\nlocking) 或表级锁，默认为页面锁\n InnoDB 支持行级锁 (row-level\nlocking) 和表级锁，默认为行级锁\n\nInnodb 中的行锁与表锁\n前面提到过，在 Innodb 引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表，什么时候或只锁住一行呢？\nInnoDB 行锁是通过给索引上的索引项加锁来实现的，这一点 MySQL 与 Oracle 不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB 这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！\n值得注意的是，DBMS\n对于主键会自动生成唯一索引，所以主键也是一个特殊的索引。即通过主键进行查询也能实现行级锁。\n在实际应用中，要特别注意 InnoDB 行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。\n行级锁都是基于索引的，如果一条 SQL 语句用不到索引是不会使用行级锁的，会使用表级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。\n行级锁与死锁\nMyISAM 中是不会产生死锁的，因为 MyISAM 总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在 InnoDB 中，锁是逐步获得的，就造成了死锁的可能。\n在 MySQL 中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条 sql 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL 会先锁定该非主键索引，再锁定相关的主键索引。\n在 UPDATE、DELETE 操作时，MySQL 不仅锁定 WHERE 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key\nlocking。\n当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。\n发生死锁后，InnoDB 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。\n避免死锁\n如何避免死锁，这里只介绍常见的三种\n1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。\n2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；\n3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率\n共享锁，排它锁，意向锁\n行级锁分为共享锁和排他锁两种，下面将详细介绍共享锁及排他锁的概念、使用方式及注意事项等。\n共享锁 (Share Lock，SLock)\n共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。\n如果事务 T 对数据 A 加上共享锁后，则其他事务只能对 A 再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。\n语法：SELECT ... LOCK IN SHARE MODE;\n在查询语句后面增加 LOCK IN SHARE MODE，Mysql 会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。\n排他锁（eXclusive Lock，XLock）\n排他锁又称写锁，如果事务 T 对数据 A 加上排他锁后，则其他事务不能再对 A 加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。\n语法：SELECT ... FOR UPDATE;\n在查询语句后面增加 FOR UPDATE，Mysql 会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。\n意向锁（Intent Lock）\nInnoDB 还有两个表锁：\n意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说给一个数据行加共享锁前必须先取得该表的 IS 锁\n意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的 IX 锁\n意向锁是 InnoDB 自动加的，不需要用户干预。\n对于 insert、update、delete，InnoDB 会自动给涉及的数据加排他锁（X）；对于一般的 Select 语句，InnoDB 不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。\n共享锁：SELECT ... LOCK IN SHARE MODE;\n排他锁：SELECT ... FOR UPDATE;\n乐观锁，悲观锁\n数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。\n乐观并发控制 (乐观锁) 和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。\n无论是悲观锁还是乐观锁，都是人们定义出来的概念，可以认为是一种思想。其实不仅仅是关系型数据库系统中有乐观锁和悲观锁的概念，像 memcache、hibernate、tair 等都有类似的概念。\n针对于不同的业务场景，应该选用不同的并发控制方式。所以，不要把乐观并发控制和悲观并发控制狭义的理解为 DBMS 中的概念，更不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在 DBMS 中，悲观锁正是利用数据库本身提供的锁机制来实现的。\n悲观锁\n悲观并发控制（又名 “悲观锁”，Pessimistic Concurrency\nControl，缩写 “PCC”）是一种并发控制的方法。\n悲观锁，正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度 (悲观)\n，因此，在整个数据处理过程中，将数据处于锁定状态。\n悲观锁的实现，往往依靠数据库提供的锁机制\n（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）\n在数据库中，悲观锁的流程如下：\n（1）在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive\nlocking）。\n（2）如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。\n具体响应方式由开发者根据实际需要决定。\n（3）如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。\n下面讲述在 MySQL\nInnoDB 中使用悲观锁，要使用悲观锁，我们必须关闭 MySQL 数据库的自动提交属性，因为 MySQL 默认使用 autocommit 模式，也就是说，当你执行一个更新操作后，MySQL 会立刻将结果进行提交。\n//0.关闭自动提交属性set autocommit=0;//1.开始事务begin;/begin work;/start transaction; //2.查询出商品信息select status from t_goods where id=1 for update;//3.根据商品信息生成订单insert into t_orders (id,goods_id) values (null,1);//4.修改商品status为2update t_goods set status=2;//4.提交事务commit;/commit work;  \n上面的查询语句中，我们使用了 select…for update 的方式，这样就通过排他锁的实现了悲观锁。此时在 t_goods 表中，id\n为 1\n的那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。\n优点与不足：悲观并发控制实际上是 “先取锁再访问” 的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数\n乐观锁\n乐观并发控制（又名 “乐观锁”，Optimistic Concurrency\nControl，缩写 “OCC”）是一种并发控制的方法。\n乐观锁（ Optimistic Locking ）\n相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。\n相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。实现数据版本可以通过使用版本号或使用时间戳。实现流程如下：\n（1）为数据表增加一个表示版本标识的字段，用于存储版本号或时间戳\n（2）当读取数据时，将版本标识的值一同读出\n（3）当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本标识与第一次取出来的版本标识值相等，则同时更新数据和版本号，否则认为是过期数据，返回错误给用户处理\n下图为该流程的示意过程：\n\n\n\n\n\n\n\n参考: MySQL 中的行级锁，表级锁，页级锁\nMySQL 中的共享锁与排他锁\n深入理解乐观锁与悲观锁\nmysql 悲观锁总结和实践\nmysql 乐观锁总结和实践\n\n\n\n","categories":["杂"],"tags":["数据库"]},{"title":"Maven 的安装、配置及使用入门","url":"/2016/02/18/Maven%E7%9A%84%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8/","content":"这是一篇关于 maven 入门的相当好的文章，文章有点长，但是非常值得看。原文链接\nMaven 简介\n何为 Maven\nMaven 这个词可以翻译为 “知识的积累”，也可以翻译为 “专家” 或 “内行”。本书将介绍 Maven 这一跨平台的项目管理工具。作为 Apache 组织中的一个颇为成功的开源项目，Maven 主要服务于基于 Java 平\n台的项目构建、依赖管理和项目信息管理。无论是小型的开源类库项目，还是大型的企业级应用；无论是传统的瀑布式开发，还是流行的敏捷模式，Maven 都能\n大显身手。\n\n#### 何为构建\n不管你是否意识到，构建（build）是每一位程序员每天都在做的工作。早上来到公司，我们做的第一件事情就是从源码库签出最新的源码，然后进行单元测试，如果发现失败的测试，会找相关的同事一起调试，修复错误代码。接着回到自己的工作上来，编写自己的单元测试及产品代码，我们会感激 IDE 随时报出的编译错误提示。\n忙到午饭时间，代码编写得差不多了，测试也通过了，开心地享用午餐，然后休息。下午先在昏昏沉沉中开了个例会，会议结束后喝杯咖啡继续工作。刚才在会上经理要求看测试报告，于是找了相关工具集成进 IDE，生成了像\n模像样的测试覆盖率报告，接着发了一封电子邮件给经理，松了口气。谁料 QA 小组又发过来了几个 bug，没办法，先本地重现再说，于是熟练地用 IDE 生成了一个 WAR 包，部署到 Web 容器下，启动容器。看到熟悉的界面了，遵循 bug 报告，一步步重现了 bug…… 快下班的时候，bug 修好了，提交代码，通知\nQA 小组，在愉快中结束了一天的工作。\n仔细总结一下，我们会发现，除了编写源代码，我们每天有相当一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。如果我们现在还手工这样做，那成本也太高了，于是有人用软件的方法让这一系列工作完全自动化，使得软件的构建可以像全自动流水线一样，只需要一条简单的命令，所有烦琐的步骤都能够自动完成，很快就能得到最终结果。\nMaven 是优秀的构建工具\n前面介绍了 Maven 的用途之一是服务于构建，它是一个异常强大的构建工具，能够帮我们自动化构建过程，从清理、编译、测试到生成报告，再到打包和部署。我们不需要也不应该一遍又一遍地输入命令，一次又一次地点击鼠标，我们要做的是使用 Maven 配置好项目，然后输入简单的命令 (如 mvn\nclean install)，Maven 会帮我们处理那些烦琐的任务。\nMaven 是跨平台的，这意味着无论是在 Windows 上，还是在 Linux 或者 Mac 上，都可以使用同样的命令。\n我们一直在不停地寻找避免重复的方法。设计的重复、编码的重复、文档的重复，当然还有构建的重复。Maven 最大化地消除了构建的重复，抽象了构建生命周期，并且为绝大部分的构建任务提供了已实现的插件，我们不再需要定义过程，甚至不需要再去实现这些过程中的一些任务。最简单的例子是测试，我们没必要告诉 Maven 去测试，更不需要告诉 Maven 如何运行测试，只需要遵循 Maven 的约定编写好测试用例，当我们运行构建的时候，这些测试便会自动运行。\n想象一下，Maven 抽象了一个完整的构建生命周期模型，这个模型吸取了大量其他的构建脚本和构建工具的优点，总结了大量项目的实际需求。如果遵循这个模型，可以避免很多不必要的错误，可以直接使用大量成熟的 Maven 插件来完成我们的任务（很多时候我们可能都不知道自己在使用 Maven 插件）。此外，如果有非常特殊的需求，我们也可以轻松实现自己的插件。\nMaven 还有一个优点，它能帮助我们标准化构建过程。在 Maven 之前，十个项目可能有十种构建方式；有了 Maven 之后，所有项目的构建命令都是简单一致的，这极大地避免了不必要的学习成本，而且有利于促进项目团队的标准化。\n综上所述，Maven 作为一个构建工具，不仅能帮我们自动化构建，还能够抽象构建过程，提供构建任务实现；它跨平台，对外提供了一致的操作接口，这一切足以使它成为优秀的、流行的构建工具。\nMaven 不仅仅是构建工具\nJava 不仅是一门编程语言，还是一个平台，通过 JRuby 和 Jython，我们可以在 Java 平台上编写和运行 Ruby 和 Python 程序。我们也应该认识到，Maven 不仅是构建工具，还是一个依赖管理工具和项目信息管理工具。\n它提供了中央仓库，能帮我们自动下载构件。\n在这个开源的年代里，几乎任何 Java 应用都会借用一些第三方的开源类库，这些类库都可通过依赖的方式引入到项目中来。随着依赖的增多，版本不一致、版本冲突、依赖臃肿等问题都会接踵而来。手工解决这些问题是十分枯燥的，幸运的是 Maven 提供了一个优秀的解决方案，它通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标 Maven 能够找到任何一个\nJava 类库（如 jar 文件）。Maven 给这个类库世界引入了经纬，让它们变得有秩序，于是我们可以借助它来有序地管理依赖，轻松地解决那些繁杂的依赖问题。\nMaven 还能帮助我们管理原本分散在项目中各个角落的项目信息，包括项目描述、开发者列表、版本控制系统地址、许可证、缺陷管理系统地址等。这些微小的变化看起来很琐碎，并不起眼，但却在不知不觉中为我们节省了大量寻找信息的时间。除了直接的项目信息，通过 Maven 自动生成的站点，以及一些已有的插件，我们还能够轻松获得项目文档、测试报告、静态分析报告、源码版本日志报告等\n非常具有价值的项目信息。\nMaven 还为全世界的 Java 开发者提供了一个免费的中央仓库，在其中几乎可以找到任何的流行开源类库。通过一些 Maven 的衍生工具（如 Nexus），我们还能对其进行快速地搜索。只要定位了坐标，Maven 就能够帮我们自动下载，省去了手工劳动。\n使用 Maven 还能享受一个额外的好处，即 Maven 对于项目目录结构、测试用\n例命名方式等内容都有既定的规则，只要遵循了这些成熟的规则，用户在项目间切换的时候就免去了额外的学习成本，可以说是约定优于配置\n（Convention Over Configuration）。\n为什么需要 Maven\nMaven 不是 Java 领域唯一的构建管理的解决方案。本节将通过一些简单的例子解释 Maven 的必要性，并介绍其他构建解决方案，如 IDE、Make 和 Ant，并将它们与 Maven 进行比较。\n组装 PC 和品牌 PC\n笔者初中时开始接触计算机，到了高中时更是梦寐以求希望拥有一台自己的计算机。我的第一台计算机是赛扬 733 的，选购是一个漫长的过程，我先阅读了大量的杂志以了解各类配件的优劣，CPU、内存、主板、显卡，甚至声卡，我都仔细地挑选，后来还跑了很多商家，调货、讨价还价，组装好后自己装操作系统和驱动程序…… 虽然这花费了我大量时间，但我很享受这个过程。可是事实证明，装出来的机器稳定性不怎么好。\n一年前我需要配一台工作站，这时候我已经没有太多时间去研究电脑配件了。我选择了某知名 PC 供应商的在线商店，大概浏览了一下主流的机型，选择了我需要的配置，然后下单、付款。接着 PC 供应商帮我组装电脑、安装操作系统和驱动程序。一周后，物流公司将电脑送到我的家里，我接上显示器、电源、鼠标和键盘就能直接使用了。这为我节省了大量时间，而且这台电脑十分稳定，商家在把电脑发送给我之前已经进行了很好的测试。对了，我还能享受两年的售后服务。\n使用脚本建立高度自定义的构建系统就像买组装 PC，耗时费力，结果也不一定很好。当然，你可以享受从无到有的乐趣，但恐怕实际项目中无法给你那么多时间。使用 Maven 就像购买品牌 PC，省时省力，并能得到成熟的构建系统，还能得到来自于 Maven 社区的大量支持。唯一与购买品牌 PC 不同的是，Maven 是开源的，你无须为此付费。如果有兴趣，你还能去了解 Maven 是如何工作的，而我们无法知道那些 PC 巨头的商业秘密。\nIDE 不是万能的\n当然，我们无法否认优秀的 IDE 能大大提高开发效率。当前主流的 IDE 如 Eclipse 和 NetBeans 等都提供了强大的文本编辑、调试甚至重构功能。虽然使用简单的文本编辑器和命令行也能完成绝大部分开发工作，但很少有人愿意那样做。然而，IDE 是有其天生缺陷的：\n\nIDE 依赖大量的手工操作。编译、测试、代码生成等工作都是相互独立的，很难一键完成所有工作。手工劳动往往意味着低效，意味着容易出错。\n\n很难在项目中统一所有的 IDE 配置，每个人都有自己的喜好。也正是由于这个原因，一个在机器 A 上可以成功运行的任务，到了机器 B 的 IDE 中可能就会失败。\n\n我们应该合理利用 IDE，而不是过多地依赖它。对于构建这样的任务，在 IDE 中一次次地点击鼠标是愚蠢的行为。Maven 是这方面的专家，而且主流 IDE 都集成了 Maven，我们可以在 IDE 中方便地运行 Maven 执行构建。\nMake\nMake 也许是最早的构建工具，它由 Stuart\nFeldman 于 1977 年在 Bell 实验室创建。Stuart\nFeldman 也因此于 2003 年获得了 ACM 国际计算机组织颁发的软件系统奖。目前 Make 有很多衍生实现，包括最流行的 GNU\nMake 和 BSD Make，还有 Windows 平台的 Microsoft nmake 等。\nMake 由一个名为 Makefile 的脚本文件驱动，该文件使用 Make 自己定义的语法格式。其基本组成部分为一系列规则（Rules），而每一条规则又包括目标（Target）、依赖（Prerequisite）和命令（Command）。Makefile 的基本结构如下：\nTARGET… : PREREQUISITE…  COMMAND  …  …  ```  Make通过一系列目标和依赖将整个构建过程串联起来，同时利用本地命令完成每个目标的实际行为。**Make的强大之处在于它可以利用所有系统的本地命令，尤其是UNIX/Linux系统，丰富的功能、强大的命令能够帮助Make快速高效地完成任务。**但是，Make将自己和操作系统绑定在一起了。也就是说，使用Make，就不能实现（至少很难）跨平台的构建，这对于Java来说是非常不友好的。此外，Makefile的语法也成问题，很多人抱怨Make构建失败的原因往往是一个难以发现的空格或Tab使用错误。#### AntAnt不是指蚂蚁，而是意指“另一个整洁的工具”（Another Neat Tool），它最早用来构建著名的Tomcat，其作者James Duncan Davidson创作它的动机就是因为受不了Makefile的语法格式。我们可以将Ant看成是一个Java版本的Make，也正因为使用了Java，Ant是跨平台的。此外，Ant使用XML定义构建脚本，相对于Makefile来说，这也更加友好。与Make类似，Ant有一个构建脚本build.xml，如下所示：  ```xml  &lt;?xml version=\"1.0\"?&gt;  &lt;project name=\"Hello\" default=\"compile\"&gt;    &lt;target name=\"compile\" description=\"compile the Java source code to class files\"&gt;      &lt;mkdir dir=\"classes\"/&gt;      &lt;javac srcdir=\".\" destdir=\"classes\"/&gt;    &lt;/target&gt;    &lt;target name=\"jar\" depends=\"compile\" description=\"create a Jar file \"&gt;      &lt;jar destfile=\"hello.jar\"&gt;        &lt;fileset dir=\"classes\" includes=\"**/*.class\"/&gt;        &lt;manifest&gt;          &lt;attribute name=\"Main.Class\" value=\"HelloProgram\"/&gt;        &lt;/manifest&gt;      &lt;/jar&gt;    &lt;/target&gt;  &lt;/project&gt;  \nbuild.xml 的基本结构也是目标（target）、依赖（depends），以及实现目标的任务。比如在上面的脚本中，jar 目标用来创建应用程序 jar 文件，该目标依赖于 compile 目标，后者执行的任务是创建一个名为 classes 的文件夹，编译当前目录的 java 文件至 classes 目录。compile 目标完成后，jar 目标再执行自己的任务。Ant 有大量内置的用 Java 实现的任务，这保证了其跨平台的特质，同时，Ant 也有特殊的任务 exec 来执行本地命令。\n和 Make 一样，Ant 也都是过程式的，开发者显式地指定每一个目标，以及完成该目标所需要执行的任务。针对每一个项目，开发者都需要重新编写这一过程，这里其实隐含着很大的重复。\nMaven 是声明式的，项目构建过程和过程各个阶段所需的工作都由插件实现，并且大部分插件都是现成的，开发者只需要声明项目的基本元素，Maven 就执行内置的、完整的构建过程。这在很大程度上消除了重复。\nAnt 是没有依赖管理的，所以很长一段时间 Ant 用户都不得不手工管理依赖，这是一个令人头疼的问题。幸运的是，Ant 用户现在可以借助 Ivy 管理依赖。而对于 Maven 用户来说，依赖管理是理所当然的，Maven 不仅内置了依赖管理，更有一个可能拥有全世界最多 Java 开源软件包的中央仓库，Maven 用户无须进行任何配置就可以直接享用。\n不重复发明轮子\n【该小节内容整理自网友 Arthas 最早在 Maven 中文 MSN 的群内的讨论，在此表示感谢】\n小张是一家小型民营软件公司的程序员，他所在的公司要开发一个新的 Web 项目。经过协商，决定使用 Spring、iBatis 和 Tapstry。jar 包去哪里找呢？公司里估计没有人能把 Spring、iBatis 和 Tapstry 所使用的 jar 包一个不少地找出来。大家的做法是，先到 Spring 的站点上去找一个 spring.with.dependencies，然后去 iBatis 的网站上把所有列出来的 jar 包下载下来，对 Tapstry、Apache\ncommons 等执行同样的操作。项目还没有开始，WEB.INF/lib 下已经有近百个 jar 包了，带版本号的、不带版本号的、有用的、没用的、相冲突的，怎一个 “乱” 字了得！\n在项目开发过程中，小张不时地发现版本错误和版本冲突问题，他只能硬着头皮逐一解决。项目开发到一半，经理发现最终部署的应用的体积实在太大了，要求小张去掉一些没用的 jar 包，于是小张只能加班加点地一个个删……\n小张隐隐地觉得这些依赖需要一个框架或者系统来进行管理。\n小张喜欢学习流行的技术，前几年 Ant 十分流行，他学了，并成为了公司这方面的专家。小张知道，Ant 打包，无非就是创建目录，复制文件，编译源代码，使用一堆任务，如 copydir、fileset、classpath、ref、target，然后再 jar、zip、war，打包就成功了。\n项目经理发话了：“兄弟们，新项目来了，小张，你来写 Ant 脚本！”\n“是，保证完成任务！” 接着，小张继续创建一个新的 XML 文件。target clean;\ntarget compile; target jar; ……\n不知道他是否想过，在他写的这么多的 Ant 脚本中，有多少是重复劳动，有多少代码会在一个又一个项目中重现。既然都差不多，有些甚至完全相同，为什么每次都要重新编写？\n终于有一天，小张意识到了这个问题，想复用 Ant 脚本，于是在开会时他说：“以后就都用我这个规范的 Ant 脚本吧，新的项目只要遵循我定义的目录结构就可以了。” 经理听后觉得很有道理：“嗯，确实是个进步。”\n这时新来的研究生发言了：“经理，用 Maven 吧，这个在开源社区很流行，比 Ant 更方便。” 小张一听很惊讶，Maven 真比自己的 “规范化 Ant” 强大？其实他不知道自己只是在重新发明轮子，Maven 已经有一大把现成的插件，全世界都在用，你自己不用写任何代码！\nMaven 的安装和配置\n前面介绍了 Maven 是什么，以及为什么要使用 Maven，我们将从本章实际开始实际接触 Maven。本章首先将介绍如何在主流的操作系统下安装 Maven，并详细解释 Maven 的安装文件；其次还会介绍如何在主流的 IDE 中集成 Maven，以及 Maven 安装的最佳实践。\n在 Windows 上安装 Maven\n检查 JDK 安装\n在安装 Maven 之前，首先要确认你已经正确安装了 JDK。Maven 可以运行在 JDK\n1.4 及以上的版本上。本书的所有样例都基于 JDK\n5 及以上版本。打开 Windows 的命令行，运行如下的命令来检查你的 Java 安装：\nC:\\Users\\Juven Xu&gt;echo %JAVA_HOME%  \nC:\\Users\\Juven Xu&gt;java -version\n结果如下图所示：\n\n上述命令首先检查环境变量 JAVA_HOME 是否指向了正确的 JDK 目录，接着尝试运行 java 命令。如果 Windows 无法执行 java 命令，或者无法找到 JAVA_HOME 环境变量。你就需要检查 Java 是否安装了，或者环境变量是否设置正确。关于环境变量的设置，请参考 2.1.3 节。\n下载 Maven\n请访问 Maven 的下载页面：http://maven.apache.org/download.html，其中包含针对不同平台的各种版本的 Maven 下载文件。对于首次接触 Maven 的读者来说，推荐使用 Maven3.0，因此下载 apache-maven-3.0-bin.zip。当然，如果你对 Maven 的源代码感兴趣并想自己构建 Maven，还可以下载 apache-maven-3.0\n-src.zip。该下载页面还提供了 md5 校验和（checksum）文件和 asc 数字签名文件，可以用来检验 Maven 分发包的正确性和安全性。\n在本书编写的时候，Maven 2 的最新版本是 2.2.1，Maven 3 基本完全兼容 Maven\n2，而且较之于 Maven\n2 它性能更好，还有不少功能的改进，如果你之前一直使用 Maven\n2，现在正犹豫是否要升级，那就大可不必担心了，快点尝试下 Maven 3 吧！\n本地安装\n将安装文件解压到你指定的目录中，如：\nD:\\bin&gt;jar xvf \"C:\\Users\\Juven Xu\\Downloads\\apache-maven-3.0--bin.zip\"\n这里的 Maven 安装目录是 D:-maven-3.0，接着需要设置环境变量，将 Maven 安装配置到操作系统环境中。\n打开系统属性面板（桌面上右键单击 “我的电脑”→“属性”），点击高级系统设置，再点击环境变量，在系统变量中新建一个变量，变量名为 M2_HOME，变量值为 Maven 的安装目录 D:-maven-3.0。点击确定，接着在系统变量中找到一个名为 Path 的变量，在变量值的末尾加上 % M2_HOME%;，注意多个值之间需要有分号隔开，然后点击确定。至此，环境变量设置完成\n现在打开一个新的 cmd 窗口（这里强调新的窗口是因为新的环境变量配置需要新的 cmd 窗口才能生效），运行如下命令检查 Maven 的安装情况：\nC:\\Users\\Juven Xu&gt;echo %M2_HOME%  \nC:\\Users\\Juven Xu&gt;mvn -v\n第一条命令 echo\n% M2_HOME% 用来检查环境变量 M2_HOME 是否指向了正确的 Maven 安装目录；而 mvn\n–version 执行了第一条 Maven 命令，以检查 Windows 是否能够找到正确的 mvn 执行脚本。\n升级 Maven\nMaven 还比较年轻，更新比较频繁，因此用户往往会需要更新 Maven 安装以获得更多更酷的新特性，以及避免一些旧的 bug。\n在 Windows 上更新 Maven 非常简便，只需要下载新的 Maven 安装文件，解压至本地目录，然后更新 M2_HOME 环境变量便可。例如，假设 Maven 推出了新版本 3.1，我们将其下载然后解压至目录 D:-maven-3.1，接着遵照前一节描述的步骤编辑环境变量 M2_HOME，更改其值为 D:-maven-3.1。至此，更新就完成了。同理，如果你需要使用某一个旧版本的 Maven，也只需要编辑 M2_HOME 环境变量指向旧版本的安装目录。\n在基于 Unix 的系统上安装 Maven\nMaven 是跨平台的，它可以在任何一种主流的操作系统上运行，本节将介绍如何在基于 Unix 的系统（包括 Linux、Mac\nOS 以及 FreeBSD 等）上安装 Maven。\n下载和安装\n首先，与在 Windows 上安装 Maven 一样，需要检查 JAVA_HOME 环境变量以及 Java 命令，细节不再赘述，命令如下：\njuven@juven-ubuntu:~$ echo $JAVA_HOME  \njuven@juven-ubuntu:~$ java –version\n接着到 http://maven.apache.org/download.html\n下载 Maven 安装文件，如 apache-maven-3.0-bin.tar.gz，然后解压到本地目录：\njuven@juven-ubuntu:bin$ tar -xvzf apache-maven-3.0-bin.tar.gz\n现在已经创建好了一个 Maven 安装目录 apache-maven-3.0，虽然直接使用该目录配置环境变量之后就能使用 Maven 了，但这里我更推荐做法是，在安装目录旁平行地创建一个符号链接，以方便日后的升级：\njuven@juven-ubuntu:bin$ ln -s apache-maven-3.0 apache-maven  juven@juven-ubuntu:bin$ ls -l  total 4  lrwxrwxrwx 1 juven juven   18 2009-09-20 15:43 apache-maven -&gt; apache-maven-3.0  drwxr-xr-x 6 juven juven 4096 2009-09-20 15:39 apache-maven-3.0  \n接下来，我们需要设置 M2_HOME 环境变量指向符号链接 apache-maven-，并且把 Maven 安装目录下的 bin / 文件夹添加到系统环境变量 PATH 中去：\njuven@juven-ubuntu:bin$ export M2_HOME=/home/juven/bin/apache-maven  juven@juven-ubuntu:bin$ export PATH=$PATH:$M2_HOME/bin  ```  一般来说，需要将这两行命令加入到系统的登录shell脚本中去，以我现在的Ubuntu 8.10为例，编辑~/.bashrc文件，添加这两行命令。这样，每次启动一个终端，这些配置就能自动执行。至此，安装完成，我们可以运行以下命令检查Maven安装：    juven@juven-ubuntu:bin$ echo $M2_HOME      juven@juven-ubuntu:bin$ mvn –version#### 升级Maven**在基于Unix的系统上，可以利用符号链接这一工具来简化Maven的升级，不必像在Windows上那样，每次升级都必须更新环境变量。**前一小节中我们提到，解压Maven安装包到本地之后，平行地创建一个符号链接，然后在配置环境变量时引用该符号链接，这样做是为了方便升级。现在，假设我们需要升级到新的Maven 3.1版本，同理，将安装包解压到与前一版本平行的目录下，然后更新符号链接指向3.1版的目录便可：    juven@juven-ubuntu:bin$ rm apache-maven  juven@juven-ubuntu:bin$ ln -s apache-maven-3.1/ apache-maven      juven@juven-ubuntu:bin$ ls -l      total 8      lrwxrwxrwx 1 juven juven   17 2009-09-20 16:13 apache-maven -&gt; apache-maven-3.1 /      drwxr-xr-x 6 juven juven 4096 2009-09-20 15:39 apache-maven-3.0drwxr-xr-x 2 juven juven 4096 2009-09-20 16:09 apache-maven-3.1同理，可以很方便地切换到Maven的任意一个版本。现在升级完成了，可以运行mvn -v进行检查。### 安装目录分析  本章前面的内容讲述了如何在各种操作系统中安装和升级Maven。现在我们来仔细分析一下Maven的安装文件。#### M2_HOME前面我们讲到设置M2_HOME环境变量指向Maven的安装目录，本书之后所有使用M2_HOME的地方都指代了该安装目录，让我们看一下该目录的结构和内容：```  bin  boot  conf  lib  LICENSE.txt  NOTICE.txt  README.txt  \n\nBin：\n该目录包含了 mvn 运行的脚本，这些脚本用来配置 Java 命令，准备好 classpath 和相关的 Java 系统属性，然后执行 Java 命令。其中 mvn 是基于 UNIX 平台的 shell 脚本，mvn.bat 是基于 Windows 平台的 bat 脚本。在命令行输入任何一条 mvn 命令时，实际上就是在调用这些脚本。该目录还包含了 mvnDebug 和 mvnDebug.bat 两个文件，同样，前者是 UNIX 平台的 shell 脚本，后者是 windows 的 bat 脚本。那么 mvn 和 mvnDebug 有什么区别和关系呢？打开文件我们就可以看到，两者基本是一样的，只是 mvnDebug 多了一条 MAVEN_DEBUG_OPTS 配置，作用就是在运行 Maven 时开启 debug，以便调试 Maven 本身。此外，该目录还包含 m2.conf 文件，这是 classworlds 的配置文件，稍微会介绍 classworlds。\nBoot： 该目录只包含一个文件，以 maven\n3.0 为例，该文件为 plexus-classworlds-2.2.3.jar。plexus-classworlds 是一个类加载器框架，相对于默认的 java 类加载器，它提供了更丰富的语法以方便配置，Maven 使用该框架加载自己的类库。更多关于 classworlds 的信息请参考 http://classworlds.codehaus.org/。对于一般的 Maven 用户来说，不必关心该文件。\nConf：\n该目录包含了一个非常重要的文件 settings.xml。直接修改该文件，就能在机器上全局地定制 Maven 的行为。一般情况下，我们更偏向于复制该文件至 /.m2 / 目录下（这里表示用户目录），然后修改该文件，在用户范围定制 Maven 的行为。本书的后面将会多次提到该 settings.xml，并逐步分析其中的各个元素。\nLib：\n该目录包含了所有 Maven 运行时需要的 Java 类库，Maven 本身是分模块开发的，因此用户能看到诸如 mavn-core-3.0.jar、maven-model-3.0.jar 之类的文件，此外这里还包含一些 Maven 用到的第三方依赖如 common-cli-1.2.jar、google-collection-1.0.jar 等等。（对于 Maven\n2 来说，该目录只包含一个如 maven-2.2.1-uber.jar 的文件原本各为独立 JAR 文件的 Maven 模块和第三方类库都被拆解后重新合并到了这个 JAR 文件中）。可以说，这个 lib 目录就是真正的 Maven。关于该文件，还有一点值得一提的是，用户可以在这个目录中找到 Maven 内置的超级 POM，这一点在 8.5 小节详细解释。其他：\nLICENSE.txt 记录了 Maven 使用的软件许可证 Apache License Version 2.0；\nNOTICE.txt 记录了 Maven 包含的第三方软件；而 README.txt 则包含了 Maven 的简要介绍，包括安装需求及如何安装的简要指令等等。\n\n~/.m2\n在讲述该小节之前，我们先运行一条简单的命令：mvn\nhelp:system。该命令会打印出所有的 Java 系统属性和环境变量，这些信息对我们日常的编程工作很有帮助。这里暂不解释 help:system 涉及的语法，运行这条命令的目的是为了让 Maven 执行一个真正的任务。我们可以从命令行输出看到 Maven 会下载 maven-help-plugin，包括 pom 文件和 jar 文件。这些文件都被下载到了 Maven 本地仓库中。\n现在打开用户目录，比如当前的用户目录是 C:Xu，你可以在 Vista 和 Windows7 中找到类似的用户目录。如果是更早版本的 Windows，该目录应该类似于 C:and\nSettingsXu。在基于 Unix 的系统上，直接输入 cd\n回车，就可以转到用户目录。为了方便，本书统一使用符号～\n指代用户目录。\n在用户目录下，我们可以发现.m2 文件夹。默认情况下，该文件夹下放置了 Maven 本地仓库.m2/repository。所有的 Maven 构件（artifact）都被存储到该仓库中，以方便重用。我们可以到～/.m2/repository/org/apache/maven/plugins/maven-help-plugins/ 目录下找到刚才下载的 maven-help-plugin 的 pom 文件和 jar 文件。Maven 根据一套规则来确定任何一个构件在仓库中的位置，这一点本书第 6 章将会详细阐述。由于 Maven 仓库是通过简单文件系统透明地展示给 Maven 用户的，有些时候可以绕过 Maven 直接查看或修改仓库文件，在遇到疑难问题时，这往往十分有用。\n默认情况下，/.m2 目录下除了 repository 仓库之外就没有其他目录和文件了，不过大多数 Maven 用户需要复制 M2_HOME/conf/settings.xml 文件到 /.m2/settings.xml。这是一条最佳实践，我们将在本章最后一小节详细解释。\n设置 HTTP 代理\n有时候你所在的公司由于安全因素考虑，要求你使用通过安全认证的代理访问因特网。这种情况下，就需要为 Maven 配置 HTTP 代理，才能让它正常访问外部仓库，以下载所需要的资源。\n首先确认自己无法直接访问公共的 Maven 中央仓库，直接运行命令 ping\nrepo1.maven.org 可以检查网络。如果真的需要代理，先检查一下代理服务器是否畅通，比如现在有一个 IP 地址为 218.14.227.197，端口为 3128 的代理服务，我们可以运行 telnet\n218.14.227.197\n3128 来检测该地址的该端口是否畅通。如果得到出错信息，需要先获取正确的代理服务信息；如果 telnet 连接正确，则输入 ctrl+]，然后 q，回车，退出即可。\n检查完毕之后，编辑～/.m2/settings.xml 文件（如果没有该文件，则复制 $M2_HOME/conf/settings.xml）。添加代理配置如下：\n&lt;settings&gt;  …  &lt;proxies&gt;      &lt;proxy&gt;        &lt;id&gt;my-proxy&lt;/id&gt;        &lt;active&gt;true&lt;/active&gt;        &lt;protocol&gt;http&lt;/protocol&gt;        &lt;host&gt;218.14.227.197&lt;/host&gt;        &lt;port&gt;3128&lt;/port&gt;        &lt;!--        &lt;username&gt;***&lt;/username&gt;        &lt;password&gt;***&lt;/password&gt;        &lt;nonProxyHosts&gt;repository.mycom.com|*.google.com&lt;/nonProxyHosts&gt;        --&gt;      &lt;/proxy&gt;    &lt;/proxies&gt;    …  &lt;/settings&gt;  ```  这段配置十分简单，proxies下可以有多个proxy元素，如果你声明了多个proxy元素，则默认情况下第一个被激活的proxy会生效。这里声明了一个id为my-proxy的代理，active的值为true表示激活该代理，protocol表示使用的代理协议，这里是http。当然，最重要的是指定正确的主机名（host元素）和端口（port元素）。上述XML配置中我注释掉了username、password、nonProxyHost几个元素，当你的代理服务需要认证时，就需要配置username和password。nonProxyHost元素用来指定哪些主机名不需要代理，可以使用 | 符号来分隔多个主机名。此外，该配置也支持通配符，如*.google.com表示所有以google.com结尾的域名访问都不要通过代理。### 安装m2eclipse  Eclipse是一款非常优秀的IDE。除了基本的语法标亮、代码补齐、XML编辑等基本功能外，最新版的Eclipse还能很好地支持重构，并且集成了JUnit、CVS、Mylyn等各种流行工具。可惜Eclipse默认没有集成对Maven的支持。幸运的是，由Maven之父Jason Van Zyl创立的Sonatype公司建立了m2eclipse项目，这是Eclipse下的一款十分强大的Maven插件，可以访问http://m2eclipse.sonatype.org/ 了解更多该项目的信息。本小节将先介绍如何安装m2eclipse插件，本书后续的章节会逐步介绍m2eclipse插件的使用。现在我以Eclipse 3.6为例逐步讲解m2eclipse的安装。启动Eclipse之后，在菜单栏中选择Help，然后选择Install New Software…，接着你会看到一个Install对话框，点击Work with:字段边上的Add按钮，你会得到一个新的Add Repository对话框，在Name字段中输入m2e，Location字段中输入http://m2eclipse.sonatype.org/sites/m2e，然后点击OK。Eclipse会下载m2eclipse安装站点上的资源信息。等待资源载入完成之后，我们再将其全部展开，就能看到下图所示的界面：![][3]如图显示了m2eclipse的核心模块Maven Integration for Eclipse (Required)，选择后点击Next &gt;，Eclipse会自动计算模块间依赖，然后给出一个将被安装的模块列表，确认无误后，继续点击Next &gt;，这时我们会看到许可证信息，m2eclipse使用的开源许可证是Eclipse Public License v1.0，选择I accept the terms of the license agreements，然后点击Finish，接着就耐心等待Eclipse下载安装这些模块，如下图所示：![][4]除了核心组件之外，m2eclipse还提供了一组额外组件，主要是为了方便与其它工具如Subversion进行集成，这些组件的安装地址为http://m2eclipse.sonatype.org/sites/m2e-extras。使用前面类似的安装方法，我们可以看到如下图的组件列表：![][5]下面简单解释一下这些组件的用途：**1.重要的**   Maven SCM handler for Subclipse(Optional）：Subversion是非常流行的版本管理工具，该模块能够帮助我们直接从Subversion服务器签出Maven项目，不过前提是需要首先安装Subclipse（http://subclipse.tigris.org/）。   Maven SCM Integration (Optional）：Eclipse环境中Maven与SCM集成核心的模块，它利用各种SCM工具如SVN实现Maven项目的签出和具体化等操作。**2. 不重要的**   Maven issue tracking configurator for Mylyn 3.x (Optional）：该模块能够帮助我们使用POM中的缺陷跟踪系统信息连接Mylyn至服务器。   Maven SCM handler for Team/CVS (Optional）：该模块帮助我们从CVS服务器签出Maven项目，如果你还在使用CVS，就需要安装它。   Maven Integration for WTP (Optional）：使用该模块可以让Eclipse自动读取POM信息并配置WTP项目。、   M2eclipse Extensions Development Support (Optional)：用来支持扩展m2eclipse，一般用户不会用到。   Project configurators for commonly used maven plugins (temporary)：一个临时的组件，用来支持一些Maven插件与Eclipse的集成，建议安装。  读者可以根据自己的需要安装相应组件，具体步骤不再赘述。待安装完毕后，重启Eclipse，现在让我们验证一下m2eclipse是否正确安装了。首先，点击菜单栏中的Help，然后选择About Eclipse，在弹出的对话框中，点击Installation Details按钮，会得到一个对话框，在Installed Software标签栏中，检查刚才我们选择的模块是否在这个列表中  ![][6]  如果一切没问题，我们再检查一下Eclipse现在是否已经支持创建Maven项目，依次点击菜单栏中的File→New→Other，在弹出的对话框中，找到Maven一项，再将其展开，你应该能够看到如下所示的对话框：![][7]如果一切正常，说明m2eclipse已经正确安装了。最后，关于m2eclipse的安装，需要提醒的一点是，你可能会在使用m2eclipse时遇到类似这样的错误：09-10-6 上午01时14分49秒: Eclipse is running in a JRE, but a JDK is required  Some Maven plugins may not work when importing projects or updating source folders.这是因为**Eclipse默认是运行在JRE上的，而m2eclipse的一些功能要求使用JDK**，解决方法是配置Eclipse安装目录的eclipse.ini文件，添加vm配置指向JDK，如：    --launcher.XXMaxPermSize      256m      -vm      D:\\java\\jdk1.6.0_07\\bin\\javaw.exe      -vmargs      -Dosgi.requiredJavaVersion=1.5      -Xms128m      -Xmx256m### Maven安装最佳实践  本节介绍一些在安装Maven过程中不是必须的，但十分有用的实践。#### 设置MAVEN_OPTS环境变量本章前面介绍Maven安装目录时我们了解到，**运行mvn命令实际上是执行了Java命令，既然是运行Java，那么运行Java命令可用的参数当然也应该在运行mvn命令时可用**。这个时候，MAVEN_OPTS环境变量就能派上用场。我们通常需要设置MAVEN_OPTS的值为：-Xms128m -Xmx512m，因为**Java默认的最大可用内存往往不能够满足Maven运行的需要，比如在项目较大时，使用Maven生成项目站点需要占用大量的内存，如果没有该配置，我们很容易得到java.lang.OutOfMemeoryError。**因此，一开始就配置该变量是推荐的做法。关于如何设置环境变量，请参考前面设置M2_HOME环境变量的做法，尽量不要直接修改mvn.bat或者mvn这两个Maven执行脚本文件。因为如果修改了脚本文件，升级Maven时你就不得不再次修改，一来麻烦，二来容易忘记。同理，我们应该尽可能地不去修改任何Maven安装目录下的文件。#### 配置用户范围settings.xml**Maven用户可以选择配置$M2_HOME/conf/settings.xml或者~/.m2/settings.xml。前者是全局范围的，整台机器上的所有用户都会直接受到该配置的影响，而后者是用户范围的，只有当前用户才会受到该配置的影响。**我们推荐使用用户范围的settings.xml，主要原因是为了避免无意识地影响到系统中的其他用户。当然，如果你有切实的需求，需要统一系统中所有用户的settings.xml配置，当然应该使用全局范围的settings.xml。除了影响范围这一因素，配置用户范围settings.xml文件还便于Maven升级。直接修改conf目录下的settings.xml会导致Maven升级不便，每次升级到新版本的Maven，都需要复制settings.xml文件，如果使用~/.m2目录下的settings.xml，就不会影响到Maven安装文件，升级时就不需要触动settings.xml文件。#### 不要使用IDE内嵌的Maven无论是Eclipse还是NetBeans，当我们集成Maven时，都会安装上一个内嵌的Maven，这个内嵌的Maven通常会比较新，但不一定很稳定，而且往往也会和我们在命令行使用的Maven不是同一个版本。这里有会出现两个潜在的问题：首先，较新版本的Maven存在很多不稳定因素，容易造成一些难以理解的问题；其次，除了IDE，我们也经常还会使用命令行的Maven，如果版本不一致，容易造成构建行为的不一致，这是我们所不希望看到的。因此，我们应该在IDE中配置Maven插件时使用与命令行一致的Maven。在m2eclipse环境中，点击菜单栏中的Windows，然后选择Preferences，在弹出的对话框中，展开左边的Maven项，选择Installation子项，在右边的面板中，我们能够看到有一个默认的Embedded Maven安装被选中了，点击Add…然后选择我们的Maven安装目录M2_HOME，添加完毕之后选择这一个外部的Maven，如下图所示：  ![][8]  ### 小结  本章详细介绍了在各种操作系统平台上安装Maven，并对Maven安装目录进行了深入的分析，在命令行的基础上，本章又进一步介绍了Maven与主流IDE Eclipse及NetBeans的集成，本章最后还介绍了一些与Maven安装相关的最佳实践。本书下一章会创建一个Hello World项目，带领读者配置和构建Maven项目。## Maven使用入门  到目前为止，我们已经大概了解并安装好了Maven，现在，我们开始 创建一个最简单的Hello World项目。如果你是初次接触Maven，我建议你按照本章的内容一步步地编写代码并执行，可能你会碰到一些概念暂时难以理解，不用着急，记下这些疑 难点，相信本书的后续章节会帮你逐一解答。 **就像Make的Makefile，Ant的build.xml一样，Maven项目的核心是pom.xml。POM（Project Object Model，项目对象模型）定义了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。**### 编写pom.xml文件  现在我们先为Hello World项目编写一个最简单的pom.xml。首先创建一个名为hello-world的文件夹（本书中各章的代码都会对应一个以ch开头的项目），打开该文件夹，新建一个名为pom.xml的文件，输入其内容如代码清单3-1：代码清单3-1：Hello World的POM```xml  &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"           xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0  http://maven.apache.org/maven-v4_0_0.xsd\"&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.juvenxu.mvnbook&lt;/groupId&gt;    &lt;artifactId&gt;hello-world&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;name&gt;Maven Hello World Project&lt;/name&gt;  &lt;/project&gt;  \n代码的第一行是 XML 头，指定了该 xml 文档的版本和编码方式。紧接着是 project 元素，project 是所有 pom.xml 的根元素，它还声明了一些 POM 相关的命名空间及 xsd 元素，虽然这些属性不是必须的，但使用这些属性能够让第三方工具（如 IDE 中的 XML 编辑器）帮助我们快速编辑 POM。\n根元素下的第一个子元素 modelVersion 指定了当前 POM 模型的版本，对于 Maven2 及 Maven\n3 来说，它只能是 4.0.0。\n这段代码中最重要的是 groupId，artifactId 和 version 三行。这三个元素定义了一个项目基本的坐标，在 Maven 的世界，任何的 jar、pom 或者 war 都是以基于这些基本的坐标进行区分的。\n\ngroupId 定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联，譬如你在 googlecode 上建立了一个名为 myapp 的项目，那么 groupId 就应该是 com.googlecode.myapp，如果你的公司是 mycom，有一个项目为 myapp，那么 groupId 就应该是 com.mycom.myapp。本书中所有的代码都基于 groupId\ncom.juvenxu.mvnbook。\nartifactId 定义了当前 Maven 项目在组中唯一的 ID，我们为这个 Hello\nWorld 项目定义 artifactId 为 hello-world，本书其他章节代码会被分配其他的 artifactId。而在前面的 groupId 为 com.googlecode.myapp 的例子中，你可能会为不同的子项目（模块）分配 artifactId，如：myapp-util、myapp-domain、myapp-web 等等。\nversion 指定了 Hello\nWorld 项目当前的版本 ——1.0-SNAPSHOT。SNAPSHOT 意为快照，说明该项目还处于开发中，是不稳定的版本。随着项目的发展，version 会不断更新，如升级为 1.0、1.1-SNAPSHOT、1.1、2.0 等等。本书的 6.5 小节会详细介绍 SNAPSHOT，第 13 章介绍如何使用 Maven 管理项目版本的升级发布。\n\n最后一个 name 元素声明了一个对于用户更为友好的项目名称，虽然这不是必须的，但我还是推荐为每个 POM 声明 name，以方便信息交流。\n没有任何实际的 Java 代码，我们就能够定义一个 Maven 项目的 POM，这体现了 Maven 的一大优点，它能让项目对象模型最大程度地与实际代码相独立，我们可以称之为解耦，或者正交性，这在很大程度上避免了 Java 代码和 POM 代码的相互影响。比如当项目需要升级版本时，只需要修改 POM，而不需要更改 Java 代码；而在 POM 稳定之后，日常的 Java 代码开发工作基本不涉及 POM 的修改。\n编写主代码\n项目主代码和测试代码不同，项目的主代码会被打包到最终的构件中（比如 jar），而测试代码只在运行测试时用到，不会被打包。默认情况下，Maven 假设项目主代码位于 src/main/java 目录，我们遵循 Maven 的约定，创建该目录，然后在该目录下创建文件 com/juvenxu/mvnbook/helloworld/HelloWorld.java，其内容如代码清单 3-2：\n代码清单 3-2：Hello World 的主代码\npackage com.juvenxu.mvnbook.helloworld;  public class HelloWorld  {     public String sayHello()     {       return \"Hello Maven\";     }    public static void main(String[] args)    {    System.out.print( new HelloWorld().sayHello());    }  }  \n这是一个简单的 Java 类，它有一个 sayHello () 方法，返回一个 String。同时这个类还带有一个 main 方法，创建一个 HelloWorld 实例，调用 sayHello () 方法，并将结果输出到控制台。\n关于该 Java 代码有两点需要注意。首先，在 95% 以上的情况下，我们应该把项目主代码放到 src/main/java/ 目录下（遵循 Maven 的约定），而无须额外的配置，Maven 会自动搜寻该目录找到项目主代码。其次，该 Java 类的包名是 com.juvenxu.mvnbook.helloworld，这与我们之前在 POM 中定义的 groupId 和 artifactId 相吻合。一般来说，项目中 Java 类的包都应该基于项目的 groupId 和 artifactId，这样更加清晰，更加符合逻辑，也方便搜索构件或者 Java 类。\n代码编写完毕后，我们使用 Maven 进行编译，在项目根目录下运行命令\nmvn clean compile，我们会得到如下输出：\n[INFO] Scanning for projects...  \n[INFO] ------------------------------------------------------------------------  \n[INFO] Building Maven Hello World Project  \n[INFO]    task-segment: [clean, compile]  \n[INFO] ------------------------------------------------------------------------  \n[INFO] [clean:clean {execution: default-clean}]  \n[INFO] Deleting directory D:\\code\\hello-world\\target  \n[INFO] [resources:resources {execution: default-resources}]  \n[INFO] skip non existing resourceDirectory D: \\code\\hello-world\\src\\main\\resources  \n[INFO] [compiler:compile {execution: default-compile}]  \n[INFO] Compiling 1 source file to D: \\code\\hello-world\\target\\classes  \n[INFO] ------------------------------------------------------------------------  \n[INFO] BUILD SUCCESSFUL  \n[INFO] ------------------------------------------------------------------------  \n[INFO] Total time: 1 second  \n[INFO] Finished at: Fri Oct 09 02:08:09 CST 2009  \n[INFO] Final Memory: 9M/16M  \n[INFO] ------------------------------------------------------------------------  \nclean 告诉 Maven 清理输出目录 target，compile 告诉 Maven 编译项目主代码，从输出中我们看到 Maven 首先执行了 clean:clean 任务，删除 target / 目录，默认情况下 Maven 构建的所有输出都在 target / 目录中；接着执行 resources:resources 任务（未定义项目资源，暂且略过）；最后执行 compiler:compile 任务，将项目主代码编译至 target/classes 目录 (编译好的类为 com/juvenxu/mvnbook/helloworld/HelloWorld.Class）。\n上文提到的 clean:clean、resources:resources，以及 compiler:compile 对应了一些 Maven 插件及插件目标，比如 clean:clean 是 clean 插件的 clean 目标，compiler:compile 是 compiler 插件的 compile 目标，后文会详细讲述 Maven 插件及其编写方法。\n至此，Maven 在没有任何额外的配置的情况下就执行了项目的清理和编译任务，接下来，我们编写一些单元测试代码并让 Maven 执行自动化测试。\n编写测试代码\n为了使项目结构保持清晰，主代码与测试代码应该分别位于独立的目录中。3.2 节讲过 Maven 项目中默认的主代码目录是 src/main/java，对应地，Maven 项目中默认的测试代码目录是 src/test/java。因此，在编写测试用例之前，我们先创建该目录。\n在 Java 世界中，由 Kent Beck 和 Erich\nGamma 建立的 JUnit 是事实上的单元测试标准。要使用 JUnit，我们首先需要为 Hello\nWorld 项目添加一个 JUnit 依赖，修改项目的 POM 如代码清单 3-3：\n代码清单 3-3：为 Hello World 的 POM 添加依赖\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"           xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0  http://maven.apache.org/maven-v4_0_0.xsd\"&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.juvenxu.mvnbook&lt;/groupId&gt;    &lt;artifactId&gt;hello-world&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;name&gt;Maven Hello World Project&lt;/name&gt;    &lt;dependencies&gt;      &lt;dependency&gt;         &lt;groupId&gt;junit&lt;/groupId&gt;         &lt;artifactId&gt;junit&lt;/artifactId&gt;         &lt;version&gt;4.7&lt;/version&gt;         &lt;scope&gt;test&lt;/scope&gt;      &lt;/dependency&gt;    &lt;/dependencies&gt;  &lt;/project&gt;  \n代码中添加了 dependencies 元素，该元素下可以包含多个 dependency 元素以声明项目的依赖，这里我们添加了一个依赖 ——groupId 是 junit，artifactId 是 junit，version 是 4.7。前面我们提到 groupId、artifactId 和 version 是任何一个 Maven 项目最基本的坐标，JUnit 也不例外，有了这段声明，Maven 就能够自动下载 junit-4.7.jar。也许你会问，Maven 从哪里下载这个 jar 呢？在 Maven 之前，我们可以去 JUnit 的官网下载分发包。而现在有了 Maven，它会自动访问中央仓库（http://repo1.maven.org/maven2/），下载需要的文件。读者也可以自己访问该仓库，打开路径 junit/junit/4.7/，就能看到 junit-4.7.pom 和 junit-4.7.jar。本书第 6 章会详细介绍 Maven 仓库及中央仓库。\n上述 POM 代码中还有一个值为 test 的元素 scope，scope 为依赖范围，若依赖范围为 test 则表示该依赖只对测试有效，换句话说，测试代码中的 import\nJUnit 代码是没有问题的，但是如果我们在主代码中用 import\nJUnit 代码，就会造成编译错误。如果不声明依赖范围，那么默认值就是 compile，表示该依赖对主代码和测试代码都有效。\n配置了测试依赖，接着就可以编写测试类，回顾一下前面的 HelloWorld 类，现在我们要测试该类的 sayHello () 方法，检查其返回值是否为 “Hello\nMaven”。在 src/test/java 目录下创建文件，其内容如代码清单 3-4：\n代码清单 3-4：Hello World 的测试代码\npackage com.juvenxu.mvnbook.helloworld;  import static org.junit.Assert.assertEquals;  import org.junit.Test;  public class HelloWorldTest  {      @Test      public void testSayHello()      {          HelloWorld helloWorld = new HelloWorld();          String result = helloWorld.sayHello();          assertEquals( \"Hello Maven\", result );      }  }  \n一个典型的单元测试包含三个步骤：\n1. 准备测试类及数据；\n2. 执行要测试的行为；\n3. 检查结果。\n上述样例中，我们首先初始化了一个要测试的 HelloWorld 实例，接着执行该实例的 sayHello () 方法并保存结果到 result 变量中，最后使用 JUnit 框架的 Assert 类检查结果是否为我们期望的”Hello\nMaven”。在 JUnit\n3 中，约定所有需要执行测试的方法都以 test 开头，这里我们使用了 JUnit\n4，但我们仍然遵循这一约定，在 JUnit\n4 中，需要执行的测试方法都应该以 @Test 进行标注。\n测试用例编写完毕之后就可以调用 Maven 执行测试，运行\nmvn clean test ：\n[INFO] Scanning for projects...  [INFO] ------------------------------------------------------------------------  [INFO] Building Maven Hello World Project  [INFO]    task-segment: [clean, test]  [INFO] ------------------------------------------------------------------------  [INFO] [clean:clean {execution: default-clean}]  [INFO] Deleting directory D:\\git-juven\\mvnbook\\code\\hello-world\\target  [INFO] [resources:resources {execution: default-resources}]  …  Downloading: http://repo1.maven.org/maven2/junit/junit/4.7/junit-4.7.pom  1K downloaded  (junit-4.7.pom)  [INFO] [compiler:compile {execution: default-compile}]  [INFO] Compiling 1 source file to D: \\code\\hello-world\\target\\classes  [INFO] [resources:testResources {execution: default-testResources}]  …  Downloading: http://repo1.maven.org/maven2/junit/junit/4.7/junit-4.7.jar  226K downloaded  (junit-4.7.jar)  [INFO] [compiler:testCompile {execution: default-testCompile}]  [INFO] Compiling 1 source file to D:\\ code\\hello-world\\target\\test-classes  [INFO] ------------------------------------------------------------------------  [ERROR] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Compilation failure  D:\\code\\hello-world\\src\\test\\java\\com\\juvenxu\\mvnbook\\helloworld\\HelloWorldTest.java:[8,5] -source 1.3 中不支持注释  （请使用 -source 5 或更高版本以启用注释）      @Test  [INFO] ------------------------------------------------------------------------  [INFO] For more information, run Maven with the -e switch    …  \n不幸的是构建失败了，不过我们先耐心分析一下这段输出（为了本书的简洁，一些不重要的信息我用省略号略去了）。命令行输入的是 mvn\nclean\ntest，而 Maven 实际执行的可不止这两个任务，还有 clean:clean、resources:resources、compiler:compile、resources:testResources 以及 compiler:testCompile。暂时我们需要了解的是，在 Maven 执行测试（test）之前，它会先自动执行项目主资源处理，主代码编译，测试资源处理，测试代码编译等工作，这是 Maven 生命周期的一个特性，本书后续章节会详细解释 Maven 的生命周期。\n从输出中我们还看到：Maven 从中央仓库下载了 junit-4.7.pom 和 junit-4.7.jar 这两个文件到本地仓库（~/.m2/repository）中，供所有 Maven 项目使用。\n构建在执行 compiler:testCompile 任务的时候失败了，Maven 输出提示我们需要使用 - source\n5 或更高版本以启动注释，也就是前面提到的 JUnit\n4 的 @Test 注解。这是 Maven 初学者常常会遇到的一个问题。由于历史原因，Maven 的核心插件之一 compiler 插件默认只支持编译 Java\n1.3，因此我们需要配置该插件使其支持 Java 5，见代码清单 3-5：\n代码清单 3-5：配置 maven-compiler-plugin 支持 Java 5\n&lt;project&gt;  …    &lt;build&gt;      &lt;plugins&gt;         &lt;plugin&gt;           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;           &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;           &lt;configuration&gt;             &lt;source&gt;1.5&lt;/source&gt;             &lt;target&gt;1.5&lt;/target&gt;           &lt;/configuration&gt;         &lt;/plugin&gt;      &lt;/plugins&gt;    &lt;/build&gt;  …  &lt;/project&gt;  \n该 POM 省略了除插件配置以外的其他部分，我们暂且不去关心插件配置的细节，只需要知道 compiler 插件支持 Java\n5 的编译。现在再执行 mvn clean test，输出如下：\n[INFO] Compiling 1 source file to D: \\code\\hello-world\\target\\test-classes  \n[INFO] [surefire:test {execution: default-test}]  \n[INFO] Surefire report directory: D:\\code\\hello-world\\target\\surefire-reports  \n\n T E S T S  \n\nRunning com.juvenxu.mvnbook.helloworld.HelloWorldTest  \nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 sec  \nResults :  \nTests run: 1, Failures: 0, Errors: 0, Skipped: 0  \n[INFO]  \n[INFO] BUILD SUCCESSFUL  \n[INFO] \n我们看到 compiler:testCompile 任务执行成功了，测试代码通过编译之后在 target/test-classes 下生成了二进制文件，紧接着 surefire:test 任务运行测试，surefire 是 Maven 世界中负责执行测试的插件，这里它运行测试用例 HelloWorldTest，并且输出测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少。显然，我们的测试通过了 ——BUILD\nSUCCESSFUL。\n打包和运行\n将项目进行编译、测试之后，下一个重要步骤就是打包（package）。Hello\nWorld 的 POM 中没有指定打包类型，使用默认打包类型 jar，我们可以简单地执行命令\nmvn clean package 进行打包，可以看到如下输出：\n…  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  [INFO] [jar:jar {execution: default-jar}]  [INFO] Building jar: D:\\code\\hello-world\\target\\hello-world-1.0-SNAPSHOT.jar  [INFO]   [INFO] BUILD SUCCESSFUL  …  ```  类似地，Maven会在打包之前执行编译、测试等操作。这里我们看到jar:jar任务负责打包，实际上就是jar插件的jar目标将项目主代码打包成一个名为hello-world-1.0-SNAPSHOT.jar的文件，**该文件也位于target/输出目录中，它是根据artifact-version.jar规则进行命名的**，如有需要，我们还可以使用finalName来自定义该文件的名称，这里暂且不展开，本书后面会详细解释。至此，我们得到了项目的输出，如果有需要的话，就可以复制这个jar文件到其他项目的Classpath中从而使用HelloWorld类。但是，**如何才能让其他的Maven项目直接引用这个jar呢？**我们还需要一个安装的步骤，执行 `mvn clean install`：   [INFO] [jar:jar {execution: default-jar}]     [INFO] Building jar: D: \\code\\hello-world\\target\\hello-world-1.0-SNAPSHOT.jar     [INFO] [install:install {execution: default-install}]     [INFO] Installing D:\\code\\hello-world\\target\\hello-world-1.0-SNAPSHOT.jar to C:\\Users\\juven\\.m2\\repository\\com\\juvenxu\\mvnbook\\hello-world\\1.0-SNAPSHOT\\hello-world-1.0-SNAPSHOT.jar     [INFO]     [INFO] BUILD SUCCESSFUL  在打包之后，我们又执行了安装任务install:install，从输出我们看到该任务将项目输出的jar安装到了Maven本地仓库中，我们可以打开相应的文件夹看到Hello World项目的pom和jar。之前讲述JUnit的POM及jar的下载的时候，我们说只有构件被下载到本地仓库后，才能由所有Maven项目使用，这里是同样的道理，只有将Hello World的构件安装到本地仓库之后，其他Maven项目才能使用它。我们已经将体验了Maven最主要的命令：`mvn clean compile`、`mvn clean test`、`mvn clean package`、`mvn clean install`。**执行test之前是会先执行compile的，执行package之前是会先执行test的，而类似地，install之前会执行package**。我们可以在任何一个Maven项目中执行这些命令，而且我们已经清楚它们是用来做什么的。到目前为止，我们还没有运行Hello World项目，不要忘了HelloWorld类可是有一个main方法的。默认打包生成的jar是不能够直接运行的，因为带有main方法的类信息不会添加到manifest中(我们可以打开jar文件中的META-INF/MANIFEST.MF文件，将无法看到Main-Class一行)。为了生成可执行的jar文件，我们需要借助maven-shade-plugin，配置该插件如下：```  &lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;    &lt;version&gt;1.2.1&lt;/version&gt;    &lt;executions&gt;      &lt;execution&gt;        &lt;phase&gt;package&lt;/phase&gt;        &lt;goals&gt;          &lt;goal&gt;shade&lt;/goal&gt;        &lt;/goals&gt;        &lt;configuration&gt;          &lt;transformers&gt;            &lt;transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"&gt;            &lt;mainClass&gt;com.juvenxu.mvnbook.helloworld.HelloWorld&lt;/mainClass&gt;           &lt;/transformer&gt;         &lt;/transformers&gt;       &lt;/configuration&gt;       &lt;/execution&gt;    &lt;/executions&gt;  &lt;/plugin&gt;  ```  plugin元素在POM中的相对位置应该在`&lt;project&gt;&lt;build&gt;&lt;plugins&gt;`下面。我们配置了mainClass为com.juvenxu.mvnbook.helloworld.HelloWorld，项目在打包时会将该信息放到MANIFEST中。现在执行 mvn clean install ，待构建完成之后打开target/目录，我们可以看到hello-world-1.0-SNAPSHOT.jar和original-hello-world-1.0-SNAPSHOT.jar，前者是带有Main-Class信息的可运行jar，后者是原始的jar，打开hello-world-1.0-SNAPSHOT.jar的META-INF/MANIFEST.MF，可以看到它包含这样一行信息：    Main-Class: com.juvenxu.mvnbook.helloworld.HelloWorld现在，我们在项目根目录中执行该jar文件：    D: \\code\\hello-world&gt;java -jar target\\hello-world-1.0-SNAPSHOT.jar      Hello Maven控制台输出为 Hello Maven，这正是我们所期望的。本小节介绍了Hello World项目，侧重点是Maven而非Java代码本身，介绍了POM、Maven项目结构、以及如何编译、测试、打包，等等。### 使用Archetype生成项目骨架  Hello World项目中有一些Maven的约定：在项目的根目录中放置pom.xml，在src/main/java目录中放置项目的主代码，在src/test/java中放置项目的测试代码。我之所以一步一步地展示这些步骤，是为了能让可能是Maven初学者的你得到最实际的感受。我们称这些基本的目录结构和pom.xml文件内容称为项目的骨架，当你第一次创建项目骨架的时候，你还会饶有兴趣地去体会这些默认约定背后的思想，第二次，第三次，你也许还会满意自己的熟练程度，但第四、第五次做同样的事情，就会让程序员恼火了，为此**Maven提供了Archetype以帮助我们快速勾勒出项目骨架。**还是以Hello World为例，我们使用maven archetype来创建该项目的骨架，离开当前的Maven项目目录。  如果是Maven 3，简单的运行：    mvn archetype:generate如果是Maven 2，最好运行如下命令：    mvn org.apache.maven.plugins:maven-archetype-plugin:2.0-alpha-5:generate很多资料会让你直接使用更为简单的 mvn archetype:generate 命令，但在Maven2中这是不安全的，因为该命令没有指定archetype插件的版本，于是Maven会自动去下载最新的版本，进而可能得到不稳定的SNAPSHOT版本，导致运行失败。然而在Maven 3中，即使用户没有指定版本，Maven也只会解析最新的稳定版本，因此这是安全的，具体内容见7.7小节。我们实际上是在运行插件**maven-archetype-plugin**，注意冒号的分隔，其格式为 groupId:artifactId:version:goal ，org.apache.maven.plugins 是maven官方插件的groupId，maven-archetype-plugin 是archetype插件的artifactId，2.0-alpha-5 是目前该插件最新的稳定版，generate是我们要使用的插件目标。紧接着我们会看到一段长长的输出，有很多可用的archetype供我们选择，包括著名的Appfuse项目的archetype，JPA项目的archetype等等。每一个archetype前面都会对应有一个编号，同时命令行会提示一个默认的编号，其对应的archetype为maven-archetype-quickstart，我们直接回车以选择该archetype，紧接着Maven会提示我们输入要创建项目的groupId、artifactId、 version、以及包名package，如下输入并确认：```  Define value for groupId: : com.juvenxu.mvnbook  Define value for artifactId: : hello-world  Define value for version:  1.0-SNAPSHOT: :  Define value for package:  com.juvenxu.mvnbook: : com.juvenxu.mvnbook.helloworld  Confirm properties configuration:  groupId: com.juvenxu.mvnbook  artifactId: hello-world  version: 1.0-SNAPSHOT  package: com.juvenxu.mvnbook.helloworld   Y: : Y  \nArchetype 插件将根据我们提供的信息创建项目骨架。在当前目录下，Archetype 插件会创建一个名为 hello-world（我们定义的 artifactId）的子目录，从中可以看到项目的基本结构：基本的 pom.xml 已经被创建，里面包含了必要的信息以及一个 junit 依赖；主代码目录 src/main/java 已经被创建，在该目录下还有一个 Java 类 com.juvenxu.mvnbook.helloworld.App，注意这里使用到了我们刚才定义的包名，而这个类也仅仅只有一个简单的输出 Hello\nWorld! 的 main 方法；测试代码目录 src/test/java 也被创建好了，并且包含了一个测试用例 com.juvenxu.mvnbook.helloworld.AppTest。\nArchetype 可以帮助我们迅速地构建起项目的骨架，在前面的例子中，我们完全可以在 Archetype 生成的骨架的基础上开发 Hello\nWorld 项目以节省我们大量时间。\n此外，我们这里仅仅是看到了一个最简单的 archetype，如果你有很多项目拥有类似的自定义项目结构以及配置文件，你完全可以一劳永逸地开发自己的 archetype，然后在这些项目中使用自定义的 archetype 来快速生成项目骨架，本书后面的章节会详细阐述如何开发 Maven\nArchetype。\nm2eclipse 简单使用\n介绍前面 Hello\nWorld 项目的时候，我们并没有涉及 IDE，如此简单的一个项目，使用最简单的编辑器也能很快完成，但对于稍微大一些的项目来说，没有 IDE 就是不可想象的，本节我们先介绍 m2eclipse 的基本使用。\n导入 Maven 项目\n第 2 章介绍了如何安装 m2eclipse，现在，我们使用 m2ecilpse 导入 Hello\nWorld 项目。选择菜单项 File，然后选择 Import，我们会看到一个 Import 对话框，在该对话框中选择 General 目录下的 Maven\nProjects，然后点击 Next，就会出现 Import\nProjects 对话框，在该对话框中点击 Browse… 选择 Hello\nWorld 的根目录（即包含 pom.xml 文件的那个目录），这时对话框中的 Projects: 部分就会显示该目录包含的 Maven 项目，如下图所示：\n\n点击 Finish 之后，m2ecilpse 就会将该项目导入到当前的 workspace 中，导入完成之后，我们就可以在 Package\nExplorer 视图中看到如下图的项目结构：\n\n我们看到主代码目录 src/main/java 和测试代码目录 src/test/java 成了 Eclipse 中的资源目录，包和类的结构也十分清晰，当然 pom.xml 永远在项目的根目录下，而从这个视图中我们甚至还能看到项目的依赖 junit-4.7.jar，其实际的位置指向了 Maven 本地仓库（这里我自定义了 Maven 本地仓库地址为 D:，后续章节会介绍如何自定义本地仓库位置）。\n创建 Maven 项目\n创建一个 Maven 项目也十分简单，选择菜单项 File -&gt; New -&gt;\nOther，在弹出的对话框中选择 Maven 下的 Maven Project，然后点击 Next\n&gt;，在弹出的 New Maven\nProject 对话框中，我们使用默认的选项（不要选择 Create a simple\nproject 选项，那样我们就能使用 Maven Archetype），点击 Next\n&gt;，此时 m2eclipse 会提示我们选择一个 Archetype，我们选择 maven-archetype-quickstart，再点击 Next\n&gt;。由于 m2eclipse 实际上是在使用 maven-archetype-plugin 插件创建项目，因此这个步骤与上一节我们使用 archetype 创建项目骨架类似，输入 groupId,、artifactId、version、package（暂时我们不考虑 Properties），如下图所示：\n\n注意，为了不和前面已导入的 Hello\nWorld 项目产生冲突和混淆，我们使用不同的 artifactId 和 package。OK，点击 Finish，Maven 项目就创建完成了，其结构与前一个已导入的 Hello\nWorld 项目基本一致。\n运行 mvn 命令\n我们需要在命令行输入如 mvn clean\ninstall 之类的命令来执行 maven 构建，m2eclipse 中也有对应的功能，在 Maven 项目或者 pom.xml 上右击，再选择 Run\nAs，就能看到如下的常见 Maven 命令，如下图所示：\n\n选择想要执行的 Maven 命令就能执行相应的构建，同时我们也能在 Eclipse 的 console 中看到构建输出。这里常见的一个问题是，默认选项中没有我们想要执行的 Maven 命令怎么办？比如，默认带有 mvn\ntest，但我们想执行 mvn clean test，很简单，选择 Maven buid…\n以自定义 Maven 运行命令，在弹出对话框中的 Goals 一项中输入我们想要执行的命令，如 clean\ntest，设置一下 Name，点击 Run 即可。并且，下一次我们选择 Maven\nbuild，或者使用快捷键 Alt + Shift + X,\nM 快速执行 Maven 构建的时候，上次的配置直接就能在历史记录中找到。\n","categories":["工具使用"],"tags":["转载","Java","工具使用"]},{"title":"Practical Wisdom from \"The Almanack of Naval Ravikant\":Happiness","url":"/2023/05/28/Practical%20Wisdom%20from%20%22The%20Almanack%20of%20Naval%20Ravikant%22:%20Happiness/","content":"\"The Almanack of Naval\nRavikant\" is a book that compiles the wisdom and insights of\nentrepreneur and investor Naval Ravikant. This book mainly talks about\ntwo topics, wealth and happiness. It offers practical advice on how to\nlive a more fulfilling and purposeful life.\nThrough his own experiences and perspectives, Naval provides readers\nwith valuable insights into topics like success, motivation, and\npersonal growth, making the book a useful guide for anyone looking to\nimprove their life and achieve their goals\nI have benefited greatly from reading this book, and I want to write\nsome important takeaways from this book. As this book was written in\nEnglish, I want to give it a try to write it in English, too. Hoping it\nwill be beneficial to you, this passage is about the second topic:\nHappiness\nThe passage about the first topic: wealth, can be found here\n\nThe book claims that three big ones in life are wealth,\nhealth, and happiness. And usally we pursue them in that order,\nbut their importance is reverse.\nAs to the definition to happiness, it is just different to different\npeople at different age or situation\n\nHappiness is a very evolving thing.\nWhat’s happiness? The answer I would have given you a year ago will\nbe different than what I tell you now. And the answer that works for me\nis going to be nonsense to you, and vice versa. Whatever happiness means\nto me, it means something different to you.\n\nBut luckily, happiness is not something you inherit or even choose,\nbut a choice you make and a skill you develop, like\nfitness or nutrition.\nSo in the following we will focus on how to learn the skill of being\nhappy and avoid those enemies of hapiness.\nPresence and Peace\nHappiness requires presence and peace\nAt any given time, when we’re walking down the streets, a very small\npercentage of your brain is focused on the present. The rest is planning\nthe future or regretting the past.\nThis keeps you from having an incredible experience. It’s keeping you\nfrom seeing the beauty in everything and for being grateful for where\nyou are.\nSo what we should do is not to believe in anything from my past.\nAnything. No memories. No regrets. No people. No trips. Nothing.\nA lot of our unhappiness comes from comparing things from the\npast to the present.\nBesides presence, peace is also very important for happiness, because\nhappiness is more about peace than it is about joy.\nWhen a lot of people say happiness, they mean joy or bliss, but the\nauthor believes it more about peace.\nIf you ever just sit down and try and do nothing, nothing. Doing\nnothing, means not to read a book, not to listen to music, literally\njust sit down and do nothing. You will find you can’t do it, because\nthere’s anxiety always trying to make you get up and go, get up and go,\nget up and go. So it’s important just being aware the anxiety is\nmaking you unhappy. The anxiety is just a series of running\nthoughts.\nWhen facing anxiety, we don't need to fight it, we just need to be\naware we're anxious because of all these thoughts. We're actually making\na choice, “Would I rather be having this thought right now, or would I\nrather have my peace?”\nA happy person isn’t someone who’s happy all the time. It’s\nsomeone who effortlessly interprets events in such a way that they don’t\nlose their innate peace.\nDesire and Jealousy\nDesire and Jealousy are enemies of happiness\nAs humans, we are highly judgmental survival-and-replication\nmachines. We constantly walk around thinking, “I need this,” or “I need\nthat,” trapped in the web of desires.\n\nHappiness is what’s there when you remove the sense that something is\nmissing in your life.\n\nHappiness is the state when nothing is missing. When nothing is\nmissing, your mind shuts down and stops running into the past or future\nto regret something or to plan something.\nSo happiness is about the absence of desire,\nespecially the absence of desire for external things.\nThere is a fundamental delusion for many people: There is\nsomething out there that will make me happy and fulfilled\nforever.\nSuppose we bought a new car. Now we're waiting for the new car to\narrive. Of course, every night, we're on the forums reading about the\ncar. I know the instant the car arrives we won’t care about it anymore.\nThe thing is, we're addicted to the desiring. We're\naddicted to the idea of this external thing bringing me some kind of\nhappiness and joy, and this is completely delusional.\nNot to say you shouldn’t do things on the outside. You absolutely\nshould. You’re a living creature. You’re meant to do something. You’re\nnot just meant to lie there in the sand and meditate all day long. You\nshould self-actualize. You should do what you are meant to do.\nSo the key point is what kind of desire and what extent we\nshould achieve, but the book does not provide a definitive answer to\nthis or, in other words, the answer varies for each\nindividual.\nUsally, desire and jealousy are twins, they appear at the same time.\nWhen we are desired for something but do not own them yet, jealousy will\ncome up seeing other people who already get those things.\nJealousy was a very hard emotion to overcome. It’s such a poisonous\nemotion because, at the end of the day, you’re no better off with\njealousy. You’re unhappier, and the person you’re jealous of is still\nsuccessful or good-looking or whatever they are.\nWe should realize with all these people we were jealous of,\nwe couldn’t just choose little aspects of their\nlife.\nWe couldn’t say we want his body, we want her money, we want his\npersonality. You have to be that person. Do you want to actually be that\nperson with all of their reactions, their desires, their family, their\nhappiness level, their outlook on life, their self-image? If you’re not\nwilling to do a wholesale, 24/7, 100 percent swap with who that person\nis, then there is no point in being jealous.\nSingle-Player Game\nSocially, we’re told, “Go work out. Go look good.” That’s a\nmulti-player competitive game. Other people can see if\nI’m doing a good job or not. We’re told, “Go make money. Go buy a big\nhouse.” Again, external multiplayer competitive game.\nBut training yourself to be happy is completely\ninternal. There is no external progress, no external\nvalidation. You’re competing against yourself—it is a single-player\ngame.\nThe reality is life is a single-player game. You’re\nborn alone. You’re going to die alone. All of your interpretations are\nalone. All your memories are alone. You’re gone in three generations,\nand nobody cares. Before you showed up, nobody cared. It’s all single\nplayer.\nWe’re like bees or ants. We are such social creatures, we’re\nexternally programmed and driven. We don’t know how to play and win\nthese single-player games anymore. Perhaps one reason why yoga\nand meditation are hard to sustain is they have no extrinsic\nvalue. Purely single-player games.\nThere is no endpoint to self-awareness and\nself-discovery. It’s a lifelong process you hopefully keep\ngetting better and better at.\nFreedom from Expectations\nIf you hurt other people because they have expectations of you,\nthat’s their problem. If they have an agreement with you, it’s your\nproblem\nCourage is not caring what other people think.\nValue your time. It is all you have. It’s more important than your\nmoney. It’s more important than your friends. It is more important than\nanything. Your time is all you have. Do not waste your time.\nWhen reading this part in the book, it just reminds me of the speech\nof Steve Jobs in Standford\n\nYour time is limited, so don't waste it living someone else's life.\nDon't be trapped by dogma - which is living with the results of other\npeople's thinking.\nDon't let the noise of other's opinions drown out your own inner\nvoice. And most important, have the courage to follow your heart and\nintuition. They somehow already know what you truly want to become.\nEverything else is secondary.\n\nHappiness Habits\nAs mentioned above, happiness is skill, just like nutrition, dieting,\nworking. It's not what you are born with. It starts with realizing\nthey’re skills you can learn.\nThe most important trick to being happy is to realize happiness is a\nskill you develop and a choice you make. You choose to be happy, and\nthen you work at it. It’s just like building muscles.\nSo how does someone build the skill of happiness? This book offers\nsome advices for building habits\n\nBe very aware in every moment. If I catch myself\njudging somebody, I can stop myself and say, “What’s the positive\ninterpretation of this?”\n\nDropping caffeine and alcohol\n\nNot eating sugar will keep your mood more stable.\nNot going on Facebook, Snapchat, or Twitter will keep your mood more\nstable.\n\nWorking out every day, if you have peace of body,\nit’s easier to have peace of mind\n\nTell your friends you’re a happy person. Then, you’ll be forced to\nconform to it\n\nUse meditation, music, and exercise to reset your\nmood\n\nIncrease serotonin in the brain without drugs: Sunlight,\nexercise, positive thinking, and tryptophan\n\nBe grateful with what you have\n\n\nAs humans, we’re used to taking everything for granted. Like what you\nand I are doing right now. We’re sitting indoors, wearing clothes,\nwell-fed, and communicating with each other through space and time. We\nshould be two monkeys sitting in the jungle right now watching the sun\ngoing down, asking ourselves where we are going to sleep.\n\n\nStop Judging\n\n\nThe more you judge, the more you separate yourself.\nYou’ll feel good for an instant, because you feel good about yourself,\nthinking you’re better than someone. Later, you’re going to feel lonely.\nThen, you see negativity everywhere. The world just reflects your own\nfeelings back at you.\n\nAs to how to change a habit, the book give the follwing steps\n\nPick one thing. Cultivate a desire. Visualize it.\n\nPlan a sustainable path.\n\nIdentify needs, triggers, and substitutes.\n\nTell your friends.\n\nTrack meticulously.\n\nSelf-discipline is a bridge to a new self-image.\n\nBake in the new self-image. It’s who you are—now\n\n\nFirst, you know it. Then, you understand it. Then, you can explain\nit. Then, you can feel it. Finally, you are it.\n\nCare for Yourself\nWe can depend on nobody but just ourselves,\n\nDoctors won’t make you healthy.\nNutritionists won’t make you slim.\nTeachers won’t make you smart.\nGurus won’t make you calm.\nMentors won’t make you rich.\nTrainers won’t make you fit.\nUltimately, you have to take responsibility.\n\nThe number one priority in life, above happiness, above family and\nwork, is health, which can be mainly summarized as two parts:\nphysical health and mental health\nPhysical Health\n\nTo have peace of mind, you have to have peace of body first.\n\n\nDiet\n\nA correct diet should probably look closer to a paleo diet, mostly\neating vegetables with a small amount of meat and berries\nThe combination of sugar and fat together is really deadly. You’ve\ngot to watch out for that in your diet\nMost fit and healthy people focus much more on what they eat than how\nmuch. Quality control is easier than (and leads to) quantity control\n\nExercise\n\nFor the question: What habit would you say most positively impacts\nyour life? Navel gave the answer as follows\n\nThe daily morning workout. That has been a complete game-changer.\nIt’s made me feel healthier, younger. It’s made me not go out late.\nThe harder the workout, the easier the day.\n\nMake hard choices instead of easy ones, you will be easy in\nthe long term\nIf you are making the hard choices right now in what to eat, you’re\nnot eating all the junk food you want, and making the hard choice to\nwork out. Your life long-term will be easy. You won’t be sick. You won’t\nbe unhealthy.\nThe same is true of values. The same is true of saving up for a rainy\nday. The same is true of how you approach your relationships. If you\nmake the easy choices right now, your overall life will be a lot\nharder\nMental Health\nIn this part, this book highlights the importance of meditation for\nmetal health\nFor your entire life, things have been happening to you. Some good,\nsome bad, most of which you have processed and dissolved, but a few\nstuck with you. Over time, more and more stuck with you, and they almost\nbecame like these barnacles stuck to you.\nYou lost your childhood sense of wonder and of being present and\nhappy. You lost your inner happiness because you built up this\npersonality of unresolved pain, errors, fears, and desires that glommed\nonto you like a bunch of barnacles.\nJust like too much sugar leads to a heavy body, and too many\ndistractions lead to a heavy mind. Meditation is the\nintermittent fasting for the mind.\nOne kind of meditation recommended by Navel is called\nChoiceless Awareness, or Nonjudgmental Awareness\nAs you’re going about your daily business and you’re not talking to\nanybody else, you practice learning to accept the moment you’re in\nwithout making judgments. You don’t think, “Oh, there’s a homeless guy\nover there, better cross the street”or look at someone running by and\nsay, “He’s out of shape, and I’m in better shape than him.”\nYou don’t make any decisions. You don’t judge anything. You just\naccept everything. Naval said if he does that for ten or fifteen minutes\nwhile walking around, he will end up in a very peaceful, grateful\nstate.\nAnother kind of meditation is transcendental\nmeditation, which is where you’re using repetitive chanting to\ncreate a white noise in your head to bury your thoughts.\nYou can just very keenly and very alertly be aware of your thoughts\nas they happen. As you watch your thoughts, you realize how many of them\nare fear-based. The moment you recognize a fear, without even trying it\ngoes away. After a while, your mind quiets.\nThe book recommends if you really want to try meditation, try sixty\ndays of one hour a day, first thing in the morning. After about sixty\ndays, you will be tired of listening to your own mind. You will have\nresolved a lot of issues, or you have heard them enough to see through\nthose fears and issues.\nI think it may be a little hard for people working in modern society\nto spend extra one hour in the morning, when they have to get up early\nfor work and may need work late at night. But there is a very useful\nlife-hack: When in bed, meditate. Either you will have\na deep meditation or fall asleep. Victory either way.\nMeditation doesn’t mean you’re suddenly going to gain the superpower\nto control your internal state. The advantage of meditation is\nrecognizing just how out of control your mind is.\nRealizing the fact will drag you out of all those fantasy-future\nplanning or uncontrollable fear about the future. It will bring you to\nthe present, which is the requirement for happiness.\n\nInsight meditation lets you run your brain in debug mode until you\nrealize you’re just a subroutine in a larger program.\n\nBesides meditation, another tip in this book is cold\nshowers.\nIt's hard for many people, and the book recommends a method called\nWim Hof breathing method. It involves hyperventlating to get more oxygen\ninto your blood, which raises your core temperature. Then, you can go\ninto the shower.\nThe first few cold showers may be scary, but sooner it will not seem\nso hard. A very important lesson from this: most of our\nsuffering comes from avoidance. Most of the suffering from a\ncold shower is the tip-toeing your way in. Once you’re in, you’re in.\nIt’s not suffering.\nHere is a video of a uploader on bilibili about cold showers，which\nrecords the change of his body with cold showers in a series of 30 days.\nIt also explains some principles behind cold showers 【连续洗 30 天冷水澡】我的身体发生了什么变化？\nPhilosophy\nWhen it comes to real happiness in life, we can not avoid some topics\nrelated to philosophy\nMeaning of Life\nThis is a big question, and almost anyone will ask themselves this\nquestion in their lives, especially when they come to their 30s.\nThere is no standark answer to this just like the definition of\nhappiness. But one possible answer to this is there's no meaning\nof life, or it's personal if exists.\nAnything you do will fade. It will disappear, just like the human\nrace will disappear and the planet will disappear. No one is going to\nremember you past a certain number of generations, whether you’re an\nartist, a poet, a conqueror, a pauper, or anyone else. There’s no\nmeaning.\nFrom another aspect, however, it indicates the truth that you have to\ncreate your own meaning, because it will be personal if it really\nexists. Any piece of wisdom anybody else gives you, whether it’s Buddha\nor the book, is going to sound like nonsense. Fundamentally, you have to\nfind it for yourself, so the important part is not the answer, it’s the\nquestion. You just have to sit there and dig with the question. It might\ntake you years or decades. When you find an answer you’re happy with, it\nwill be fundamental to your life.\nUsally it is related to relationship with people around us, after all\nHumanity\nis a the summation of social relationships\nLive by Your Values\nThe book listed some core values of Naval, again, it just varies with\ndifferent people.\nHonesty is a core, core, core value.If I disconnect\nwhat I’m thinking from what I’m saying, it creates multiple threads in\nmy mind. I’m no longer in the moment—now I have to be future-planning or\npast-regretting every time I talk to somebody.\nPlay long-term games with long-term people. All benefits in life come\nfrom compound interest, whether in money,\nrelationships, love, health, activities, or habits. I only want to be\naround people I know I’m going to be around for the rest of my life. I\nonly want to work on things I know have long-term payout.\nDon’t believe in anger anymore. Anger was good when I was young and\nfull of testosterone, but now I like the Buddhist saying, “Anger\nis a hot coal you hold in your hand while waiting to throw it at\nsomebody.” I don’t want to be angry, and I don’t want to be\naround angry people. I just cut them out of my life.\nPresence is All We Have\nThere is actually nothing but this moment.\nNo one has ever gone back in time, and no one has ever been able to\nsuccessfully predict the future in any way that matters.\nLiterally, the only thing that exists is this exact point\nwhere you are in space at the exact time you happen to be\nhere.\nPerhaps we’re dying and being reborn at every\nmoment. It’s up to you whether to forget or remember that.\nSummary\nThis passage is about the second topic of this book: happiness\nIt's the most important thing in life, or we can say it's the\nultimate goal. Because when we chase for wealth, power, health etc,\nwe're not chasing these things themselves, but the happiness we belive\nwe will get when we own these things.\nHowever, these things may not bring real happiness. The book belives\nthat happiness requires presence and peace, while desire and jealousy\nare enemies of it. Finally, we will realize that life is a single-player\ngame. There's no one else except for yourself, so just free yourself\nfrom other people's expections.\nSome useful and pratical habits for happiness are recommended in the\nbook, I think we can follow them just from now on. Among them, I belive\nthe habits for health is extremely important, inlucding physical health\nand mental health.\nBecause to have peace of mind, you have to have peace of body first,\ndiet, exercise and sleeping are important on this. As to mental health,\nmeditation is highly recommended.\nOn the journey of chasing happiness, we may eventually come to the\nrealization that there's no meaning of life, or it's personal if exists.\nLife is just what it is, we can only construct the meaning for ourselves\nliving every moment to the best with people around us, because presence\nis all we have and humanity is a the summation of social\nrelationships.\nHope we can all find it someday\n","categories":["读书"],"tags":["纳瓦尔宝典","读书"]},{"title":"Multi-channel Budget Allocation and Bidding","url":"/2024/12/22/Multi-Channel%20Budget%20Allocation%20and%20Bidding/","content":"最近一段时间在研究 multi-channel bidding 的问题，套用 Google Ads 的 Power\nMore Conversions and Value through Cross-Channel Bid Optimization\n是这么定义这个问题的\n\nTraditionally, advertisers have applied automated bidding to\ncampaigns that target a single channel. For example, they might use a\nbid strategy that maximizes conversion value on separate campaigns for\nSearch, Display, and Video. But there are limitations to this siloed\napproach. But multi-channel bid optimization can help you to drive\nbetter results compared to single-channel bid optimization by maximizing\nmarginal CPA or ROAS in each and every auction\n\n简单来说，就是一个计划在更多流量位上同时投放，能够更好优化预算的边际收益（marginal\nCPA or\nROAS），这个也很好理解，因为有了更丰富的流量库存，同一笔预算效率理论上是能做到更优的；这跟国内当前各个媒体平台近年提的 “通投”、“全站投放” 等产品，本质上也是类似的。这类产品广告主提供了使用门槛更低的产品，省去了广告主在不同\nchannel\n分配预算或设置出价的过程，同时平台能够通过算法能力把预算的效率变得更高\n从技术视角上来看，这里的 multi-channel 有两个问题值得讨论\n（1）统一出价是否是最优的？如果不是，分 channel 出价要怎么做\n（2）预算是否要显式分配到各个 channel\n上面讨论的 multi-channel 的例子都是在一个大的平台下的\nchannel，这种情况下各 channel\n的预算和出价是比较容易共享的；而另外一个常见的 multi-channel\n的定义是跨平台的，比如说很多广告主会同时在 google 和 meta 两个 channel\n投放，这个时候显然预算和出价是不能共享的；那站在广告主的视角下，怎么分配是最优的也是一个值得讨论的问题\n本文主要讨论前者，即在同一平台的多个 channel 同时投放时，budget\nallocation 和 bidding\n的相关问题；同时会提一下在不同平台（channel）投放时的一些研究，祝开卷有益～\n\n上面提到，在同一平台上的多 channel\n投放，有两个问题需要解决，一是出价，即是否要分 channel\n独立出价；二是预算分配，即是否要限制各个 channel\n使用的预算，下面分别谈一下这两部分\n统一出价 vs 分 channel 出价\n在 multi-channel 中常见的一个问题是，统一出价（即在各个 channel\n中使用同一个出价）是否是最优的\n直观来看，把所有 channel\n当做一个整体的广告位，然后统一来出价似乎是比较合理的？但如果进一步分析，结论不一定如此；可以粗略把出价产品分为成本类出价和非成本类出价\n对于成本类出价（如 target-cost、cost-cap），往往是基于后验做\nbidding，这一做法本质上是在为模型预估不准做兜底；当各个 channel\n的模型预估准确时，统一出价是最优的，但这一点往往很难成立，更常见的情况是在\nchannel A 高估，在 channel B\n低估，或者高估程度不太一致；而如果仅靠一个统一的出价，无法很好处理各个\nchannel 高低估不一致的问题，比如说统一提价能解决低估的 channel\n问题，但是会加剧高估 channel 的问题\n非成本类出价（如 lowest-cost）没有上面的问题，但非成本类往往会通过\nbudget allocation 来间接控制成本，而 budget allocation\n曲线的一个很重要的信号就是 channel\n的流量分布，会倾向于把预算更多分配到流量更多的时间片，而各个 channel\n的流量分布往往是不一致的，即各个 channel\n的最优预算分配曲线往往也是不一致的\n因此，无论是成本类出价还是非成本类出价，比较合理的的做法往往是各个\nchannel 独立出价，而这又有常见的两种方式\n（1）所有 channel 的出价都独立，即独立搜集数据，独立 pacing\n（2）基于所有 channel 的后验计算出一个基础的出价，然后各个 channel\n基础的出价上再做扰动\n选择哪种方式，也跟出价类型、数据稀疏程度等因素有关\n成本类出价理论上两种方式都可用，如 target-cost 和 cost-cap\n这一类产品，广告主是有明确的成本目标 / 限制的，各个 channel\n只要锚定广告主明确的诉求，其实就可独立做出价，这里还有个问题是数据的稀疏程度，方式（1）有效的前提是各个\nchannel 的后验数据都比较充足，否则数据过于稀疏冷启问题会比较严重\n而对于 lowest-cost 这一类非成本类出价的产品，使用方式 1\n就有点困难了，主要原因有两个\n一是方式 1 要求每个 channel\n的预算是明确的，但这在通投场景是没有的（可通过算法分配一个，后面预算分配会提到）\n二是 nobid 虽然没有明确的成本约束，但是 channel\n之间的成本差异不能过大，一个很直观的想法是，成本更高的 channel\n向成本更低的 channel 的成本看齐，这样会就是一个动态的 target-cost\n产品，且预算花完是没有保证的；因此实际中往往是一个主 channel\n负责把钱花完，其他 channel 把成本对齐主 channel\n综上，这部分基本的观点是分 channel 出价在实际中往往是更优的，而分\nchannel 出价是否要做到各 channel\n完全独立，取决于多种因素。在广告主明确表达了成本的产品上，理论上是可以做到完全独立出价的，但需要各\nchannel 后验数据充足；lowest-cost 因为没有显式的分 channel\n预算，做独立出价有点难，最直观的做法是在主 channel\n的出价基础上做一些扰，做成一个动态 target-cost （主 channel\n的成本）的产品，后面也会提到的一个做法是在不同 channel\n之间做预算分配\n预算分配\n预算分配跟出价是紧密相关的，尤其是对于 lowest-cost\n这一类出价，预算分配是最主要的调控手段；这里的预算分配的主要问题是预算是否要显式分到各个\nchannel；而这部分的做法同样跟具体的出价类型相关\n成本类出价\n对于 target-cost 或 cost-cap 这一类有广告主明确的 given_cpa\n的出价，由于是有明确的成本目标，理论上是可以共享预算，即各个 channel\n都锚着固定成本去 pacing\n后验成本达到这个目标，这种情况下不需要对预算做显式的划分，哪个 channel\n能在 given_cpa\n的约束下能花更多的钱，这个渠道就是更优的；不过共享预算 + 基于后验 pacing\n的 target-cost 也有一些其他问题\n除了上面说的数据稀疏，基于后验成本 pacing\n的成本类出价一直面临的另一个挑战是，后验数据回传延迟（delayed\nfeedback）问题，这个问题会导致当前计算的后验成本不准确，往往需要建模来解决（模型也会面临这个问题，关于这个问题的一些解决方法可参考 Delayed\nFeedBack In Computational Advertising）\n也有业界的做法是做了预算的划分的，比如说这篇 paper 《Spending\nProgrammed Bidding: Privacy-friendly Bid Optimization with ROI\nConstraint in Online\nAdvertising》，就是对成本类出价的预算做了划分，并且通过实验验证效果是更优，文章建模的思路挺值得学习的\nSPB\n大致的做法就是先基于后验数据（不一定要非常实时，整体还是要考虑数据的时效性与有效性的\ntrade_off），离线建模 cost2convert 的函数，然后基于 given_cpa 计算一个\nbudget，最后基于预算分配曲线把 budget\n花完，这样理论上收到上面的影响会更小，paper 中的是这么说的\n\nSPB is a two-stage framework that separates long horizon delivery\nspend planning (the macro stage) and short horizon bidding execution\n(the micro stage). The macro stage models the target ROI to achieve\nmaximum utility and derives the expected spend, whereas the micro stage\noptimizes the bid price given the expected spend.\n\n这个方法的关键点是建模 cost2convert 的函数 \\(f(cost) =\nconvert\\)，常见的一个先验假设是随着 cost\n增加，转化数的增加速度逐渐减小，或者说边际成本是逐步增加的，如下图所示（横轴是消耗，纵轴是转化数）；这个假设也比较直观，因为对于一个计划而言，便宜的转化拿完了，需要出更高的价才能拿得到转化，转化成本是随之增加的\n\n根据上面的定义，可以得到成本的公式为 \\(\\frac{cost}{f(cost)}\\),\n而边际成本则是上面曲线的斜率的倒数即 \\(\\frac{1}{f'(cost)}\\)，基于上面的边际成本随着\ncost 逐步增加的假设，可以进一步假设边际成本与转化数 \\(f(cost)\\)\n成线性关系（实际中不一定是线性，保证是递增的关系即可）, 如下所示，其中\n\\(\\omega\\) 和 \\(b\\) 是需要拟合得到的参数\n\\[\\frac{1}{f'(cost)} = \\omega *\nf(cost) + b\\]\n通过如下推导\n\n可以得到 \\(f(cost)\\)\n的形式为 (这里省略了常数 \\(C\\), 且因为\n\\(f(cost)\\)\n不会为负数，因此只取了为正数的那一项)\n\\[f(cost) = \\frac{\\sqrt{2 * \\omega * cost\n+ b^2} - b}{\\omega}\\]\n上面的推导，其实也可以直接基于边际成本上升直接给 \\(f(cost)\\) 一个先验的函数形式，\n这里是直接对斜率做了假设然后推导的，但其实跟直接的假设没有多大的本质区别\n利用客户给定的 given_cpa, 可以得到下面的等式\n\\[given\\_cpa = \\frac{cost}{f(cost)} =\n\\frac{\\omega * cost}{\\sqrt{2 * \\omega * cost + b^2} - b}\\]\n进一步求解上面的方程可得到分配的预算为\n\\[cost = \\frac{(2 * given\\_cpa - b)^2 -\nb^2}{2 * \\omega}\\]\n因此，只要通过历史数据拟合得到 \\(\\omega\\) 和 \\(b\\)，就能沟通过客户给定的 given_cpa\n得到规划的预算\n上述的方法应用到多个 channel 的情况，可以为每个 channel\n拟合出自己的 \\(\\omega\\) 和 \\(b\\)，然后基于上面公式来计算每个 channel\n应该分配的预算\n但这种方式也有其局限性，首先这种建模的精度不一定是非常准确的（因为如果能够准确建模出\ncost2convert\n的关系，相当于可以预估计划的实际成本了）；其次是假设的先验形式，在实际中不一定是所有的计划都服从这样的形式（分段线性拟合的能力理论上要比这种纯先验的要好一些），甚至不都服从随着\ncost 增加，边际成本上涨的假设；其次是由于 cost2convert\n对转化数有要求，因此在 convert\n数不足时，需要走探索出价，只有在转化数置信后才能建模\n非成本类出价\n在统一出价中提到，对于 lowest-cost\n这一类非成本类出价的产品，使用完全独立出价有点困难了，主要原因是这种方式要求每个\nchannel\n的预算是明确的，但这在通投场景是没有的明确的预算分配，那能不能通过算法算出一个？\n我们在上面的 SPB 的 paper\n里提到的计算过程，不就是一个分配预算的过程么？能否复用上面的方法呢\n对于 lowest cost 这类没有广告主给定的出价，没有\ngiven_cpa，理论上没法直接套用上面的那种模式；但我们能否给定一个 margin_cpa\n作为 given_cpa，然后做到预算花完的前提下 margin_cpa\n最小？答案是可以的，首先看上面 cost2convert 的形式\n\\[f(cost) = \\frac{\\sqrt{2 * \\omega * cost\n+ b^2} - b}{\\omega}\\]\n如果直接看这个函数，大概长下面这个样子的；从红线到黑线，代表 \\(\\omega\\) 从 1 变成了 3，\\(b\\) 不变，从红线到绿线，代表 \\(b\\) 从 1 变成了 5.2, \\(\\omega\\) 不变，两者在花相同的 cost\n前提下，转化数都更少了，即转化成本上涨了\n\n因为前面的推导中有个假设是边际成本与转化数 \\(f(cost)\\) 成线性关系（如下所示），则 \\(\\omega\\) 和 \\(b\\)\n的上涨就代表了成本的上涨，或者说每个 channel 拟合出来的 \\(\\omega\\) 和 \\(b\\)\n代表了其转换成本的高低情况，\\(\\omega\\) 和 \\(b\\) 越大，代表成本越高\n\\[\\frac{1}{f'(cost)} = \\omega *\nf(cost) + b\\]\n而从最终的预算分配公式可知，一个 channel \\(\\omega\\) 和 \\(b\\)\n越大，分得得预算会越少，代表整体算法会把预算更多分配到成本更低的 channel\n上\n\\[cost = \\frac{(2 * given\\_cpa - b)^2 -\nb^2}{2 * \\omega}\\]\n同时从上面的公式可知，\\(cost\\) 和\ngiven_cpa 是正相关，given_cpa\n过低，能够花出去的钱也少，因为便宜的转化就那么多；有了这个单调性的特性，就可以通过二分查找的方式来找个一个最小的\nmargin_cpa，使得预算能刚好花完\n基于这个查找到的 margin_cpa，便能够套用上面公式计算出各个 channel 的\nbudget，然后基于这个分配好的 budget，lowest-cost\n这类出价能够很自然地做预算分配来出价投放\n跨平台的 multi-channel\n上面说的都是在同一个平台的 multi-channel\n的预算分配和出价，也有一些研究是针对多平台的 multi-channel\n的预算分配\n这篇 paper 《Multi-channel\nAutobidding with Budget and ROI Constraints》\n就是站在广告主视角下，如何通过设置各个 channel 的 budget 和 roi\n来达到营销效果的最优（这也是广告主唯二影响他们投放效果的手段）\n论文的篇幅比较长，这里重点说一下论文的一个核心结论：Solely\noptimizing per-channel budgets are sufficient to maximize\nconversion，即只优化各个 channel 的 budget 便能做到 max\nconversion，理论证明过程比较复杂，这里就不详细展开了，套用 paper\n的话是这么说的\n\nIn Theorem 3.2 of Section 3, we show that solely optimizing for\nper-channel ROIs is inadequate to optimize conversion across all\nchannels possibly resulting in arbitrary worse total conversions\ncompared to the hypothetical global optimal where advertisers can\noptimize over individual auctions. In contrast, in Theorem 3.3 and\nCorollary 3.4 we show that solely optimizing for per-channel budgets\nallows an advertiser to achieve the global optimal.\n\n在上面的结论基础上，paper 设计了一个算法，用于做各个 channel\n的预算分配，整体算法流程如下图所示\n\n这个算法结合了 SGD 和 UCB，整体目标是在 budget\n约束下最大化总转换数；这里基本就是套用了 Multi-Arm Bandits\n的范式，将连续预算空间离散化为有限臂（候选预算值），然后基于 MAB\n的探索与利用机制：迭代选择臂、观测反馈、更新统计信息，优化预算分配策略；不同的点是引入了对偶变量和\nsgd（上图的公式 10），用来处理预算约束，传统的 MAB 没有涉及到这一点\n但是回到实际中，对于各类广告主尤其是 SMB\n这种中小广告主，如果想直接用这个方法是比较困难的，因为使用门槛会比较高；而实际中，广告主往往会先在各平台投一小笔钱看效果，然后根据各平台效果再决定在哪个平台投入更多的预算，这其实跟上面的\nSGD_UCB 算法的模式也比较像，只是精度没有那么高\n小结\n本文主要讨论了同一平台下 multi-channel\n下的出价和预算分配问题；无论是成本类出价还是非成本类出价，比较合理的做法往往是各个\nchannel\n独立出价；而对于成本类出价或非成本类出价，都有两种技术范式，选择哪种基本得看哪种更适合当前业务现状，而这基本又得靠实验来决策了\n对于成本类出价，理论上是可以共享预算，然后独立 channel\n来搜集数据和出价；但除了这种方式，另一种技术范式就是类似 Spending\nProgrammed Bidding 这种算法：通过建模 cost2convert 的关系，然后基于\ngiven_cpa 在不同 channel 之间做显式的预算分配\n非成本类出价同样也有两种技术范式，一种是共享预算，然后以一个主\nchannel 的成本作为后验成本，当做一个动态 target-cost\n产品；另一种也是借鉴类似上面的预算分配的思路，通过搜索出一个\nmargin_cpa，套用同样的算法，来在不同 channel\n之间做预算分配，然后基于预算分配曲线来投放\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"《Modeling Delayed Feedback in Display Advertising》 阅读笔记","url":"/2020/05/17/Modeling%20Delayed%20Feedback%20in%20Display%20Advertising%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"在计算广告中，转化是有延迟的，即在点击发生后过一段时间用户可能才会发生转化，且往往转化漏斗越深，延迟的时间越长；因此在训练\ncvr/deepcvr\n模型时，会有两种情况出现（1）过早把样本送入模型，把最终会转化但是还没回传\nlabel\n的事件当做负例，导致模型低估（2）过晚把样本送入模型，即让所有样本都等待一个足够长的时间才送入模型，导致模型没能及时更新\n因此在建模时需要对转化的回传延时进行建模，这篇 paper 《Modeling Delayed\nFeedback in Display Advertising》是 criteo\n针对这个问题提供的一个解决方法，主要思想就是对于还未观察到\nconversion 的样本，不直接将其当做负样本，而是当前考虑 click\n已发生的时间长短给模型不同大小的 gradient；paper 里称在 criteo\n的真实的数据上验证了该方法的有效性。此外，文章从问题的建模到求解的思路不错，值得一看。\n\n为什么要对延迟建模\n与转化的回传紧密相关的一个话题就是归因（attribution），即将转化归到哪一个\nclick 上，paper 中用了常用的 last-click 归因，归因窗口固定在 30\n天内，即在点击后 30 天以内回传的转化才认为是有效的。\n对回传延迟建模往往要先分析延迟的分布，且对于不同的广告主、行业等，这个值的差异往往还是比较大的，在 paper 中统计结果如下图一所示，结果显示约 35% 的转化会在一小时内回传，50%\n会在 24 小时内回传。\n此外，paper\n中还提到了新计划的问题，主要的观点是每天都会有大量的新计划被创建（如下图二所示），如果不能尽快将这些新计划对应的样本喂给模型，会导致模型在这些新计划上预估得不好\n\n\ngraph\n\n上面这两张图正好对应着文章开头说的要对 delayed conversion\n进行建模的原因；图一表明如果样本不延迟做归因，会把最终会转化但是还没回传\nlabel\n的事件当做负例，导致模型低估，图二则表明如果等待过长时间才把样本送入模型，即让所有样本都等待一个足够长的时间才送入模型，会导致模型没能及时更新\n问题建模\n下面的建模采用到的一些符号及其含义如下所示\n\n\\(X\\): 特征\n \\(Y \\in \\lbrace 0, 1\\rbrace\\):\n当前时刻 conversion 是否已经发生了\n \\(C \\in \\lbrace 0,\n1\\rbrace\\)：conversion 最终是否会发生\n \\(D\\)：回传延迟的真正时间\n \\(E\\)：当前已过去的时间\n\n则当前如果\n\n观察到转化还没发生即 \\(Y=0\\)，有两种可能\n\n\n转化最终不会发生 \\(C=0\\)\n\n\n 转化最终会发生，但是 \\(D&gt;E\\)\n\n\n 观察到转化已发生即 \\(Y=1\\),\n则肯定有 \\(C=1\\)；即 \\(Y=1\\) 是 \\(C=1\\) 的充分条件\n\n且 paper 中做了如下的假设，当前已过去的时间 \\(E\\)\n与最终是否的转化时间以及是否会转化无关，即\n\\[\\begin{align} P(C,D|X,E) = P(C,D|X)\n\\end{align}\\]\n这个假设也是比较合理的，因为最终是否转化及转化的延迟时间与当前已经过去的时间长短无关。\npaper 中对将问题分为两部分来建模，第一部分是常见的 ctr\n预估模型，如下公式 (1) 所示，第二部分是通过指数分布来建模回传延迟 \\(D\\), 如下公式 (2) 所示；除了指数分布，weibull\n分布、gamma\n分布、log-normal\n分布 也常被用来建模事件发生的时间间隔，这些分布是 Survival\nanalysis 这个研究领域里常用的分布。\n\\[\\begin{align}\nP(C=1|X=x)=p(x)=\\frac{1}{1+\\exp(-w_cx)} \\tag{1} \\end{align}\\]\n\\[\\begin{align} P(D=d|X=x,\nC=1)=\\lambda(x)\\exp(-\\lambda(x)d) \\tag{2} \\end{align}\\]\n其中 \\(\\lambda(x)\\) 在 survival\nanalysis 中被称为 hazard\nfunction，其含义表示的是事件发生的频率 / 强度 ;\n如对于指数分布，如果这个值越大，表示事件发生频率越大，事件间的时间间隔越短，则概率密度函数会越陡峭，如下图所示。此外，为了保证\n\\(\\lambda(x) \\gt 0\\), 这里令 \\(\\lambda(x) = \\exp(w_dx)\\)\n\n\nexp prob density\n\n此外，指数分布用来建模两个事件发生的时间间隔，而泊松分布则用来建模某段时间里事件发生的次数，详细的讲解可参考知乎上的这个回答\n指数分布公式的含义是什么？\n有了公式 (1) 和公式 (2) 后，我们就可以推导在考虑回传延迟下 pvr\n预估问题了；下面可分两种情况写出样本的似然：即已观察到\nconversion 的样本和未观察到 conversion 的样本\n已观察到 conversion\n的样本的似然\n即当前观察到 conversion 的概率可写成\n\\[\\begin{align}\n\\begin{split}\np_1&amp;=p(Y=1, D=d_i|X = x_i, E=e_i) \\\\\\\n&amp;= p(Y=1,D=d_i|X = x_i) \\\\\\\n&amp;= p(C=1,D=d_i|X = x_i) \\\\\\\n&amp;= p(D=d_i|X=x_i, C=1)*p(C=1|X=x_i)\n\\end{split}\n\\end{align}\\]\n上面的推导的三步主要利用了以下三点\n\n针对该问题的假设：当前已过去的时间 \\(E\\)\n与最终是否会转化无关，同时与转化时间无关\n \\(Y=1\\) 是 \\(C=1\\) 的充分条件\n条件概率公式 \\(p(a|b,c) * p(b|c) =\np(a,b|c)\\)\n\n最后可将概率表示成公式 (1) 和公式（2）的乘积，则\n\\[\\begin{align} p_1=p(Y=1, D=d_i|X = x_i,\nE=e_i)=\\lambda(x_i)\\exp(-\\lambda(x_i)d_i) * p(x_i) \\tag{3}\n\\end{align}\\]\n未观察到 conversion\n的样本的似然\n同理，可写出当前还没观察到 conversion 的样本的概率 \\(p_0\\) 为\n\\[\\begin{align}\n\\begin{split}\np_0&amp;=p(Y=0|X = x_i, E=e_i) \\\\\\\n&amp;=p(Y=0|C=0, X = x_i, E=e_i)p(C=0|X=x_i) +\\\\\\\n&amp;p(Y=0|C=1, X = x_i, E=e_i)p(C=1|X=x_i) \\\\\\\n&amp;=p(C=0|X=x_i) + p(Y=0|C=1, X = x_i, E=e_i)p(C=1|X=x_i)\n\\end{split}\n\\end{align}\\]\n上面的推导的两步主要利用了以下两点\n\n全概率公式 + 条件概率\n\n\\(p(a|c)=\\sum_{b_i}p(ab_i)\\)\n\\(p(ab_i|c)=\\sum_{b_i}p(a|b_i,\nc)p(b_i|c)\\)\n\n\\(p(Y=0|C=0, X = x_i,\nE=e_i)=1\\)\n\n则上面的推导关键点在于求出条件概率 \\(p(Y=0|C=1, X = x_i, E=e_i)\\),\n该条件概率的含义是当前未观察到 conversion\n的样本最终会转化的概率，换种描述就是最终的延时 \\(D\\) 大于当前已过去时间 \\(E\\) 的概率，即可表示成如下公式\n\\[\\begin{align}\n\\begin{split}\n&amp;p(Y=0|C=1, X = x_i, E=e_i) \\\\\n&amp;=p(D&gt;E|C=1,X=x_i, E=e_i) \\\\\n&amp;=\\int_{e_i}^{\\infty} \\lambda(x)\\exp(\\lambda(x)t)dt \\\\\n&amp;=\\exp(-\\lambda(x)e_i)\n\\end{split}\n\\end{align}\\]\n即最终有 \\[p_0=p(Y=0|X = x_i,\nE=e_i)=1-p(x_i) + p(x_i) * \\exp(-\\lambda(x_i)e_i) \\tag{4}\\]\n小结\n综上\n\n对于当前观察到 conversion 的样本，其似然函数为公式 (3) 即，\n\n\\[\\begin{align}\np_1=\\lambda(x_i)\\exp(-\\lambda(x_i)d_i) *\np(x_i)  \\end{align}\\]\n\n对于当前未观察到 conversion 的样本，其似然函数为公式 (4) 即\n\n\\[\\begin{align} p_0=1-p(x_i) + p(x_i) *\n\\exp(-\\lambda(x_i)e_i)  \\end{align}\\]\n可以看到，这里 \\(p_0 + p_1 \\ne 1\\),\n跟我们常见的建模方式不一样\n问题求解\n上面的问题的解法的基本原理是\nMLE (极大似然估计)，但是根据是否显式地写出变量 \\(C\\) 的条件概率有两种解法，第一种是将 \\(C\\) 当做隐变量，通过 EM\n算法求解；第二种没有直接写出隐变量 \\(C\\) 的概率密度函数，而是直接写出样本集的\nlikelihood，因为从上面的问题建模中已经基于当前观察到的 \\(y_i\\) 写出对应的似然函数了。\nEM 算法\n关于 EM 算法的推导，CS229 的讲义 EM algorithm\n里讲得比较详细了，证明的基本流程就是\n\n在似然函数中通过全概率公式引入隐变量\n通过 Jensen’s inequality 把隐变量提出来\n优化目标转转为最大化 Jensen’s inequality 的下界，让 Jensen’s\ninequality 取等号得到隐变量的概率密度函数 (E-step)\n 将隐变量的概率密度函数带入似然函数中做 MLE 即可 (M-step)\n\n知乎上也有一个比较通俗的回答人人都懂 EM 算法，因此证明过程这里就不再展开赘述了\n简单来说，EM 算法中的 E-step 是为了通过条件概率 \\(p(z|x;\\theta)\\) 来表示隐变量 \\(H\\)，从而替换掉似然函数中的隐变量；进而在\nM-step 中只针对 \\(\\theta\\)\n来进行进行极大似然即可. 如下图所示是 CS229\n讲义里的一次迭代的流程，里面的 \\(i\\)\n表示隐变量可能的状态\n\n\nEM\n\nEM 算法的证明中通过证明了算法会随着 E-step + M-step\n逐步收敛的，但是不能保证收敛到全局最优，如果我们的优化目标是凸的，则 EM 算法可以保证收敛到全局最优，因为对于凸函数局部最优即全局最优。\n回到这个问题，由于 \\(C\\)\n是隐变量，则 E-step 和 M-step 的过程分别如下\nE-step\n给定一个样本 \\((x_i, y_i, e_i)\\),\n令隐变量的条件概率为\n\\[\\begin{align} p(C=1|X=x_i, Y=y_i) := w_i\n\\end{align}\\]\n相比于上面的 EM 算法的流程，我们这还多了一个 \\(y_i\\), 因此还要对 \\(y_i\\) 的值进行分类讨论\n\n当 \\(y_i=1\\) 时，可知 \\(w_i=1\\)（因为 \\(Y=1\\) 是 \\(C=1\\) 的充分条件）\n当 \\(y_i=0\\) 时，其计算方法如下，\n即\n\n\\[\\begin{align}\n\\begin{split}\nw_i&amp;=p(C=1|Y=0,X=x_i,E=e_i) \\\\\\\n&amp;=p(Y=0|C=1, X = x_i, E=e_i)p(C=1|X=x_i) \\\\\\\n&amp;=p(x_i) * \\exp(-\\lambda(x_i)e_i)\n\\end{split}\n\\end{align}\\]\n计算出 \\(w_i=p(C=1|X=x_i, Y=y_i)\\)\n后，\\(p(C=0|X=x_i, Y=y_i) = 1-w_i\\)\nM-step\nM step 最大化的似然函数根据 \\(Y\\) 和\n\\(C\\) 的值可分为四项，分别对应与上图中\nM-step 中的两层 \\(\\sum\\) 嵌套，且在\n\\(Y=1\\) 的情况下 \\(C=0\\) 的概率为 0，最终 M-step\n的似然函数如下所示\n\\[\\begin{align}\n\\begin{split}\nL&amp;=\\sum_{i,y_i=1}w_i \\* \\log p(Y=1,D=d_i|X=x_i,E=e_i) + \\\\\\\n&amp;\\sum_{i,y_i=0}[ w_i \\* \\log p(Y=0,C=1|X=x_i,E=e_i) + \\\\\\\n&amp;(1-w_i) \\* \\log p(Y=0,C=0|X=x_i,E=e_i)]\n\\end{split}\n\\end{align}\\]\n且由前面可知，\\(w_i\\) 取值为\n\\[\\begin{align}\nw_i = \\begin{cases}\n1 &amp; &amp;y_i =1 \\\\\\\np(x_i) * \\exp(-\\lambda(x_i)e_i) &amp; &amp;y_i =0\n\\end{cases}\n\\end{align}\\]\n因此，利用公式 (3) 和公式 (4) 的推导，导入上式最终化简得到结果为\n\\[\\begin{align}\n\\begin{split}\nL&amp;=\\sum_{i}w_i \\log p(x_i) + (1-w_i) \\log(1-p(x_i)) \\\\\\\n&amp;+\\sum_{i} \\log(\\lambda(x_i))y_i - \\lambda(x_i)t_iw_i\n\\end{split}\\tag{5}\n\\end{align}\\]\n除了 \\(w_i\\) 的值要根据 \\(y_i\\) 的值变化外，公式 (5) 中的 \\(t_i\\) 也需要根据 \\(y_i\\) 的值变化，即如下所示\n\\[\\begin{align}\nt_i = \\begin{cases}\ne_i &amp;y_i =1 \\\\\\\nd_i &amp;y_i =0\n\\end{cases}\n\\end{align}\\]\n至此，可通过公式 (5) 对 \\(p\\) 和 \\(\\lambda\\) 进行 MLE\n了，且从公式 (5) 中可看到，对 \\(p\\) 执行的优化是一个 weighted logistics\nregression 的过程，而对 \\(\\lambda\\)\n执行的优化构成则是一个 exponential regression\nJoint Optimization\n在上面的问题建模中，基于当前观察到的 \\(y_i\\)\n已经能写出样本对应的似然函数了，因此虽然问题中包含隐变量 \\(C\\), 但是不一定要显示的将 \\(C\\)\n的条件概率写出来（当无法写出样本对应的似然函数了需要这一步，这个时候就一定要通过\nEM 算法求解了）\n根据 MLE，求解的问题可表示成如下形式\n\\[\\begin{align} \\arg \\min_{w_c,w_d}\nL(w_c,w_d)+\\frac{\\mu}{2}(||w_c||_2^2 + ||w_d||_2^2)\n\\end{align}\\]\n上面的公式中加入了 L2 regularization，要求教的 \\(w_c,w_d\\) 分别是 \\(p\\) 和 \\(\\lambda\\) 的参数；根据公式 (3) 和 (4),\n可写出 \\(L(w_c,w_d)\\) 为\n\\[\\begin{align}\nL(w_c,w_d) = -\\sum_{i, y_i=1} \\log(p(x_i)+ \\log\n\\lambda(x_i)-\\lambda(x_i)d_i  - \\\\\\\n\\sum_{i, y_i=0} \\log[1-p(x_i) + p(x_i) \\exp(-\\lambda(x_i)e_i)]\n\\end{align}\\]\n其中，\\(p(x)=\\frac{1}{1+\\exp(-w_cx)}\\) , \\(\\lambda(x) = \\exp(w_dx)\\)\n则对 \\(w_c\\) 和 \\(w_d\\) 的导数如下图所示\n\n\ngradient\n\n从上面的求导的表达式可知，对于当前 \\(y_i=0\\) 即 label 还没回传的样本，其对 \\(w_c\\) 的影响可从下面两方面去了解\n(1) 当 \\(\\lambda(x_i)e_i \\ll 1\\),\n即当前已经过去的时间 \\(e_i\\)\n远小于平均的回传时间 \\(\\lambda(x_i)^{-1}\\) 时，\\(\\sum_{i, y_i}\\)\n这一项几乎为 0，表示 click 发生的时间还很短，没法完全确认最终是没有\nconversion 的 (2)\\(\\lambda(x_i)e_i\n\\gg 1\\), 即当前已经过去的时间 \\(e_i\\) 远大于平均的回传时间 \\(\\lambda(x_i)^{-1}\\) 时，对 \\(w_c\\) 贡献的梯度是 \\(1/(1-p(x_i))\\), 相当于在 logistics\nregression\n中的一个负样本的梯度，表示已经过了较长时间还没观察到 conversion，可认为这个样本是负样本了\n小结\n前面主要讲了求解建模好的问题的两种解法，第一种方法是显式地写出了隐变量\n\\(C\\) 的条件概率，并通过 EM\n算法求解；第二种方法则是直接写出所有样本的似然函数直接求解。值得注意的是，在 serving 的时候只使用 ctr 预估模型，而不用建模延时的指数回归模型，因为在做反向传播过程中已经考虑了回传延迟对参数进行了修正\n实验\n实验数据：实验数据使用了 criteo 里的真实数据（但是 paper\n里附的实验数据链接打不开了）；7 天测试数据，每天的测试数据对应的训练数据是其前 3 周的数据（约 600w 条样本）\n实验设置：为了与实验提出来的 DFM\n模型作对比，对照组里提供了若干个模型\n\nShort Term Conversion (STC): 采用了两个模型\n\n第 1 个模型用来预估样本在某段时间内转化的概率，即 \\(p(C=1,D \\le 1 day|X=x)\\)\n 第 2 个模型用来预估在这段时间内转化的样本占所有转化样本的比例，即\n\\(p(D \\le 1 day|C=1, X=x) = p(C=1,D \\le 1\nday|X=x)/p(C=1|X=x)\\)\n 则最终预估的概率是 \\(p(C=1|X=x)=\\),\n即是上面两个模型预估值之比\n\n NAIVE: 不考虑样本的转化的回传延迟\n SHIFTED：所有样本都等待 30 天后，确认每个样本的 label\n都是准确后才把模型送入样本进行训练\n ORACLE:\n拥有上帝视角的模型，即在样本送入模型时就能知道这个样本最终是否会转化\n RESCALE：与 NAIVE 一样，但是预估的 cvr 会除以一个常数，\n\n实验结果如下图所示，评估指标是平均 negative\nlog-likelihood(NLL)，因为 NLL 比起 AUC 等指标更能反映预估 cvr\n值的准确性。\n\n\nexp\n\n实验中提供了两套评估数据集，overall 和\nrecent，前者是全集，后者则是专门针对新计划的，从实验结果来看，DFM\n模型的预估结果次优于有上帝视角的 ORACLE 模型。\n小结\n综上，paper\n针对转化回传有延迟的问题提出了一种建模方法，建模的思想是不把还没观察到\nconversion 的样本直接当做负样本去处理，而是考虑其距离当前 click\n发生的时间长短给予模型不同权重的梯度。\n除了之外，这个问题另一种思路就是直接用 regression model\n来建模回传延迟的时间。而转化回传的延迟除了会影响 cvr 模型的预估，还会影响以保成本为目标的\nbidding 方式，即当前 conversion\n还没回传时，不能直接认为没有 conversion，而是要考虑一种更为平滑的 pacing\n策略。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"Practical Wisdom from \"The Almanack of Naval Ravikant\":Wealth","url":"/2023/05/20/Practical%20Wisdom%20from%20%22The%20Almanack%20of%20Naval%20Ravikant%22:Wealth/","content":"\"The Almanack of Naval\nRavikant\" is a book that compiles the wisdom and insights of\nentrepreneur and investor Naval Ravikant. This book mainly talks about\ntwo topics, wealth and happiness. It offers practical advice on how to\nlive a more fulfilling and purposeful life.\nThrough his own experiences and perspectives, Naval provides readers\nwith valuable insights into topics like success, motivation, and\npersonal growth, making the book a useful guide for anyone looking to\nimprove their life and achieve their goals\nI have benefited greatly from reading this book, and I want to write\nsome important takeaways from this book. As this book was written in\nEnglish, I want to give it a try to write it in English, too. Hoping it\nwill be beneficial to you, this passage is about the first topic:\nWealth\n\nWealth\nWe have been pursuing wealth almost all our lives, and financial\nfreedom seems to be the ultimate goal of everyone\nBut firstly，what's the definition of wealth? The book gives the\ndefinition of wealth as follows\n\nWealth is assets that earn while you sleep.\nWealth is the factory, the robots, cranking out things. Wealth is the\ncomputer program that’s running at night, serving other customers.\nWealth is even money in the bank that is being reinvested into other\nassets, and into other businesses.\n\nWe should purse wealth, not money or status. Because money is how we\ntransfer time and wealth, and status is your place in the social\nhierarchy.\nYou will get rich by giving society what it wants but does not yet\nknow how to get at scale.\nSo the following will focus on some of the key factors and abilities\nthe author believes are necessary to achieve financial freedom\nEquity\nYou’re not going to get rich renting out your time. You must\nown equity—a piece of a business to gain your financial\nfreedom.\nIf you are paid for renting out your time, you can make some money,\nbut you’re not going to make the money that gives you financial freedom.\nYou’re not going to have passive income where a business is earning for\nyou while you are on vacation.\nEverybody who really makes money at some point owns a piece of\na product, a business, or some IP.\nYou have to work up to the point where you can own equity in\na business. You could own equity as a small shareholder where\nyou bought stock, or stock options if you work at a tech company. You\ncould also own it as an owner where you started the company.\nBut usually, the real wealth is created by starting your own\ncompanies or even by investing. In an investment firm, they’re\nbuying equity. These are the routes to wealth. It doesn’t come through\nthe hours.\nSpecific Knowledge\nSpecific knowledge is knowledge you cannot be trained for. If society\ncan train you, it can train someone else and replace you.\nThe specific knowledge is sort of this weird combination of unique\ntraits from your DNA, your unique upbringing, and your response to it.\nIt’s almost baked into your personality and your\nidentity. It is found much more by pursuing your innate\ntalents, your genuine curiosity, and your passion. It’s not by going to\nschool for whatever is the hottest job; it’s not by going into whatever\nfield investors say is the hottest.\nSome specific knowledge listed in the book are as follows\n\nWith all the specific knowledge above, we will focus on the\nsales skills.\nIf someone is a natural at sales, obviously they learned somewhere,\nbut they didn’t learn it in a classroom setting. They learned probably\nin their childhood in the school yard, or they learned negotiating with\ntheir parents. Maybe some is a genetic component in the DNA\nBut you can improve sales skills. You can read Robert Cialdini, you\ncan go to a sales training seminar, you can do door-to-door sales. It is\nbrutal but will train you very quickly. Specific knowledge cannot be\ntaught, but it can be learned.\n\nLearn to sell. Learn to build. If you can do both, you will\nbe unstoppable.\n\nThe most important skill for getting rich is becoming a\nperpetual learner.\nIt’s much more important today to be able to become an expert in a\nbrand-new field in nine to twelve months than to have studied the\n“right” thing a long time ago\nLevarage\nArchimedes once said, \"Give me a lever long enough and a fulcrum on\nwhich to place it, and I shall move the world.\"\nFortunes also require leverage. Business leverage comes from\ncapital, labor, and products with no marginal cost of replication (code\nand media).\nCapital means money. To raise money, apply your specific knowledge\nwith accountability and show resulting good judgment. For example,\nworking is the most common way to do this\nLabor means people working for you. It’s the oldest and most\nfought-over form of leverage. Labor leverage will impress your parents,\nbut don’t waste your life chasing it.\nI think we should always remember the leverage we can use\n\nCapital and labor are permissioned leverage.\nEveryone is chasing capital, but someone has to give it to you. Everyone\nis trying to lead, but someone has to follow you.\n\nCode and media are permissionless leverage. They’re\nthe leverage behind the newly rich. You can create software and media\nthat works for you while you sleep.\n\nThe most interesting and the most important form of leverage is\nthe idea of products that have no marginal cost of\nreplication. This is the new form of leverage. This was only\ninvented in the last few hundred years. It started with the printing\npress. It accelerated with broadcast media, and now it’s really blown up\nwith the internet and with coding. Now, you can multiply your efforts\nwithout involving other humans and without needing money from other\nhumans.\nAlso, they are permissionless. They don’t require\nsomebody else’s permission for you to use them or succeed. For labor\nleverage, somebody has to decide to follow you. For capital leverage,\nsomebody has to give you money to invest or to turn into a product.\nCoding, writing books, recording podcasts, tweeting, YouTubing—these\nkinds of things are permissionless.\nThe book offers two useful advice, which can be applied to most\npeople\n\nAn army of robots is freely available—it’s just packed in data\ncenters for heat and space efficiency. Use it.\n\nIf you can’t code, write books and blogs, record videos and\npodcasts.\n\nProductize yourself\n\"Productize\" has leverage. \"Yourself\" has accountability.\"Productize\"\nhas specific knowledge. \"Yourself\" also has specific knowledge in\nthere.\nSociety will pay you for creating things it wants. But society\ndoesn’t yet know how to create those things, because if it did, they\nwouldn’t need you. They would already be stamped out.\nSo if you want to be wealthy, you want to figure out which one of\nthose things you can provide for society that it does not yet know how\nto get but it will want and providing it is natural to you, within your\nskill set, and within your capabilities.\nThen, you have to figure out how to scale it because if you\nonly build one, that’s not enough. You’ve got to build\nthousands, or hundreds of thousands, or millions, or billions of them so\neverybody can have one\nSo I think we can keep asking the question: Am I productizing\nit? Am I scaling it? Am I scaling with labor or with capital or with\ncode or with media?\nLong-Term Game and\nCompound Interest\nCompound interest is a very powerful concept. Compound interest\napplies to more than just compounding capital.It can also be applied in\nbusiness relationships, reputation and individual people working\nwith.\nPlay iterated games. All the returns in life, whether in wealth,\nrelationships, or knowledge, come from compound interest.\nPrioritize and Focus\nSpend more time making the big decisions. There are\nbasically three really big decisions you make in your early life:\nwhere you live, who you’re with, and what you do.\nWe spend very little time deciding which relationship to get into. We\nspend so much time in a job, but we spend so little time deciding which\njob to get into. Choosing what city to live in can almost completely\ndetermine the trajectory of your life, but we spend so little time\ntrying to figure out what city to live in.\nIf you’re going to live in a city for ten years, if you’re going to\nbe in a job for five years, if you’re in a relationship for a decade,\nyou should be spending one to two years deciding these things. These are\nhighly dominating decisions. Those three decisions really matter.\nPatient\nEverybody wants to get rich immediately, but the world is an\nefficient place; immediate doesn’t work. You do have to put in the time.\nYou do have to put in the hours, and so I think you have to put yourself\nin the position with the specific knowl- edge, with accountability, with\nleverage, with the authentic skill set you have, to be the best in the\nworld at what you do.\nYou have to enjoy it and keep doing it, keep doing it, and keep doing\nit. Don’t keep track, and don’t keep count because if you do, you will\nrun out of time.\nJudgement\nHard work is important. Without hard work, you’ll develop neither\njudgment nor leverage. But nowadays it is really overrated, how hard you\nwork matters a lot less in the modern economy.\nAt the same time, judgment is underrated. The\ndirection you’re heading in matters more than how fast you move,\nespecially with leverage.\nSo how can we develop our judgment?\nThink Clearly\nNavel believes that “Clear thinker” is a better compliment than\n“smart.”\nThis is a good point in the book, and I think it can be used in life\nin any circunstances\n\nIf someone is using a lot of fancy words and a lot of big concepts,\nthey probably don’t know what they’re talking about\nI think the smartest people can explain things to a child. If\nyou can’t explain it to a child, then you don’t know it. It’s a\ncommon saying and it’s very true.\n\nThis is exactly what \"The Feynman Learning\nTechnique\" is about. The really smart thinkers are clear thinkers.\nThey understand the basics at a very, very fundamental level.\nSo we should understand the basics really well rather than\nmemorize all kinds of complicated concepts we can’t stitch\ntogether and can’t rederive from the basics.\nIf you can’t rederive concepts from the basics as you need them,\nyou’re lost. You’re just memorizing. And this is exactly what \"First\nprinciple\" is about\nSee Reality\nMake sure you’re dealing with reality when you’re making decisions,\nto achive this, we need to see reality\nOne thing clouding us from being able to see reality is we have\npreconceived notions of the way it should be, which\nwill stop us from seeing the truth.\nTo see the truth, you have to get your ego out of the way\nbecause your ego doesn’t want to face the truth. The smaller\nyou can make your ego, the less conditioned you can make your reactions,\nthe less desires you can have about the outcome you want, the easier it\nwill be to see the reality.\nOur egos are constructed in our formative years—our first two\ndecades. They get constructed by our environment, our parents, society.\nThen, we spend the rest of our life trying to make our ego happy, which\nis exactly the reason that ego will stop you from seeing the true.\nTo solve this problem, one important thing is to be able to\nuncondition yourself, to be able to take your habits\napart and say, “Okay, this is a habit I probably picked up when I was a\ntoddler trying to get my parent’s attention. Now I’ve reinforced it and\nreinforced it, and I call it a part of my identity. Does it\nstill serve me? Does it make me happier? Does it make me healthier? Does\nit make me accomplish whatever I set out to accomplish?”\nTo see reality, we also need to focus on data and\nanalysis, instead of feeling. Because what you feel tells you\nnothing about the facts, it merely tells you something about your\nestimate of the facts.\nThis book also gives another interesting advice about empty space\n\nIt’s actually really important to have empty space. If you don’t have\na day or two every week in your calendar where you’re not always in\nmeetings, and you’re not always busy, then you’re not going to be able\nto think.\nI also encourage taking at least one day a week (preferably two,\nbecause if you budget two, you’ll end up with one) where you just have\ntime to think.\n\nSkills of\nDecision-Making: Mental Models\nJudgment requires experience but can be built faster by learning\nfoundational skills.\nThis book emphasizes that there is no skill called “business”. We\nshould void business magazines and business classes. Instead, study\nmicroeconomics, game theory, psychology, persuasion, ethics,\nmathematics, and computers, which are the mental models the book\nrecommends.\nBecause during decision-making, the brain is a memory prediction\nmachine. A lousy way to do memory prediction is “X happened in the past,\ntherefore X will happen in the future.” It’s too based on specific\ncircumstances. What you want is principles. You want mental models.\nSome mental models the book recommends and the key points of them are\nas follows\n\nevolution: a lot of modern society can be\nexplained through evolution\n\ninversion: being successful is just about\nnot making mistakes\n\ncomplexity theory: the limits of our knowledge\nand the limits of our prediction capability\n\neconomics: understanding of\nsupply-and-demand\n\nprincipal-agent problem: if you want it done\nright, then you have to go yourself and do it\n\ncompound interest: in the intellectual domain,\ncompound interest rules\n\nbasic math: arithmetic, probability, and\nstatistics\n\nbalck swans: extreme probabilities\n\ncalculus: measure the change in small discrete\nor small continuous events\nIF YOU CAN’T DECIDE, THE ANSWER IS NO.\n\nThis is a very rule in the book, when you're faced with a difficult\nchoice, such as: Should I marry this person? Should I take this job?\nShould I buy this house? Should I move to this city?. etc\nThe reason listed in the book is\n\nIf you cannot decide, the answer is no. And the reason is, modern\nsociety is full of options. When you choose something, you get locked in\nfor a long time. Starting a business may take ten years. You start a\nrelationship that will be five years or maybe more. You move to a city\nfor ten to twenty years. These are very, very long-lived decisions. It’s\nvery, very important we only say yes when we are pretty certain\n\nBut I doubt that a little, cause life is always full of uncertainty.\nIf we have to make decision when we are very certain, perhaps we can\nmake no decision. I prefer the quotes in the book \"West\nwith the Night\"\n\nI have learned that if you must leave a place that you have lived in\nand loved and where all your yesteryears are buried deep, leave it any\nway except a slow way, leave it the fastest way you can. Never turn back\nand never believe that an hour you remember is a better hour because it\nis dead. Passed years seem safe ones, vanquished ones, while the future\nlives in a cloud, formidable from a distance.\n\n\nRUN UPHILL\n\nWhen you have two choices to make, and they’re relatively equal\nchoices, take the path more difficult and more painful in the\nshort term.\nWhat’s actually going on is one of these paths requires short-term\npain. And the other path leads to pain further out in the future. Your\nbrain is overvaluing the side with the short-term happi- ness and trying\nto avoid the one with short-term pain.\nSo we have to cancel the tendency out (it’s a powerful subconscious\ntendency) by leaning into the pain. As you know, most of the gains in\nlife come from suffering in the short term so you can get paid in the\nlong term\nWhat are the most efficient ways to build new mental models?\nRead a lot, just read. Reading science, math, and philosophy one hour\nper day will likely put you at the upper echelon of human success within\nseven years.\nSummary\nIn summary, the first part of this book is about wealth, which mainly\ntalks about the definition of wealth and how to build wealth\nAll the methods mentioned in this book about building wealth can be\nsummarized into two words: Productize Yourself\n\"Productize\" has leverage. \"Yourself\" has accountability.\"Productize\"\nhas specific knowledge. \"Yourself\" also has specific knowledge in there.\nYou need to figure out which one of those things you can provide for\nsociety that it does not yet know how to get but it will want and\nproviding it is natural to you, within your skill set, and within your\ncapabilities. Then you have to figure out how to scale it\nAnother important point is about judgement, which is always\nunderrated compared to hard work.\nTo build judgement, we need to think clearly and see\nreality, with some very useful mental models.\n","categories":["读书"],"tags":["纳瓦尔宝典","读书"]},{"title":"Segment Tree 简介","url":"/2016/08/05/Segment%20Tree%20%E7%AE%80%E4%BB%8B/","content":"简介\n本文主要通过实际例子介绍 segment tree 这种数据结构及其应用。以 LeetCode 上的一道题目\n307.\nRange Sum Query - Mutable 为例说明。\n\n这道题目 307. Range\nSum Query -\nMutable要求求数组的区间和，但是有个额外条件，就是会进行多次数组区间求和以及数组元素的更新的操作。\n从正常的思路出发，每次求和的时间复杂度为 \\(O(n)\\)， 更新数组元素的时间复杂度为 \\(O(1)\\), 因此总体的时间复杂度为 \\(O(n)\\)。而参考 303.\nRange Sum Query - Immutable 可以实现求和的时间复杂度为 \\(O(1)\\), 但更新数组元素的时间复杂度为 \\(O(n)\\)，所以总体的时间复杂度也是 \\(O(n)\\)。\n上面两种方法的总体时间复杂度均为 \\(O(n)\\), 但是通过我们下面要介绍的 Segment\nTree，能够将求和以及更新数组元素操作的时间复杂度均变为  \\(O(log_2n)\\)。\nSegment\nTree 是一棵二叉树，其特点为叶子节点个数与数组的长度相同\n从左到右依次为数组中下标从小到大的元素的值，父节点的值为其左右的叶子节点的值的和。如下图是一个简单的例子\n\n因此可以看到每个非叶子节点的值均是代表了数组某个区间的和。下面分别讲述如何构造这棵树，更新某个元素的值以及对特定区间求和。\n建树\n虽然逻辑上是一棵二叉树，但是实际存储时可以通过数组来实现，通过父子节点的下标的数值关系可以访问父节点的子节点。然后需要求出数组的大小，因为这是一棵满二叉树（full\nbinary\ntree，具体定义见下），而且数组下标必须是连续的，因此需要的最大空间为 \\({\\displaystyle \\sum\n_{k=0}^{m}2^k}\\), 其中 m 为二叉树的高度 (从 0 开始计算，如上图的高度为 3)。\n\n具体实现则通过递归，每次记录当前的节点的下标以及表示的数组的范围，如下为建树的 python 代码，其中 seg_tree 为建立的 segment\ntree，nums 为原数组，curr 为 segmen\ntree 中当前节点的下标，start、end 为以 curr\n包含的 nums 数组的下标范围。\ndef build_tree(start, end, curr):        if start &gt; end: return         if start == end:            seg_tree[curr] = nums[start]        else:            mid = start + (end - start)/2            seg_tree[curr] = build_tree(start, mid, curr*2+1) + build_tree(mid+1, end, curr*2+2)        return seg_tree[curr]\n更新元素\n更新元素需要更新两个地方，一是原数组对应的下标的值，另外一个是包含了这个元素的 segment\ntree 中的节点的值。具体也是通过递归实现，下面是更新 segment\ntree 中所有包含原数组下标为 idx\n的元素的节点的值的 python 代码，\ndiff 是下标为 idx 的新值与旧值之差。可见时间复杂度为 \\(O(log_2n)\\),n 为原数组元素的个数。\ndef update_sum( start, end, idx, curr, diff):        seg_tree[curr] += diff        if start == end: return        mid = start + (end - start)/2        if start &lt;= idx &lt;= mid:            update_sum(start, mid, idx, curr*2+1, diff)        else:            update_sum(mid+1, end, idx, curr*2+2, diff)\n求区间和\n求区间和也是通过递归实现，关键在于根据当前节点表示的范围以及需要求的区间的范围的关系进行求和。下面是实现的求区间 [qstart, qend] 的和的 python 代码。可见时间复杂度为 \\(O(log_2n)\\),n 为原数组元素的个数。\ndef get_sum(start, end, qstart, qend, curr):        mid = start + (end - start)/2        if qstart &gt; end or qend &lt; start:            return 0        elif start &gt;= qstart and end &lt;= qend:            return seg_tree[curr]        else:            return get_sum(start, mid, qstart, qend, curr*2+1) + get_sum(mid+1, end, qstart, qend, curr*2+2)\n实际例子\n下面结合上面讲述的三个步骤以及 LeetCode 上的题目 307. Range\nSum Query - Mutable 给出完整的 AC 代码入下:\nclass NumArray(object):    def __init__(self, nums):        \"\"\"        initialize your data structure here.        :type nums: List[int]        \"\"\"        n = len(nums)        if n == 0: return        max_size = 2 * pow(2, int(math.ceil(math.log(n, 2)))) - 1        self.seg_tree = [0 for i in xrange(max_size)]        self.nums = nums[:]        self.build_tree(0, n-1, 0)    def build_tree(self, start, end, curr):        if start &gt; end: return # empty list        if start == end:            self.seg_tree[curr] = self.nums[start]        else:            mid = start + (end - start)/2            self.seg_tree[curr] = self.build_tree(start, mid, curr*2+1) + self.build_tree(mid+1, end, curr*2+2)        return self.seg_tree[curr]            def update(self, i, val):        \"\"\"        :type i: int        :type val: int        :rtype: int        \"\"\"        diff = val - self.nums[i]        self.nums[i] = val        self.update_sum(0, len(self.nums)-1, i, 0, diff)    def update_sum(self, start, end, idx, curr, diff):        self.seg_tree[curr] += diff        if start == end: return        mid = start + (end - start)/2        if start &lt;= idx &lt;= mid:            self.update_sum(start, mid, idx, curr*2+1, diff)        else:            self.update_sum(mid+1, end, idx, curr*2+2, diff)    def sumRange(self, i, j):        \"\"\"        sum of elements nums[i..j], inclusive.        :type i: int        :type j: int        :rtype: int        \"\"\"        return self.get_sum(0, len(self.nums)-1, i, j, 0)    def get_sum(self, start, end, qstart, qend, curr):        mid = start + (end - start)/2        if qstart &gt; end or qend &lt; start:            return 0        elif start &gt;= qstart and end &lt;= qend:            return self.seg_tree[curr]        else:            return self.get_sum(start, mid, qstart, qend, curr*2+1) + self.get_sum(mid+1, end, qstart, qend, curr*2+2)\n","categories":["python"],"tags":["python","树"]},{"title":"Quantitative Trading:How to Build Your Own Algorithmic Trading Business(2)","url":"/2023/10/02/Quantitative%20Trading:%20How%20to%20Build%20Your%20Own%20Algorithmic%20Trading%20Business(2)/","content":"\"Quantitative\nTrading: How to Build Your Own Algorithmic Trading Business\" by\nErnie Chan is a comprehensive guide that explores the world of\nquantitative trading and provides practical advice for building a\nalgorithmic trading business, especially for individuals interested in\nquantitative trading.\nIt covers essential concepts, methodologies, and practical tips to\nhelp readers develop and implement their own algorithmic trading\nstrategies while effectively managing risk and building a sustainable\ntrading business.\nDue to the significant benefits I received from reading this book, I\nwant to write down some of the most important takeways I get from this\nbook, with the hope that it can also be useful to you.\nThis passage is about the last four chapters, which introduces\nexecution system in actual trading(automated and semi-automated), how to\nminimize transaction cost and determine the optimal leverage using the\nKelly Criterion. It also talks about some special topics or common sense\nin trading. Finally, it lists some advantages of individuals investors\nover institutional investors.\n\nExecution Systems\nThis chapter is about building an automated trading system and ways\nto minimize trading costs and divergence with your expected performance\nbased on your backtests.\nAutomated Trading System\nAn automated trading system is a piece of software that automatically\ngenerates and transmits orders to your brokerage account based on your\ntrading strategy.\n\nIt eliminates manual operation so that you can simultaneously run\nmultiple strategies.\n\nMost importantly, it allows speedy transmissions of orders,which is\nessential to high-frequency trading strategies.\n\nThere are two kinds of trading systems: fully automated and\nsemiautomated, and the choice depends on the frequency of your\ntrading.\nFully Automated Trading\nSystem\nA fully automated trading system can run the trading algorithm in a\nloop again and again, constantly scanning the latest prices and\ngenerating new waves of orders throughout the trading day. The\nsubmission of orders through an API to your brokerage account is\nautomatic, so you would not need to load the trades to a basket trader\nor spread trader\nA fully automated system has the advantage that it minimizes human\nerrors and delays. For certain high-frequency systems, a fully automated\nsystem is indispensable, because any human intervention will cause\nenough delay to seriously derail the performance.\nHowever, a fully automated system is also complicated and costly to\nbuild, often requiring professional programmers with knowledge of\nhigh-performance programming languages such as Java, C, or C++ in order\nto connect to your brokerage’s application program- ming interface\n(API).\nSemiautomated Trading System\nIn a semiautomated trading system, the trader still needs to manually\nupload a text file containing order details to a basket trader or spread\ntrader, and manually press a button to transmit the orders at the\nappropriate time. However, the order text file can be automatically\ngenerated by a program such as Python.\nWhether you have built a semiautomated or a fully automated trading\nsystem, there is often a need for input data beyond the prices that your\nbrokerage or data vendor can readily provide you. For example,\nearnings estimates or dividends data are often not\nprovided as part of the real-time data stream.\nThese non-price data are typically available free of charge from many\nweb sites, but are usually embedded in an HTML format and not readily\nusable. Hence, an automated system also must be able to retrieve such\nweb pages, parse them, and reformat them into a tabular format that your\ntrading strategy can utilize\nMinimizing Transaction Costs\nIn the previous chapter, we had talked about how to reduce\ntransaction costs. Besides changing your brokerage or proprietary\ntrading firm to one that charges a lower commission, there are a few\nthings you can do in your execution method to minimize the transaction\ncosts.\nAnd the book regards this mainly as a matter of not allowing\nyour order size to be too big relative to its average trading volume and\nrelative to its market capitalization\nCut Down on Commissions\nYou should refrain from trading lowprice stocks.\nTypically, institutional traders do not trade any stocks with prices\nlower than $5\nBecause low-price stocks increase your total commissions costs (since\nyou need to buy or sell more shares for a fixed amount of\ncapital)（Why？？）\nAlso it has a wider bid-ask spread and therefore\nincrease your total liquidity costs\nThe definition of bid-ask spread from ChatGPT is as follow\n\nThe bid-ask spread is a concept used in financial markets,\nparticularly in securities trading such as stocks, bonds, commodities,\nand foreign exchange. It refers to the difference between the highest\nprice a buyer is willing to pay (the bid price) and the lowest price a\nseller is willing to accept (the ask price) for a particular asset or\nsecurity.\nA wider spread can indicate lower liquidity or higher\ntransaction costs. Market conditions, supply and demand\ndynamics, trading volume, and the nature of the asset being traded can\nall influence the bid-ask spread.\n\nMinimize Market Impact Cost\nWe should limit the size (number of shares) of your orders based on\nthe liquidity of the stock, on which we can do two things:\n\nlimit the size of order according to the certain measurement(like\naverage daily volume)\n\nallocate assets effectively between large-cap and small-cap\nstocks\n\nOne common measure of liquidity is the average daily volume (it is\nyour choice what lookback period you want to average over). As a\nrule of thumb, each order should not exceed 1 percent of the average\ndaily volume. As an independent trader, you may think that it\nis not easy to reach this 1 percent threshold, and you would be right\nwhen the stock in question is a large-cap stock belonging to the S&amp;P\n500. However, you may be surprised by the low liquidity of some\nsmall-cap stocks out there\nAnother way to reduce market impact is to scale the size of your\norders based on the market capitalization of a stock.\nA linear scale (i.e., scaling the capital of a stock to be linearly\nproportional to its market capitalization) would result in\npractically zero weights for most small- and micro- cap stocks in your\nportfolio, and this will take away any benefits of\ndiversification\nIf we were to use linear scale, the capital weight of the largest\nlarge-cap stock will be about 10,000 of the smallest small-cap stock. To\nreap the benefits of diversification, we should not allow that\nratio to be more than 10 or so, provided that the liquidity\n(volume) constraint described above is also satisfied.\nSlippage\nMany institutional traders who desire to execute a large order will\nbreak it down into many smaller orders and execute them over time. This\nmethod of trading will certainly reduce market impact; however, it\nengenders another kind of transactions costs, namely, slippage，\nReducing market impact in this way may increase slippage, it\nis not really suitable for retail traders whose order size is usually\nnot big enough to require this remedy\nSlippage is usually outside of your control, perhaps your brokerage’s\nexecution speed is simply too slow, due to either software issues (their\nsoftware processes your orders too slowly), risk control issues (your\norder has to be checked against your account’s buying power and pass\nvarious risk control criteria before it can be routed to the exchange),\nor pipeline issues (the brokerage’s speed of access to the exchanges).\nOr perhaps your brokerage does not have access to deep enough\n“dark-pool” liquidity. These execution costs and issues should affect\nyour choice of brokerages\nPaper Trading\nPaper trading refers to the practice of simulating trades in a\nrisk-free environment using virtual money instead of real capital.\nWhen performing paper trading, there are several key aspects to pay\nattention to in order to make the experience as valuable and realistic\nas possible\nYou should be able run your ATS, execute paper trades, and then\ncompare the paper trades and profit and loss (P&amp;L) with the\ntheoretical ones generated by your backtest program using the\nlatest data. If the difference is not due to transaction costs\n(including an expected delay in execution for the paper trades), then\nyour software likely has bugs.\nIt gives you better intuitive understanding of your\nstrategy, including the volatility of its P&amp;L, the typical\namount of capital utilized, the number of trades per day, and the\nvarious operational difficulties including data issues.\nThere are some common problems in paper trading, some of them are\nlisted in the following\nThe moment you start paper trading you will realize that there is a\nglaring look-ahead bias in your strategy—there may just be no way you\ncould have obtained some crucial piece of data before you enter an\norder\nIf you are able to run a paper trading system for a month or longer,\nyou may even be able to discover data-snooping bias, since paper trading\nis a true out-of-sample test.\nBacktesting also won’t reveal the operational difficulties, such as\nhow fast you can download all the needed data before the market opens\neach day and how you can optimize your operational procedures in actual\nexecution, but paper trading will make you feel all these.\nIn summary, paper trading allows you to:\n\nDiscover software bugs in your trading strategy and execution\nprograms.\n\nDiscover look-ahead or even data-snooping bias.\n\nDiscover operating difficulties and plan for operating\nschedules.\n\nEstimate transaction costs more realistically.\n\nGain important intuition about P&amp;L volatility, capital usage,\nportfolio size, and trade frequency.\n\nActual Performance V.S\nExpectations\nWhat do you do in the situation when your live trading underperforms\nyour backtest?\nThe list of what possibly may have caused this divergence from\nexpectation is as follows\n\nDo you have bugs in your ATS software?\n\nDo the trades generated by your ATS match the ones generatedby your\nbacktest program?\n\nAre the execution costs much higher than what you expected?\n\nAre you trading illiquid stocks that caused a lot of market\nimpact?\n\nYou can start by addressing the usual problems: Eliminate bugs in the\nstrategy or execution software; reduce transaction costs; and simplify\nthe strategy by eliminating parameters. But, fundamentally, your\nstrategy still may have suffered\nfrom data-snooping bias or regime shift.\n\ndata-snooping bias\n\nYou can verify this cause by trying to eliminate as many rules and as\nmany parameters in your strategy as possible. If the backtest\nperformance completely fell apart after this exercise, chances are you\ndo have this bias and it is time to look for a new strategy.\n\nregime shifts\n\nWhen the financial market structure or the macroeconomic environment\nundergoes a drastic change so much so that trading strategies that were\nprofitable before may not be profitable\nOne regime shift is relevant if your strategy shorts\nstocks.\nMany stocks, especially the small-cap ones or the ones with low\nliquidity, are “hard to borrow.” For you to be able to short a stock,\nyour broker has to be able to borrow it from someone else (usually a\nlarge mutual fund or other brokerage clients) and lend it to you for\nselling.\nIf no one is able or willing to lend you their stock, it is deemed\nhard to borrow and you would not be able to short it. Hence, again, a\nvery profitable historical short position may not actually have been\npossible due to the difficulty of borrowing the stock.\nMoney and Risk Management\nKelly Criterion\nThis chapter provides an important tool for risk management: the\ndetermination of the optimal leverage using the Kelly\nCriterion.\nIt also determines the optimal allocation of capital among different\nstrategies, based on the covariance of their returns.\nThe general form of the Kelly\nCriterion is as follows:\n\\[\\begin{align} f^{*} = \\frac{p}{a} -\n\\frac{q}{b} \\end{align}\\]\nwhere\n\n\\(f^{*}\\) is the fraction of the\nassets to apply to the security.\n\n\\(p\\) is the probability that the\ninvestment increases in value.\n\n\\(q\\) is the probability that the\ninvestment decreases in value(\\(q=1-p\\))\n\n\\(a\\) is the fraction that is\nlost in a negative outcome. If the security price falls\n10%, then \\(a=0.1\\)\n\n\\(b\\) is is the fraction that is\ngained in a positive outcome. If the security price\nrises 10%, then \\(b=0.1\\)\n\nIn an intuitive understanding of the Kelly Criterion, it involves\ntaking the probability of winning divided by the multiple of the wagered\ncapital when you lose (typically 1 in the case of gambling), and then\nsubtracting the probability of losing divided by the multiple of the\ncapital gained when you win.\nSince this formula can result in Kelly fractions higher than\n1. In this case, it is theoretically advantageous to use\nleverage to purchase additional securities on\nmargin.\nThe proof of Kelly Formula is also easy to understand, the details in\nwiki is as follows\n\nBut it's important to note that the Kelly Criterion is not\nsuitable for all situations. It assumes that you know the odds or\nreturns and expected probabilities, while ignoring market\nuncertainties and other factors. Therefore, in practical applications,\nthe Kelly Criterion is often combined with real-world considerations and\nrisk tolerance to formulate more reasonable and feasible money\nmanagement strategies.\nAlso, the Kelly criterion is valid only for known outcome\nprobabilities, which is not the case with investments. Risk averse\ninvestors should not invest the full Kelly fraction.\nMind-Set\nThe ultimate risk management mind-set is very simple: Do not succumb\nto either despair or greed. To gain practice in this psychological\ndiscipline, one must proceed slowly with small position\nsize, and thoroughly test various aspects of the trading\nbusiness (model, software, operational procedure, money and risk\nmanagement) before scaling up according to the Kelly formula.\nIn order to proceed slowly and cautiously, it is helpful to have\nother sources of income or other businesses to help sustain yourself\neither financially or emotionally (to avoid the boredom associated with\nslow progress). It is indeed possible that finding a diversion, whether\nincome producing or not, may actually help improve the long-term growth\nof your wealth.\nSpecial topics\nThe special topics in this book includes\n\nmean reversion and momentum\n\nregime switching\n\nstationarity and cointegration\n\nfactor model\n\nseasonal trading models\n\nhigh-frequency trading.\n\nMean Reversion and Momentum\nTrading strategies can be profitable only if securities prices are\neither mean-reverting or trending. Otherwise, they are random-walking,\nand trading will be futile.\nAt any given time, stock prices can be both mean reverting and\ntrending depending on the time horizon you are interested in.\nConstructing a trading strategy is essentially a matter of\ndetermining if the prices under certain conditions and for a certain\ntime horizon will be mean reverting or trending, and what the\ninitial reference price should be at any given time.(When the prices are\ntrending, they are also said to have “momentum,” and thus the\ncorresponding trading strategy is often called a momentum strategy.)\nThere are multiple reasons for momentum, slow diffusion of\ninformation, private liquidity needs and herdlike behavior\n\nslow diffusion of information\n\nMomentum can be generated by the slow diffusion of information—as\nmore people become aware of certain news, more people decide to buy or\nsell a stock, thereby driving the price in the same direction.\nStock prices may exhibit momentum when the expected earnings have\nchanged. This can happen when a company announces its quarterly\nearnings, and investors either gradually become aware of this\nannouncement or they react to this change by incrementally executing a\nlarge order .\n\nprivate liquidity needs\n\nBesides the slow diffusion of information, momentum can be caused by\nthe incremental execution of a large order due to the liquidity needs or\nprivate investment decisions of a large investor.\nWith the advent of increasingly sophisticated execution algorithms\nadopted by the large brokerages, it is, however, increasingly difficult\nto ascertain whether a large order is behind the observed momentum.\n\nherdlike behavior\n\nMomentum can also be generated by the herdlike behavior of investors:\ninvestors interpret the (possibly random and meaningless) buying or\nselling decisions of others as the sole justifications of their own\ntrading decisions.\nBut momentum regimes generated by private liquidity needs and\nherdlike behavior have highly unpredictable time horizons.\nThe most important things we need to pay attention to may be the\neffects of increasing competition from traders with the same\nstrategies\n\nFor mean-reverting strategies, the effect typically is\nthe gradual elimination of any arbitrage opportunity, and thus gradually\ndiminishing returns down to zero. When the number of arbitrage\nopportunities has been reduced to almost zero, the mean-reverting\nstrategy is subject to the risk that an increasing percentage of trading\nsignals are actually due to fundamental changes in stocks’ valuation and\nthus is not going to mean revert.\nFor momentum strategies, the effect of competition is\noften the diminishing of the time horizon over which the trend will\ncontinue. As news disseminates at a faster rate and as more\ntraders take advantage of this trend earlier on, the equilibrium price\nwill be reached sooner. Any trade entered after this equilibrium price\nis reached will be unprofitable.\n\nRegime Switching\nThe desire to predict regime switches, which are also commonly known\nas turning points, is also as old as financial markets themselves\nAcademic attempts to model regime switches in stock prices generally\nproceed along these lines（Markov regime switching or hidden Markov\nmodels）\n\nPropose that the two (or more) regimes are characterized\nby different probability distributions of the prices. In the\nsimplest cases, the log of the prices of both regimes may be represented\nby normal distributions, except that they have different means and/or\nstandard deviations.\nAssume that there is some kind of transition\nprobability among the regimes.\nDetermine the exact parameters that specify the regime\nprobability distributions and the transition probabilities by fitting\nthe model to past prices, using standard statistical methods\nsuch as maximum likelihood estimation.\nBased on the fitted model above, find out the expected regime of\nthe next time step and, more importantly, the expected stock\nprice.\n\nThe method above assumes constant transition probabilities among\nregimes at all times. In practice, this means that at any time, there is\nalways a very small probability for the stock to transition from a\nnormal, quiescent regime to a volatile regime. But this is\nuseless to traders who want to know when—and under what precise\nconditions—the transition probability will suddenly peak. This\nquestion is tackled by the turning points models.\nStationarity AND\nCointegration\nA time series is “stationary” if it never drifts farther and farther\naway from its initial value. In technical terms, stationary time series\nare “integrated of order zero\"\nIt is obvious that if the price series of a security is\nstationary, it would be a great candidate for a mean-reversion strategy.\nUnfortunately, most stock price series are not stationary—they exhibit a\ngeometric random walk that gets them farther and farther away from their\nstarting\nIf you find** a pair of stocks such that if you long one and short\nthe other, the market value of the pair is stationary.** If this is the\ncase, then the two individual time series are said to be cointegrated.\nbecause a linear combination of them is integrated of order zero.\nUsually, two stocks that form a cointegrating pair are from the same\nindustry group. Traders have long been familiar with this so-called\npair-trading strategy. They buy the pair portfolio when\nthe spread of the stock prices formed by these pairs is low, and\nsell/short the pair when the spread is high—in other words, a classic\nmean-reverting strategy\nIf a price series (of a stock, a pair of stocks, or, in general, a\nportfolio of stocks) is stationary, then a mean-reverting strategy is\nguaranteed to be profitable, as long as the stationarity persists into\nthe future (which is by no means guaranteed).\nHowever, the converse is not true. You don’t necessarily need a\nstationary price series in order to have a successful mean-reverting\nstrategy. Even a nonstationary price series can have many\nshort-term reversal opportunities that one can exploit, as many\ntraders have discovered.\nFactor Models\nAssume excess returns \\(R\\) can be\nobtained in the formula\n\\[\\begin{align} R = Xb+u\n\\end{align}\\]\nThere are multiple components in the formula\n\nFactor returns, \\(X\\) are the common drivers of stock\nreturns, and are therefore independent of a particular stock.\nFactor exposures, \\(b\\) are the sensitivities to each of these\ncommon drivers.\nSpecific return, \\(u\\), any part of a stock’s return that\ncannot be explained by these common factor returns is deemed a specific\nreturn (i.e., specific to a stock and essentially regarded as just\nrandom noise within the APT framework). Each stock’s specific return is\nassumed to be uncorrelated to another stock’s.\n\nA famous factor model is Three-Factor model, that is\nexcess return of a stock depends linearly on only three factor\nexposures: beta, market capitalization and book-to-price\nratio\nThese factor exposures are obviously different for each stock and for\neach time period. (Factor exposures are often normalized such that the\naverage of the factor exposures within a universe of stocks is zero, and\nthe standard deviation is 1.)\nWhat about the factor returns and specific returns? We cannot\ndirectly compute the factor returns and specific returns—we have to\ninfer their values by running a multivariate linear regression of the\nexcess returns of stocks against the factor exposures.\nNote that each stock represents one data point in this linear\nregression, and we have to either run a separate linear regression for\neach time period or, if we want an average value over many time periods,\naggregate the values from all these time periods into one training set\nand run one regression against them all.\nBut the limitation of factor models are also obvious, one can make a\ngeneral observation that factor models that are dominated by\nfundamental and macroeconomic factors have one major drawback—they\ndepend on the fact that investors persist in using the same metric to\nvalue companies\nFor example, even though the value (book-to-price ratio) factor\nreturns are usually positive, there are periods of time when investors\nprefer growth stocks such as during the Internet bubble in the late\n1990s\nTherefore, it is not uncommon for factor models to experience\nsteep drawdown during the times when investors’ valuation method\nshifts, even if only for a short duration. But then, this\nproblem is common to practically any trading model that holds stocks\novernight.\nExit Strategy\nThere isn’t usually much variety in the way exit signals are\ngenerated. They are based on one of these\n\nA fixed holding period\n\nA target price or profit cap\n\nThe latest entry signals\n\nA stop price\n\nFixed Holding Period\nFor momentum model, the optimal period typically decreases due to the\nincreasing speed of the diffusion of information and the increasing\nnumber of traders who catch on to this trading opportunity. Hence\na momentum model that has worked well with a holding period\nequal to a week in the backtest period may work only with a one-day\nholding period now\nFor mean-reverting model, there is a more statistically robust way to\ndetermine the optimal holding period that does not depend on the limited\nnumber of actual trades, that is Ornstein-Uhlenbeck\nformula, which describes the behavior of a continuous-time\nstochastic (random) process\nThe Ornstein-Uhlenbeck process is a stationary Gaussian process that\nexhibits mean-reverting behavior. In simpler terms, it models a system\nthat tends to return to a central equilibrium or mean value over time,\nsimilar to a particle undergoing Brownian motion but with a restoring\nforce pulling it back towards a mean position.\nThe definition of the Ornstein-Uhlenbeck\n\nMathematicians tell us that the average value of \\(X(t)\\) follows an exponential decay to its\nmean \\(\\mu\\), and the half-life of this\nexponential decay is equal to \\(ln(2)/θ\\)\nTarget Price\nIf you believe that your security is mean reverting, then you also\nhave a ready-made target price—the mean value of the historical prices\nof the security, or \\(\\mu\\) in the\nOrnstein-Uhlenbeck formula.\nBut target prices are not as easily justified in momentum models as\nin mean-reverting models.\nLatest entry signals\nSuppose you are running a trading model, and you entered into a\nposition based on its signal. Some time later, you run this model again.\nIf you find that the sign of this latest signal is opposite to your\noriginal position (e.g., the latest signal is “buy” when you have an\nexisting short position), then you have two choices. Either you simply\nuse the latest signal to exit the existing position and become flat or\nyou can exit the existing position and then enter into an opposite\nposition\nSeasonal Trading\nThese strategies recommend that you buy or sell certain securities at\na fixed date of every year\nHowever, from my own experience, much of the seasonality in equity\nmarkets has weakened or even disappeared in recent years, perhaps due to\nthe wide spread knowledge of this trading opportunity, whereas some\nseasonal trades in commodity futures are still profitable\nThe most famous seasonal trade in equities is called the\nJanuary effect.\nOne version states that small-cap stocks that had the worst returns\nin the previous calendar year will have higher returns in January than\nsmall-cap stocks that had the best returns. The rationale for this is\nthat investors like to sell their losers in December to benefit from tax\nlosses, which creates additional downward pressure on their prices.\nHigh-Frequency Trading\nIn general, if a high Sharpe ratio is the goal of your trading\nstrategy (as it should be), then you should be trading at high\nfrequencies, rather than holding stocks overnight.\nThe reason why these strategies have Sharpe ratio is simple: Based on\nthe “law of large numbers,” the more bets you can place, the smaller the\npercent deviation from the mean return you will experience. With\nhigh-frequency trading, one can potentially place hundreds if not\nthousands of bets all in one day.\nTherefore, provided the strategy is sound and generates\npositive mean return, you can expect the day-to-day deviation\nfrom this return to be minimal. With this high Sharpe ratio, one can\nincrease the leverage to a much higher level than longer-term strategies\ncan\nBut no strategy can actually guarantee positive mean return, besides,\nhigh-frequency trading also requires harder backtesting and more\npowerful infrastructure\nThough successful high-frequency strategies have such numerous\nmerits, it is not easy to backtest such strategies when the average\nholding period decreases to minutes or even seconds. Just having\nhigh-frequency data with last prices is not sufficient—data with bid,\nask, and last quotes is needed to find out the profitability of\nexecuting on the bid versus the ask\nHigh-speed execution may account for a large part of the actual\nprofits or losses. Professional high-frequency trading firms have been\nwriting their strategies in C instead of other, more user-friendly\nlanguages, and locating their servers next to the exchange or a ma- jor\nInternet backbone to reduce the microsecond delays.\nEven though the Sharpe ratio is appealing and the returns\nastronomical, truly high-frequency trading is not by any means easy for\nan independent trader to achieve in the beginning. But there is no\nreason not to work toward this goal gradually as expertise and resources\naccrue\nCan Independent Traders\nSucceed?\nThis section primarily outlines some advantages that individual\ninvestors have in quantitative trading compared to institutional\ninvestors.\ncapacity\nA significant difference between individual and institutional quant\ntrading lies in the size of capital (capacity).\nThe book states that it is far, far easier to generate a high Sharpe\nratio trading a 100,000 account than a 100 million account\nDifferent capital sizes lead to different strategies, and it's\ngenerally easier to profit in small-scale quant trading compared to\nmanaging large amounts of capital. And there are many simple and\nprofitable strategies that can work at the low capacity end that would\nbe totally unsuitable to hedge funds.\nMore details about this can be obtained from this question on zhihu\nThe specific reason for such phenomenon, as explained in the book,\nstarts with liquidity: small capital provides liquidity, while\nlarge capital demands liquidity.\n\nMost profitable strategies that have low capacities are acting as\nmarket makers: providing short-term liquidity when it is needed\nand taking quick profits when the liquidity need\ndisappears.\nHowever, you have billions of dollars to manage, you now\nbecome the party in need of liquidity, and you have to pay for\nit. To minimize the cost of this liquidity demand, you\nnecessarily have to hold your positions over long periods of time. When\nyou hold for long periods, your portfolio will be subject to\nmacroeconomic changes (i.e., regime shifts) that can cause great damage\nto your portfolio.\n\nAnother issue with large-scale funds is that they engage in\ncompetition with different hedge funds, ultimately resulting in\nsimilar holdings and similar returns\nThe intense competition among hedge funds means the strategies become\nless profitable. The lowered returns in turn pressure the fund manager\nto overleverage.\nThe fundamental market inefficiency that they are trying to exploit\nmay remain the same, and thus their portfolios may still end up holding\nvery similar positions. When market environment changes, a stampede out\nof similar losing positions can (and did) cause a complete meltdown of\nthe market.\nconstraints\nAnother difference between individual and institutional quant trading\nis the constraints during trading.\nFor example, as a trader in a quantitative fund, you may be\nprohibited from trading a long-only strategy, but long-only strategies\nare often easier to find, simpler, more profitable, and if traded in\nsmall sizes, no more risky than market-neutral strategies. Or you may be\nprohibited from trading futures. You may be required to be not only\nmarket neutral but also sector neutral. You may be asked to find a\nmomentum strategy when you know that a mean-reverting strategy would\nwork.And on and on. Many of these constraints are imposed for risk\nmanagement reasons, but many others may be just whims, quirks, and\nprejudices of the management.\nWhen your strategy shows initial profits, these managers may impose\nenormous pressure for you to scale up quickly, and when your strategy\nstarts to lose, they may force you to liquidate the portfolios and\nabandon the strategy immediately.\nAs every student of mathematical optimization knows, any constraint\nimposed on an optimization problem decreases the optimal objective\nvalue. Similarly, every institutional constraint imposed on a trading\nstrategy tends to decrease its returns\nSummary\nThis passage firstly introduces two kinds of execution system,\nautomated and semi-automated, and the choice depends on the frequency of\nyour trading. But no matter which one to choose, minimizing transaction\ncost is very necessary, we should cut down on commissions, minimize\nmarket impact cost and try to avoid slippage as much as possible.\nIn actual trading, risk management is very necessary. One important\npoint is to determine the optimal leverage using the Kelly Criterion.And\nthe mind-set is also very important, do not succumb to either despair or\ngreed, proceed slowly with small position size, and thoroughly test\nvarious aspects of the trading business.\nIt also talks about some special topics or common sense in trading,\nincluding mean reversion strategy and momentum startegy, regime\nswitching, stationarity and cointegration, factor model, seasonal\ntrading models, high-frequency trading. These can be also regarded as\ncommon sense in quantative trading\nFinally, it lists some advantages of individual investors over\ninstitutional investors, capacity and constraints. Individual investors\ncan use many more strategies than institutional investors with smaller\ncapacity, and also face fewer constraints.\nOnce you have automated everything and your equity is growing\nexponentially, can you just sit back, relax, and enjoy your wealth?\nUnfortunately, experience tells us that strategies do lose their\npotency over time as more traders catch on to them. It takes ongoing\nresearch to supply you with new strategies. So keep studying\nand thinking, this will be an endless game.\n","categories":["读书"],"tags":["读书","量化交易"]},{"title":"Quantitative Trading:How to Build Your Own Algorithmic Trading Business(1)","url":"/2023/07/30/Quantitative%20Trading:%20How%20to%20Build%20Your%20Own%20Algorithmic%20Trading%20Business(1)/","content":"\"Quantitative\nTrading: How to Build Your Own Algorithmic Trading Business\" by\nErnie Chan is a comprehensive guide that explores the world of\nquantitative trading and provides practical advice for building a\nalgorithmic trading business, especially for individuals interested in\nquantitative trading.\nIt covers essential concepts, methodologies, and practical tips to\nhelp readers develop and implement their own algorithmic trading\nstrategies while effectively managing risk and building a sustainable\ntrading business.\nDue to the significant benefits I received from reading this book, I\nwant to write down some of the most important takeways I get from this\nbook, with the hope that it can also be useful to you.\nThis passage is about the first four chapters, which introduce basic\nrequirements for independent traders, including search for ideas,\nperform backtest, and what we need before conducting real trading.\n\nQuantitative trading, also known as algorithmic trading, is the\ntrading of securities based strictly on the buy/sell decisions of\ncomputer algorithms.\nIt relies on advanced mathematical and statistical models, computer\nalgorithms, and data analysis to make trading decisions. It involves the\nuse of quantitative techniques to identify patterns, trends, and\nopportunities in financial markets and execute trades automatically\nwithout human intervention.\nThe quantitative trading this book focuses on is called\nstatistical arbitrage trading, which deals with the\nsimplest financial instruments: stocks, futures, and sometimes\ncurrencies. The book argues that one does not need an advanced degree to\nbecome a statistical arbitrage trader, compared to those institutional\nquantitative traders who received their advanced degrees as physicists,\nmathematicians, engineers, focusing on trading complex derivative\ninstruments\nIndependent Quantative\nTrading\nAs this book is focus on independent quantitative traders, it lists\nsome demands for anyone who wants to venturing into this field, the most\nimportant one I though is the following one\nWhen one plunges into independent trading, fear of losses and of\nbeing isolated from the rest of the world is natural, and so it helps to\nhave both a prior appreciation of risks and some savings to lean on. It\nis important not to have a need for immediate profits to sustain your\ndaily living, as strategies have intrinsic rates of returns that cannot\nbe hurried\nThe ideal independent quantitative trader is therefore\nsomeone who has some prior experience with finance or computer\nprogramming, who has enough savings to withstand the inevitable losses\nand periods without income, and whose emotion has found the right\nbalance between fear and greed.\nThe book also talks about how much time to take for independent\nquantative trading, since most people who enter this field aims for\nfreedom of wealth so as to have more time for their lives.And it depends\nvery much on the degree of automation you have achieved. The author\nlists himself as an example\n\nThe largest block of time I need to spend is in the morning before\nthe market opens: I typically need to run various programs to download\nand process the latest historical data, read company news that comes up\non my alert screen, run programs to generate the orders for the day, and\nthen launch a few baskets of orders before the market opens and start a\nprogram that will launch orders automatically throughout the day. I\nwould also update my spreadsheet to record the previous day’s profit and\nloss (P&amp;L) of the different strategies I run based on the\nbrokerages’ statements. All of this takes about two hours.\nBut the two hours reffered above is just the operational side of the\nbusiness. If you want to grow your business, or keep your current\nprofits from declining due to increasing competition, you will need to\nspend time doing research and backtesting on new strategies. But\nresearch and development of new strategies is the creative part of any\nbusiness, and it can be done whenever you want to. And this may take a\nlot of time\n\nIf you are convinced that you want to become a quantitative trader, a\nnumber of questions immediately follow, which will be answered in the\nfollowing chapters\n\nHow do you find the right strategy to trade?\n\nHow do you recognize a good versus a bad strategy even before\ndevoting any time to backtesting them?\n\nHow do you rigorously backtest them?\n\nIf the backtest performance is good, what steps do you need to take\nto implement the strategy, in terms of both the business structure and\nthe technological infrastructure?\n\nIf the strategy is profitable in initial real-life trading, how does\none scale up the capital to make it into a growing income stream while\nmanaging the inevitable losses that come with trading?\n\nFishing for Ideas\nThis part mainly answers the first question, How do you find the\nright strategy？\nFinding a strategy is not hard, there are hundreds, if not thousands,\nof trading ideas that are in the public sphere at any time, accessible\nto anyone at little or no cost. But they may not be useful, especially\nfor academics\nThe author states in the book that he had found that many strategies\ndescribed by academics are either too complicated, out of date (perhaps\nthe once-profitable strategies have already lost their power due to\ncompetition), or require expensive data to backtest (such as historical\nfundamental data). Furthermore, many of these academic strategies work\nonly on small-cap stocks, whose illiquidity may render actual\ntrading profits far less impressive than their backtests would\nsuggest.\nHowever, numerous publicly accessible strategies may not be highly\nrigorous, but one can consider making slight adjustments to these\nstrategies to achieve profitability. For example, the strategies from\ntraders’ forums may have worked only for a little while, or they work\nfor only a certain class of stocks, or they work only if you don’t\nfactor in transaction costs. However, the trick is that you can often\nmodify the basic strategy and make it profitable\nCapital Matters\nThere are many factors to consider to get the most suitable strategy\nfor you, including working hours, programming skills, trading capital,\netc. Here we jsut focus on the factor of capital. Because capital\navailability affects many choices\n\naccount\n\nThe first is whether you should open a retail brokerage account or a\nproprietary trading account, which will be explained with more details\nin chapter 4\n\nleverage\n\nWith a low-capital account, we need to find\nstrategies that can utilize the maximum leverage\navailable. Trading futures, currencies, and options can offer you higher\nleverage than stocks; intraday positions allow a Regulation T leverage\nof 4, while interday (overnight) positions allow only a leverage of\n2\nCapital (or leverage) availability determines whether you should\nfocus on directional trades (long or short only) or dollar-neutral\ntrades (hedged or pair trades).\nA dollar-neutral\ntrades means that hold equal dollar amounts of long and short\npositions, sometimes it’s also called market neutral strategies, and\na dollar-neutral portfolio or market-neutral portfolio requires\ntwice the capital or leverage of a long or short-only\nportfolio.\n\ninfrastructure\n\nCapital availability also imposes a number of indirect constraints.\nIt affects how much you can spend on various infrastructure, data, and\nsoftware.\nFor example, if you have low trading capital, your online brokerage\nwill not be likely to supply you with real-time market data for too many\nstocks, so you can’t really have a strategy that requires real-time\nmarket data over a large universe of stocks.\nSimilarly, clean historical stock data with high frequency costs more\nthan historical daily stock data. And historical stock data without\nsurvivorship bias are much more expensive than those that have such a\nbias. Yet if your data have survivorship bias, the backtest result can\nbe unreliable.\n\nholding period\n\nThere is a misconception aired by some investment advisers, that is\nif your goal is to achieve maximum long-term capital growth, then the\nbest strategy is a buy-and-hold one. This notion has been shown to be\nmathematically false.\nIn reality, maximum long-term growth is achieved by finding a\nstrategy with the maximum Sharpe ratio (defined in the next section),\nprovided that you have access to sufficiently high leverage. Therefore,\ncomparing a short-term strategy with a very short holding\nperiod, small annual return, but very high Sharpe ratio, to a long-term\nstrategy with a long holding period, high annual return, but lower\nSharpe ratio, it is still preferable to choose the short-term strategy\neven if your goal is long-term growth, barring tax\nconsiderations and the limitation on your margin borrowing (more on this\nsurprising fact later in Chapter 6 on money and risk management).\nA long holding period does not guarantee a good strategy; however, in\nmany cases, it is really hard to judge the market trend correctly, which\nleads to the truth that long-term approach is more profitable than\nshort-term approach\nThe following image includes the difference between low capital and\nhigh capital\n\nAnd the autor's experience may also bring us some insights\n\nI started my life as an independent quantitative trader with $100,000\nat a retail brokerage account (I chose Interactive Brokers), and I\ntraded only directional, intraday stock strategies at first. But when I\ndeveloped a strategy that sometimes requires much more leverage in order\nto be profitable, I signed up as a member of a proprietary trading firm\nas well.\n\nBefore Backtesting\nEven before doing an in-depth backtest of the strategy, you can\nquickly filter out some unsuitable strategies if they fail one or more\nof these tests:\n\nDoes it outperform a benchmark?\n\nDoes it have a high enough Sharpe\nratio?\n\nDoes it have a small enough drawdown and short enough\ndrawdown duration?\n\nDoes the backtest suffer from survivorship\nbias?\n\nDoes the strategy lose steam in recent years compared to its earlier\nyears?\n\nDoes the strategy have its own “niche” that protects it from intense\ncompetition from large institutional money managers?\n\nWe will talk about the above questions one by one\n\nbenchmark\n\nTo compare strategy with a benchmark is important, but we also need\nto realize that different assets require different benchmarks\nEverybody seems to know that if a long-only strategy returns 10\npercent a year, it is not too fantastic because investing in an index\nfund will generate as much, if not better, return on average. However,\nif the strategy is a long-short dollar-neutral strategy (i.e., the\nportfolio holds long and short positions with equal capital), then 10\npercent is quite a good return, because then the benchmark of comparison\nis not the market index, but a riskless asset such as the yield of the\nthree-month U.S. Treasury bill (which at the time of this writing is\nabout 4 percent).\nAnd Excess Returns = Portfolio Returns − Benchmark Returns\n\nsharpe ratio and drawdown\n\nDon't just consider the return rate, Sharpe ratio is also\nimportant\nThere is often a trade-off between the return rate and the sharpe\nratio; however, the book suggests that higher Sharpe ratio often leads\nto higher returns because leverage can be applied to benefit, and the\npartial risk of leverage can be hedged by the high Sharpe ratio.\nIf a strategy has deep (e.g., more than 10 percent) or lengthy (e.g.,\nfour or more months) drawdowns, it is unlikely that it will have a high\nSharpe ratio\nAny strategy with a Sharpe ratio less than 1 is not considered a good\nstrategy, and strategies that are profitable every day typically have a\nSharpe ratio greater than 3.\n\nsteam and transaction costs\n\nIn addition to commission fees, there are also costs incurred due to\nliquidity; for example, the inability to sell at a specific price or the\nprice drop after selling.\nThe book explains this point with the following example\n\nwhen you buy or sell a large chunk of securities, you will not be\nable to complete the transaction without impacting the prices at which\nthis transaction is done. This effect on the market prices due to your\nown order is called market impact, and it can contribute to a large part\nof the total transaction cost when the security is not very liquid\n\nThe term \"slippage\" is used to describe the situation mentioned\nabove, slippage is the difference between the price that triggers the\norder and the execution price. Of course, this slippage can be of either\nsign, but on average it will be a cost rather than a gain to the\ntrader\nTaking transaction costs into account, the Sharpe ratio of a\nstrategy may shift from positive to negative.\n\ndata\n\nUsually there are two problems related to data, surviveship bias and\nregime shifts\nSurvivorship bias in data refers to the exclusion of assets that have\nalready delisted or disappeared from the backtesting data.\nSome strategies are significantly impacted by this bias, such as\nthose specifically focus on buying cheap stocks. These strategies may\nnot be targeting undervalued assets, but rather assets that are on the\nverge of delisting or disappearing.\nAs to \"regime shifts\" in the financial markets, it means that\nfinancial data from an earlier period simply cannot be fitted to the\nsame model that is applicable today\nThis point may be hard to swallow for many statistically minded\nreaders. Many of them may think that the more data there is, the more\nstatistically robust the backtest should be. This is true only when the\nfinancial time series is generated by a stationary process.\nUnfortunately, financial time series is famously\nnonstationary\n\nfly under the Radar\n\nFor personal traders, avoid blindly following strategies of\nsuperstitious large institutions; instead, find a well-considered and\nindependent strategy that suits you, or \"strategy fly under the\nRadar\"\nYou should look for those strategies that fly under the radar of most\ninstitutional investors, for example, strategies that have very low\ncapacities because they trade too often, strategies that trade very few\nstocks every day, or strategies that have very infrequent positions\nBacktesting\nThere are many nuts and bolts involved in creating a realistic\nhistorical backtest and in reducing the divergence of the future\nperformance of the strategy from its backtest performance.\nIssues discussed here include:\n\nData: Split/dividend adjustments, noise in daily\nhigh/low, and survivorship bias.\n\nPerformance measurement: Annualized Sharpe ratio and maximum\ndrawdown.\n\nLook-ahead bias: Using unobtainable future information for past\ntrading decisions.\n\nData-snooping bias: Using too many parameters to fit historical\ndata, and avoiding it using large enough sample, out-of-sample testing,\nand sensitivity analysis.\n\nStrategy refinement: Common ways to make small variations on the\nstrategy to optimize performance.\n\nTransaction cost: Impact of transaction costs on performance.\n\nData\nThere are serveral aspects to which we need to pay attention when\ndealing with data in backtesting\n\nDividend and Split\n\nIn the financial field, a dividend refers to a portion of a company's\nprofits distributed to its shareholders. When a company makes a profit,\nit can choose to return a portion of the earnings to its shareholders in\nthe form of cash or stock. This is known as a dividend payment. There\nare two main types of dividends:\n\nCash Dividend: A dividend paid by the company directly in the form\nof cash to its shareholders.\n\nStock Dividend: An extra share of stock distributed to shareholders\nin the form of additional shares.\n\nAs to split, a stock split refers to a company dividing its existing\nshares into a larger number of shares at a certain ratio, thereby\nincreasing the number of shares while reducing the price per share. For\nexample, a 2-for-1 stock split means each share is split into two\nshares, but the price per share is reduced by half. Stock splits do not\nchange the market value of the company or the total equity of\nshareholders.\nThe viewpoint of this book is that we need to refresh the price of\nthe stock when there is dividen or split, according the the exact number\nof dividen or split\n\nHigh and Low Data\n\nA backtest that relies on high and low data is less reliable than one\nthat relies on the open and close.\nAfter retrieving the data from a database, it is often advisable to\ndo a quick error check. The simplest way to do this is to\ncalculate the daily returns based on the data. If you\nhave open, high, low, and close prices, you can calculate the various\ncombinations of daily returns such as from the previous high to today’s\nclose as well. You can then examine closely those days with returns that\nare, say, 4 standard deviations away from the average.Typically, an\nextreme return should be accompanied by a news announcement, or should\noccur on a day when the market index also experienced extreme returns.\nIf not, then your data is suspect\n\nSurvivorship Bias\n\nOne way to overcome this problem is to start collecting point-in-time\ndata yourself for the benefit of your future backtest. If you save the\nprices each day of all the stocks in your universe to a file, then you\nwill have a point-in-time or survivorship-bias-free database to use in\nthe future. Another way to lessen the impact of survivorship bias is to\nbacktest your strategies on more recent data so that the results are not\ndistorted by too many missing stocks.\nPerformance measurement\nThe point in this book is that sharpe ratio and\ndrawdowns are the two most important measurements in\nbacktesting.\nBut usually it did not include average annualized returns, the\nmeasure most commonly quoted by investors. The reason in the book is if\nyou use this measure, you have to tell people a number of things about\nwhat denominator you use to calculate returns.\nIn general, if you calculate your average and standard deviation of\nreturns based on a certain trading period T, whether T is a month, a\nday, or an hour, and you want to annualize these quantities, you have to\nfirst find out how many such trading periods there are in a year (call\nit NT). Then Annualized Sharpe Ratio = (NT) × Sharpe\nRatio\nCommon Pitfalls\nThese pitfalls are actually some common problems in machine\nlearning\n\nLook-Ahead Bias: same as the problem of data leakage in machine\nlearning\nData-Snooping Bias: same as the problem of overfitting in machine\nlearning, the author offers his experience as follows\n\n\nUsually, an erroneous backtest would produce a historical performance\nthat is better than what we would have obtained in actual trading\n\n\nAs a rule of thumb, I would not employ more than five parameters,\nincluding quantities such as entry and exit thresholds, holding period,\nor the lookback period, in computing moving averages.\nLet’s assume that the number of data points needed for\noptimizing your parameters is equal to 252 times the number of free\nparameters your model has.(purely on experience.)\n\n\nSensitivity Analysis: to test if a strategy if robust\n\n\nOnce you have optimized your parameters as well as various features\nof your model and have verified that its performance on a test set is\nstill reasonable, vary these parameters or make some small qualitative\nchanges in the features of the model and see how the performance changes\non both the training and the test sets.\n\nTransaction Costs\nIn actual financial trading, there are various costs involved, like\ncommission, liquidity cost, opportunity cost, market impact, and\nslippage, which we had reffered in the previous chapter\nThese costs can greatly impact the final returns in backtesting.And\nit should not surprise you to find that a strategy with a high Sharpe\nratio before adding transaction costs can become very unprofitable after\nadding such costs\nSo you should consider transaction costs in backtesting\nStrategy Refinement\nUsaually there are some very simple strategies that are fairly well\nknown in traders’ circles and are still somewhat profitable, though\ntheir returns seem to be diminishing. An example is the pair trading of\nstocks. The reason they are diminishing in returns is that too\nmany traders are taking advantage of this arbitrage opportunity and\ngradually erasing the profit margin. However, it is often\npossible to introduce minor variations in the basic strategy, which will\nboost its returns.\nThese minor variations are often far less well known than the basic\nstrategy, and therefore far less well exploited by traders. Sometimes\nthey involve excluding certain stocks or groups of stocks from\nthe universe. For example, traders often prefer to exclude\npharmaceutical stocks from their technical trading program because of\nthe dramatic impact of news on their prices, or else they may exclude\nstocks that have pending merger or acquisition deals. Other traders\nchange the entry and exit timing or frequency of the\ntrades. And a strategy that has a very good Sharpe\nratio when it is applied to small-cap stocks becomes very unprofitable\nwhen applied to large-cap stocks.\nWhen introducing these refinements to your strategy, it is\npreferable that the refinement has some basis in fundamental economics\nor a well-studied market phenomenon, rather than some arbitrary rule\nbased on trial and error. Otherwise, data-snooping bias\nlooms.\nSetting up your business\nRetail or Proprietray\nProprietary trading firms are financial companies specialized in\nproprietary trading. This means they use their own capital for trading,\naiming to profit from market fluctuations. These firms typically have\nhighly specialized trading teams that employ complex trading strategies.\nThey use their own funds for trading, enabling them to seek profits in\nhigh-risk, high-return markets.\nRetail brokerage accounts are accounts opened by individual investors\nwith a brokerage or securities firm. Through retail brokerage accounts,\nindividual investors can buy and sell financial products such as stocks,\nbonds, and funds. These accounts are typically geared towards the\ngeneral public and offer relatively simple investment options and\ntrading functionalities. The funds in these accounts come from the\nindividual investors themselves and are used to meet their personal\ninvestment needs.\nTheir difference are listed as follows in this book\n\nIn a nutshell, retail brokerages give you complete freedom\nand better capital protection but smaller leverage, while\nproprietary trading firms give you less freedom and less capital\nprotection but much high leverage. You can choose to have both retail\nand proprietary accounts, each tailored to the specific needs of your\nstrategies\nThe decision whether to go retail or to join a proprietary trading\nfirm is generally based on your need of capital, the style of your\nstrategy, and your skill level.\nFor example, if you run a low-risk, market-neutral strategy that\nnevertheless requires a much higher leverage than allowed by Regulation\nT in order to generate good returns, a proprietary account may be better\nfor you. However, if you engage in high-frequency futures trading that\ndoes not require too much capital, a retail account may save you a lot\nof costs and hassles. Similarly, a very experienced trader with strong\nrisk man- agement practices and emotional stability probably doesn’t\nneed the guidance given by a proprietary firm, but less experienced\ntraders may benefit from the imposed restraints.\nRegardless of whether you have chosen to trade in a retail bro-\nkerage or join a proprietary trading firm, you need to make sure their\ntrading account and systems have these features:\n\nRelatively low commissions.\n\nTrade a good variety of financial\ninstruments.\n\nAccess to deep pool of liquidity.\n\nAPI for real-time data retrieval and order transmission.\n\nA \"deep pool of liquidity\" refers to a financial market or trading\nenvironment where there is a significant volume of buy and sell orders\navailable for a particular asset (such as a stock, currency, or\ncommodity) at various price levels. In other words, it is a market with\na large number of participants willing to trade that particular asset,\nresulting in a high level of trading activity.\nPhysical Infrastructure\nIn quantitative trading, high-performance servers and reliable\nnetwork connections are critical because they directly affect the\nefficiency of executing trading strategies and the stability of\ntrades.\nHere are some aspects that need to be cosidered in this part\n\nServer Selection: Quantitative trading requires\nthe use of high-performance servers, typically servers with powerful\nprocessing capabilities and large memory capacity. These servers need to\nprocess market data rapidly, run complex trading strategies, and execute\ntrade instructions efficiently.\nLow Latency: The success of quantitative trading\noften relies on quick execution and response times. Therefore, servers\nneed to have low latency characteristics to ensure that trade\ninstructions reach the exchange and receive a response as quickly as\npossible, reducing the impact of network latency on trade\nslippage.\nRedundant Systems: As quantitative trading\ninvolves high-frequency, automated trading, server reliability is\ncrucial. Redundant systems, such as redundant power supplies, redundant\nhard drives, and redundant network interfaces, are used to prevent\nsingle points of failure and ensure the continuous operation of\nservers.\nServer Location: Server location is also an\nimportant consideration. To minimize trade execution latency, some\nquantitative traders place servers in data centers close to the\nexchange, reducing the transmission time of trade instructions as much\nas possible.\nNetwork Connectivity: High-speed and stable\nnetwork connections are crucial for quantitative trading. Quantitative\ntraders often choose high-quality dedicated network connections or\nleased lines to ensure fast and reliable data transmission.\nSecurity: Quantitative trading involves a\nsignificant amount of trade data and sensitive information. Therefore,\nservers and network infrastructure need to have robust security measures\nto protect traders' data and funds.\n\nThe above requirements may seem too strict for the trading which\nfrequency is relatively lower, since it is usually for\nultra-high-frequency trading applications. But some infrastructure is\nstill required to support the development and execution of trading\nstrategies. For example, server, network ans security, and it highly\ndepends on what kind of trading you're performing.\nSummary\nIn summary, this passage discusses various aspects of quantitative\ntrading and provides practical advice for building an algorithmic\ntrading business. It emphasizes the importance of strategy selection,\nbacktesting, and setting up the necessary infrastructure. Here are the\nkey points covered:\nQuantitative Trading Overview and Independent Quantitative\nTrading: Quantitative trading, also known as algorithmic\ntrading, uses computer algorithms, mathematical models, and data\nanalysis to make buy/sell decisions automatically. The ideal independent\nquantitative trader should have some prior experience in finance or\ncomputer programming, sufficient savings to withstand losses, and\nemotional balance between fear and greed.\nFinding Trading Ideas: Traders can find trading\nstrategies from various sources, but it is essential to verify and\nrefine them. Minor adjustments to existing strategies can lead to\nprofitable results. Factors Influencing Strategy Selection: Capital\navailability, infrastructure, and holding period influence the choice of\ntrading strategy.\nBacktesting: Backtesting involves testing a\nstrategy's performance using historical data.Considerations include\ndividend/split adjustments, data accuracy, perfromance measurements,\ntransaction costs, and sensitivity analysis.\nSetting Up Your Business: Traders can choose between\nretail brokerage accounts and proprietary trading firms. Important\nfeatures to consider include low commissions, access to a variety of\nfinancial instruments, deep liquidity, and real-time data retrieval and\norder transmission. And traders also need for set up some physical\ninfrastructure for real trading.\n","categories":["读书"],"tags":["读书","量化交易"]},{"title":"ROC 曲线与 PR 曲线","url":"/2018/06/16/ROC%20%E6%9B%B2%E7%BA%BF%E4%B8%8E%20PR%20%E6%9B%B2%E7%BA%BF/","content":"ROC 曲线和 PR\n曲线是评估机器学习算法性能的两条重要曲线，两者概念比较容易混淆，但是两者的使用场景是不同的。本文主要讲述两种曲线的含义以及应用的场景。\n\n定义\nROC 曲线和 PR 曲线都是用在二分类中，且涉及到下图的几个概念 (摘自 The Relationship\nBetween Precision-Recall and ROC Curves)\n\n\nroc vs pr\n\n上面四个指标用大白话解释如下\n\nRecall：查全率，正样本中被预测出来是正的比例 (越大越好)\nPrecision：查准率，预测的正样本中被正确预测的比例 (越大越好)\nTrue Positive Rate：跟 Recall 定义一样 (越大越好)\nFPR : 负样本中被预测为正的比例 (越小越好)\n\n对于一个二分类问题，往往要设定一个 threshold，当预测值大于这个\nthreshold 时预测为正样本，小于这个 threshold 时预测为负样本。如果以\nRecall 为横轴，Precision 为纵轴，那么设定一个 threshold\n时，便可在坐标轴上画出一个点，设定多个 threshold\n则可以画出一条曲线，这条曲线便是 PR 曲线。\nPR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC 曲线则是以\nFPR 为横轴，TPR 为纵轴。\n那么两者的关系是怎样的？\n对比\nThe\nRelationship Between Precision-Recall and ROC Curves\n中证明了以下两条定理\n定理 1：对于一个给定的的数据集，ROC 空间和 PR 空间存在一一对应的关系，因为二者包含完全一致的混淆矩阵。我们可以将 ROC 曲线转化为 PR 曲线，反之亦然。\n定理 2：对于一个给定数目的正负样本数据集，曲线 A 在\nROC 空间优于曲线 B ，当且仅当在 PR 空间中曲线 A 也优于曲线 B。\n定理 2 中 “曲线 A 优于曲线 B” 是指曲线 B 的所有部分与曲线 A 重合或在曲线\nA\n之下。而在 ROC 空间，ROC 曲线越凸向左上方向效果越好。与 ROC 曲线左上凸不同的是，PR 曲线是右上凸效果越好。\n从定理 2 来看，ROC 空间和 PR\n空间两个指标似乎具有冗余性，那么为什么还需要这同时两个指标呢？答案是在两者在样本不均衡的情况下表现有较大差异。\n下图是 ROC 曲线和 Precision-Recall 曲线的对比，摘自 An\nintroduction to ROC analysis\n\n\nROC_PR.png-89.5kB\n\n图 (a) 和 (b) 是在样本正负比例为 1:1 下的 ROC 曲线和 PR 曲线，图 (c) 和\n(d) 是在样本正负比例为 1:100 下的 ROC 曲线和 PR 曲线。\n从结果来看：当测试集中的正负样本的分布变化的时候，ROC 曲线能够保持不变。\n文章 ROC 和 AUC 介绍以及如何计算 AUC\n以及 An\nintroduction to ROC analysis\n中都认为这是个优点，原因是在实际的数据集中经常会出现类不平衡（class\nimbalance）现象，即负样本比正样本多很多（或者相反），而 ROC\n这种对不平衡样本的鲁棒性使得其曲线下的面积 AUC\n不会发生突变。\n那么，AUC 意味这什么？首先 AUC\n值是一个概率值，表示随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的 Score 值将这个正样本排在负样本前面的概率。\n因此，AUC 值实际上反映了模型的 rank\n能力，AUC 值越大，当前的分类算法越有可能将正样本排在负样本前面。这个指标尤其适用在某些场景下 (如\nCTR 预估)，每次要返回的是最有可能点击的若干个广告 (根据 CTR 排序，\n选择排在前面的若干个)，实际上便是在考验模型的排序能力。除此之外，CTR\n中存在着样本不均衡的问题，正负样本比例通常会大于 1:100，如果采用 PR\n曲线，则会导致 AUC 发生剧变，无法较好反映模型效果。\n然而，ROC\n曲线不会随着类别分布的改变而改变的优点在一定程度上也是其缺点。因为\nROC 曲线这种不变性其实影响着的是 AUC\n值，或者说是评估分类器的整体性能。但是在某些场景下，我们会更关注正样本，这时候就要用到\nPR 曲线了。\n比如说信用卡欺诈检测，我们会更关注 precision 和\nrecall，比如说如果要求预测出为欺诈的人尽可能准确，那么就是要提高\nprecision；而如果要尽可能多地预测出潜在的欺诈人群，那么就是要提高\nrecall。一般来说，提高二分类的 threshold 就能提高 precision，降低\nthreshold 就能提高 recall，这时便可观察 PR 曲线，得到最优的\nthreshold。\n除此之外，Quora 上的问题 What\nis the difference between a ROC curve and a precision-recall curve? When\nshould I use each? 中也举了一下的例子说明了在欺诈检测的问题中，PR\n曲线更能反映结果的变化。\n\nLet's take an example of fraud detection problem where there are 100\nfrauds out of 2 million samples.\n\nAlgorithm 1: 90 relevant out of 100 identified\nAlgorithm 2: 90 relevant out of 1000 identified\n\nEvidently, algorithm 1 is more preferable because it\nidentified less number of false positive.\nIn the context of ROC curve, - Algorithm 1: TPR=90/100=0.9, FPR=\n10/1,999,900=0.00000500025 - Algorithm 2: TPR=90/100=0.9,\nFPR=910/1,999,900=0.00045502275 The FPR difference is 0.0004500225\nFor PR Curve - Algorithm 1: precision=0.9, recall=0.9 - Algorithm 2:\nPrecision=90/1000=0.09, recall= 0.9 Precision difference= 0.81\nThe difference is more apparent in PR curve\n\n总结\n综上，有以下几条结论（参考 机器学习之类别不平衡问题\n(2) —— ROC 和 PR 曲线）\n\nROC 曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能 (通常是会计算 AUC，表示模型的 rank 性能)，相比而言 PR 曲线完全聚焦于正例。\n如果有多份数据且存在不同的类别分布。比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则 ROC 曲线比较适合，因为类别分布改变可能使得 PR 曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则 PR 曲线比较适合。\n如果想要评估在相同的类别分布下正例的预测情况，则宜选 PR 曲线。类别不平衡问题中，ROC 曲线通常会给出一个乐观的效果估计，所以大部分时候还是 PR 曲线更好。(参考上面\nQuora 的例子)\n 最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的 precision，recall，f1\nscore 等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。\n\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"Spark 集群部署和 Jupyter Notebook 配置注意事项","url":"/2017/09/13/Spark%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E5%92%8C%20Jupyter%20Notebook%20%E9%85%8D%E7%BD%AE%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","content":"文章主要记录了 spark 2.1.0 集群部署注意事项以及如何通过 Jupyter\nNotebook 访问 spark 集群。\n\nspark 集群安装注意事项\n安装过程主要参考了 spark 官方文档\nhttps://spark.apache.org/docs/latest/spark-standalone.html\n一些需要注意的事项如下\n\n每台机器需要先安装 Java，并配置环境变量 JAVA_HOME 和\nPATH\n安装包解压到每台机器上，保持目录一致性，master 能够通过密钥 ssh\n到其他 workers\n 修改 /etc/hosts 文件，保证每台主机能够通过主机名称\nping 通其他机器，注意，如果原来 /etc/hosts 文件中有类似于\n127.0.0.1 hostname 的解释需要注释掉（因为会与前面设置\nip hostname 冲突）\nmaster 创建 conf/slaves, 每行包含一个 worker 的\nhostname\n 各个主机的 python\n路径和版本要一致，在各台主机上的 conf/spark-env.sh 中定义如下\nexport PYSPARK_PYTHON=/opt/miniconda2/bin/pythonexport PYSPARK_DRIVER_PYTHON=ipython\n通过 sbin 中的 script start-all.sh\n启动整个集群\n默认情况下 worker (slave) 所有的 cores\n都会使用，但是内存默认只会使用 1g (可通过\nhttp://SparkMaster:8080/ 查看，SparkMaster 为 master\n的 ip)，如果需要修改可用的资源，需要修改\nconf/spark-defaults.conf 文件，如下是设置了每个 worker 分配\n4 个 core 和 6g 内存给 executor，同时设定了 driver program 使用的 core\n的数量和内存大小。 spark.executor.cores            4spark.executor.memory           6gspark.driver.cores              4spark.driver.memory             6g\n\n这里需要注意的是 conf/spark-env.sh\n也有内存和 cores 相关的设定，但是设定的是可使用的最大值，并不是实际的使用值，设定实际的使用值必须要在文件\nconf/spark-defaults.conf 中设置，且当\nconf/spark-defaults.conf 设定值大于\nconf/spark-env.sh\n时，该项不生效，也就是实际的资源值会变为 0。\n另外，spark\n只是一个计算框架，并不提供存储的功能，往往需要结合其他的分布式数据库\nHBase 或分布式文件系统 HDFS 等使用，\n从中读取数据并进行将结果保存在其中。\njupyter notebook 配置\njupyter notebook 的前身是\nipython notebook，网上的基本是通过 ipython\n建立 profile 文件来解决，但是 jupyter notebook 已经不支持\nprofile 参数了，因此这种方法无效。\n通过 Apache Toree 可以建立 jupyter notebook\n使用的 kernel，从而将 kernel\n连接到 spark 上，本来通过 pip install toree 可以简单地安装 toree，但是由于使用\nspark 的版本是 2.1.0, 其对应的 Scala 的版本是 2.11, 这个版本的 Scala 与\ntoree 中的 2.10 的 Scala\n版本不符。因此需要重新编译 toree 并安装，编译 toree 并安装可参考以下教程。\n主要过程如下\n1. 如果没有安装 sbt，需要先安装 sbt echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823sudo apt-get updatesudo apt-get install sbt\n2. 下载并编译 toree 的源码\ngit clone https://github.com/apache/incubator-toreecd incubator-toree/\n编译前需要先修改 MakeFile 文件中的\nAPACHE_SPARK_VERSION、SCALA_VERSION 为对应的版本。我这里使用 spark\n2.1.0，因此修改成如下\nAPACHE_SPARK_VERSION?=2.1.0SCALA_VERSION?=2.11\n然后执行 make dist &amp;&amp; make release 这时会报错\n/bin/sh: 1: docker: not found, 主要原因是没安装 docker，可以忽略这个错误，因为 docker 主要作用是将安装文件打包在一起再通过 pip 安装，但是这些文件已经生成在\ndist\n目录下，可以直接安装，因此可以执行下面的命令进行安装\ncd dist/toree-pip/python setup.py install\n\n通过 toree 生成相应的 kernel 供 jupyter notebook 使用\n\n可以通过\njupyter toree install --spark_home= path_to_spark-2.1.0 --interpreters=PySpark/SparkR/SQL/Scala\n安装对应的 kernel，然后在 web 界面选择相应的 kernel 即可。\n最后的问题，toree\n默认使用的是单机模式，如果需要使用集群模式，需要通过环境变量设置提交时的参数，从这个脚本文件\n/usr/local/share/jupyter/kernels/apache_toree_pyspark/bin/run.sh\n可以看出提交任务时的参数通过 SPART_OPTS\n获取，因此可以将这个换件变量的值设置为\n--master spark://ip:7077 从而通过集群运行任务。\n通过上面搭建的环境与 Spark 集群进行交互的一些 Jupyter Notebook\n样例可参考 https://github.com/WuLC/MachineLearningWithSpark\n\n参考资料\nMachine\nLearning with Jupyter using Scala, Spark and Python: The Setup Installing\nToree+Spark 2.1 on Ubuntu 16.04\n","categories":["spark"],"tags":["分布式","spark"]},{"title":"Supervisor 的简介与使用","url":"/2016/08/23/Supervisor%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E4%BD%BF%E7%94%A8/","content":"Supervisor 是\nLinux\n下一个进程管理的工具，主要的功能包括让程序自动启动、程序奔溃后自动重启，指定进程的数目等。本文主要讲述 Supervisor 在 Linux 下的安装与使用。\n\n安装\n由于 Supervisor 是用 python 写的，因此推荐利用\neasy_install supervisor 或 pip install supervisor\n进行安装。除此之外，还可通过 Linux\n的包管理命令进行安装（源中需要包含这个包），如 Centos 下可通过\nyum  install supervisor 进行安装，Ubuntu\n下可以通过 apt install supervisor 安装。\n配置\nSupervisor\n的配置文件就只有一个，在安装完成后通过 echo_supervisord_conf &gt; /etc/supervisord.conf\n将创建一个默认的配置文件 /etc/supervisord.conf, 当然也可以指定配置文件在其他目录下。配置文件以 [] 来隔离每部分的配置内容，并且以 ; 为注释符号。\n因为 Supervisor\n由三部分组成：supervisord、supervisorctl、inet_http_server。因此配置文件也分别根据这三部分阐述。需要注意的是 supervisorctl、inet_http_server 并非是必须要配置的，这两个均是连接 supervisord 的客户端，用于观察和管理\nsupervisord 监控的程序。\nsupervisord\nsupervisord\n是 Supervisor 的核心，主要用与启动程序，在程序奔溃时自动重启，设定程序的进程数目、输出的日志文件路径等。\nsupervisored\n有多个参数，下面主要讲述其中几种较为重要的最简配置，每个参数的含义可参考官方文档。\n下面就是 supervisord 配置的一个简单例子 [supervisord]logfile=/var/log/supervisor/supervisord.log ;日志文件的目录pidfile=/var/run/supervisord.pid ; pid文件目录; logfile_maxbytes = 50MB ;默认的日志文件的大小; loglevel = info ; 默认的日志的记录等级; umask = 002; 默认的进程umask; nodaemon = false ;默认在后台启动，若为true则在前台启动\n上面注释掉的配置项为 supervisord\n的默认配置项，可以不配置。\n除了配置 supervisord\n外，还需要配置被其管理的程序。详细的参数可见官方文档，下面是一个简单的例子\n[program:robot] ; 标示一个程序[program:XXX],XXX为自定义的程序名称command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.py;运行程序所需命令;autostart=true ;默认跟随supervisord启动而启动autorestart=true ;程序退出后自动重启; startretries = 3; 程序出错时，连续重启的最大次数，超过该次数后，进程进入FATAL转态; startsecs = 1; 程序启动后多少秒内认为其启动成功; numprocs = 1  ;启动的进程数目，默认为1; priority = 999 ; 程序的优先级，默认为999，该值越小，表示优先级越高user=root ;程序启动的用户，只用root用户才能指定这一项；不指定时该值为启动supervisord的用户stdout_logfile = /home/amazon/log/Robot.log ;存储程序标准输出流的文本文件stderr_logfile = /home/amazon/log/Robot_err.log ; 存储程序出错时错误提示的文件; stopasgroup = false;以进程组的方式停止进程，默认为false，以上面为例，假如为false时，停止该程序时只会停止 python运行的程序，而不会停止 xvfb 程序\n上面注释的配置项为程序默认的配置，可以不配置。上面给出的 supervisord 和 program 为最简配置，仅配置这两项就可以运行supervisor。运行方式为\nsupervisord -c /etc/supervisord.conf,\n-c 参数指定了配置文件的路径，不指定该参数时会以一定的路径顺序寻找配置文件，并且会抛出warning，因此建议启动时要带有此参数。\n上面的配置虽然能启动这些程序，但是当 supervisord 管理多个程序时，需要关闭或开启其中的一个程序就必须关闭 supervisord，然后修改配置文件并重启。为了单独管理这些程序，并直观看到每个程序的运行状态，就需要配置下面的 supervisorctl 和 inet_http_server。\ninet_http_server\ninet_http_server 是 supervisord\n内置的一个 http 浏览器，用于查看、管理每个程序的运行状态，配置项如下：\n[inet_http_server]port = 110.64.55.128:9001username = XXXXpassword = XXXX\n上面的配置应该比较容易理解，访问 port 后输入用户名和密码验证身份后即可观察到程序运行的状态，下图为访问时观察到的效果。\n\n上图可以看到每个程序的运行状态，pid 以及运行时长，还可以改变程序的运行状态。\nsupervisorctl\nsupervisorctl 的功能与 inet_http_server 一样，只是 inet_http_server 是有界面的，而 supervisorctl 是在命令行下使用的，配置项如下：\n[supervisorctl]serverurl = http://110.64.55.128:9001 ;http服务器的地址username = XXXX ;与 [inet_http_server] 配置项的username相同password = XXXX ;与 [inet_http_server] 配置项的password相同\n通过\nsupervisorctl 即可观察到程序运行的状态，如下图所示\n\n同时可以通过\nsupervisorctl start|stop|restrt XXX 来启动、停止、重启程序 XXX，其中 XXX 为配置 [program:XXX] 指定的名称。\n下面是综合以上所说的完整的 supervisord.conf 配置文件\n[supervisord]logfile=/var/log/supervisor/supervisord.log  pidfile=/var/run/supervisord.pid  [program:robot] command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.pyautorestart=true stdout_logfile = /home/amazon/log/Robot.log stderr_logfile = /home/amazon/log/Robot_err.log [inet_http_server]port = 110.64.55.128:9001username = XXXXpassword = XXXX[supervisorctl]serverurl = http://110.64.55.128:9001 username = XXXX password = XXXX \n从上面可知，既然 supervisorctl 提供的功能和 inet_http_server 的相同，那么是否可以不启动 http 服务器，仅仅通过 supervisorctl 进行观察呢？\n答案是可以的，但是要通过 unix socket 与 supervisord\n通信，将上面的 [inet_http_server] 部分改成 [unix_http_server]，并修改 [supervisorctl] 的 serverurl 部分，完整的配置文件如下。\n[supervisord]logfile=/var/log/supervisor/supervisord.log  pidfile=/var/run/supervisord.pid  [program:robot] command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.pyautorestart=true stdout_logfile = /home/amazon/log/Robot.log stderr_logfile = /home/amazon/log/Robot_err.log [unix_http_server]file=/var/run/supervisor.sock   ; (the path to the socket file)username = XXXXpassword = XXXX[supervisorctl]serverurl = unix:///var/run/supervisor.sockusername = XXXX password = XXXX \n使用\n综上，supervisor 的一般的使用方法为如下：\n1）配置好需要启动的程序\n2）通过 supervisord -c /etc/supervisord.conf 启动 supervisord\n3）通过 supervicorctl 和日志文件查看每个程序状态，通过 supervicorctl start|stop|restart XXX 在不影响其他程序的情况下改变某个程序的运行状态。\n更详细的内容请参考官方文档\n","categories":["工具使用"],"tags":["Linux","工具使用"]},{"title":"Wand 算法介绍与实现","url":"/2018/03/18/Wand%20%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E7%8E%B0/","content":"本文主要介绍 Wand (Weak And) 算法的原理和实现，Wand\n算法是一个搜索算法，应用在 query 有多个关键词或标签，同时每个 document\n也有多个关键词或标签的情形（如搜索引擎）；尤其是在 query\n中的关键词或标签较多的时候，通过 Wand 能够快速的选择出 Top n 个相关的\ndocument，算法的原始论文见 Efficient\nQuery Evaluation using a Two-Level Retrieval\nProcess，本文主要讲述这个算法的原理以及通过 python\n实现这个算法。\n\n一般来说，检索往往会利用倒排索引，倒排索引能够根据 query\n中的关键词快速检索到候选文档，然而当候选文档集合较大时，遍历整个候选文档所需要的时间也很大。\n但是检索需要得到的往往只是 Top n\n个结果，在遍历候选文档过程中能否跳过一些与 query\n相关性较低的文档，从而加速检索的过程呢？Wand 算法就是干这个事的。\nWand 原理介绍\nWand\n算法通过计算每个词的贡献上限来估计文档的相关性上限，并与预设的阈值比较，进而跳过一些相关性一定达不到要求的文档，从而得到提速的效果。\n上面这句话涵盖了 Wand 算法的思想，下面进行详细说明：\nWand 算法首先要估计每个词对相关性贡献的上限（upper\nbound），最简单的相关性就是\nTF-IDF，一般 IDF 是固定的，因此只需要估计一个词在各个文档中的词频 TF 上限 (即这个词在各个文档中最大的 TF)，该步骤通过线下计算即可完成。\n线下计算出各个词的相关性上限，可以计算出一个 query\n和一个文档的相关性上限值，就是他们共同出现的词的相关性上限值的和，通过与预设的阈值比较，如果 query\n与文档的相关性大于阈值，则进行下一步的计算，否则丢弃。\n在上面过程中，如果还是将 query\n和一个一个文档分别计算相关性，并没有减少时间复杂度， Wand\n算法通过一种巧妙的方式使用倒排索引，从而能够跳过一些相关性肯定达不到要求的文档。\nWand 算法步骤如下\n\n建立倒排索引，记录每个单词所在的所有文档 ID (DID)，ID\n按照从小到大排序\n初始化 posting 数组，使得 posting [pTerm] 为词 pTerm\n倒排索引中第一个文档的 index\n 初始化 curDoc = 0（文档 ID 从 1 开始）\n\n接着可以执行下面的 next 函数 (摘自原始论文),\n\n\nnext function\n\n上面流程中用到的几个函数的含义如下\n1. sort(terms, posting)：根据 posting\n数组指向的当前文档 ID，对所有的 terms 从小到大排序。如下是三个 term\n及其对应的索引文档的 ID，此时的 posting 数组为 [1, 0, 1],\n则根据各个 term 当前文档 ID 排序的结果应该是 t1, t2, t3\nt0: [3, 26] t1: [4, 10, 100] t2:\n[2, 5, 56]\n2. findPivotTerm(terms,\nθ)：按照之前得到的排序，从第一个 term 开始累加各个 term\n的相关性贡献的上限（upper\nbound，UB），这个在之前已经通过离线计算出来；直到累加和大于等于设定的阈值\nθ, 返回当前的 term。这里应用这篇文章的一个例子，下面为通过\nsort (terms, posting) 后的倒排索引，假设阈值 θ = 8\n\n\npivot term\n\n对于 doc 2，其可能的最大得分为 2&lt;8\n对于 doc 4，其可能的最大得分为 2+1=3&lt;8\n对于 doc 5，其可能的最大得分为 2+1+4=7&lt;8\n对于 doc 23，其可能的最大得分为 2+1+4+3=10&gt;8 因此，t3 为 pivotTerm，doc\n23 为 pivot\n3.\npickTerm(terms[0..pTerm])：在 0 到 pTerm (不包含 pTerm) 中选择一个 term，关于选择策略，当然是以可以跳过最多的文档为原则，论文中选择了\nIDF 最大的 term。以上面的图为例子，此时可以选择 t2, t1 或 t4, 根据其 IDF\n值选择最大的 term 即可\n4. aterm.iterator.next(n)：返回 aterm\n这个单词对应的倒排索引中的文档 ID (DID)，这个 DID 要满足 DID &gt;= n。则\nposting[aterm] ← aterm.iterator.next(n) 其实就是更新了\naterm 在 posting 数组中的当前文档，从而跳过 aterm\n对应的索引中一些不必要计算的文档。\n还是以上面的图为例子，假如选择的 aterm 为 t2, 则 t2 中指向 2\n的指针要往后移动直至 DID &gt;= 23 , 这样便跳过了部分不必计算文档。\n实际上，t1, t4 也可以执行上面这个操作，因为在 doc 23 之前的\ndoc 的得分不可能达到阈值 θ(因为 DID 是经过排序的) ，所以 t2、t1、t4 对应的\nposting\n数组中的项都可以直接跳到大于等于 doc23 的位置，但是论文中每次只选择一个\nterm ，虽然多迭代几次也能达到同样效果，但是我认为这里可以三个 Term\n可以一起跳。\n介绍了上面过程中几个重要函数，下面来看一下上面的几个分支分别表示情况\n\nif (pTerm = null) return (NoMoreDocs) 表示当前所有 term\n的 upper bound 和达不到阈值 θ ，结束算法\nif (pivot = lastID) return (NoMoreDocs)\n表示当前已经没有满足相关性大于阈值 θ 的文档，结束算法\nif (pivot ≤ curDoc) 表示当前 pivot 指向的 DID\n已经计算过相关性，需要跳过，这部分代码会在下面第 4 步执行后在进入循环时执行\nif (posting[0].DID = pivot) 表示当前 pivot\n对应的文档的相关性有可能满足大于阈值 θ ，返回这篇文档的 ID\n并计算这篇文档和 query\n的相关性；posting[0].DID = pivot\n表示从第一个 term 到当前的 term 所指向的文档都是同一篇\nif (posting[0].DID = pivot) 对应的else语句\n表示前面遍历过的那些 term 的当前 DID 都不可能满足大于阈值\nθ，因此需要跳过，也正是这里大大减少了需要计算相关性的文档数量\n\nWand 的实现代码\n实现 Wand 算法的 Python 代码见这里，参考这篇文章的代码进行了修改，并增加了评估文档和 query 相似性的函数，代码中有以下几点需要注意\n\n当一个 term 对应的所有 document\n遍历完后，有两种处理方法。第一种方法是直接删除，这样会降低每次排序的时间复杂度和内存占用率，但是每次删除时候是要在一个有序列表内删除，时间复杂度为\n\\(O(n)\\), \\(n\\) 为 terms 的个数；第二种方法是在每个\nterm 的 document list\n最后增加一个比所有文档 ID 都要大的数 (LastID)，这样被遍历完的 term 会自然被排序到最后，整个代码更加简洁。两种方法都尝试了一下，详细代码可见上面的代码连接的提交历史\npickTerm 方法原论文采用的是选择 idf\n最大值的 term，这里直接选择第一个，因为代码仅用于阐述算法的流程，各个\nterm 没有 idf 值。当然，如果有各个 term 的 idf 值，是可以根据 idf\n选择的\n上面伪代码的算法流程中最后的 else 语句是选择 pivotTerm\n中的任意一个并跳过相关性低的文档，但是从前面的解释可知，可以 pivotTerm\n前面的所有 term 都可进行这一操作，因此代码里面的这部分跟伪代码不同\n\n这里还是给出完整代码，可以对照着上面的伪代码看，命名方法基本都保持了一致，如有错漏，欢迎指出\nimport heapqUB = {\"t0\":0.5,\"t1\":1,\"t2\":2,\"t3\":3,\"t4\":4} #upper bound of term's valueLAST_ID = 999999999999 # a large number, larger than all the doc id in the inverted indexTHETA = 2 # theta, threshold for chechking whether to calculate the relevence between query and docTOPN = 3 #max result number class WAND:    def __init__(self, InvertIndex):        \"\"\"init inverted index and necessary variable\"\"\"        self.result_list = [] #result list        self.inverted_index = InvertIndex #InvertIndex: term -&gt; docid1, docid2, docid3 ...        self.current_doc = 0        self.current_inverted_index = {} #posting        self.query_terms = []        self.sort_terms = []        self.threshold = THETA        self.last_id = LAST_ID    def __init_query(self, query_terms):        \"\"\"init variable with query\"\"\"        self.current_doc = 0        self.current_inverted_index = {}        self.query_terms = []        self.sort_terms = []        for term in query_terms:            if term in self.inverted_index:  # terms may not appear in inverted_index                doc_id = self.inverted_index[term][0]                self.query_terms.append(term)                self.current_inverted_index[term] = [doc_id, 0] #[ docid, index ]                self.sort_terms.append([doc_id, term])    def __pick_term(self, pivot_index):        \"\"\"select the term before pivot_index in sorted term list         paper recommends returning the term with max idf, here we just return the firt term,         also return the index of the term instead of the term itself for speeding up\"\"\"        return 0    def __find_pivot_term(self):        \"\"\"find pivot term\"\"\"        score = 0        for i in range(len(self.sort_terms)):            score += UB[self.sort_terms[i][1]]            if score &gt;= self.threshold:                return [self.sort_terms[i][1], i] #[term, index]        return [None, len(self.sort_terms)]    def __iterator_invert_index(self, change_term, docid, pos):        \"\"\"find the new_doc_id in the doc list of change_term such that new_doc_id &gt;= docid,        if no new_doc_id satisfy, the self.last_id\"\"\"        doc_list = self.inverted_index[change_term]        # new_doc_id, new_pos = self.last_id, len(doc_list)-1 # the case when new_doc_id not exists        for i in range(pos, len(doc_list)):            if doc_list[i] &gt;= docid:   # since doc_list contains self.last_id, this inequation will always be satisfied                new_pos = i                new_doc_id = doc_list[i]                break        return [new_doc_id, new_pos]    def __advance_term(self, change_index, doc_id ):        \"\"\"change the first doc of term self.sort_terms[change_index] in the current inverted index        return whether the action succeed or not\"\"\"        change_term = self.sort_terms[change_index][1]        pos = self.current_inverted_index[change_term][1]        new_doc_id, new_pos = self.__iterator_invert_index(change_term, doc_id, pos)        self.current_inverted_index[change_term] = [new_doc_id, new_pos]        self.sort_terms[change_index][0] = new_doc_id    def __next(self):        while True:            self.sort_terms.sort() #sort terms by doc id            pivot_term, pivot_index = self.__find_pivot_term() #find pivot term &gt; threshold            if pivot_term == None: #no more candidate                return None            pivot_doc_id = self.current_inverted_index[pivot_term][0]            if pivot_doc_id == self.last_id: # no more candidate                return None            if pivot_doc_id &lt;= self.current_doc:                change_index = self.__pick_term(pivot_index)                self.__advance_term(change_index, self.current_doc + 1)            else:                first_doc_id = self.sort_terms[0][0]                if pivot_doc_id == first_doc_id:                    self.current_doc = pivot_doc_id                    return self.current_doc # return the doc for fully calculating                else:                    # pick all preceding term instead of just one, then advance all of them to pivot                    change_index = 0                    while change_index &lt; pivot_index:                        self.__advance_term(change_index, pivot_doc_id)                        change_index += 1            # print(self.sort_terms, self.current_doc, pivot_doc_id)    def __insert_heap(self, doc_id, score):        \"\"\"store the Top N result\"\"\"        if len(self.result_list) &lt; TOPN:            heapq.heappush(self.result_list, (score, doc_id))        else:            heapq.heappushpop(self.result_list, (score, doc_id))    def __calculate_doc_relevence(self, docid):        \"\"\"fully calculate relevence between doc and query\"\"\"        score = 0        for term in self.query_terms:            if docid in self.inverted_index[term]:                score += UB[term]        return score    def perform_query(self, query_terms):        self.__init_query(query_terms)        while True:            candidate_docid = self.__next()            if candidate_docid == None:                break            #insert candidate_docid to heap            print('candidata doc', candidate_docid)            full_doc_score = self.__calculate_doc_relevence(candidate_docid)            self.__insert_heap(candidate_docid, full_doc_score)            print(\"result list \", self.result_list)        return self.result_listif __name__ == \"__main__\":    testIndex = {}    testIndex[\"t0\"] = [1, 3, 26, LAST_ID]    testIndex[\"t1\"] = [1, 2, 4, 10, 100, LAST_ID]    testIndex[\"t2\"] = [2, 3, 6, 34, 56, LAST_ID]    testIndex[\"t3\"] = [1, 4, 5, 23, 70, 200, LAST_ID]    testIndex[\"t4\"] = [5, 14, 78, LAST_ID]    w = WAND(testIndex)    final_result = w.perform_query([\"t0\", \"t1\", \"t2\", \"t3\", \"t4\"])    print(\"=================final result=======================\")    for i in reversed(range(len(final_result))):        print(\"doc {0}, relevence score {1}\".format(final_result[i][1], final_result[i][0]))\n\n参考资料\nwand (weak\nand) 算法基本思路 WAND 算法核心部分梳理\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"Systematic Trading-Avoid Flawed Human Brain with Systematic Trading Rules","url":"/2024/03/10/Systematic%20Trading/","content":"《Systematic\nTrading: A unique new method for designing trading and investing\nsystems》 is a comprehensive guide for traders and investors seeking\na structured approach to the markets.\nAimed at both novices and experienced traders, the book offers\npractical insights and applications, making it a pretty good resource\nfor anyone looking to enhance their trading methodology with a\nsystematic approach.\nThis is not a book totally about automating trading strategies. It’s\npossible to trade systematically using an entirely manual process with\njust a spreadsheet to speed up calculations, so automation is not\nnecessary, the key word if \"systematic\". This passage mainly covers the\nfirst two chapters, providing a rough overview of this book. Hope you\ncan enjoy it\n\nFlawed Human Brain\nThree types of traders\nThere are three types of traders in this book. To a certain extent,\nthese three types are determined and classified by the degree of\nautomation with which they execute transactions.\n1. asset allocating investor\nAsset allocators can use systematic methods to avoid the short-term\nchasing of fads and fashions that they know will reduce their returns.\nThey might be lazy and wise amateur investors, or managing institutional\nportfolios with long horizons such as pension funds.\nAsset allocators are sceptical about those who claim to get extra\nreturns from frequent trading. For this reason the basic asset\nallocation principle assumes you can’t forecast how asset prices will\nperform.\n2. semi-automatic trader\nSemi-automatic traders think they are superior to simple rules when\nit comes to forecasting by how much prices will go up or down; instead\nthey make their own educated guesses\nThey would like to place those bets inside a systematic\nframework which will ensure their positions and risk are properly\nmanaged. This frees them up to spend more time making the right\ncall on the market.\n3. staunch systems trader\nThe staunch systems trader is a true believer in the benefits of\nfully systematic trading. Unlike the semi-automatic trader and the asset\nallocating investor, they embrace the use of systematic trading rules to\nforecast price changes, but within the same common framework for\nposition risk management.\nAvoid mistake instead of\nsilver bullet\nThis book claims that there is no magic system that will\nautomatically make huge profits for you, and you should be wary of\nanyone who says otherwise, especially if they want to sell it to\nyou.\nInstead, success in systematic trading is mostly down to\navoiding common mistakes such as over complicating your system,\nbeing too optimistic about likely returns, taking excessive risks, and\ntrading too often.\nThe aim of this book isn’t just to offer a single predefined system\nfor trading, but to provide a modular framework which can be adapted to\nmeet your needs. Part three describes the framework in detail. Just like\nyou can choose different engines and tyres on a car, my framework\nincludes options for different trading rules and position sizing\ncalculations.\nOverconfidence leads\nto flawed human brain\nMeddling is due to the biggest cognitive bias of all:\noverconfidence. We think we are cleverer than the\ntrading system and we are. To stop meddling with our trading systems we\nrequire what economists call a commitment\nmechanism.\nCreating a purely objective system is a powerful\ncommitment mechanism.\nA highly subjective rule would be something like ‘Sell for small\nlosses, and hold on to large profit making positions’.\nAnother advantage of objective systems is that they can be automated,\nand automation is a good commitment mechanism\nBut automation doesn’t stop meddling, having a system which requires\na human to do the actual execution of automatically generated trades is\nstill prone to meddling, so it must be done with a well designed\nsystem in which you have full confidence.\nOne simple principle is systems whose performance and behaviour can\nbe explained will be more preferred, while there are some\npitfalls to avoid when designing trading systems: over-fitting,\novertrading and over-betting\n\nover-fitting\n\nIf we’re not careful the rules selected will fit the data too well.\nThey will be highly tuned to one set of market conditions and perform\nworse in actual trading than a simpler rule would.\n\novertrading\n\nBoth amateur investors and professional managers have a tendency to\ntrade too frequently.There are extra costs involved in trading more\noften.\nOvertrading suggests overconfidence in your own relative ability to\novercome the higher hurdle of bigger costs.\n\nover-betting\n\nIf your confidence is unbounded you can bet the maximum that your\nbroker allows you to. Potentially this could be a ten-fold leverage on\nan individual equity bet, equating to an annual standard deviation of\nreturns of around 200%.\nSystematic Trading Rules\nThis section mostly relates to how trading rules are fitted. This is\nless applicable to asset allocating investors and semi-automatic\ntraders\nIdeas first or data first\n\nThe author favors the ideas first method. This usually results in\nintuitive, simpler and more transparent rules. In the author's\nexperience, consistently profitable trading comes out of careful\nresearch, done by thoughtful and knowledgeable people, who seek to\nunderstand where their profits come from.\nBut some situations the data first process could be better; for\nexample in high frequency trading where there is plenty of data, rules\ncan be refitted regularly and novel ideas are more likely to be found as\nmarket structure evolves.\nWhat makes a good trading\nrule\nThis book identifies the following four aspects as the most\ncrucial:\n\nexplainable profits\n\nWith an ideas first approach you can easily explain why a strategy\nwas profitable, since the explanation is inherent in the original idea.\nFor example you might have tested a trend following rule, which you\nthink makes money because of cognitive biases explained by prospect\ntheory.\nWith data first any explanation has to follow the fitting process.\nYou first examine how a rule behaves and then infer what might be\ncausing the effect. The more complex a data driven trading rule is the\nharder it will be to explain.\nRemember the narrative fallacy, another cognitive bias: humans like\nstories. To trust a trading rule the author both a good story and a\nrigorous back-test.\n\nintuitively understandable behaviour\n\nA complicated system might buy or sell when prices moved higher,\ndepending on the exact pattern. This would make its behaviour less\nobvious and more unpredictable.\n\nas simple as possible\n\nIt is much easier to explain the profitability and behaviour of\nsimple trading rules, those with few moving parts and no weird\ninteractions.\nIdeas first rules should be simple unless you begin with a very\nconvoluted idea, or over complicate it to get a more profitable\nback-test, neither of which is recommended. Data first rules can be very\nsimple or extremely complicated, depending on how many parameters are\nfitted. A data first rule with more parameters will be less explainable,\nand more vulnerable to over-fitting.\n\ncan be systematised\n\nIdeas need to translated into systematised rules. Not all styles of\ntrading are entirely suitable for this, because they are inherently\nsubjective or because of data\nlimitations.\n\nsubjective\n\nMany methods of trading cannot be systematised because it’s\nimpossible to write down a set of relatively simple, objective and\ngeneric rules. One example would be merger arbitrage, where you have to\nassess the likelihood of a deal going through based on analysis of a\nnumber of hard to quantify factors.\n\ndata limitations\n\nEven if a strategy is objective the necessary data might be\nunavailable. Even if you could write trading rules for merger arbitrage\nthe necessary information about legal and regulatory issues cannot\neasily be converted into an algorithm friendly format.\nThere might also be a shortage of data, cause many years of data are\nneeded to properly test a strategy.\nWhy certain rules make\nprofit\nThis book lists multiple cases to explain why certain rules are\nprofitable\n\nrisk premia\n\nliquidity and size\n\nwhen others have to trade\n\nbarriers to entry, returns to effort and cost\n\nbehavioural effects\n\npure alpha and skill\n\n...\nrisk premia\n\nThere are three types risk premia mentioned in this book\n\npersistent risk premium\n\nSome risk premia persist for long periods, like the extra return\nyou’d expect from investing in equities versus safer assets like\nbonds.\n\ntiming varying risk premium\n\nYou can make profits buying cheap premia and selling expensive ones.\nThis is a form of mean reversion trading, where you assume premia, and\nhence prices, will revert to some long- term equilibrium.\n\nskew and unlikely events premium\n\nThe rational investor of classical financial theory only cares about\nan asset’s Sharpe ratio (SR) – it’s average returns adjusted for their\nstandard deviation.(Formally it is the mean return for a particular time\nperiod divided by the standard deviation of returns for the same time\nperiod; daily v.s annual)\nThis only makes sense if all assets have symmetrically distributed\nreturns.in practice assets with the same SR could make steady losses\nwith occasional large returns or steady gains with occasional large\nlosses\n\npositive skew VS negative skew\n\nWhen an asset has a higher chance of a large down move than an\nequivalent up move, it is said to have a negative skew. If large up\nmoves are more likely then it has positive skew.\n\nWith the same Sharpe ratio, the returns from a positively skewed\nasset will contain more losing days than for those of a negatively\nskewed asset. But the losing days will be relatively small in\nmagnitude.\nEquities normally have mildly negative skew. ‘Safe haven’ assets like\ngold and Swiss francs tend to have positive skew.\n\nliquidity and size\n\nThe degree of liquidity in an asset is how easily we can buy or sell\nwithout unduly affecting the price.\nThe premia for liquidity, size and leverage can vary over time,\npresenting opportunities for well timed buying and selling.\n\nwhen others have to trade\n\nNot everyone trades because they are trying to make money. Some are\nforced to.\nA foreign exchange carry trading rule that borrows in low interest\nrate currencies like Switzerland and invests in high interest rate\ncurrencies will be consistently profitable as a result, at least until\nthe currency policy is abandoned\n\nbarriers to entry, returns to effort and cost\n\nSome trading rules have barriers to entry in the form of costs or\ninvestments that need to be made to realise profits from them. High\nfrequency strategies require renting expensive servers located\nphysically within exchange buildings, as well as developing specialised\nsoftware.\nIf an opportunity requires a lot of work to exploit, then it may be\npassed up by most investors and additional returns would not be\navailable.\n\nbehavioural effects\n\nYou can create rules which extract returns from other people’s\nbehavioural weaknesses.\n\npure alpha and skill\n\nThose who exhibit pure skill can adapt to changing market\nopportunities in a way that a systematic rule never could. If you think\nyou are part of this elite group then the semi- automatic trader example\nis for you.\nTrading styles\nstatic vs dynamic\nStatic means you invest your portfolio and then do nothing, while\ndynamic is to rebalance your protfolio the from time to time\nThe most intuitive dynamic rule is to re-balance your portfolio so\nyou keep the same cash value in each company.\nYet another rebalance rule is to equal risk instead of equal cash\nvalue, which is risk parity\ninvesting. This method assumes risk is equal to the recent standard\ndeviation of the daily percentage changes in price for each firm.\n\nRisk parity is an advanced portfolio technique often used by hedge\nfunds and sophisticated investors. It requires a complex quantitative\nmethodology, which makes its allocations more advanced than simplified\nallocation strategies. The goal of risk parity investing is to\nearn the optimal level of return at the targeted risk\nlevel.\nSimplified allocation strategies such as the 60%/40% stocks-bonds\nportfolio make use of MPT. MPT provides a standard for diversification\nwithin one's investment portfolio that maximizes expected return for a\ngiven level of risk. In simplified MPT strategies using just stocks and\nbonds, allocations are usually more heavily weighted toward equities for\ninvestors who wish to take on more risk. Risk-averse investors will\ninstead have a higher weight in bonds for capital preservation.\n\n\npredictable risk vs unpredictable risk\n\nIt’s very hard to forecast what the return will be each day over the\nnext few weeks for an asset or portfolio. But you can model and estimate\nwhat the variation in returns is likely to be. Estimates based entirely\non assuming that recent levels of variation will persist tend to be\nrelatively good\nskew: positive vs negative\nThe term \"skew\" mentioned here, just like the \"skew\" referred to in\nthe context of risk premium earlier, which usually can be classified\ninto positive skew and negative skew\nIf you are using a systematic trading rule and have access to\nback-testing technology you can measure your skew. Otherwise you will\nneed to make a judgment on what it is likely to be based on your trading\nstyle.\nThe characteristic of positive skew and nagative can be listed as\nfollows\n\nSome examples of positive skew and negative skew\n\nBe aware that if you are making steady profits nearly every day, and\nmost of your trades are winners, then there is a good chance you are\nengaged in negative skew trading. It’s just that you haven’t yet seen\nany rare large losses.\ntrading speed: fast vs slow\nTrading speed can be roughly classified into three types: very slow,\nmedium and fast, depends on the average holding period\nVery slow systems usually hold assets for several months to\nmany years. While the average holding period of medium is\nusually a few hours or days, to several months. But for\nfast system, the holding period is usually microseconds to one\nday\nVery slow systems look almost like static portfolios, with additional\ngradual changes in position coming from their trading rules. Rules often\ninvolve mean reversion to very long run equilibrium such as relative\nvalue equity portfolios that buy past losers, and sell recent\nwinners.\nIn medium systems, trading costs need to be accurately measured to\ndecide whether an instrument should be traded at a faster or slower\nspeed within this region. Large institutional traders also need to\ndetermine if a market has the capacity to absorb their trading. These\nsubjects will be covered in chapter twelve, ‘Speed and Size’.\nFor fast system, typical raw Sharpe ratios could be very high due to\nthe number of trades made, but costs will chew up a big chunk of\nprofits. Special execution algorithms are needed to reduce costs below\nnormal levels. There are higher barriers to entry than at slower speeds;\nco-located servers and fully automated software is needed. Also faster\nstrategies are likely to have limited capacity. The domain of high\nfrequency trading mostly falls outside the scope of this book.\n\nLAW OF ACTIVE MANAGEMENT\n\nThere is a law called LAW\nOF ACTIVE MANAGEMENT, which states that the Sharpe ratio of\na trading strategy will be proportional to the square root of the number\nof independent bets made per year.\nThis law gives us some idea of how profits should vary with trading\nspeed. Suppose you’re holding one asset at a time, and making one ‘bet’\nper year (buying, holding for 12 months before selling, then repeating),\nand that you expect an SR of 0.15 from this activity.\nIf you decide to make four ‘bets’ a year, holding the asset for three\nmonths, then because 2 is the square root of 4 you should expect an SR\nof 2 × 0.15 = 0.30.\nIf you begin betting every single business day, then with around 256\nbusiness days in a year the SR will be 16 times larger than 0.15 or\n2.4\nThe other important implication of the law is that diversification\nacross assets can substantially improve returns. If you can find four\nassets which have zero correlation then you can double your Sharpe\nratio.\nBut the flaws of this law is also obvious, it assumes that the\n‘skill’ of the trader is constant across holding periods (but the skills\nrequired to day trade are very different from those a long-term investor\nneeds). It also ignores transaction costs；these can seriously damage\nthe returns of fast traders\n### technical vs fundamental\nStrategies vary in the source of data they use, either using\ntechnical or fundamental information, or both.\nPurely technical rules only use price data. While\nfundamental data, comes in two main flavours: micro and\nmacro. Micro data is about a specific asset, for example the\nyield of a particular bond or the PE ratio of a company. Macro data such\nas inflation and GDP growth covers entire economies.\nTechnical systems are easier to build and run, but in another example\nof barriers to entry the additional effort required for including\nfundamental rules is usually rewarded with higher returns. The examples\nin this book are all technical\nportfolio sizep\nAs the law of active management shows that diversification is the\nbest source of additional risk adjusted returns. Both traders and\ninvestors should hold more positions when they can; ideally across\nseveral asset classes to get the greatest possible benefit. With larger\nportfolios you’re also less exposed to instrument specific problems such\nas bad data or temporary liquidity issues.\nHowever smaller portfolios make sense for semi-automatic traders or\nfor those running entirely manual systems. In chapter twelve, ‘Speed and\nSize’, those with relatively small accounts also have to limit the\nnumber of positions they take.\nleverage: more risk and\nyet more skew\nLeverage is borrowing to invest or trade. The borrowing can be\nexplicit as when trading equities on margin, or implicit as with\nderivatives like futures. Usually one needs leverage when the natural\nreturn and risk of an asset is less than desired.\nFor example, if you want a 10% return on average, and your asset is a\nbond with an expected Sharpe ratio of 0.5, but only a 5% annualised\nstandard deviation, then you need to lever up four times\nLeverage can be dangerous, especially for negative skew, where we\ngain small consistent gains until the trade goes terribly wrong. These\nsteady gains tricked you into thinking that the position was low risk,\nand lured you into using leverage to improve your returns. So be aware\nof gearing up on apparently low risk with an asset or with a style of\ntrading which is likely to have negative skew, even if you haven’t yet\nseen any evidence of the large downside\ncontrarians,\nmarket followers and crowded trades\nThree types of trades: contrarians, market followers and crowded\ntrades\nMean reversion and relative value traders act as contrarians, they\nseek to take advantage of mis-pricing which means buying low after falls\nand selling when the price has risen. Contrarian traders like to catch\nfalling knives.\nMarket followers usually prefer various forms of trend following.\nTrend followers will close positions that have started to lose money,\nlike the early loss taker trading rule.\nWhen the majority of market participants have the same bet on, which\ncould be from behaving as contrarians or market followers.As the story\ngoes when the shoeshine boy or the taxi driver is in the market, it’s\ntime to get out, cause the market if full of\ncrowded trades.\nAchievable Sharpe ratios\nIt’s important to have a realistic sense of what level of Sharpe\nratios (SR) are achievable. Inflated expectations can lead to over\nbetting, which will be discussed more in chapter nine, ‘Volatility\nTargeting’ and overtrading\nThe simplest possible risky investments, is a long only equity\nposition in one company, excess equity returns on single equities will\nprobably average around 3% a year in the future; with annualised\nstandard deviation of around 20%, this implies an SR of 3% ÷ 20% =\n0.15 is realistic\nWith the law of active management, investing in a portfolio of\nequities will do slightly better. If you invest globally across multiple\ncountries you can probably get to a Sharpe ratio of around\n0.25\nTo do much better you’d need to allocate across multiple asset\nclasses: equities, bonds, commodities and so on. Since correlations\nbetween asset classes are usually low, a portfolio covering several of\nthese types of assets can probably expect to reach a Sharpe ratio of\naround 0.40\nMany traders have highly unrealistic expectations of back-tested\nSharpe ratios of 2.0, 3.0 or even higher; just on single instruments!\nThese values are far too optimistic and are caused by over-fitting.\nIn reality SR consistently greater than 1.0 are rarely\nachieved, even by sophisticated institutional investors.\nUsually there are two ways to increase SR, (1) trade negative\nskew strategies (2) trade more quickly\nHowerever, taking into account the cost of each trading, trading more\nquickly may not lead to high SR, as show in the image below\n\nConslusion\nThis passage roughly talks about how to avoid flawd human brain with\nsystematic trading rules, which empahsizes why we need trading rules,\nand how to measure whether rules are good or not. Different types of\ntraders and trading styles are also introduced in this passage, showing\nthose various factors that will affect your profits\nIt’s important that you understand and can cope with the risks of\nyour trading system. Finding the best trading rules is less important\nthan designing your trading system in the correct way. A balanced\ncombination of trading rules, with different styles that work in\ndifferent environments, is better than any single alternative.\n","categories":["读书"],"tags":["读书","量化交易"]},{"title":"Yet Another Overview of an AD System","url":"/2024/04/05/Yet%20Another%20Overview%20of%20An%20AD%20System/","content":"之前写的 An\nOverview of an AD System,\n从技术原理上介绍了各个模块（召回、精排、出价、冷启动等）的基本职责和原理，几年过去了，这部分的认知虽然还没过时，但是经历了更多业务后，对整体的商业化也有一个更全面认知，本文尝试从另一个更系统的角度去理解一个广告系统\n传统认知中的 ad system\n一般是三方：广告主 / 代理、平台、用户；但是随着内容平台（如抖音、快手、小红书、bilibili 等）的迅速发展，涌现了越来越多的\nUGC\n内容，创作者在商业变现中的影响也越来越难被忽视，所以这里基于三方增加了代表创作者的第四方，如下图所示\n\n以上的四方比较复杂的关系，一般是存在于 “一方流量”（参考一方数据的概念）上，即抖音 / 快手 / 小红书 /bilibili 这类有能力搭建自己的一方流量的变现团队，在自家的流量上变现；相较于 “一方流量”，“三方流量” 的场景一般只需要关注客户和平台的关系，典型的就是联盟的场景（穿山甲、优量汇、快手联盟等），对用户侧没有强体验约束，因为本质上联盟就是个倒卖流量的生意，相关技术与一方流量差不多，但是对\nC 端的用户体验以及创作者部分基本不怎么关注。\n本文重点在一方流量上，下面的内容会根据上图中提到四方依次讨论每一方本身的一些职责、与其他各方的关系，内容会比较发散，祝开卷有益～\n\n上图中的箭头可以理解为一方与另一方之间的需要交付的目标，以平台为例，作为连接各方的枢纽，平台需要承担以下的职责\n\n对于用户，需要保证营销内容对用户体验损失在红线以内，一般量化的指标是\ndau loss；常见的 load\n约束、频控、负反馈策略，混排时的一些硬约束等，都是在尝试在优化这部分；而商业化的目标，就是在\ndau loss 红线内，最大化商业化收入\n\n对于客户 (广告主)，是平台需要重点关注的对象，因为客户就是平台的平台的金主，一句话描述这部分：需要在客户表达预算和素材后，保证约定的成本约束下，最大化跑量；为了精细化地服务好客户，还会分行业（电商、游戏、教育等）、分客户类型（KA、SMB）等去进行优化，提供更丰富的营销目标与更好的投放体验，为客户在平台的长期经营提供更好的土壤\n\n对于创作者，平台需要为创作者提供变现商机（促成创作者与客户的合作，\n在星图、蒲公英、聚星、花火等官方合作平台上完成），平台在这个过程中也需要考虑各种问题（创作者生态、内容生态、水下问题等）；同时也要考虑创作者成长，为其提供一些机制策略，各种流量加热工具（付费\nor 免费），相应地创作者需要为平台创作好的内容\n\n平台\n前面提到，作为连接各方的枢纽，平台肩负着体验（C 端与 B\n端）、变现 (平台与创作者)、生态 (客户、创作者与内容) 等各个职责，需要为各方交付相应的业务指标\n体验：C 端与 B 端\n根据用户的分类，这里的体验又可以分为 C 端用户与 B 端的广告主\n平台需要为 C 端交付的体验，就是用户体验；最直接衡量指标就是 dau\nloss，即不能因为营销内容过多或者内容不符合用户的兴趣，而导致了用户的流失；技术手段上，无论是\nload\n约束、频控 / 负反馈 / 分人群等策略、或者混排时的一些硬约束（广告首位，min-gap\n等），都是尝试在优化这部分；在后续的用户部分会展开这部分\n平台需要为 B\n端交付的体验，就是广告主的体验；一般直接的度量指标就是广告主的 nps\n调研，但这个指标往往没法做直接的优化；而计划起量、掉量、跑量、成本、系统\nvariance 等问题，都会影响广告主的体验，直接或间接地影响广告主的 nps\n以及在平台上的长期预算，解决这些问题是有可能提升 B\n端的用户体验的；其中成本问题是基础也是最重要，往往是通过 ctr、cvr\n的预估准确性做基本保障，同时通过 “出价 + 计费” 的方式来做兜底，另外针对计划的掉量、拿量波动的问题有稳定性专项、针对计划的起量问题有冷启动专项等\n前面提到，商业化的目标粗略来说，就是在对 C\n端用户体验损害的红线约束内，最大化商业化收入， C\n端的用户体验这部分有比较明确的度量指标：ad load 与 dau\nloss，前者比较好度量，后者往往观测性没那么好，需要把观测周期拉的非常长，往往需要找一些关联的中间指标如时长等来做直接的优化，但总体优化方向是较为明确的\n但对于 B 端广告主，没有那么直接可度量的指标，nps\n也许算一个，但不是能够直接优化的指标，所以往往会将广告主体验部分拆成好几个方向来进行，如起量（通过冷启动扶持来加速计划的起量）、掉量（掉量的识别与扶持等）、系统\nvariance（计划的复制与裁剪）、成本（纠偏与出价）等，这些体验的交付，最终是为了\nB 端的广告主能够在平台长期投入更多预算\n变现：平台与创作者\n变现也分为两部分：平台变现与创作者变现：平台变现就是常规的通过卖流量的广告变现，创作者变现指的创作者通过为客户或平台创作好内容获得报酬，也是近几年在内容平台发展过程中逐步衍生出来的一个方向\n\n平台变现\n\n平台的变现即广告变现，往往需要把下面三件事情做好，而一些互联网公司也会根据这几个方向来建设相应的组织架构（排序组、流量组、行业组）\n1、排序效率：提升链路的排序效率，如模型\nauc，链路一致性\n2、流量机制：明确链路的排序公式和一些辅助策略，以满足流量需要交付的目标（消耗、体验、生态等）\n3、客户产品：服务于客户的诉求（营销目标多样性、成本、跑量、低门槛投放、数据隐私等）为客户提供的各种广告主产品，这部分会在客户部分详细展开\n从技术原理上来讲，上面这几件事情都是在让平台匹配效率最大化，或者说流量售卖效率的最大化，因为本质上\n收入 = cpm × ad_load ；ad_load\n基本是一个固定的值（虽然有很多变相扩 load\n的方法如全域推广），在技术方向上的需要优化的点就是 cpm\n的增长；如果简单拆解 cpm，可以粗略认为\ncpm ≈ bid × ctr × cvr，为了提升\ncpm，需要分别考虑这几项：bid、ctr、cvr\n\n因素一：bid\n\n先看 bid，直观上看就是竞价流量的 bid 平均水位，对于大部分的 oCPX\n广告，这里的 bid 也往往是广告主的平均出价水位（nobid\n类产品没有广告主出价，但是 bid\n也能一定程度上反映广告主的预算），系统不能在广告主的 bid\n基础上无约束地提，因为广告主的出价表达了其对转化成本的预期，如果一个劲儿地提\nbid，成本是没法很好地保证的，所以这里的 bid\n基本上能理解为 “广告主在某个媒体平台上愿意为一个转化出的价格”，而影响广告主出的\nbid 高低有很多，这里重点讲以下 2 个因素：\n（1）广告主的生意模式：一个比较关键的因素是转化对于广告主而言有没有除了这个转化以外的价值\n一般的白牌商家的预算都属于纯效果预算，都是有比较严格的 roi\n的考核的，这种情况下广告主愿意出的价格在各个媒体上也是比较固定的，与之相反的是纯品牌预算，即对转化基本没有太多考核，像\ngd\n很多时候就是在溢价买流量，这个时候广告主会根据平台的影响力，或者说流量价值来出价；\n而实际中，很多预算其实是介于纯效果预算和纯品牌预算之间的，所以广告主的出价的时候也会根据平台的转化是否能带来外溢价值，来对出价做相应的调整；\n如广告主在小红书等种草平台是能接受赞藏、加粉等成本，比抖音、快手等竞品要更高，因为在广告主心里会认为这些种草平台的一个转化能够带来更多外溢价值\n（2）平台的竞争激烈程度：即广告主能否在平台上卷起来\n在一个媒体平台上，如果竞争不够激烈，如最极端情况下只有一个广告主，那转化的定价权基本上就掌握在广告主的手里（而这也是广告主生态需要解决的问题之一）；当广告主数量变多后，蓝海变成红海，广告主在总体\nroi\n还是为正的前提下，还是有动力降低自己的利润、在平台提升转化出价的成本来做营销的\n所以，假设广告主出价是理性的，那要提高公式里的\nbid，平台长期要做的就是提升其流量价值（比较虚的概念，与市场认可度与声量相关），满足更多广告主的营销诉求，引入更多的广告主进入平台，营造一个良性的竞争环境，这部分主要上面提到的第\n3 部分即客户产品来承当，当然也会涉及到第 1 部分（如\nctr、cvr 预估准确性）第 2 部分（如流量策略影响广告主生态）\n\n因素二：ctr × cvr\n\n其他两项是 ctr 和 cvr，因为比较相似，就放在这里一并说了，提升 ctr 和\ncvr 也有多个因素，这里重点说以下两个：\n（1）首先能想到的就是模型的排序效率，模型的职责就是在用户到来时，为用户选择出\nctr、cvr 最高的 item，所以模型的 auc 提升，理论上是能够带来 ctr、cvr\n的提升\n但实际上在广告系统里排序公式不是 ctr，cvr 最大化，往往是\necpm + hidden_cost, 前者 ecpm=bid × ctr × cvr\n是收入最大化，后者 hidden_cost\n则是为了满足收入以外的目标而加上的各项（如冷启动、掉量等问题）；因此，提升模型的\nauc ，不一定能带来最终的 ctr、cvr 提升，因为本身的排序公式就不是仅由\nctr、cvr 决定的（比如说 ctr、cvr 不是最高的但是 bid\n足够高，综合起来是有可能排在第一位的）\n但是总体来说，模型长期的迭代预期是能带来后验 ctr、cvr\n上涨的，因为对于广告主来说，用低 ctr、cvr\n素材但是高出价不是一种良性的方式，且不是平台所提倡的（平台会引导广告主制作好素材）；因此，对于广告主而言，通过制作好素材提升\nctr、cvr 来跑量，才是一种更加可持续的方式\n（2）另一个重要的因素就是素材，素材质量的优劣直接影响着用户的点击率、转化率等指标，需要提高优质素材的供给；比如说通过撮合平台让\nK 给 B 提供更多高质量素材，通过 AIGC\n的方式生成更多优质素材等，然后平台侧结合模型的排序效率、素材优选等方式，提升总体的\nctr、cvr，平台侧这些动作的一些拓展文章可参考 Dynamic\nCreative Optimization in Online Display Advertising 和 A\nHybrid Bandit Model with Visual Priors for Creative Ranking in Display\nAdvertising\n这部分主要上面提到的第 1 部分（模型效率）和第 2\n部分（排序公式，hidden_cost 的组成）来支持\n\n创作者变现\n\n创作者随着内容平台的发展而发展，当前已经在广告系统中是一个重要组成方，肩负着创作好素材的职责，与之相关的就是创作者变现这个大话题，毕竟创作者不能单纯 “用爱发电”，而是需要有稳定的收入来支撑做内容\n创作者的变现的途径有很多，包括来自 B\n端（如商单推广、直播带货）、来自平台（如平台的补贴）、来自 C\n端（如直播打赏、充电等）等，参考 B\n站的创作者变现介绍\n\n来自 B\n端的商单是最常见的方式，客户（以下简称 B）侧找到创作者（以下简称 K）侧写对应的推广的内容，这是一个典型的撮合过程，很多内容平台也建立相关的官方平台（星图、蒲公英、聚星、花火）来促成 B 与 K 的合作，B\n侧走官方平台报备，除了需要为 K\n提供一口价结算或效果化结算（即根据转化数来结算），还需要为平台提供一定比例的佣金，这部分的内容我们称为 “水上内容”\n但除了 “水上内容”，还有不少内容会走 “水下”，即 B 与 K\n不走以上提到的官方平台，而是直接达成合作，B\n是有动力去做这件事的，因为这样最直接的收益就是 B\n侧没有了额外的需要支付的佣金，且内容更贴近 ugc\n的（一些平台对于走了撮合的内容是要打标的）；但是对于平台而言，肯定是希望这些内容都能够走官方合作平台的，一是佣金；二是平台会希望能够对\nK 制作的内容了解并有一定的管控能力，包括了解 K\n变现的行业、粉丝量分布，并通过影响 K 的商机分配来影响 K\n内容制作方向等\n因此，在创作者变现上，平台的愿景往往是：（1）更多的创作者能够在平台变现（走水上的方式），即变现人数与撮合流水能够持续增长（2）为平台共享贡献好的内容\n而在这个过程中，平台需要解决的一个大问题，就是\n“水下翻水上”，即如何让 B\n侧更多的预算走水上，这部分会在后面创作者重点介绍\n除了来自 B\n端，平台的补贴也是创作者变现的一个来源，一般的补贴包括现金和流量两种；如很多内容平台都会有创作者激励计划，即根据内容播放量或浏览量来给创作者结算；也会有一些流量扶持，如流量券、特定话题下作品的流量扶持等\n生态：客户、创作者与内容\n生态一般是平台发展到一定规模后才需要考虑的问题；这里的生态指的是非红线的生态问题，即不会一旦出现了平台就要倒闭的那种（比如政治敏感话题）；而是短期不管看不出什么大问题，但是放任不管，不利于平台长期可持续发展，如计划冷启动、广告主的多样性、素材多样性、创作者变现分布等等\n这是个比较典型的规模与生态的问题，规模小的时候这类问题不显著，或者说当时阶段增长是最重要的点，但是当规模变大后，必然会出现一些不健康的问题，通俗点讲就是 “林子大了，什么鸟都有”，这个时候生态问题就不能忽略了\n\n客户生态\n\n常见的客户生态包括：客户结构（如大广告主与小广告主、广告主的行业分布情况）、客户操作习惯（如频繁改出价、复制计划、微改素材等行为）等\n前面提到，最极端情况下平台只有一个广告主，这样流量的定价权就在这个广告主手里了；因此平台肯定希望提升广告主的多样性，让平台总体\ncpm 水位能在广告主的竞争中水涨船高\n但是广告系统里往往又存在着马太效应，即大部分的钱是由小部分的广告主花的；其原因是很多长尾的中小广告主由于竞价能力、投放经验不足等问题与大广告主不太可比，这部分广告主在投放链路中会由于数据量少，链路相关模型学习不充分等原因，进一步加剧这个问题。这个时候需要平台对这部分中小的广告主做一些特定的优化，成立对应的中小专项来优化，这部分的一些手段会在客户部分详细说明\n另外一个常见的客户生态问题就是客户的投放操作问题，比如说频繁改出价，正常\noCPX\n广告中，广告主的出价就是其对转化成本的预期，而这个成本理论上应该是固定的，不过会因为在投放初期由于没有固定成本预期而去调价做试探，比如说一个点赞在不同媒体平台上的成本是不一样的，广告主在一个平台切换到另一个平台后需要做调整，但这种改动也不会很频繁；所以频繁改动出价往往是广告主\nhack 平台的一种手段，如把出价从低调到高来 hack\n赔付的漏洞，或者在预算不足的时候出一个很大的 bid 来 hack\n刹车漏洞，因次平台要有相应的规则来回避这些问题，同时需要有相应的投放引导\n除了出价，广告主往往也存在不断复制计划来 hack\n系统的行为，因为系统的排序模型往往会使用很多 id 类特征，带来的就是系统的\nvariance 问题，如两个计划的投放设置、素材都一致，但是很可能因为计划 id\n不一致，导致投放效果的不一致，所以广告主有动力通过复制计划来跑更多的量，但这样给平台带来的问题有（1）工程上的压力，如召回的计划或素材变多了（2）素材重复度高；这其实也算是素材生态问题，微改素材的问题也属于同一类问题；因此，平台侧需要考虑使用\nid 类特征带来的问题，或者使用了 id\n类特征后需要通过额外的策略来保证广告主的复制是无效的，如托管控制客户复制计划的行为、相似计划的限制剪裁策略、相似素材识别等\n\n创作者生态\n\n创作者的生态，跟客户生态一样，也需要分结构和行业来看，因为创作者变现本身也是个供需的生意，需要明确分粉丝段或分行业的供需问题，如创作者变现的粉丝量分布是否健康，是否只有头部作者有变现，但大部分长尾的创作者压根没有变现；行业上也是类似的，因为内容平台是希望提升其内容的多样性的，对应的需要提升创作者行业的多样性，因为创作者是\nugc 平台素材的一个很重要的供给\n除了上面的问题，前面也提到内容平台往往存在 “水下” 问题，即创作者的撮合不走官方撮合平台，而是\nB 直接找 K\n来达成合作，这会影响平台对营销内容的管控、平台的营收等；这部分会在下面创作者部分详细展开\n\n内容生态\n\n在内容生态上，除了一些红线问题（如政治、色情内容等），更多的生态问题在于内容的多样性和内容质量等问题上\n内容多样性问题前面也提到，分别涉及到客户投流内容多样性和创作者创作的内容多样性\n对于客户，多样性问题体现在客户投放不把重心放在素材制作，而是放在了计划复制等各种投放的 “骚操作” 上，导致素材重复度高，用户审美疲劳，长期对广告交付的用户体验指标、广告相关转化率都有负向影响；针对这一点，需要平台侧应做相关的引导，引导客户去把精力放到素材制作上，巨量引擎的\n创意型自动化投放平台\n做的就是这件事情，巨量平台上对广告主的引导也是希望广告主通过创意驱动生意（下图来自 2024\n巨量引擎营销通案）\n\n对于创作者，多样性问题体现在其内容是否足够多样，一般而言，内容的多样性与创作者的多样性挂钩，因为往往每个创作者都有自己的创作偏好；因此平台首先需要引进足够多样的创作者，同时平台侧会希望能够有相应的抓手来影响创作者的创作倾向，一般是商机和流量，即把商机和流量更多倾向给平台侧需要扶持的创作者类型\n内容质量问题是个很泛的问题，如视频拍摄质量、内容抄袭、内容反智、营销感强等等都算内容质量问题，这些一般需要依靠审核（人 + 机）的方式来识别与降权；另外针对创作者的撮合往往还存在着 “代写代发” 的问题，即创作者并非在真诚分享，而是直接发布\nB 写好的笔记（B\n为了更多创作者为品牌背书会这么做，创作者也只需要发布写好的笔记就能拿到钱），这样不仅容易导致总体内容营销感强，同时也会带来创作者变现变现的不良风气和现象，需要平台通过机制来抑制这样的趋势\n客户\n前面提到，客户在内容平台上有营销诉求，为了服务好客户的各种诉求：如成本、跑量、低门槛投放、营销目标多样性、数据隐私等，平台需要为客户提供的各种投放产品；\n同时需要给客户交付好投放体验与投放效果，投放体验是客户在平台上投放时，产品能否满足广告主的具体需求，如易用性是否足够好；投放效果也有多种，最常见的是成本稳定，跑量最大化，同时也会有跑量稳定性等诉求\n在投放效果上，成本问题是最基础也是最重要的部分，从技术上来讲，ctr、cvr\n的预估准确性是最基本保障（所以相较于推荐，广告在 ctr、cvr 目标上一般会有\ncalibration 这个模块），同时通过出价和计费来做兜底；这里的兜底主要是指对\nctr、cvr 预估不准的兜底，即出价会基于后验的转化与消耗来做 pacing\n来保成本；除了出价，在扣费阶段，也可以基于计划当前的状态对计划少收钱或多收钱。出价和计费，除了是对\nctr、cvr\n预估不准确的兜底，也是对常见的二价收费带来的二价差的一种缓解。这里不详细展开这部分了，相关的内容可以看之前写的\nAn\nOverview of an AD System\n下面主要从 “平台希望客户做长期经营” 的思路，介绍一些相关动作\n行业化 / 场景化\n一般广告系统在发展到一定的规模后，就会出现分行业的优化诉求，其原因不同行业的营销目标与范式等不太一致，只用一个大一统的方案往往很难满足广告主的所有诉求，所以需要分行业来进行优化和推荐；行业的分类有很多标准，如果参考巨量给出的标准，主要是 4\n个场景 / 行业：电商、下载、本地、线索\n\n电商\n\n电商一般分为闭环电商和引流电商两种，前者是站内转化，预算类型商家预算，后者是站外转化，预算类型是商家预算 + 平台预算；商家预算的营销目标基本是商品购买、进店 / 直播间等，平台预算的基本目标就是拉新或拉活。因为预算类型有重合，闭环电商与引流电商的优化目标与路径也有较多相似之处，较大不同的点有这两个\n（1）数据的完备性与实时性，由于转化在站外，引流电商的数据完备性和实时性是不如闭环电商，往往需要通过外部数据合作的方式来获取外部的数据进行建模；也容易遇到数据安全等问题，一般需要通过隐私计算等方式来进行跨域的数据交互\n（2）全域推广，做引流的商家的经营阵地可能不在对应的媒体平台上（如从抖音引流到淘宝的商家，主要经营阵地在淘宝），这种商家一般不会在抖音上做日常内容的运营（即自然流量少），投引流也是考核\nroi\n是否达标；但是闭环电商的商家是有动力在平台上做内容来获取自然流量的，对于这部分商家，业界推出了 “全域推广” 类产品来让商家更好获取自然 + 广告流量\n\n\n下载\n\n下载更准确来说是个转化目标，会如游戏、网服、电商的平台预算等行业都有下载类预算；游戏是下载类行业中优化目标网深去做的最早行业，从开始的下载，到后续的次留、付费次数，付费金额等目标，游戏广告主是最早接受深度转化目标优化的一批广告主\n优化目标变深，更贴近广告主的营销诉求，是平台在往效果广告演进中不得不经历的一个过程；在这个过程中往往会遇到从浅度目标切到深度目标后，roi\n变好但是跑量下降的问题；这个时候需要权衡能否接受短期的消耗的 “损失”，但是给广告主带来了更多价值的决策，因为只有服务好广告主，才能给平台可持续的收入和增长\n\n线索\n\n线索也是一个比较泛的概念，涵盖非常多的行业；房产、汽车、教育、医美等需要提供服务等行业，都涉及到线索；常见的电话表单或私信等方式，都算线索，是广告平台需要优化的转化目标\n但是线索会面临的一个问题是线索的有效性，即很多提供了电话或私信的客户发生后续转化的概率并不高，这样并不能给商家带来实际的转化价值，因此线索的深度转化也是线索行业中的一个老大难问题，基本解决思路就是跟广告主有更多的建联，把更多的深度转化数据汇创到平台侧，巨量提出了一个方法论，如下图所示\n\n线索存在着很多中小商家的行业（电商也一样），因为在很多卖服务和商品的行业，有很多规模不大的生意，这部分小商家也是有做营销的意向的，但是因为其预算小，投放经验不足等问题，往往会遇到更多问题，这部分的一些解决思路会在后面的中小商家提到\n\n本地\n\n本地与前面几个场景不少内容有重叠，其一大特点其服务在地理上是会被限制在一定半径内的，且需要到店才能提供相关服务（在这一点上其实跟线索广告很像）；在优化手段上跟其他行业没有太大差异 (除了地域上需要做一些限制)\n\nKA 与 SMB\n根据客户的经营规模、预算等因素，可以粗略把客户分为 KA（Key\nAccount）客户和 SMB（Small and Medium-sized\nBusiness）客户两大类，前者往往有较为充足的预算与成熟的投放能力，后者则往往是预算不多、投放经验一般较少；且\nSMB 客户的数量级往往会高于 KA 客户\n但是广告系统里又存在着马太效应，即大部分的钱是由小部分的广告主花的，其原因是很多长尾的\nSMB 广告主由于竞价能力、投放经验不足等问题与 KA\n广告主不太可比，这部分广告主在投放链路中会由于数据量少，链路相关模型学习不充分等原因，进一步加剧这个问题\n但平台长期的收入、cpm\n的增长，会比较依赖广告主数量增加带来的更剧烈的竞价环境，因此 SMB\n广告主对平台而言也是不可忽略的，要服务好这部分\nSMB，需要平台对这部分中小的广告主做一些特定的优化\n困扰 SMB\n客户的两个问题主要是素材和投放；前者是因为客户不是专门的内容创作者，没法制作很好的素材，同时往往没有额外的素材预算，没法买到很好的素材；后者则是对平台的规则、投放的流程等不熟悉，操作成本高\n因此平台针对这部分 SMB\n广告主的投放产品要做到易用性足够好，包括提供更低便捷的投放入口和投放引导，简化创编流程，同时提供一些自动的素材生成和编辑能力，提升这部分广告主使用平台的便捷性\n此外在流量分发上，SMB 广告主由于竞价能力弱 ecpm 低，是竞争不过 KA\n广告主的，往往需要平台给予相应的流量的激励和扶持，而这种扶持是会对平台大盘的消耗是可能有损的，因为破坏了平台以收入为最大化的分发机制\n针对线索行业的中小客户，巨量给出的解决方案的思路基本就是上面提到的，同时还提供了私信的智能客服\n\n电商的中小的解决方案类似\n\n经营\n随着营销的精细化，平台会倡导客户在平台做营销过程中往经营方向去转变；因为站在客户和平台视角，相对于仅靠广告来买流量来触达用户，另一种更可持续的方式是通过广告来度过冷启，然后通过好内容 + 好服务持续吸引与留住用户\n巨量把这个定义成营销力（见下图），即客户除了仅砸钱投广告，还需要有制作好素材能力（获取更多自然流量）以及提供优质服务的能力（口碑有保证，留住用户），对客户和平台来说，才是一种更可持续发展的方向；而这也是笔者这里讲的 “经营” 所表达的意思是一样的，经营在笔者的理解就是通过好内容或钱来获取流量（前者是自然流量，后者是广告流量），然后基于这些流量触达到的客户，来进一步拓展用户群\n\n平台在这个过程中，需要提供一些更好的方法论和工具来指导广告主，如抖音的 “星推搜直” 方法论、小红书的\nKFS方法论，都是在指导广告主怎么更好地整合平台提供的营销能力，包括素材的生产、投放的方法论等\n\n除了方法论，平台也需要提供对应的能力和工具，如经营诊断、资产管理（人群资产如抖音的\n5A 人群），具体可参考下面这张图，这里就不详细展开了\n\n创作者\n创作者是平台优质内容的重要来源，为了让创作者能够持续在平台上创作出好内容，平台往往需要为创作者提供两个能力：商机和加热；商机就是创作者变现的撮合机会，也是上面提到的星图等各个平台的支持的能力；加热则是为创作者提供的买流量的产品，是创作者成长的一个很重要的工具\n商机\n通俗来讲，平台为创作者提供的商机就是 B 与 K 的撮合，即 B 付费给 K\n写笔记 + 发布笔记，买的是 K 的内容和流量\n因此平台需要保证撮合能力的完备性与易用性，完备性指的是平台提供的撮合能力需要涵盖基本的范式，如一般的撮合有\n1v1 的精准模式、1vN 的招募模式，计算方式有一口价结算，也有根据效果的 cps\n类结算\n\n1v1 与 1vN\n\n1v1 的撮合和一口价是比较基础的模式，就是 B 在平台上选择对应的\nK，然后付款让 K 写笔记后发布；而 1vN 和 cps\n结算类，是更偏效果化的方式，同时为更多的中长尾 K\n提供变现机会，其模式一般是 B 表达其需求，然后平台将需求发布给一批 K，B\n再根据 K\n创作的笔记带来的实际效果（如曝光、点赞等）进行结算，这种模式相较于\n1v1，相当于把 B 的预算从原来的只分配给 1 个 K，变成了分配给多个\nK；星图的发行人计划就是个典型的\n1vN 模式\n\n\n水上与水下\n\n这类官方撮合平台，往往面临着预算走 “水下” 的问题；这里有个概念需要说明，水上即是走官方合作平台的\nBK 撮合，需要给平台交一定的手续费；而水下指的是 B 不通过平台与 K\n直接建联\n站在平台的视角，肯定是希望 BK\n撮合都走水上，因为除了手续费还能对平台的这类商机有更好的管控；但在 B\n的视角，走水下既能省手续费，内容又能更原生（有些平台对水上的内容会明确打标），因此水下与水上的博弈，会是一个平台长期需要解决的问题\n为了让更多的预算从水下翻水上，平台需要做好两件事\n（1）提升撮合平台的易用性与效率，即平台需要让 B 和 K 的建联更容易\n（2）流量分发上，水上需要建立与水下有差异的分发机制，并对 B\n侧透出笔记走水上的权益\n这里重点说一下第（2）点，因为 BK 撮合的笔记跟 K\n自己发布的笔记在形式上基本是无差异的，一般都是走大盘的分发，在这种方式下，相当于水上笔记与水下笔记的流量分发没有任何差异，这样\nB 走水上的意愿就没有那么高了\n所以要做水下翻水上，水上的流量分发需要做出差异，需要更多考虑 B\n侧的权益（因为一般创作者的笔记都是在推荐分发，B\n侧的相关目标往往不是大盘分发的目标）；而实际上，K 与 B\n的目标并不是二元对立的，本质上是平台对 B 的钱袋子在不同 K\n的分配之间做干预，所以首先要服务好愿意出钱的 B，然后需要考虑如何在不同 K\n之间做好分配\n而 B\n侧往往是有营销诉求的，在星图等平台上买到的素材会是其投广素材的一个很重要的来源，且这个素材的在\nK 发布后获取到的自然流量的效果，是影响 B\n拿这个素材投广的一个很关键因素；有差异的流量分发，就是指这部分自然流量的分发方式和效果，在实际中，可以考虑为这部分流量加入一些\nB 侧更关注的影响目标作为排序公式的一部分\n\n内容投广\n\n前面也提到了， B 往往是有营销诉求的，会利用 K\n创作好的素材进行投广等操作，因此创作者为 B\n创作的内容，是商业化素材很重要的一个来源；所以在水上的流量分发中，除了考虑并透出更多水上流量的权益，还能引导客户用更多的素材投广，因为水上流量的效果，往往是客户判断素材优劣的一个很重要因素\n通过素材在水上流量的效果让客户决策投广，是一个比较间接的方式；相较于这种方式，星广联投\n则是一个更直接的方式，这种方式跟前面提到的全域推广有点像，也是同时考虑了客户在自然流量和广告流量的效果\n\n加热\n如果说商机是创作者变现的部分，加热就是创作者付费的部分：付费为自己的内容加热，像市面上的\ndou+、粉条、\n薯条等都是创作者加热工具\n加热本质上就是花钱买流量，这一点跟广告是一样的，最大的区别是创作者对投放与商业不是非常熟悉，跟中小客户一样需要提供一些低门槛的投放产品和能力\n而在流量分发上，虽然这部分流量是创作者花钱买的，理论上应该跟广告混竞，但是创作者一般的竞价能力是不如大广告主的，与广告混竞往往会存在着竞争不过大广告主的问题，或者说相关的曝光、转化成本会跟商业流量的水位一样，这样给创作者带来的体感就是流量很贵，性价比远不如自己发内容获取到的自然流量\n但平台是需要提供一个付费买流量的工具给到创作者的，因为这是也是创作者成长很重要的一个能力；因此这部分流量是需要独立来分发的，避免被广告的挤压，且在混排时有更宽松且独立的首位、\nmin_gap\n等限制；同时，这部分素材一般比起硬广的素材质量也会更优，素材也往往不用打标\n用户\n在商业系统中，与用户较为相关就是体验相关的事项，量化的指标往往是 dau\nloss；从这个角度来讲，商业化就是在用 dau loss\n来换钱，且需要持续把这个兑换比变得更高；但 dau loss\n需要长期的实验才能观测到显著的变化，所以往往会把 dau loss\n拆解出很多中间指标 (如使用时长、次留、负反馈等) 进行观测\n为了防止 dau loss 超红线， ad load\n限制是最基础的约束，在此之上就是各种体验的流量策略，包括频控、排序公式加入体验项、混排约束等\n流量策略\n频控是最常见的体验策略，即控制同一内容 / 同类内容等在 xx\n天内不能给用户曝光超过 xx\n次之类的，这类频控比较硬，所以往往也会有针对不同给用户 xx\n是不同的软频控策略，本质上其实是个分人群策略\n排序公式里加入体验项也是常见手段，商业化的分发公式中的基础项 ecpm\n，是以收入最大化作为目标的，在 hidden_cost\n中可以加入负反馈项、停留时长项等影响相关的体验项，但这破坏了以收入最大化的分发目标，对收入来说肯定不是最优的，而这本质上就是在用收入来兑换相关的体验\n混排一般会有较多的约束，其目的也是约束广告对用户体验的损害，如广告的首位，广告位置之间的\ngap 约束，广告曝光的时间 gap\n的限制，但是很多策略其实都在尝试把硬规则软化，所以也有一些思路是把这些比较硬的规则做成软化的\nloss，详见 混排的那些事儿\n上面提到的这些策略，其实都在做的都是个性化策略，或者说分人群策略；因为不同用户之间的差异是比较大的，如果对用户做聚类，一般可以从\n“价值 × 对广告敏感” 的角度把用户划分成 4\n个类比；不同类比的用户往往需要不同策略（个性化策略），如广告多出在高价值低敏感的用户，少出在低价值高敏感的用户上，带来的\ndau 收益和商业化收入的收益，肯定是要更大的\n回到上面的软频控、排序公式加入体验项打分、混排硬约束的软化，其实都是在做分人群的策略，都会把广告更多地出在那些高价值低敏感的用户，这不是个显式的策略，是各个策略基于后验数据建模，再作用到系统上的自然结果\n值得注意的是，高价值不敏感用户的标签并非会一直不变，当各类营销内容（电商、直播、广告）都在挑这部分人群时，有可能因为营销内容过多而离开平台；所以监控\ndau loss 时，也可以分人群做监控，把这部分核心人群单独监控起来\n素材\n另一个优化用户体验的重要方向就是素材了，包括素材的质量与多样性，这部分在前面都提及到了，这里就不再展开\n小结\n传统的广告系统一般需要处理好三方的关系：广告主、平台和用户，但是随着内容平台的发展，\n创作者在商业变现中的影响也越来越难被忽视，本文主要就是讲了这四方的相互关联、相互作用而构成的商业系统\n作为连接各方的核心枢纽，平台是最重要也是最需要发挥主观能动性的一方；这里主要从体验、变现和生态三个方面讲述平台的职责。在体验上，需要为\nC 端用户交付好 dau loss，同时也要给 B\n端用户提供良好的投放体验；在变现上，需要最大化售卖 ad\nload，同时为创作者提供一个好的变现环境；在生态上，客户与创作者的生态都需要分结构来看总体分布，而在内容上，也都需要保证内容的优质性\n客户作为平台的金主，是平台需要重点服务的客户，无论是分行业 / 场景的优化，还是针对\nSMB\n客户有更多低门槛的投放，都是在尝试把每个广告主服务好，让广告主能更好的做好自己生意，在平台上做长期的经营\n创作者是平台优质内容的重要来源，无论是给 C 端的用户的消费还是 B\n端的用户的营销；创作者在平台上持续创作出好内容，往往会有两个诉求：商机和加热，前者是供创作者变现，后者则是创作者的成长，平台需要提供相应的能力\n用户是平台的根基，商业变现不能无底线地损失用户体验，需要在体验的红线指标内最大化分发效率，通过各种精细化的流量策略，优质的素材等，降低对用户体验的损失\n从最开始门户网站的 banner ads，到搜索引擎的 search\nads，再到内容平台的 feed\nads，每一次的变革都为广告业带来了新的可能性和挑战，商业变现的多样性与复杂性也是前所未有的；本文所简述的内容也只是冰山一角，很多内容没有详细展开，有更多在冰山之下，隐藏着更多值得我们去探索和了解的知识，也欢迎交流指导～\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"ipdb 使用小记","url":"/2018/12/21/ipdb%20%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0/","content":"最近在魔改 loss\nfunction，涉及到很多矩阵运算，而矩阵运算中维度的对齐免不了要多次的调试；沿袭着之前的\nprint 大法弄了一段时间后，不仅代码凌乱不堪，而且心累：每次\nimport tensorflow as tf 都要十几秒，然后 print\n完之后想进一步看其他变量的信息，又要重新执行一遍。后来找到了 ipdb\n这个好用的工具，才发现自己过去调试程序的方法是多么的低效和 naive。\n\npython 提供了一个默认的 debugger：pdb，而 ipdb\n则是 pdb 的增强版，提供了补全、语法高亮等功能，类似于 ipython 与 python\n默认的交互终端的关系，通过 pip install ipdb 即可安装\nipdb。\n使用方式\nipdb\n的使用方法一般有两种：集成到源代码或通过命令交互。\n集成到源代码可以直接在代码指定位置插入断点。如下所示：\nimport ipdbvar1 = 23ipdb.set_trace()...\n上面的代码会在执行完 var1 = 23 这条语句之后停止，展开\nipython 环境，之后就可以自由地调试了。\n上面的方式虽然简单，但是存在着两个较为比较明显的问题：\n\n插入的断点代码会污染原来的代码空间\n每次插入断点都需要修改源码\n\n因此，相比于上面的方式，交互式的命令式调试方法更加方便。启动命令式调试环境的方法也很简单：\npython -m ipdb code.py\n接着就是通过一些常用的命令来进行\ndebug 了，如上面插入断点的样例代码就可以通过以下命令达到同样效果：\n$python -m ipdb code.pyipdb&gt; b 3Breakpoint 1 at /test.py:3ipdb&gt; c\n上面的命令 b 3 表示在第三行设置一个断点，然后通过命令\nc 一直执行至断点处，接着就会展开 ipython\n环境进行调试了。（b 和 c 分别代表了 break 和\ncontinue，可以用整条命令，也可以只用首字母）\n常用命令\n上面的设置断点和一直执行至断点是比较常见的用法，除此之外，还有其他一些常用命令。\n帮助\n通过命令 h 可以列出所有命令，后面跟上具体的命令如\nh command\n则可以显示出这条命令的具体作用，非常有用，依靠这条命令能够节省不少\ngoogle 的次数。\n断点\n上面提到了断点的一种常见用法，即命令 b line_number 和\nc 的组合，b line_number\n默认是对当前文件设置断点，也可以在 line_number\n前加上其他文件名（比如说要将要引用的其他文件），即\nb file_name:line_number；file_name 需要在\nsys.path 中，当前目录已经默认存在 sys.path ,\n也可通过 .. 引用上一层目录的文件。\n另外，通过 b 设置的断点在重新运行 debug 程序\n(命令 restart 或 run)\n后会依然保留，如果要忽略这些断点，有两种做法\n\n通过 disable 关闭这些断点，enable\n打开这些断点\n通过命令 clear 或 cl 清除这些断点\n\n此外，除了上面那种一直存在的断点，ipdb\n中还有一种 只生效一次的断点，命令为 tbreak,\n使用方法同命令 b。\n上面的断点都是直接指定的，pdb\n中还有一种条件断点，即只有当某个条件成立时，才设置断点，其使用命令为\ncondition line_num bool_expression, condition\n为关键字，line_num 为设定断点的位置，只有当 expression 为 true 时，\n才会设置这个断点。\n如果需要列出已经设置的所有断点，可以单独使用命令 b。\n逐行执行\n有两条命令可以进行逐行执行： s（step) 或\nn（next),\n两个命令的主要区别是：假如当前行调用了某个函数，s\n会进入这个函数，n\n则不会。因此，如果需要了解函数内部执行的细节，需要\ns 命令进入函数内部进行 debug。\n进入了函数之后，通过命令 a（argument）\n可列出当前的函数的参数，通过 r（return）则可以直接执行至\nreturn 语句。\n忽略某段代码\n使用 j line_number 可以忽略某段代码，下一步直接从\nline_number 开始执行。\n查看源码\n通过命令 l 或 ll 可查看源码，\nll 是查看整个源码文件， l\n可指定需要查看的行数，默认是当前往后 11 行，也可指定具体的范围，如\nl 2,5 是查看第 2-5 行的源码。\n重启或退出 debugger\n上面已经提到了重启 debugger 可通过 restart 或\nrun 命令，需要注意的是，重启 debugger\n后断点、debugger 的设置等是会保留的。如果要一个全新的\ndebugger，可通过命令 q、quit 或\nexit 退出 debugger 后进入。\n小结\n以上就是 ipdb 的一些基本用法，除此之外，更多的用法可参考 pdb\n的官方文档，ipdb 的命令跟 pdb 是一样的。另外，gdb\n也是一个类似的命令行 debugger，只是一般用来调试 C/C++\n而已，使用的方法类似，甚至很多命令的名称更 pdb 都一样，具体可参考 用 GDB 调试程序。\n","categories":["工具使用"],"tags":["python","工具使用"]},{"title":"format 函数常用语法","url":"/2018/06/03/format%20%E5%87%BD%E6%95%B0%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95/","content":"python 的 format\n函数能够对输出做格式化从而使得符合输出的要求，这里记录其一些常见用法，主要参考了博客\n飘逸的 python\n- 增强的格式化字符串 format 函数\n\n位置映射\n下面是通过参数的位置 (从 0 开始) 将参数映射到字符串具体位置\nIn [1]: '{0},{1}'.format('kzc',18)  Out[1]: 'kzc,18'  In [2]: '{},{}'.format('kzc',18)  Out[2]: 'kzc,18'  In [3]: '{1},{0},{1}'.format('kzc',18)  Out[3]: '18,kzc,18'\n字符串的 format\n函数可以接受不限个参数，位置可以不按顺序，可以不用或者用多次\n名称映射\nIn [5]: '{name},{age}'.format(age=18,name='kzc')  Out[5]: 'kzc,18'\n对象映射\nclass Person:      def __init__(self,name,age):          self.name,self.age = name,age          def __str__(self):              return 'This guy is {self.name},is {self.age} old'.format(self=self)  In [2]: str(Person('kzc',18))  Out[2]: 'This guy is kzc,is 18 old'\nlist/tuple 的下标映射\nIn [7]: p=['kzc',18]In [8]: '{0[0]},{0[1]}'.format(p)Out[8]: 'kzc,18'\n填充与对齐\n填充常跟对齐一起使用\n^、&lt;、&gt; 分别是居中、左对齐、右对齐，后面带宽度\n: 号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充\nIn [15]: '{:&gt;8}'.format('189')Out[15]: '     189'In [16]: '{:0&gt;8}'.format('189')Out[16]: '00000189'In [17]: '{:a&gt;8}'.format('189')Out[17]: 'aaaaa189'\n精度与类型 f\n精度常跟类型 f 一起使用 In [44]: '{:.2f}'.format(321.33345)Out[44]: '321.33'\n其中.2 表示长度为 2 的精度，f 表示 float 类型。\n进制转换\n主要就是进制了，b、d、o、x\n分别是二进制、十进制、八进制、十六进制，用 ,\n号还能用来做金额的千位分隔符。 In [54]: '{:b}'.format(17)Out[54]: '10001'In [55]: '{:d}'.format(17)Out[55]: '17'In [56]: '{:o}'.format(17)Out[56]: '21'In [57]: '{:x}'.format(17)Out[57]: '11'In [58]: '{:,}'.format(1234567890)Out[58]: '1,234,567,890'\n","categories":["python","语法"],"tags":["python"]},{"title":"/etc/ld.so.conf 文件详解","url":"/2015/11/21/etc-ld-so-conf%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/","content":"可执行程序的类型\nLinux 系统上有两类不同的 Linux 可执行程序。\n\n第一类是静态链接的可执行程序。静态可执行程序包含执行所需的所有函数\n—\n换句话说，它们是 “完整的”。因为这一原因，静态可执行程序不依赖任何外部库就可以运行。\n第二类是动态链接的可执行程序。动态可执行程序是 \"不完整\" 的程序，它依靠外部共享库来提供运行所需的许多函数。\n\n\n静态可执行程序与动态可执行程序比较\n我们可以用 ldd\n命令来确定某一特定可执行程序是否为静态链接的：\n#ldd /sbin/sln  \nnot a dynamic executable\n“not a dynamic executable” 是 ldd 说明 sln\n是静态链接的一种方式。现在，让我们比较 sln 与其非静态同类 ln\n的大小：\n# ls -l /bin/ln /sbin/sln  \n　　-rwxr-xr-x    1 root     root        23000 Jan 14 00:36 /bin/ln  \n　　-rwxr-xr-x    1 root     root       381072 Jan 14 00:31 /sbin/sln\nsln 的大小超过 ln 十倍。ln 比 sln 小这么多是因为它是动态可执行程序.\n动态链接相关性\n查看 ln 依赖的所有共享库的列表，可以使用 ldd\n命令：\n# ldd /bin/ln  \n　libc.so.6 =&gt; /lib/libc.so.6 (0x40021000)  \n　/lib/ld-linux.so.2 =&gt; /lib/ld-linux.so.2 (0x40000000)\n可见，ln\n依赖外部共享库 libc.so.6 和 ld-linux.so.2。通常，动态链接的程序比其静态链接的等价程序小得多。不过，静态链接的程序可以在某些低级维护任务中发挥作用。例如，sln\n是修改位于 /lib 中的不同库符号链接的极佳工具。但通常您会发现几乎所有\nLinux 系统上的可执行程序都是某种动态链接的变体。\n动态装入器\n那么，如果动态可执行程序不包含运行所需的所有函数，Linux\n的哪部分负责将这些程序和所有必需的共享库一起装入，以使它们能正确执行呢？答案是动态装入器（dynamic\nloader），它实际上是在 ln 的 ldd 清单中看到的作为共享库相关性列出的\nld-linux.so.2\n库。动态装入器负责装入动态链接的可执行程序运行所需的共享库。那么，动态装入器如何在系统上找到适当的共享库？\n动态装入器找到共享库要依靠两个文件\n/etc/ld.so.conf 和 /etc/ld.so.cache\n/etc/ld.so.conf 文件\n文件进行 cat 操作，您可能会看到一个与下面类似的清单：\n  $ cat /etc/ld.so.conf  \n　　/usr/X11R6/lib  \n　　/usr/lib/gcc-lib/i686-pc-linux-gnu/2.95.3  \n　　/usr/lib/mozilla  \n　　/usr/lib/qt-x11-2.3.1/lib  \n　　/usr/local/lib\nld.so.conf 文件包含一个所有目录（/lib 和 /usr/lib\n除外，它们会自动包含在其中）的清单，动态装入器将在其中查找共享库。\n### /etc/ld.so.cache 文件\n在动态装入器能 “看到”/etc/ld.so.conf 里面的信息之前，必须将它转换到\nld.so.cache 文件中。可以通过运行 ldconfig\n命令做到这一点：\n# ldconfig\n当 ldconfig 操作结束时，您会有一个最新的\n/etc/ld.so.cache 文件，它反映对\n/etc/ld.so.conf\n所做的更改。从这一刻起，动态装入器在寻找共享库时会查看在\n/etc/ld.so.conf 中指定的所有新目录。\nldconfig 技巧\n要查看 ldconfig 可以 “看到” 的所有共享库，可以输入：\n# ldconfig -p | less\n还有另一个方便的技巧可以用来配置共享库路径。有时候希望告诉动态装入器在尝试任何\n/etc/ld.so.conf\n路径以前先尝试使用特定目录中的共享库。在运行的较旧的应用程序不能与当前安装的库版本一起工作的情况下，可以通过 LD_LIBRARY_PATH 这个环境变量来实现，计入需要指示动态装入器首先检查某个目录，需要将\nLD_LIBRARY_PATH\n变量设置成希望搜索的目录。多个路径之间用冒号分隔；例如：\n# export LD_LIBRARY_PATH=\"/usr/lib/old:/opt/lib\"\n执行上面命令后，所有从当前 shell 启动的动态链接可执行程序都将使用\n/usr/lib/old 或 /opt/lib\n中的库，如果仍不能满足一些共享库相关性要求，则转回到\n/etc/ld.so.conf 中指定的库。\n","categories":["Linux"],"tags":["Linux"]},{"title":"docker 使用小记","url":"/2017/11/25/docker%20%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0/","content":"由于最近需要在几台电脑上配置相同的环境，几台电脑的操作系统不一样，而且配置一台所需要的步骤是比较繁琐的，因此就想到了用\ndocker，下面是使用 docker\n构造镜像并且在不同的电脑上使用这个镜像的记录。\n\ndocker 支持多个平台，各个平台上具体的安装步骤可参考官方文档。\ndocker\n提供了一个镜像仓库，当从 docker 镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改\n\n从已经创建的容器中更新镜像，并且提交这个镜像\n使用 Dockerfile 指令来创建一个新的镜像\n\n这里采用的是第一种方法，由于我这里需要的是 python 环境，因此先 pull\n一个 python 镜像作为基础镜像（可以通过 docker search python\n找到相关的镜像，这里 pull 的是官方的 pytohn 3.5 镜像）, 命令如下\ndocker pull python:3.5\n等到镜像 pull 下来后，可以通过以下命令进入镜像中\ndocker run -t -i python:3.5 /bin/bash\n这里 -t -i 含义如下\n-t : 在新容器内指定一个伪终端或终端。 -i :\n允许你对容器内的标准输入 (STDIN) 进行交互\n这样便可进入装有 python\n3.5 的系统（默认是 ubuntu），然后在其中像普通的系统一样通过 apt 和 pip\n配置所需要的软件和库即可\n配置完成后注意不能马上退出这个容器，因为在这个容器中的修改默认是不会影响到原来的镜像的，也就是说如果退出后在进入\npython:3.5\n镜像所创建的容器中，所安装的这些库会完全消失，因此需要将这个配置过的容器另存为一个新的镜像，具体做法如下\n首先原来的容器不能退出，另开一个终端，通过 docker ps\n命令获得修改过的容器的 id ，如下图所示\n\n\ndocker ps\n\n然后 docker commit id new_image\n命令将这个修改过的容器存为新的镜像，这里的 id\n不一定要写全，只要能跟其他的容器 id 区分开来，写前几个字符也可以。\n如下所示是将前面的容器另存为镜像 modified_python,\n并且通过 docker images 查看到该创建的镜像的时间和大小。\n\n\ndocker commit\n\n如果要将这个容器分发到其他机器，可以先将这个镜像上传到 docker\n官方的中央仓库 (需要注册账号），其他机器再从中央仓库 pull\n下来，但是这样可能会存在着网速过慢的问题，因此可以在本地导出容器，然后直接拷贝到其他机器导入，具体操作如下\n首先通过 docker ps -a 查看本地使用的容器，然后通过\ndocker export id &gt; tar_file 将容器导出到\ntar_file 中，其中容器 id\n的书写规则同上。如下是将上面配置过的 python 环境导出到\nubuntu_python.tar\n\n\ndocker export\n\n通过这个文件导入为镜像也很简单，通过\ncat tar_file | docker import image_name 即可将\ntar_file 导入为 image_name 镜像\n\n\ndocker import\n\n其他的一些值得注意的地方就是 -v 参数可以将本地的目录挂到 docker\n的目录中，从而可以在容器中写入本地磁盘，具体语法为\n-v local_dir:contain_dir, 当容器中的\ncontainer_dir 不存在时会自动创建这个目录。\n\n参考资料：\nInstall\nDocker Docker 教程\n关于 Docker 目录挂载的总结\n保存对容器的修改\n导出和导入容器\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"python 中 * 与 ** 的参数传递","url":"/2016/09/01/python%20%E4%B8%AD%E6%98%9F%E5%8F%B7%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92/","content":"在 python\n中，经常可以看到定义函数或调用函数时 f(*arg) 和 f(**args) 类型的参数，本文主要讲述这两个形式的参数的含义以及应用。\n\n定义函数时参数加上 * 和\n**\n首先这两个类型的参数都表示不确定具体参数个数，怎么理解这句话呢？通常在定义函数的时候，定义了几个参数，调用是也要传入几个参数 (默认参数除外，可传可不传)，但是只要在定义函数的时候将参数写成 * 或 ** 的形式，就可以传入多个参数。如下面的例子：\n&gt;&gt;&gt; def a(*args):...     print type(args)...     for i in args:...         print i...&gt;&gt;&gt; a(1,2,3,4)&lt;type 'tuple'&gt;1234&gt;&gt;&gt; a([3,2,3])&lt;type 'tuple'&gt;[3, 2, 3]&gt;&gt;&gt; a()&lt;type 'tuple'&gt;\n从上面的例子可以看到，通过 *\n声明的参数在调用是可以传入 0~n 个参数，且不管传入的参数为何类型，在函数内部都被存放在以形参名为标识符的 tuple 中，无法进行修改。\n同理，通过 ** 声明的参数也是可以传入多个参数，但是传入的参数类型需要为 k1=v1,k2=v2..... 的类型，且参数在函数内部将被存放在以形式名为标识符的 dictionary 中，这种方法在需要声明多个默认参数的时候特别有用。\n&gt;&gt;&gt; def a(**args):...     print type(args)...     print args...     for k, v in args.items():...         print k,v...     print args['k1']...&gt;&gt;&gt; a(k1=1, k2=2, k3=3, k4=4)&lt;type 'dict'&gt;{'k3': 3, 'k2': 2, 'k1': 1, 'k4': 4}k3 3k2 2k1 1k4 41\n下面显示了如何通过这两个参数使代码变得简洁 (注意 *args 和\n**kwargs 可以同时在函数的定义中，但是 *args 必须在 **kwargs\n前面)\ndef sum(*values, **options):    s = 0    for i in values:        s = s + i    if \"neg\" in options and options[\"neg\"]:        s = -s    return ss = sum(1, 2, 3, 4, 5)            # returns 15s = sum(1, 2, 3, 4, 5, neg=True)  # returns -15s = sum(1, 2, 3, 4, 5, neg=False) # returns 15\n除此之外，*args 和 **kwargs\n也可以和命名参数一起混着用。命名参数首先获得参数值，然后所有的其他参数都传递给\n*args 和 **kwargs\n. 命名参数在列表的最前端。例如:\ndef table_things(titlestring, *args, **kwargs)\n调用函数时参数加上 * 和\n**\n除了在定义函数时可以加上 * 或 **,\n还可以在调用函数时加上 * 或 **,\n表示将输入的\n集合（序列）类型参数拆开，见下面的例子：\ndef sum(a, b):    return a + b# values = set()# values.add(1)# values.add(2)values = (1, 2)# values = [1,2]# values = {1:3, 2:4}s = sum(*values)\n无论是集合、列表、元组还是字典，\n在作为参数输入时加上 *，表示将里面的元素拆开，然后一个个传进去，所以上面执行的结果相当于 s = sum(1, 2), 由于字典比较特殊，传入参数时只会拆开\nkey 然后传入。下面结合上面定义函数时参数加上 * 来讲述这个\n* 的含义，例子如下： &gt;&gt;&gt; def sum(*args):...     print args[0]...&gt;&gt;&gt; val = (1,2)&gt;&gt;&gt; sum(val)(1, 2)&gt;&gt;&gt; sum(*val)1\n上面的 sum 函数输出传入的第一个参数，由于 sum(val) 将整个 val 元组作为参数传入，相当于 sum((1,2)), 所以会输出 (1,2); 而 sum(*val) 则会将 val 拆开，相当于 sum（1,2），因此输出为\n1\n而在调用参数时加上 **, 作用也是将传入的参数拆开，只是输入的参数必须为字典，且每个\nkey\n必须要为函数的某个形参，key 对应的 value 为该参数的值。详见下面的例子:\n&gt;&gt;&gt; def parrot(voltage, state='a stiff', action='voom'):...     print(\"-- This parrot wouldn't\", action, end=' ')...     print(\"if you put\", voltage, \"volts through it.\", end=' ')...     print(\"E's\", state, \"!\")...&gt;&gt;&gt; d = {\"voltage\": \"four million\", \"state\": \"bleedin' demised\", \"action\": \"VOOM\"}&gt;&gt;&gt; parrot(**d)-- This parrot wouldn't VOOM if you put four million volts through it. E's bleedin' demised !\n上面的 parrot(**d) 相当于 parrot(voltage = \"four million\", state = \"bleedin' demised\", action = \"VOOM\") 通过上面的方法，可以先将所有参数用字典封装，再通过\n** 传递。\n结合定义函数时参数加上 ** 有以下例子：\n&gt;&gt;&gt; def b(**args):...     for k,v in args.items():...         print k,v...&gt;&gt;&gt; val = {'keke':1, 'hehe':2}&gt;&gt;&gt; b(**val)keke 1hehe 2\n\n参考：\nhttp://stackoverflow.com/questions/2921847/what-does-the-star-operator-mean-in-python\nhttps://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n","categories":["python","语法"],"tags":["python"]},{"title":"python 中使用 SQLAlchemy","url":"/2016/06/08/python%20%E4%B8%AD%E4%BD%BF%E7%94%A8SQLAlchemy/","content":"数据库表是一个二维表，包含多行多列。通过 python 获取数据库中的内容时，可以用一个 list 表示获取的多行记录，每一个元素的类型是 tuple，表示一行记录，比如，包含 id 和 name 的 user 表：\n\n[    ('1', 'Michael'),    ('2', 'Bob'),    ('3', 'Adam')]\n通过 ORM 技术：Object-Relational Mapping，可以把关系数据库的表结构映射到对象上。如：\nclass User(object):    def __init__(self, id, name):        self.id = id        self.name = name[    User('1', 'Michael'),    User('2', 'Bob'),    User('3', 'Adam')]\nORM 框架就是用来完成这种装换的，在 Java 中最常用的是 Hibernate，而在 Python 中，最有名的 ORM 框架是 SQLAlchemy。SQLAlchemy\n的简单使用如下\n创建表\n# 导入:from sqlalchemy import Column, String, create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_base# 创建对象的基类:Base = declarative_base()# 定义User对象:class User(Base):    # 表的名字:    __tablename__ = 'user'    # 表的结构:    id = Column(String(20), primary_key=True)    name = Column(String(20))# 初始化数据库连接:engine = create_engine('mysql+mysqlconnector://root:password@localhost:3306/test')# 创建DBSession类型:DBSession = sessionmaker(bind=engine)\ncreate_engine() 用来初始化数据库连接。SQLAlchemy 用一个字符串表示连接信息：\n数据库类型+数据库驱动名称://用户名:口令@机器地址:端口号/数据库名\n插入记录\n由于有了 ORM，我们向数据库表中添加一行记录，可以视为添加一个 User 对象：\n# 创建session对象:session = DBSession()# 创建新User对象:new_user = User(id='5', name='Bob')# 添加到session:session.add(new_user)# 提交即保存到数据库:session.commit()# 关闭session:session.close()\nsession 对象可视为当前数据库连接。关键是获取 session，然后把对象添加到 session，最后提交并关闭。\n查询记录\n通过 ORM 查询出来的可以不再是 tuple，而是 User 对象。SQLAlchemy 提供的查询接口如下：\n# 创建Session:session = DBSession()# 创建Query查询，filter是where条件，最后调用one()返回第一行，如果调用all()则返回所有行:user = session.query(User).filter(User.id=='5').one()# 打印类型和对象的name属性:print 'type:', type(user)print 'name:', user.name# 关闭Session:session.close()\n运行结果如下： type: &lt;class '__main__.User'&gt;name: Bob\n关 SQLAlchemy 更详细的用法可参考官方文档\n\n参考：http://www.liaoxuefeng.com/\n","categories":["python"],"tags":["python","数据库"]},{"title":"python 中的可迭代对象 (iterable)、迭代器 (iterator) 与生成器 (generator)","url":"/2016/09/08/python%20%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1(iterable)%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8(iterator)%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8(generator)/","content":"本文主要讲述 python 中的几个概念：可迭代对象 (iterable)、迭代器 (iterator) 与生成器 (generator)。\n\n可迭代对象 (iterable) 与\n迭代器 (iterator)\n对于\nstring、list、dict、tuple\n等这类容器对象，可以使用 for\n循环对其进行遍历。像这种可以被遍历的对象被称为可迭代对象。\n通过 for 语句对遍历可迭代对象时，实际上是调用可迭代对象内部的\n__iter__() 方法（因此一个可迭代对象必须要实现\n__iter__()\n方法），调用了这个方法会返回一个迭代器 (iterator)，通过迭代器便可遍历可迭代对象。见下面的例子：\n&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; y = iter(x)&gt;&gt;&gt; z = iter(x)&gt;&gt;&gt; next(y)1&gt;&gt;&gt; y.next()2&gt;&gt;&gt; next(z)1&gt;&gt;&gt; type(x)&lt;class 'list'&gt;&gt;&gt;&gt; type(y)&lt;class 'list_iterator'&gt;\n这里 x 是一个列表，是一个可迭代对象。y 和 z\n是两个独立的迭代器，迭代器内部持有一个状态，该状态用于记录当前迭代所在的位置，以方便下次迭代的时候获取正确的元素。\n迭代器也分具体的迭代器类型，比如\nlist_iterator，set_iterator。iter(x) 语句实际上是调用了\nx 内部的 __iter__ 方法的，调用 __iter__\n方法后会返回一个迭代器，由于迭代器内部实现了 next 方法\n（python2 中是 next 方法，python3 是 __next__\n方法，一个迭代器必须实现此方法），因此可通过\nnext() 方法来遍历可迭代对象。\n因此，执行下面语句：\n x = [1, 2, 3]for elem in x:    ...\n相当于以下流程\n\n上图中调用 next()\n方法直到没有后续元素时，next() 会抛出一个\nStopIteration 异常，通知 for 语句循环结束。如\n&gt;&gt;&gt; a = [1,3]&gt;&gt;&gt; b = iter(a)&gt;&gt;&gt; b.next()1&gt;&gt;&gt; next(b)3&gt;&gt;&gt; next(b)Traceback (most recent call last):  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration\n上面说的都是 python\n自带的容器对象，它们都实现了相应的迭代器方法，那如果是自定义类需要遍历怎么办？\n方法很简单，假如我们需要自定义一个有遍历功能的类\nIterClass，那么只需要在这个类的内部实现一个\n__iter__(self) 方法，使其返回一个带有\n__next__(self) 方法的对象就可以了。如果你在\nIterClass 刚好也定义了 __next__(self)\n方法（一般使用迭代器都会定义），那在 __iter__() 里只要返回\nself 就可以。下面是具体的实例：\nclass IterClass:    def __init__(self, max):        self.max = max    def __iter__(self):        self.a = 0        self.b = 1        return self    def next(self):        fib = self.a        if fib &gt; self.max:            raise StopIteration        self.a, self.b = self.b, self.a + self.b        return fibif __name__ == '__main__':    fib = IterClass(10)    for i in fib:        print i\n上面输出的结果为: 0112358\n上面的代码定义了一个 IterClass 类，用于生成 fibonacci\n序列。用 for 遍历时会逐个打印生成的 fibonacci 数，max 是生成的 fibonacci 序列中数字大小的上限。\n在类的实现中，定义了一个 __iter__(self)\n方法，这个方法是在遍历时被 iter()\n调用，返回一个迭代器。因为在遍历的时候，是直接调用 python\n的内置函数 iter()，由 iter() 通过调用\n__iter__(self) 获得对象的迭代器。\n有了迭代器，就可以逐个遍历元素了。而逐个遍历的时候，也是使用 python\n的内置 的 next() 函数，next()\n函数通过调用对象的 next(self) 方法（python 3 为\n__next__(self) 方法）对迭代器对象进行遍历。因为同时实现\n__iter__(self) 和 next(self) ， 所以\nIterClass 既是可迭代对象，也是迭代器，在实现\n__iter__(self) 的时候，直接返回 self 就可以。\n为了更好地理解，对上面的内容的小结如下：在循环遍历自定义容器对象时，会使用\npython 内置函数 iter() 调用遍历对象的\n__iter__(self) 获得一个迭代器，之后再循环对这个迭代器使用\nnext() 调用迭代器对象的 next(self) 或\n__next__(self)。__iter__ 只会被调用一次，而\n__next__ 会被调用 n 次。\n生成器 (generator)\n生成器其实是一种特殊的迭代器，不过这种迭代器更加简洁和高效，它自动创建了\n__iter__() 和 next()\n方法（因此生成器其实既是一个可迭代对象，也是一个迭代器）,\n除了创建和保存程序状态的自动方法，当发生器终结时，还会自动抛出\nStopIteration 异常。它不需要再像上面的类一样写\n__iter__() 和 next () 方法了，只需要一个\nyiled\n关键字。生成器一定是迭代器（反之不成立）。\n一个带有关键词 yield\n的函数就是一个生成器，它和普通函数不同，生成一个 generator\n看起来像函数调用，但不会执行任何函数代码，直到对其显式或隐式地调用\nnext() (在 for 循环中会隐式自动调用\nnext())\n才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个\nyield 语句就会中断，并返回一个迭代值，下次执行时从\nyield\n的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被\nyield 中断了数次，每次中断都会通过 yield\n返回当前的迭代值（yield 暂停一个函数，next () 从其暂停处恢复其运行）。见下面的例子：\n&gt;&gt;&gt; def reverse(data):...     for index in range(len(data)-1, -1, -1):...         yield data[index]... &gt;&gt;&gt; for char in reverse('hello'):...     print(char)... olleh\n用生成器来实现上面的斐波那契数列的例子是：\ndef fib():    prev, curr = 0, 1    while True:        yield curr        prev, curr = curr, curr + prev&gt;&gt;&gt; f = fib()&gt;&gt;&gt; list(itertools.islice(f, 0, 10))[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n生成器在 Python 中是一个非常强大的编程结构，可以用更少地中间变量写流式代码，此外，相比其它容器对象它更能节省内存和 CPU，它也可以用更少的代码来实现相似的功能。如果构造一个列表的目的仅仅是传递给别的函数，\n那么就可以用生成器来代替。但凡看到类似： def something():    result = []    for ... in ...:        result.append(x)    return result\n都可以用生成器函数来替换： def iter_something():    for ... in ...:        yield x\n只需要在接收函数返回值的时候将其转为 list 类型即可。\n另外对于生成器，python 还提供了一个生成器表达式 (generator\nexpression)：类似与一个 yield\n值的匿名函数。表达式本身看起来像列表推导式，\n但不是用方括号而是用圆括号包围起来 ,\n它返回的是一个生成器对象而不是列表对象。见下面的例子：\n&gt;&gt;&gt; a = (i*i for i in xrange(5))&gt;&gt;&gt; for num in a:...     print num...014916&gt;&gt;&gt; a&lt;generator object &lt;genexpr&gt; at 0x02B707D8&gt;\n\n参考： https://segmentfault.com/a/1190000002900850\nhttp://foofish.net/blog/109/iterators-vs-generators\n","categories":["python","语法"],"tags":["python"]},{"title":"python 中的字符串与编码","url":"/2017/08/28/python%20%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%8E%E7%BC%96%E7%A0%81/","content":"本文主要参考了这篇文章，该文章比较清楚地讲述了字符串的编码问题，并且在\npython2 和 python3 中如何区分使用两者。\n\n字符编码\n因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用 8 个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是 255（二进制\n11111111 = 十进制\n255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是 65535，4 个字节可以表示的最大整数是 4294967295。\n由于计算机是美国人发明的，因此，最早只有 127 个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为\nASCII\n编码，比如大写字母 A 的编码是 65，小写字母 z 的编码是 122。\n但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和\nASCII 编码冲突，所以，中国制定了 GB2312\n编码，用来把中文编进去。\n你可以想得到的是，全世界有上百种语言，日本把日文编到\nShift_JIS 里，韩国把韩文编到 Euc-kr\n里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。\n因此，Unicode 应运而生。Unicode 把所有语言都统一到一套编码里，这样就不会再有乱码问题了。\nUnicode 标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要 4 个字节）。现代操作系统和大多数编程语言都直接支持 Unicode。\n现在，捋一捋 ASCII 编码和 Unicode 编码的区别：ASCII 编码是 1 个字节，而 Unicode 编码通常是 2 个字节。\n字母 A 用 ASCII 编码是十进制的 65，二进制的 01000001；\n字符 0 用 ASCII 编码是十进制的 48，二进制的 00110000，注意字符 '0' 和整数 0 是不同的；\n汉字中已经超出了 ASCII 编码的范围，用 Unicode 编码是十进制的 20013，二进制的 01001110\n00101101。\n你可以猜测，如果把 ASCII 编码的 A 用 Unicode 编码，只需要在前面补 0 就可以，因此，A 的 Unicode 编码是 00000000\n01000001。\n新的问题又出现了：如果统一成 Unicode 编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用 Unicode 编码比 ASCII 编码需要多一倍的存储空间，在存储和传输上就十分不划算。\n所以，本着节约的精神，又出现了把 Unicode 编码转化为 “可变长编码” 的 UTF-8 编码。UTF-8 编码把一个 Unicode 字符根据不同的数字大小编码成 1-6 个字节，常用的英文字母被编码成 1 个字节，汉字通常是 3 个字节，只有很生僻的字符才会被编码成 4-6 个字节。如果你要传输的文本包含大量英文字符，用 UTF-8 编码就能节省空间；注意 UTF-8 并不是唯一对 Unicode 进行编码的编码方式。\n\n\n\n字符\n ASCII\nUnicode\nUTF-8\n\n\n\n\nA\n01000001\n00000000 01000001\n01000001\n\n\n 中\n x\n01001110 00101101\n11100100 10111000 10101101\n\n\n\n从上面的表格还可以发现，UTF-8 编码有一个额外的好处，就是 ASCII 编码实际上可以被看成是 UTF-8 编码的一部分，所以，大量只支持 ASCII 编码的历史遗留软件可以在 UTF-8 编码下继续工作。\n搞清楚了 ASCII、Unicode 和 UTF-8 的关系，我们就可以总结一下现在计算机系统通用的字符编码工作方式：\n在计算机内存中，统一使用 Unicode 编码，当需要保存到硬盘或者需要传输的时候，就转换为 UTF-8 编码。\n如用记事本编辑的时候，从文件读取的 UTF-8 字符被转换为 Unicode 字符到内存里，编辑完成后，保存的时候再把 Unicode 转换为 UTF-8 保存到文件：\n\n\n记事本编码转换例子\n\n而浏览网页的时候，服务器会把动态生成的 Unicode 内容转换为 UTF-8 再传输到浏览器：\n\n\n浏览网页编码转换例子\n\n所以你看到很多网页的源码上会有类似\n&lt;meta charset=\"UTF-8\" /&gt;\n的信息，表示该网页正是用的 UTF-8 编码。\npython 中的字符串\n搞清楚了令人头疼的字符编码问题后，我们再来研究 Python 对 Unicode 的支持。\n因为 Python 的诞生比 Unicode 标准发布的时间还要早，所以最早的 Python 只支持 ASCII 编码，普通的字符串 'ABC' 在 Python 内部都是 ASCII 编码的。Python 提供了\nord() 和 chr()\n函数，可以把字母和对应的数字相互转换：\n&gt;&gt;&gt; ord('A')65&gt;&gt;&gt; chr(65)'A'\nPython 在后来添加了对 Unicode 的支持，以 Unicode 表示的字符串用\nu'...' 表示，比如：\n&gt;&gt;&gt; print u'中文'中文&gt;&gt;&gt; u'中'u'\\u4e2d'\n写 u'中' 和 u'\\u4e2d'\n是一样的，。因此，u'A' 和 u'\\u0041' 也是一样的。\n两种字符串如何相互转换？字符串 'xxx' 虽然是 ASCII 编码，但也可以看成是 UTF-8 编码，而 u'xxx' 则只能是 Unicode 编码。\n把 u'xxx' 转换为 UTF-8 编码的 'xxx' 用\nencode('utf-8') 方法：\n&gt;&gt;&gt; u'ABC'.encode('utf-8')'ABC'&gt;&gt;&gt; u'中文'.encode('utf-8')'\\xe4\\xb8\\xad\\xe6\\x96\\x87'\n英文字符转换后表示的 UTF-8 的值和 Unicode 值相等（但占用的存储空间不同），而中文字符转换后 1 个 Unicode 字符将变为 3 个 UTF-8 字符，上面的 \\xe4 就是其中一个字节，因为它的值是 228，没有对应的字母可以显示，所以以十六进制显示字节的数值。len() 函数可以返回字符串的长度：\n&gt;&gt;&gt; len(u'ABC')3&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; len(u'中文')2&gt;&gt;&gt; len('\\xe4\\xb8\\xad\\xe6\\x96\\x87')6\n反过来，把 UTF-8 编码表示的字符串'xxx' 转换为 Unicode 字符串 u'xxx' 用 decode('utf-8') 方法：\n&gt;&gt;&gt; 'abc'.decode('utf-8')u'abc'&gt;&gt;&gt; '\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8')u'\\u4e2d\\u6587'&gt;&gt;&gt; print '\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8')中文\n两者的转换方式总结来说就是从 Unicode 到 UTF-8 通过\nencode() 方法， 而从 UTF-8 到 Unicode 通过\ndecode() 方法，而且 UTF-8 并不是唯一的编码方式\n在编写 python\n程序时，如果代码中有中文，那么就需要在代码的开始地方加上\n# -*- coding: utf-8 -*-,\n这是为了是为了告诉 Python 解释器，按照 UTF-8 编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。而且这样声明了 UTF-8 编码并不意味着.py 文件就是 UTF-8 编码的，必须并且要确保编辑器在保存文件的时候使用\nUTF-8 编码方式。\n最后需要注意的是，由于历史遗留问题，python 2.x 里的字符串用\n'xxx' 表示 str，Unicode 字符串用\nu'xxx'\n表示 unicode，而在 3.x 中，所有字符串都被视为 unicode，因此，写\nu'xxx' 和 'xxx' 是完全一致的，而在 2.x 中以\n'xxx' 表示的 str 就必须写成\nb'xxx'，以此表示 “二进制字符串”.\n","categories":["python","语法"],"tags":["python"]},{"title":"python 语法杂记 -- 迭代器、生成器、上下文管理器","url":"/2018/12/17/python%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0--%E7%94%9F%E6%88%90%E5%99%A8%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E3%80%81%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8/","content":"本文主要介绍 python 中几个重要的\n\"器\"（迭代器、生成器、上下文管理器）的原理、实现与使用，还有一个装饰器在前面一篇文章已经进行了介绍，本文主要参考了\nPython 之旅\n中的相关章节。\n\n迭代器\n迭代器（iterator）就是用来遍历可迭代对象的（iterable），这两个概念要区分开。\niterable\n像 list，tuple 等可以通过 for..in..\n进行遍历的对象就是可迭代对象，更严谨的定义则是：\n含有 __iter__() 方法或\n__getitem__() 方法的对象称之为可迭代对象.\n可以使用 Python 内置的 hasattr ()\n函数来判断一个对象是不是可迭代的：\n&gt;&gt;&gt; hasattr((), '__iter__')True&gt;&gt;&gt; hasattr([], '__iter__')True&gt;&gt;&gt; hasattr({}, '__iter__')True&gt;&gt;&gt; hasattr(123, '__iter__')False&gt;&gt;&gt; hasattr('abc', '__iter__')False&gt;&gt;&gt; hasattr('abc', '__getitem__')True\n另外，也可使用 isinstance () 进行判断：\n&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance((), Iterable)        # 元组True&gt;&gt;&gt; isinstance([], Iterable)        # 列表True&gt;&gt;&gt; isinstance({}, Iterable)        # 字典True&gt;&gt;&gt; isinstance('abc', Iterable)     # 字符串True&gt;&gt;&gt; isinstance(100, Iterable)       # 数字False\niterator\n迭代器是一个对象，但比较特别，它需要遵循迭代器协议，具体协议如下\n\n迭代器协议（iterator protocol）是指要实现对象的\n__iter()__ 和 next() 方法（注意：Python3\n要实现 __next__() 方法），其中，__iter()__\n方法返回迭代器对象本身，next()\n方法返回容器的下一个元素，在没有后续元素时抛出 StopIteration 异常。\n\n这里需要注意的是，虽然元组、列表和字典等对象是可迭代的，但它们却不是迭代器\n首先，可以使用 hasattr () 进行判断： &gt;&gt;&gt; hasattr((1, 2, 3), '__iter__')True&gt;&gt;&gt; hasattr((1, 2, 3), 'next')  # 有 __iter__ 方法但是没有 next 方法，不是迭代器False&gt;&gt;&gt;&gt;&gt;&gt; hasattr([1, 2, 3], '__iter__')True&gt;&gt;&gt; hasattr([1, 2, 3], 'next')False&gt;&gt;&gt;&gt;&gt;&gt; hasattr({'a': 1, 'b': 2}, '__iter__')True&gt;&gt;&gt; hasattr({'a': 1, 'b': 2}, 'next')False\n同样也可以使用 isinstance() 进行判断：\n&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((), Iterator)False&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance({}, Iterator)False&gt;&gt;&gt; isinstance('', Iterator)False&gt;&gt;&gt; isinstance(123, Iterator)False\n虽然这些可迭代对象不是迭代器，但是可以使用 Python 内置的\niter() 函数获得它们的迭代器对象，如下所示：\n&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance(iter([1, 2, 3]), Iterator)  # 使用 iter() 函数，获得迭代器对象True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True\n事实上，Python 的 for 循环就是先通过内置函数\niter() 获得一个迭代器，然后再不断调用 next()\n函数实现的，即：\nfor x in [1, 2, 3]:    print i\n等价于\nit = iter([1, 2, 3])while True:    try:        x = next(it)        print x    except StopIteration:        # 没有后续元素，退出循环        break\n下面是一个斐波那契数列迭代器，根据迭代器的定义，我们需要实现\n__iter()__ 和 next() 方法（在 Python3 中是\n__next__() 方法）\nclass Fib(object):    def __init__(self):        self.a, self.b = 0, 1    # 返回迭代器对象本身    def __iter__(self):        return self    # 返回容器下一个元素    def next(self):        self.a, self.b = self.b, self.a + self.b        return self.adef main():    fib = Fib()        for i in fib:        if i &gt; 10:            break        print i\n因此，关于迭代器和可迭代对象，需要注意下面三点\n1.\n元组、列表、字典和字符串对象是可迭代的，但不是迭代器，不过我们可以通过\niter() 函数获得一个迭代器对象  2. Python 的\nfor 循环实质上是先通过内置函数 iter()\n获得一个迭代器，然后再不断调用 next() 函数实现的\n 3. 定义迭代器需要实现对象的 __iter()__和\nnext() 方法（Python3 要实现 __next__()\n方法），其中，__iter()__\n方法返回迭代器对象本身，next()\n方法返回容器的下一个元素，在没有后续元素时抛出 StopIteration\n异常。\n生成器\nyield\n生成器也是迭代器的一种，一个带有关键字\nyield 的函数就是一个生成器函数，而当我们使用 yield\n时，它帮我们自动创建了 __iter__() 和 next()\n方法，而且在没有数据时，也会抛出 StopIteration\n异常，非常简洁和高效。如下是一个简单的例子\n&gt;&gt;&gt; def generator_function():...     print 'hello 1'...     yield 1...     print 'hello 2'...     yield 2...     print 'hello 3'&gt;&gt;&gt;&gt;&gt;&gt; g = generator_function()  # 函数没有立即执行，而是返回了一个生成器&gt;&gt;&gt; g.next()  # 当使用 next()(或 next(g))的时候开始执行，遇到 yield 暂停并返回hello 11&gt;&gt;&gt; g.next()    # 从原来暂停的地方继续执行hello 22&gt;&gt;&gt; g.next()    # 从原来暂停的地方继续执行，没有 yield，抛出异常hello 3Traceback (most recent call last):  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration\n从上面的例子可知，带有 yield 的函数执行过程如下\n1.\n调用该函数的时候不会立即执行代码，而是返回了一个生成器对象；\n2. 当使用 next() (在 for 循环中会自动调用\nnext()) 作用于返回的生成器对象时，函数开始执行，在遇到\nyield 的时候会『暂停』并返回 yield 后的值\n 3. 当再次使用 next()\n的时候，函数会从原来『暂停』的地方继续执行，直到遇到 yield\n语句，如果没有 yield 语句，则抛出异常 **\n相比于迭代器，生成器这样的 lazy evaluation\n能够节省更多的内存，同时让代码更加简洁，如前面定义的斐波那契数列迭代器，通过生成器实现的代码如下\n&gt;&gt;&gt; def fib():...     a, b = 0, 1...     while True:...         a, b = b, a + b...         yield a...&gt;&gt;&gt; f = fib()&gt;&gt;&gt; for item in f:...     if item &gt; 10:...         break...     print item...112358\nsend(), throw(),\nclose()\n除了上面提到的\nyield，生成器还有一些其他的特殊方法：send(),\nthrow() 和\nclose()，分别用于给生成器发送消息、异常和关闭生成器。\n具体用法如下\nIn [2]: def generator_function():   ...:     # test send()   ...:     value1 = yield 0   ...:     print('value1 is ', value1)   ...:   ...:     # test throw()   ...:     try:   ...:         yield 'Normal'   ...:     except ValueError:   ...:         yield 'Error'   ...:   ...:     # test close()   ...:     yield 1   ...:     yield 2   ...:     yield 3   ...:In [3]: g = generator_function()In [4]: next(g)Out[4]: 0In [5]: g.send(10)value1 is  10Out[5]: 'Normal'In [6]: g.throw(ValueError)Out[6]: 'Error'In [7]: next(g)Out[7]: 1In [8]: g.close()In [9]: next(g)---------------------------------------------------------------------------StopIteration                             Traceback (most recent call last)&lt;ipython-input-9-5f315c5de15b&gt; in &lt;module&gt;()----&gt; 1 next(g)StopIteration:\n在上面的代码中，先调用 next()\n方法，使函数开始执行，代码执行到 yield 0 的时候暂停，返回了\n0；接着，执行 send()\n方法，它会恢复生成器的运行，并将发送的值赋给上次中断时 yield\n表达式的执行结果，也就是 value1，这时控制台打印出 value1\n的值，并继续执行，直到遇到 yield 后暂停，此时返回 'Normal',\n因此，简单地说，send() 方法就是 next()\n的功能，加上传值给 yield\n接着， throw() 方法向生成器函数传递了 ValueError\n异常，此时代码进入 except ValueError 语句，遇到 yield\n'Error'，暂停并返回 Error 字符串，因此，简单的说，throw () 就是\nnext () 的功能，加上传异常给 yield。\n最后使用了 close()\n方法来关闭一个生成器。生成器被关闭后，再次调用 next()\n方法，不管能否遇到 yield 关键字，都会抛出 StopIteration 异常\n上下文管理器\n__enter__() &amp;\n__exit__()\n上下文 (context) 在计算机中是个很常见的词汇，可以简单将其理解为运行时的环境，如进程上下文指的是进程在执行时\nCPU\n的所有寄存器中的值、进程的状态以及堆栈上的内容等，当系统需要切换到其他进程时，系统会保留当前进程的上下文，也就是运行时的环境，以便再次执行该进程。\n而在 python 中上下文管理器最常见的场景便是 with\n语句，with\n一般用于对资源进行访问的场景，确保执行过程中出现异常情况时也可以对资源进行回收，比如自动关闭文件等。\n类似迭代器协议（Iterator Protocol），上下文管理器（Context\nmanager）也有上下文管理协议（Context Management Protocol）。\n\n上下文管理器协议，是指要实现对象的\n__enter__() 和 __exit__() 方法。\n上下文管理器也就是支持上下文管理器协议的对象，也就是实现了\n__enter__() 和 __exit__() 方法的对象。\n\n如下是一个简单的上下文管理器的例子\nfrom math import sqrt, powclass Point(object):    def __init__(self, x, y):        print 'initialize x and y'        self.x, self.y = x, y    def __enter__(self):        print \"Entering context\"        return self    def __exit__(self, type, value, traceback):        print \"Exiting context\"    def get_distance(self):        distance = sqrt(pow(self.x, 2) + pow(self.y, 2))        return distance\n使用 with 语句调用上下文管理器如下所示\nwith Point(3, 4) as pt:    print 'distance: ', pt.get_distance()# outputinitialize x and y   # 调用了 __init__ 方法Entering context     # 调用了 __enter__ 方法distance:  5.0       # 调用了 get_distance 方法Exiting context      # 调用了 __exit__ 方法\n上面的 with 语句执行过程如下：\n\nPoint (3, 4) 生成了一个上下文管理器；\n调用上下文管理器的 __enter__() 方法，并将\n__enter__() 方法的返回值赋给 as 字句中的变量\npt;\n 执行语句体（指 with 语句包裹起来的代码块）内容，输出 distance；\n不管执行过程中是否发生异常，都执行上下文管理器的\n__exit__() 方法。\n\n一般来说，__exit__()\n方法负责执行清理工作，如释放资源，关闭文件等。如果执行过程没有出现异常，或者语句体中执行了语句\nbreak/continue/return，则以 None 作为参数调用\n__exit__(None, None, None)；如果执行过程中出现异常，则使用\nsys.exc_info 得到的异常信息为参数调用\n__exit__(exc_type, exc_value, exc_traceback).\n同时出现异常时，如果\n__exit__(type, value, traceback) 返回 False 或\nNone，则会重新抛出异常，让 with\n之外的语句逻辑来处理异常；如果返回\nTrue，则忽略异常，不再对异常进行处理。\n上面的 with 语句执行过程没有出现异常，下面是出现异常的情形：\nwith Point(3, 4) as pt:    pt.get_length()        # 访问了对象不存在的方法# outputinitialize x and yEntering contextExiting context---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)&lt;ipython-input-216-ab4a0e6b6b4a&gt; in &lt;module&gt;()      1 with Point(3, 4) as pt:----&gt; 2     pt.get_length()AttributeError: 'Point' object has no attribute 'get_length'\n对前面的 __exit__() 方法修改如下\ndef __exit__(self, type, value, traceback):    print \"Exception has been handled\"    print \"Exiting context\"    return True\n则执行相同过的代码的结果如下\nwith Point(3, 4) as pt:    pt.get_length()      # 访问了对象不存在的方法# outputinitialize x and yEntering contextException has been handledExiting context\ncontextlib\n除了在类中定义 __enter__ 和 __exit__\n方法来实现上下文管理器，我们还可以通过生成器函数和装饰器来实现上下文管理器，这个装饰器就在\npython 提供的 contextlib 模块中。如下是个简单的例子\nfrom contextlib import contextmanager@contextmanagerdef point(x, y):    print 'before yield'    yield x * x + y * y    print 'after yield'with point(3, 4) as value:    print 'value is: %s' % value# outputbefore yieldvalue is: 25after yield\n可以看到，yield 产生的值赋给了 as 子句中的 value\n变量。\n另外，需要强调的是，虽然通过使用 contextmanager\n装饰器，可以不必再编写 __enter__ 和 __exit__\n方法，但是获取和清理资源的操作仍需要我们自己编写：获取资源的操作定义在\nyield 语句之前，释放资源的操作定义在 yield 语句之后。\n","categories":["python","语法"],"tags":["python"]},{"title":"python 中使用 SQLite","url":"/2016/06/07/python%20%E4%BD%BF%E7%94%A8SQLite/","content":"SQLite 是一款轻量级的关系型数据库，相比 MySQL 等 CS 模式的数据库，SQLite 有以下特点：\n - 不需要一个单独的服务器进程或操作的系统（无服务器的）。 -\nSQLite 不需要配置，这意味着不需要安装或管理。 - SQLite\n是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于 250KiB。\n- 一个完整的\nSQLite 数据库是存储在一个单一的跨平台的磁盘文件。 -\nSQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。\n这里科普一下 ACID 的定义，摘自维基百科\n\nACID，是指数据库管理系统（DBMS）在写入 / 更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。\n原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。\n一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。\n隔离性：数据库允许多个并发事务同时对齐数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。\n持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n\n\nSQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32,\nWinCE, WinRT）中运行。\n\n由于 SQLite 本身是 C 写的，而且体积很小，所以，经常被集成到各种应用程序中，甚至在 iOS 和 Android 的 App 中都可以集成。\nPython 就内置了 SQLite3，所以，在 Python 中使用 SQLite，不需要安装任何东西，直接使用。\npython 中使用 SQLite 的步骤与使用 MySQL 的步骤非常类似，主要分为下面三步：\n1. 获取连接 2. 获取游标 3. 执行语句并提交事务\n下面是操作 SQLite 的一个简单例子\nimport sqlite3DB = 'test.db'try:    conn = sqlite3.connect(DB)    cursor = conn.cursor()    cursor.execute('create table student(id varchar(10),name varchar(20))')    cursor.execute('insert into student values(\"2012\",\"lc\")')    cursor.execute('select * from student where name=?',('lc',))    result = cursor.fetchall()    for row in result:        print rowexcept sqlite3.Error as e:    print efinally:    cursor.close()    conn.commit()    conn.close()\n以下几点将有助于更好地理解上面的代码：\n\n由于 SQLite 是一个嵌入式的本地数据库，所以连接时不需要指定服务器地址、用户等。只需要通过 sqlite3.connect(DB) 便可连接数据库 DB，假如没有该数据库时会自动创建。\n由于 SQLite 是关系型数据库，所以绝大部分的 SQL 语句规范与 MySQL 等类似\n如果执行的 SQL 语句需要从外部传入变量，则需要在 SQL 语句中将变量替换成？, 并在 execute 方法增加第二个参数（tuple 类型）\nfetchall() 会返回一个 list，list 中的每个元素都是一个 tuple，代表数据库中的一行记录\n执行完语句后需要关闭 cursor 和 conn，并且在关闭 conn 前需要进行 commit (), 否则修改不会生效\n\n","categories":["python"],"tags":["python","数据库"]},{"title":"python 并行编程概述","url":"/2016/05/29/python%20%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0/","content":"程序并行化的形式\n程序并行化有以下三种形式，分别是：并发编程（Concurrent\nProgramming）、并行编程（Parallel Programming ）、分布式编程 (Distributed\nProgramming)\n\n并发编程（Concurrent\nProgramming）\n并发编程（Concurrent Programming）的模型图如下所示：\n\n从图中可知，并发编程类似操作系统中的伪并行，任一时刻只有一个进程占用 CPU，通过调度控制不同的进程在不同时刻访问 CPU。\n并行编程（Parallel Programming\n）\n并行编程的模型图如下所示：\n\n并行编程指在多核环境中，同一时间每个核都可以允许一个进程运行，这可以认为是真正意义上的并行\n分布式编程 (Distributed\nProgramming)\n分布式编程的模型图如下所示：\n\n分布式编程指在不同机器上同时完成同一项任务，是物理上的隔离。如 Hadoop 中的 MapReduce 就是分布式编程的一个典型例子。\n并行化编程的通信方式\n由于并行化后的进程要完成的是同一项任务，所以程序间的通信是必须的。程序的通信方式一般有以下两种：共享状态（shared\nstate）、消息传递（message passing）\n共享状态（shared state）\n这种方法就是共享进程间的资源，类似于同一进程里所有的线程共享进程的资源一样。\n这种方法有以下不足：任一进程对共享资源的错误操作都会影响其他的进程；难以应用在分布式编程中。\n在这种通信方式下，对于只读的数据可以不加保护措施，但是对于可写的数据，必须要防止多个进程同时修改这个数据。如在操作系统中的互斥量（mutex），线程锁等就是这类型的防护措施。\n消息传递（message passing）\n消息传递能够避免上面提到的问题，而且也能够应用在分布式编程中。每进行一次消息传递，都会复制一份数据，因此数据的一致性大大提升。\n虽然这种方法占用的内存比第一种要大，但是这种方法有以下优势：\n\n数据的一致性大大增强\n消息能够在本地传输（多进程）或者在分布式环境中传输\n解决可伸缩问题并允许不同系统间的相互操作\n对于编程人员来说便于实现\n\n并行化编程存在的问题\n在并行化编程中有可能会遇到以下问题\n死锁 (DeadLock)\n与操作系统中的死锁问题一样，发生在多个进程中每个都需要其他进程的资源，同时又不肯释放自己的资源，导致资源的需求关系形成闭合的环状。\n如下图所示，进程 A 需要进程 C 的资源，进程 C 需要进程 B 的资源，而进程 B 需要进程 A 的资源。并且在进程释放自己的资源前，其他进程无法获取，而进程需要获得其他进程的资源才能完成自己的任务并释放资源，这样就是一个死锁的局面。\n\n饿死 (Starvation)\n饿死概念与操作系统中的也是一样，指的是某个进程一直得不到自己的资源，无法继续运行。\n如进程 A 的优先级比 B\n要高，所以优先运行 A，但是进程 A 由于需要完成的任务繁重，所以一直占用着 CPU，导致进程 B 一直无法运行，就称为进程 B 被饿死。\n竞争条件 (Race conditons)\n竞争条件是操作系统和电子电路中的一个常见概念，维基百科对其定义如下：\n\nA race condition or race hazard is the behavior of an electronic,\nsoftware or other system where the output is dependent on the sequence\nor timing of other uncontrollable events. It becomes a bug when events\ndo not happen in the order the programmer intended. The term originates\nwith the idea of two signals racing each other to influence the output\nfirst.\n\n大意就是多个具有不确定性（无法知道何时会到达或执行指定操作）的对象（进程或电子信号），必须要按照时间序列（time\nsequence）执行，假如这种同步被破坏，那么多个进程会没有顺序地修改同一个变量，导致数据出错。如下面的简单例子：\n假设图中的 husband 和 wife 是两个进程，两者同时操作账户里的钱，正常情况下是这样的\n\n而假如两者不按照 time sequences 执行操作，同步会被破坏，导致出现 race\nconditions，如下图所示：\n\npython 中实现并行化编程工具\n\nthreading\n模块，python 自带的多线程模块\n multiprocessing模块，python 自带的多进程模块\n parallel\nPython模块，具有运行过程调整进程数目、动态负载平衡的第三方模块\n Celery\n模块，用于分布式编程的一个分布式任务队列模块\n\n\n参考： Parallel Programming with Python\n","categories":["python","并行编程"],"tags":["python","操作系统"]},{"title":"Python 语法杂记","url":"/2016/05/23/python%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0/","content":"本文主要记录一些学习过程中遇到的一些比较零碎的 python 语法知识。\n\n常见易错用法\nfor 循环中修改下标的值\npython 中的 for 循环一般会写成这样 for i in range(10):    ....\n上面的语句中循环了10此，i的值从0增到9。在 Java 中可以在 for 循环中修改 i 的值，从而跳过一些 i 的值不处理，但是在上面的语法中无效,因为range实际上生成了一个0到9的list，每次i会取其中的一个值，所以如果没有break的话，i会取遍10个值。\n如果要达到修改 i 的值跳过一些值不处理，建议使用 while 语句。\n字典\n初始化\n可通过 {} 或 dict() 函数进行初始化，通过 dict() 初始化时，可以选择是否传入参数，传入参数初始化时，参数格式为包含若干 kv 的一个 list，每个 kv 用一个 tuple 表示，如\n&gt;&gt;&gt;d = dict(    [('foozelator', 123),     ('frombicator', 18),      ('spatzleblock', 34),      ('snitzelhogen', 23)    ])&gt;&gt;&gt;d{'foozelator': 123, 'frombicator': 18, 'snitzelhogen': 23, 'spatzleblock': 34}\n删除一个 key\n－　从字典中删除一个 key：dict.pop(key[, default]), 存在 key 时返回 key 对应的 value，不存在时返回 default。不存在且没有 default 时返回 KeyError。\n遍历字典\n\ndict.keys() 返回字典 dict 所有 keys 组成的一个 list\ndict.values() 返回字典 dict 所有 values 组成的一个 list\ndict.items() 返回字典 dict 所有 kv 组成的一个 list，kv 以 tuple 的形式存储\n\n对字典排序\n通过 sorted 函数可以根据字典的 key 或 value 对字典排序，并返回一个元素类型为 tuple 为的 list，每个 tuple 代表字典中的一个元素。排序不会改变原来字典中的值。\na = {1:2,2:1}# 根据key对字典排序,reverse = True表示从大到小，默认是从小到大sorted(a.items(), key = lambda x:x[0], reverse = True)# 输出为[(2,1),(1,2)]# 根据value对字典从小到大排序sorted(a.items(), key = lambda x:x[1])# 输出为[(2:1),(1:2)]\n集合\n可变集合\n用 {} 或 set () 函数来生成可变集合，集合中不含有相同元素。 s={} # 非法s={1,2,3,4} # 合法s=set() # 也可用s = set(list),用一个集合提取list中的不重复元素\n### 不可变集合\n对应于元组（tuple）与列表（list）的关系，对于集合（set），Python提供了一种叫做不可变集合（frozen\nset）的数据结构。\n创建一个不可变集合 &gt;&gt;&gt;s = frozenset([1, 2, 3, 'a', 1])&gt;&gt;&gt;sfrozenset({1, 2, 3, 'a'})\n不可变集合的一个主要应用是用来作为字典的键，例如用一个字典来记录两个城市之间的距离：\n&gt;&gt;&gt;flight_distance = {}&gt;&gt;&gt;city_pair = frozenset(['Los Angeles', 'New York'])&gt;&gt;&gt;flight_distance[city_pair] = 2498&gt;&gt;&gt;flight_distance[frozenset(['Austin', 'Los Angeles'])] = 1233&gt;&gt;&gt;flight_distance[frozenset(['Austin', 'New York'])] = 1515&gt;&gt;&gt;flight_distance{frozenset({'Austin', 'New York'}): 1515, frozenset({'Austin', 'Los Angeles'}): 1233, frozenset({'Los Angeles', 'New York'}): 2498}\n集合的一些方法\n\n添加元素，s.add(item)\n交集，s1&amp;s2 或 s1.intersection(s2), 返回集合 s1 和集合 s2 的交集\n并集，s1|s2 或 s1.union(s2), 返回集合 s1 和集合 s2 的并集\n差集，s1-s2 或 s1.defference(s2) 返回 s1 中有但 s2 中没有的元素的集合\n对称差集，s1^s2 或 s1.symmetric_difference(s2) 返回 s1 中有但 s2 中没有的元素和 s2 中有但 s1 中没有的元素的合集\n子集，s1.issubset(s2) 或 s1&lt;=s2 判断 s1 是否 s2 的子集；反之也可用 s2.issuperset(s1) 达到上面的效果\n删除一个元素 s.remove(element) 或 s.pop(element) 后者会返回这个值元素的值而前者不会；不存在该元素时均会报错。s.discard(element) 作用跟 remove 一样，区别在于不存在该元素时 discard () 不会报错\n\n列表\n列表合并\n可通过加号 + 按顺序合并两个列表，如 &gt;&gt;&gt;a = [1, 2, 3]&gt;&gt;&gt;b = [3.2, 'hello']&gt;&gt;&gt;a + b[1, 2, 3, 3.2, 'hello']\n列表的 extend() 方法也能实现相同功能。如： &gt;&gt;&gt;a = [1, 2, 3]&gt;&gt;&gt;b = [3.2, 'hello']&gt;&gt;&gt;a.extend(b)&gt;&gt;&gt;a[1, 2, 3, 3.2, 'hello']\n列表排序\n列表可用内置函数，分为两种类型：排序后改变原列表和排序后不改变列表\n排序后改变原列表的方法是 listName.sort(), 不改变原列表的方法是 sorted(listName)。具体见下面例子\n&gt;&gt;&gt;s = [1,4,3,2]&gt;&gt;&gt;s.sort()&gt;&gt;&gt;s[1, 2, 3, 4]&gt;&gt;&gt;s = [1,4,3,2]&gt;&gt;&gt;sorted(s)[1, 2, 3, 4]&gt;&gt;&gt;s[1, 4, 3, 2]\n默认是从小到大排序，也可从大到小排序，只需要加入 reverse=True 的参数即可\n&gt;&gt;&gt;s=[1, 4, 3, 2]&gt;&gt;&gt;sorted(s,reverse=True)[4, 3, 2, 1]&gt;&gt;&gt;s[1, 4, 3, 2]&gt;&gt;&gt;s.sort(reverse=True)&gt;&gt;&gt;s[4, 3, 2, 1]\n列表推导式 (List\ncomprehension)\n也叫列表生成式。\n将多条语句写成一条，如要求列表 a 中所有偶数的和，可写成 a = [1,2,3,4,5,6]print sum([i for i in a if i%2==0])\nsum()是求一个列表内所有元素的和的内置函数，传入的参可以为一个列表，而 [i for i in a if i%2==0] 则是列表推导式，该语句生成了列表[2,4,6]\n但是，Python 会生成这个列表，然后再将它放到垃圾回收机制中（因为没有变量指向它），这毫无疑问是种浪费。\n为了解决这种问题，与 rang() 和 xrange() 的问题类似，Python 使用生成器（generator）表达式来解决这个问题：\n将 sum 中代表 list 的括号去掉即可，修改后如下所示： a = [1,2,3,4,5,6]print sum(i for i in a if i%2==0)\n上面的 (i for i in a if i%2==0) 就是一个生成器，与列表生成式最大的不同是列表生成式会在执行语句的时候生成完整的列表，而生成器会在在循环的过程中不断推算出后续的元素\n除了上面这种定义生成器的方法，还可以在函数中通过 yield 关键字实现一个生成器。如下面生成斐波那契数列的例子：\ndef print_fibona(n):    a,b=0,1,    for i in range(n):        yield b        a,b = b,a+bfor i in print_fibona(8):    print i, 输出结果为： 1 1 2 3 5 8 13 21\nprint_fibona 不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。\n列表的其他一些方法\n\n查找某一元素在列表中出现了几次，list.count(element) 返回 element 在 list 中出现的次数\n查找某一元素在列表中第一次出现的位置，list.index(element) 返回 element 在 list 中第一次出现的位置，不存在 element 元素时会报错\n在特定位置插入某一元素，其他元素依次往后移动一步，list.insert(index,element) 在 index 处插入 element，原来在 index 处及后面的元素依次往后移动一位\n删除元素，有两种方法，list.remove(element) 会将 list 中第一次出现的 element 删除；list.pop(index) 则会将 list 中下标为 index 的元素删除且返回该元素的值。\n列表反转，list.reverse() 回将 list 中的元素反转\n\nmap 方法生成序列\n可以通过 map 的方式利用函数来生成序列，例子如下：\ndef sqr(x):     return x ** 2a = [2,3,4]print map(sqr, a)\n输出为 [4, 9, 16]\n其用法为 map(aFun, aSeq), 将函数 aFun\n应用到序列 aSeq\n上的每一个元素上，返回一个列表，不管这个序列原来是什么类型。\n事实上，根据函数参数的多少，map\n可以接受多组序列，将其对应的元素作为参数传入函数，例子如下：\ndef add(x, y):     return x + ya = (2,3,4)b = [10,5,3]print map(add,a,b) 结果为 [12, 8, 7]\n序列赋值\n序列（list,tuple,str) 可以将其值逐一赋值给变量，详见下面的例子\n&gt;&gt;&gt; a,b,c=[1,2,3]&gt;&gt;&gt; print a,b,c1 2 3&gt;&gt;&gt; a,b,c=(1,2,3)&gt;&gt;&gt; print a,b,c1 2 3&gt;&gt;&gt; a,b,c=\"123\"&gt;&gt;&gt; print a,b,c1 2 3\n数值\n整形（int）和长整形（long）\n整型数字的最大最小值\n\n在 32 位系统中，一个整型 4 个字节，最小值 -2,147,483,648，最大值\n2,147,483,647。\n在 64 位系统中，一个整型 8 个字节，最小值\n-9,223,372,036,854,775,808，最大值 9,223,372,036,854,775,807。\n\n当整型超出范围时，Python 会自动将整型转化为长整型，长整型就是在数字后面加上一个大写的 L\n复数\nPython 使用 j 来表示复数的虚部： a = 1 + 2jtype(a)a.real # 实部a.imag # 虚部a.conjugate() # 共轭 ###\n内置的一些数值函数 abs(n) 求n的绝对值\nround(n) 求n的整数部分，返回的是float类型\nmax(n,m) 求m，n的最大值\nmin(n,m) 求m,n的最小值\n其他的一些表示方法\n\n科学计数法，1e-6 表示 \\(10^{-6}\\)\n16 进制，前面加上 0x 修饰，后面的数字范围为 0~F\n8 进制，前面加上 0 修饰，后面的数字范围为 0~7\n2 进制，前面加上 0b 修饰，后面数字范围为 0~1\n\n字符串\n常用的字符串方法\n\ns.split(c), 以符号 c 为分隔符将字符串 s 分割，返回字符串列表\nc.join(sList)，作用跟上面的相反，以符号 c 为连接符将字符串数组 sList 连接起来\ns.repalce(a,b), 将字符串中的 a 替换为 b，并返回替换后的字符串，注意 s 本身不变\ns.upper()，将 s 中的英文字母转为大写的并返回，但是 s 本身不变\ns.lower()，将 s 中的英文字母转为小写的并返回，但是 s 本身不变\ns.strip()，去掉字符串 s 前后的空格并返回，但是 s 本身不变\n\ns.lstrip()，去掉字符串 s 前的空格并返回，但是 s 本身不变\ns.rstrip()，去掉字符串 s 后的空格并返回，但是 s 本身不变\n可通过 dir(str) 查找更多方法\n\n\n数字与字符的转换\n整数转字符串\n\n16 进制：hex(255) 返回 '0xff'\n8 进制：oct(255) 返回 '0377'\n2 进制：bin(255) 返回 '0b11111111'\n\n字符串转整数\n通过 int(s) 转换，还可以指定特定的进制，默认是十进制。如下面的方法均返回 255\n\nint('ff',16)\nint('377',8)\nint('111111111',2)\nint('255')\n\nASCII 码与字符的转换\n\n数字转 ASCII 码：chr(97) --&gt; 'a'\nASCII 码转数字：ord('A') --&gt; 65\n\n字符串的分片与索引\n索引指的是可以通过下标来寻找字符串中的某个字符，0 下标代表第一个，-1 下标代表倒数第一个，-2 下标代表倒数第二个\n分片指的是提取子字符串，一般格式为 [start:end:step],start 和 end 都是指字符串的下标，省略时默认为字符串的头和尾；step 指每次取字符串的步长，省略时为 1，也即是从 start 到 end-1 每个字符串都取，step 也可取负值，表示从后往前按 step 的绝对值来取。如 s[::-1] 表示反转字符串\n函数\n高阶函数（Higher-order\nfunction）\n把另外一个函数作为参数传入的函数称为高阶函数，函数式编程就是指这种高度抽象的编程范式。\nmap/reduce 函数\n\nmap 函数：两个参数，第一个参数为接收一个参数的函数，第二个参数为一个序列，利用第一个参数所代表的函数对序列中的每个元素操作，返回操作后的序列\n reduce 函数：两个参数，第一个参数为接收两个参数的函数，第二个参数为一个序列，利用第一个参数代表的函数对序列中的两个首元素操作，返回的结果与序列的下一元素再进行函数的操作，直到遍历完序列。\n\n例子： # 利用map函数对列表中每个数进行平方操作&gt;&gt;&gt; map(lambda x:x**2,[1,2,3])[1, 4, 9]# 利用reduce函数实现sum()函数的功能&gt;&gt;&gt; reduce((lambda x,y:x+y),[1,2,3,4,5])15\n上面均利用了 lambda 函数，也可以将lambda函数改成 def 定义好的函数。\nfilter 函数\nPython 内建的 filter () 函数用于过滤序列。\n和 map () 类似，filter () 也接收一个函数和一个序列。和 map () 不同的时，filter () 把传入的函数依次作用于每个元素，然后根据返回值是 True 还是 False 决定保留还是丢弃该元素。\n例子：过滤掉 1~100 中的素数并返回结果 import mathdef is_prime(num):    for i in range(2,int(math.sqrt(num))+1):        if num%i == 0:            return Trueprint filter(is_prime,range(1,101))\nsorted 函数\nsorted 函数除了可以用来给列表排序外，还可以通过排序函数作为传入参数，进行指定的排序。\n排序函数通常规定，对于两个元素 x 和 y，如果认为 x &lt;\ny，则返回 - 1 或负数，如果认为 x == y，则返回 0，如果认为 x &gt;\ny，则返回 1 或正数。python 内部定义的排序函数规则就是这样的，根据这样的原理，我们可以自定义一个排序函数进行降序排序。例子如下：\ndef descend_sort(x,y):    if x&gt;y:        return  -1    else:        return 1print sorted(range(1,101),descend_sort)\n上面的代码也可以简单写成 sorted(range(1,101),lambda x,y:y-x)\n返回函数\n高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回\ndef lazy_sum(*args):    def sum():        s = 0        for i in args:            s+=i        return s    return sum\n调用 lazy_sum () 时，返回的并不是求和结果，而是求和函数：\n&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function sum at 0x10452f668&gt;\n调用函数 f 时，才真正计算求和的结果：\n&gt;&gt;&gt; f()25\n在这个例子中，我们在函数 lazy_sum 中又定义了函数 sum，并且，内部函数 sum 可以引用外部函数 lazy_sum 的参数和局部变量，当 lazy_sum 返回函数 sum 时，相关参数和变量都保存在返回的函数中，这种程序称为闭包（Closure）\n另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了 f () 才执行。例子如下\ndef count():    fs = []    for i in range(1, 4):        def f():             return i*i        fs.append(f)    return fsf1, f2, f3 = count()\n在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的3个函数都返回了。\n你可能认为调用 f1 ()，f2 () 和 f3 ( ) 结果应该是 1，4，9，但实际结果是：\n&gt;&gt;&gt;print  f1(),f2(),f3()9 9 9\n全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。\n返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。\n如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变。如下面的例子\ndef count():    fs = [lambda x=y:x**2 for y in range(1,4)]    return fsf1,f2,f3 = count()print f1(),f2(),f3() 最后打印出来的结果是 1 4 9\n装饰器\n有一个函数我们希望将其运行前后打印某些信息，却又不希望改变这个函数的代码，那么久可以通过装饰器（decorator）来实现这个功能。\n例子如下： def log(func):    def wrapper():        print 'before %s' %func.__name__        return func()    return wrapper@logdef hello():    print 'hello'hello() 输出结果为： before hello_worldhello\n实际上执行hello()时相当于执行了hello=log(hello)，即将hello指向了返回的wrapper函数，而这也带来了一个问题，就是hello的__name__属性变为了wrapper的__name__属性。也就是加入在上面的程序的最后加上 print hello_world.__name__打印出来的是 wrapper。所以，**需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错**。\n这些事情不用我们自己做，Python 内置的 functools.wraps 就是干这个事的，所以，上面的规范写法如下：\nimport functoolsdef log(func):    @functools.wraps(func)    def wrapper():        print 'before %s' %func.__name__        return func()    return wrapper@logdef hello_world():    print 'hello'hello_world()print hello_world.__name__ 这时打印出来的结果是 before hello_worldhellohello_world\n上面的例子中装饰器均没有参数，下面给出装饰器带有参数的例子：\ndef log_a(text):    def decorator_a(func):        @functools.wraps(func)        def wrapper_a():            print 'args in the decorator is %s'%text            func()        return wrapper_a    return decorator_a@log_a('haha')def hello_world_a():    print 'hello'hello_world_a()print hello_world_a.__name__ 输出结果如下： args in the decorator is hahahellohello_world_a\n执行 hello_world_a() 相当于执行了 hello_world_a()=log('haha')(hello_world_a()),其中的 log('haha') 返回了装饰器函数 decorator_a.\n类与对象\n私有变量\npython 中没有 private 关键字来限定变量的私有性，假如变量名是以两根下划线开头，那么就认为是私有变量，如为类 s 定义了一个__age 的变量，那么不能通过 s.__age 在外部修改这个变量，只能通过在类的内部定义 set 和 get 方法。\n获取对象的信息\n通过 type(object) 函数或 isinstance(object，type) 函数可以判断一个类或对象的类型，通过 dir(object) 函数可以找到一个对象的所有属性和方法。通过 hasattr(object, 'x')\n判断 object 是否有属性 x\n通过 dir(object) 列出一个类的所有属性和方法会发现有很多__XXX___方法，类似__xxx__的属性和方法在 Python 中都是有特殊用途的，比如__len__方法返回长度。在 Python 中，如果你调用 len () 函数试图获取一个对象的长度，实际上，在 len () 函数内部，它自动去调用该对象的__len__() 方法，所以，下面的代码是等价的：\n&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3\n如果试图获取不存在的属性，会抛出 AttributeError 的错误\n动态绑定属性和方法\n定义了一个 class，或者创建了一个 class 的实例后，我们可以给该类或实例绑定任何属性和方法，这就是动态语言的灵活性。如下面的例子\n先定义一个类 &gt;&gt;&gt; class Student(object):...     pass... 然后，尝试给实例绑定一个属性：\n&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = 'hello' # 动态给实例绑定一个属性&gt;&gt;&gt; print s.namehello\n如果要限定能够绑定的属性，可以在原来的类中添加__slots__变量，变量的内容设为能够动态绑定的属性即可。\n隐藏 getter 和 setter 为类的属性\n通过装饰器 @property 可以隐藏类对某个属性的 get 方法和 set 方法，见下面的例子\nclass Student(object):    @property    def birth(self):        return self._birth    @birth.setter    def birth(self, value):        self._birth = value    @property    def age(self):        return 2014 - self._birth\n把一个getter方法变成属性，只需要加上 @property 就可以了，此时，@property 本身又创建了另一个装饰器@birth.setter，这个装饰器负责把一个setter方法变成属性赋值，并且在这个方法内可以限制复制的的范围等。\n调用方法如下所示 &gt;&gt;&gt;s = Student()&gt;&gt;&gt;s.birth = 2001 &gt;&gt;&gt;print s.birth,s.age2001 13 上面的 s.birth = 2001\n实际上是执行了装饰器 @birth.setter 装饰的birth方法，因此可在这个方法内加上赋值的限制条件,过滤不合法的赋值。\n也可以将一个属性定义为只读属性，只定义 getter 方法即可。如上面的 age 方法。\n类的一些内部函数\n__str__\n该函数是在直接打印对象时输出的内容，如下例子所示 class Student(object):    passs = Student()print s\n输出内容为 &lt;__main__.Student object at 0x02124DF0&gt;,表示对象在内存中的地址，可以重写这个函数的输出，见下面的例子\nclass Student(object):    def __str__(self):    \treturn 'object student's = Student()print s 再次执行的时候会输出 object student。\n__repr__\n该函数与__str__函数很类似，只是在直接显示变量调用的不是__str__()，而是__repr__()，两者的作用的区别是__str__() 返回用户看到的字符串，而__repr__() 返回程序开发者看到的字符串，也就是说，__repr__() 是为调试服务。\n`&gt;&gt;&gt; class Student(object):    def __str__(self):    \treturn 'object student'... ... ... &gt;&gt;&gt; s=Student()&gt;&gt;&gt; s&lt;__main__.Student object at 0x02859930&gt;\n__iter__\n如果一个类想被用于 for ... in 循环，类似 list 或 tuple 那样，就必须实现一个__iter__() 方法，该方法返回一个迭代对象，然后，Python 的 for 循环就会不断调用该迭代对象的 next () 方法拿到循环的下一个值，直到遇到 StopIteration 错误时退出循环。\n以斐波那契数列为例，写一个 Fib 类，可以作用于 for 循环： class Fib(object):    def __init__(self):        self.a, self.b = 0, 1 # 初始化两个计数器a，b    def __iter__(self):        return self # 实例本身就是迭代对象，故返回自己    def next(self):        self.a, self.b = self.b, self.a + self.b # 计算下一个值        if self.a &gt; 100000: # 退出循环的条件            raise StopIteration();        return self.a # 返回下一个值\n把Fib实例作用于for循环： &gt;&gt;&gt; for n in Fib():...     print n...11235...4636875025\n__getitem__\nFib 实例虽然能作用于 for 循环，看起来和 list 有点像，但是，把它当成 list 来使用还是不行，比如，取第 5 个元素：\n&gt;&gt;&gt; Fib()[5]Traceback (most recent call last):  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'Fib' object does not support indexing\n要表现得像list那样按照下标取出元素，需要实现__getitem__()方法：\nclass Fib(object):    def __getitem__(self, n):        a, b = 1, 1        for x in range(n):            a, b = b, a + b        return a```        现在，就可以按下标访问数列的任意一项了：```py&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0]1&gt;&gt;&gt; f[1]1&gt;&gt;&gt; f[2]2&gt;&gt;&gt; f[3]3\n__call__\n一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用 instance.method () 来调用。能不能直接在实例本身上调用呢？类似 instance ()？在 Python 中，答案是肯定的。\n任何类，只需要定义一个__call__() 方法，就可以直接对实例进行调用。请看示例：\nclass Student(object):    def __init__(self, name):        self.name = name    def __call__(self):        print('My name is %s.' % self.name)\n调用方式如下： &gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s()My name is Michael.\n__call__() 还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样\n判断一个对象是否能被调用，能被调用的对象就是一个 Callable 对象，比如函数和我们上面定义的带有__call ()__的类实例：\n&gt;&gt;&gt; callable(Student())True&gt;&gt;&gt; callable(max)True&gt;&gt;&gt; callable([1, 2, 3])False&gt;&gt;&gt; callable(None)False&gt;&gt;&gt; callable('string')False\n通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。\n错误、调试和测试\n概念\n常用的调试结构\ntry....except...else # 没有捕捉到exception时执行该语句....finally\nPython 所有的错误都是从 BaseException 类派生的，常见的错误类型和继承关系看这里：\nhttps://docs.python.org/2/library/exceptions.html#exception-hierarchy\nlogging 模块可以把错误记录到日志文件里，方便事后排查\n抛出错误\n因为错误是 class，捕获一个错误就是捕获到该 class 的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。Python 的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。\n如果要抛出错误，首先根据需要，可以定义一个错误的 class，选择好继承关系，然后，用 raise 语句抛出一个错误的实例：\nclass FooError(StandardError):    passdef foo(s):    n = int(s)    if n==0:        raise FooError('invalid value: %s' % s)    return 10 / n\n只有在必要的时候才定义我们自己的错误类型。如果可以选择 Python 已有的内置的错误类型（比如 ValueError，TypeError），尽量使用 Python 内置的错误类型。\n另一种错误处理的方式：\ndef foo(s):    n = int(s)    return 10 / ndef bar(s):    try:        return foo(s) * 2    except StandardError, e:        print 'Error!'        raisedef main():    bar('0')main()\n在 bar () 函数中，我们明明已经捕获了错误，但是，打印一个 Error! 后，又把错误通过 raise 语句抛出去了，这不有病么？\n其实这种错误处理方式不但没病，而且相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。\nraise 语句如果不带参数，就会把当前错误原样抛出。此外，在 except 中 raise 一个 Error，还可以把一种类型的错误转化成另一种类型：\ntry:    10 / 0except ZeroDivisionError:    raise ValueError('input error!')\n只要是合理的转换逻辑就可以，但是，决不应该把一个 IOError 转换成毫不相干的 ValueError。\n调试\nprint、assert 语句\n最基础的调试就是通过 print 语句打印出变量的值，但是这样每次调试后都要注释或删除 print 语句。\n因此也可使用 assert 语句，该语句的结构为 assert condition,'message', 只有当 condition 为 False 时，才会抛出一个 AssertionError 并打印出 message\nlogging 模块\n和 assert 比，logging 不会抛出错误，而且可以输出到文件。并且可以指定输出的信息的级别，包括有 debug，info，warning，error 等几个级别\nimport logginglogging.basicConfig(level=logging.INFO)s = '0'n = int(s)logging.info('n = %d' % n)print 10 / n\n上面的 logging.basicConfig 就是设置输出的日志的等级，logging.info 为输出的内容。\n输出的内容如下： INFO:root:n = 0Traceback (most recent call last):  File \"XX.py\", line X, in &lt;module&gt;    print 10 / nZeroDivisionError: integer division or modulo by zero\npdb\npdb (Python\nDebugger) 是 Python 的调试器，可以让程序以单步方式运行，并随时查看运行状态。\n通过 python -m pdb XXX.py 可以启动调试器调试 XXX.py，n 命令执行当前代码并转到下一行，p 变量名打印出具体的变量，q 命令退出调试程序。\n除了上面的使用方法，还可以在可能出错的地方放一个 pdb.set_trace()，相当于设置一个断点。运行代码时，程序会自动在 pdb.set_trace() 暂停并进入 pdb 调试环境，可以用命令 p 查看变量，或者用命令 c 继续运行\n多进程和多线程\n多进程\npython 提供的跨平台多进程模块为 multiprocessing,\n使用的方式如下： from multiprocessing import Processdef target_func(arg1,arg2):    ....p1 = Process(target=target_func,args=(arg1,arg2))p2 = Process()p1.start()p1.join()\n上面启动了一个进程，并执行任务 target_func,注意同时执行任务的最大进程数等于该机器的核数。\n更详细的用法参考这篇文章\n### 多线程\npython 提供的多线程模块为 thread 模块和 threading 模块，后者是高级模块，除了封装了前者还封装了很多其他方法。\n一般的使用有两种：1）继承 threading.Thread 构造自己的线程类。2）类似多进程将需要执行的任务作为参数构造线程。\n详细的语法可参考这篇文章。需要注意的是多线程同时修改进程中的公共变量时记得加线程锁。\nThreadLocal\n在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。\n但是局部变量也有问题，就是在函数调用的时候必须要通过参数传递。如下面的例子：\nimport threadingdef process_student(name):    print 'Hello, %s (in %s)' % (name, threading.current_thread().name)def process_thread(name):    process_student(name)t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join()\n当参数多了的时候，这样一层层传下去就会显得比较麻烦。因此引入了ThreadLocal的概念，可将上面的代码改写成如下的样式实现相同的功能。\nimport threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student():    print 'Hello, %s (in %s)' % (local_school.student, threading.current_thread().name)def process_thread(name):    # 绑定ThreadLocal的student:    local_school.student = name    process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join()\n全局变量local_school 就是一个 ThreadLocal 对象，每个 Thread 对它都可以读写 student 属性，但互不影响。你可以把 local_school 看成全局变量，但每个属性如 local_school.student 都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal 内部会处理。\n可以理解为全局变量 local_school 是一个 dict，不但可以用 local_school.student，还可以绑定其他变量，如 local_school.teacher 等等。\nThreadLocal 最常用的地方就是为每个线程绑定一个数据库连接，HTTP 请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。\n常用内建模块\ncollections\ncollections 提供了许多有用的集合类\n\nnamedtuple\nnamedtuple 是一个函数，它用来创建一个自定义的 tuple 对象，并且规定了 tuple 元素的个数，并可以用属性而不是索引来引用 tuple 的某个元素 \n&gt;&gt;&gt;from collections import namedtuple&gt;&gt;&gt;Coordinate = namedtuple(\"corr\",['x','y'])&gt;&gt;&gt;c = Coordinate(1,2)&gt;&gt;&gt;c.x1&gt;&gt;&gt;c.y2\ndeque\ndeque 是为了高效实现插入和删除操作的双向列表，适合用于队列和栈\n&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x'])\ndefaultdict\n使用 dict 时，如果引用的 Key 不存在，就会抛出 KeyError。如果希望 key 不存在时，返回一个默认值，就可以用 defaultdict：\n&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A'\n注意默认值是调用函数返回的，而函数在创建 defaultdict 对象时传入。\n\n除了在 Key 不存在时返回默认值，defaultdict 的其他行为跟 dict 是完全一样的。\n\nOrderedDict\n使用 dict 时，Key 是无序的。在对 dict 做迭代时，我们无法确定 Key 的顺序。如果要保持 Key 的顺序，可以用 OrderedDict：\n&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的{'a': 1, 'c': 3, 'b': 2}&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)])\nCounter\nCounter 是一个简单的计数器，例如，统计字符出现的个数： &gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':...     c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter({'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1})\nCounter 实际上也是 dict 的一个子类\n\nbase64\nBase64 是一种用 64 个字符来表示任意二进制数据的方法。\n首先要理解的问题就是为什么要用字符来表是二进制的数据。维基百科的解释如下：\n&gt;A binary-to-text encoding is encoding of data in plain text. More\nprecisely, it is an encoding of binary data in a sequence of characters.\nThese encodings are necessary for transmission of data when the channel\ndoes not allow binary data, such as when one might attach an image file\nto an e-mail message, to accomplish this, the data is encoded in some\nway, such that eight-bit data is encoded into seven-bit ASCII\ncharacters\n大意就是在数据传输时，某些协议或系统只支持字符的传输（如 email），因此如果需要传输二进制的数据，就要将二进制数据转为字符格式。而 Base64 是一种最常见的二进制编码方法。\nBase64 的原理很简单，首先，准备一个包含 64 个字符的数组：\n如 ['A', 'B', 'C', ... 'a', 'b', 'c', ... '0', '1', ... '+', '/'] 然后对二进制数据进行处理，每 3 个字节一组，一共是 3x8=24bit，划为 4 组，每组正好 6 个 bit,\n计算 6 个 bit 表示的数字大小（范围在 0~63）之间，然后查上面的表，这样我们得到 4 个数字作为索引，然后查表，获得相应的 4 个字符，就是编码后的字符串。\n因此，Base64 编码会把 3 字节的二进制数据编码为 4 个字符的文本数据\n如果要编码的二进制数据的字节数不是 3 的倍数，最后会剩下 1 个或 2 个字节怎么办？Base64 用 00 字节在二进制数据末尾补足后，再在编码的末尾加上 1 个或 2 个 = 号，表示补了一个会两个字节，解码的时候，会自动去掉。\n例子如下所示：\n&gt;&gt;&gt; import base64 as b64&gt;&gt;&gt; b64.b64encode(',')'LA=='&gt;&gt;&gt; b64.b64encode('l,')'bCw='&gt;&gt;&gt; b64.b64encode('ll,')'bGws'&gt;&gt;&gt;b64.b64decode('LA==')','\n由于标准的 Base64 编码后可能出现字符 + 和 /，在 URL 中就不能直接作为参数，所以又有一种 \"url\nsafe\" 的 base64 编码，其实就是把字符 + 和 / 分别变成 - 和_：\n&gt;&gt;&gt; base64.b64encode('i\\xb7\\x1d\\xfb\\xef\\xff')'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode('i\\xb7\\x1d\\xfb\\xef\\xff')'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')'i\\xb7\\x1d\\xfb\\xef\\xff'\nBase64 适用于小段内容的编码，比如数字证书签名、Cookie 的内容等。\n由于 = 字符也可能出现在 Base64 编码中，但 = 用在 URL、Cookie 里面会造成歧义，所以，很多 Base64 编码后会把 = 去掉 , 如下所示：\n# 标准Base64:'abcd' -&gt; 'YWJjZA=='# 自动去掉=:'abcd' -&gt; 'YWJjZA'\n去掉 = 后怎么解码呢？因为 Base64 是把 3 个字节变为 4 个字节，所以，Base64 编码的长度永远是 4 的倍数，因此，需要加上 = 把 Base64 字符串的长度变为 4 的倍数，就可以正常解码了。如下面的例子\n&gt;&gt;&gt; base64.b64decode('YWJjZA==')'abcd'&gt;&gt;&gt; base64.b64decode('YWJjZA')Traceback (most recent call last):  ...TypeError: Incorrect padding&gt;&gt;&gt; def safe_b64decode(s):...     return base64.b64decode(s+'='*(4-len(s)%4))...&gt;&gt;&gt; safe_b64decode('YWJjZA')'abcd'\nstruct\npython 中的 struct 模块的主要作用就是对 python 基本类型值与用 python 字符串格式表示的 C 语言中 struct 类型间的转化（This module\nperforms conversions between Python values and C structs represented as\nPython\nstrings.）。stuct 模块提供了很简单的几个函数，下面写几个例子。\nstruct 提供用 format specifier 方式对数据进行打包和解包（Packing and\nUnpacking）。例如:\nimport structimport binasciivalues = (1, 'abc', 2.7)s = struct.Struct('I3sf')packed_data = s.pack(*values)unpacked_data = s.unpack(packed_data)print 'Original values:', valuesprint 'Format string :', s.formatprint 'Uses :', s.size, 'bytes'print 'Packed Value :', binascii.hexlify(packed_data)print 'Unpacked Type :', type(unpacked_data), ' Value:', unpacked_data\n输出： Original values: (1, 'abc', 2.7) Format string : I3sf Uses : 12 bytes Packed Value : 0100000061626300cdcc2c40 Unpacked Type : &lt;type 'tuple'&gt;  Value: (1, 'abc', 2.700000047683716)\n代码中，首先定义了一个元组数据，包含 int、string、float 三种数据类型，然后定义了 struct 对象，并制定了 format‘I3sf’，I\n表示 int，3s 表示三个字符长度的字符串，f 表示\nfloat。最后通过 struct 的 pack 和 unpack 进行打包和解包。通过输出结果可以发现，value 被 pack 之后，转化为了一段二进制字节串，而 unpack 可以把该字节串再转换回一个元组. 但是值得注意的是对于 float 的精度发生了改变，这是由一些比如操作系统等客观因素所决定的。打包之后的数据所占用的字节数与 C 语言中的 struct 十分相似。\n关于 struct 的更多的具体用法可参考\nhttps://docs.python.org/2/library/struct.html\nhttp://www.cnblogs.com/coser/archive/2011/12/17/2291160.html\nXML\n操作 XML 有两种方法：DOM 和 SAX。DOM 会把整个 XML 读入内存，解析为树，因此占用内存大，解析慢，优点是可以任意遍历树的节点。SAX 是流模式，边读边解析，占用内存小，解析快，缺点是我们需要自己处理事件。\nSAX 只允许读 XML，而 DOM 则允许对 XML 文件进行读写操作。在只读的情况下，优先考虑 SAX，因为 DOM 实在太占内存。\n除了 python 自带的 xml 包可用于处理 XML 文件，第三发库如 lxml 也可以被用来处理 XML 文件。\npython 自带的 xml 包具体使用的实例代码可参考：\nhttp://www.tutorialspoint.com/python/python_xml_processing.htm\n参考：\nhttp://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n","categories":["python","语法"],"tags":["python"]},{"title":"python 语法杂记 -- 装饰器，类的特殊方法，常量类","url":"/2018/12/15/python%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0--%E8%A3%85%E9%A5%B0%E5%99%A8%EF%BC%8C%E7%B1%BB%E7%9A%84%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B8%B8%E9%87%8F%E7%B1%BB/","content":"最近在看 python 一些语法知识，虽然 python\n代码写了不少，但是对于一些高级语法的了解还不够深入；因此本文主要记录了一些比较生疏的知识点，主要包括了装饰器，类的特殊方法，常量类这三个方面的知识。\n\n装饰器\n装饰器本质上是一个高阶函数，以被装饰的函数为参数，并返回一个包装后的函数给被装饰函数。\n装饰器的一般使用形式如下： @decoratordef func():    pass\n等价于下面的形式： def func():    passfunc = decorator(func)\n装饰器可以定义多个，离函数定义最近的装饰器先被调用，比如：\n@decorator_one@decorator_twodef func():    pass\n等价于：\ndef func():    passfunc = decorator_one(decorator_two(func))\n对带参数的函数进行装饰\n对带参数的函数进行装饰这个需求很常见，简单来说，装饰带参数的函数时，需要将参数传递给装饰器内部需要返回的函数 (也叫内嵌包装函数)，也就是说内嵌包装函数的参数跟被装饰函数的参数对应，如下所示\ndef makeitalic(func):    def wrapped(*args, **kwargs):        ret = func(*args, **kwargs)        return '&lt;i&gt;' + ret + '&lt;/i&gt;'    return wrapped@makeitalicdef hello(name):    return 'hello %s' % name@makeitalicdef hello2(name1, name2):    return 'hello %s, %s' % (name1, name2)\n可以看到，装饰器内部需要返回的函数 wrapped 带上了参数\n(*args, **kwargs), 目的是为了适应可变参数。使用如下\n&gt;&gt;&gt; hello('python')'&lt;i&gt;hello python&lt;/i&gt;'&gt;&gt;&gt; hello2('python', 'java')'&lt;i&gt;hello python, java&lt;/i&gt;'\n带参数的装饰器\n上面的例子，我们增强了函数 hello 的功能，给它的返回加上了标签\n&lt;i&gt;...&lt;/i&gt;，现在，我们想改用标签\n&lt;b&gt;...&lt;/b&gt; 或\n&lt;p&gt;...&lt;/p&gt;。是不是要像前面一样，再定义一个类似\nmakeitalic\n的装饰器呢？其实，我们可以可以使用带参数的装饰器，简单来说，就是在原来的装饰器基础上再封装一层函数，将标签作为参数，返回一个装饰器。\ndef wrap_in_tag(tag):    def decorator(func):        def wrapped(*args, **kwargs):            ret = func(*args, **kwargs)            return '&lt;' + tag + '&gt;' + ret + '&lt;/' + tag + '&gt;'        return wrapped    return decorator\n现在，我们可以根据需要生成想要的装饰器了：\nmakebold = wrap_in_tag('b')  # 根据 'b' 返回 makebold 生成器`@makebolddef hello(name):    return 'hello %s' % name&gt;&gt;&gt; hello('world')'&lt;b&gt;hello world&lt;/b&gt;'\n上面的形式也可以写得更加简洁： @wrap_in_tag('b')def hello(name):    return 'hello %s' % name\n这就是带参数的装饰器，其实就是在装饰器外面多了一层包装，根据不同的参数返回不同的装饰器。\n私有成员\npython 不像 C++ 有 private\n之类的关键字，但是 可以在属性或方法的名称前面加上两个下划线\n__, 来限制用户访问对象的属性或方法。\nIn [1]: class Animal(object):   ...:     def __init__(self, name):   ...:         self.__name = name   ...:     def greet(self):   ...:         print ('Hello, I am %s.' % self.__name)   ...:In [2]: a = Animal(\"dog\")In [3]: a.__name---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)&lt;ipython-input-5-5d5520ef9fe0&gt; in &lt;module&gt;()----&gt; 1 a.__nameAttributeError: 'Animal' object has no attribute '__name'In [4]: a.greet()Hello, I am dog.\n类方法 vs 静态方法\npython\n中的类有两个特殊的方法：类方法和静态方法，两个方法主要有以下特点\n\n两个方法均是属于类而不是属于对象的\n两个方法都是通过内置的装饰器定义（@classmethod 和\n@staticmethod）\n类方法可以访问类属性，静态方法则不能\n\n如下是类方法的一个例子\nclass A(object):    bar = 1    @classmethod    def class_foo(cls):        print 'Hello, ', cls        print cls.bar&gt;&gt;&gt; A.class_foo()   # 直接通过类来调用方法Hello,  &lt;class '__main__.A'&gt;1\n在上面，我们使用了 classmethod 装饰方法\nclass_foo，它就变成了一个类方法，class_foo\n的参数是 cls，代表类本身，当我们使用\nA.class_foo() 时，cls 就会接收 A 作为参数。另外，被\nclassmethod 装饰的方法由于持有 cls\n参数，因此我们可以在方法里面调用类的属性、方法，比如 cls.bar\n上面的类方法是可以修改类的属性的，静态方法定义方式类似，但是不会改变类和实例状态；如下所示，静态方法没有\nself 和 cls\n参数，因此没法改变类的属性，可以把它看成是一个普通的函数，甚至可以把它写到类外面，但是有时候，类就是需要这么一类方法，如果写到外面，一是不利于类的完整性，二是不利于命名空间的整洁性。\nclass A(object):    @staticmethod    def static_foo():        print 'Hello'&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.static_foo()Hello&gt;&gt;&gt; A.static_foo()Hello\n那么，这两个方法该在什么时候使用呢？参考 class\nmethod vs static method in Python 如下\n\nWe generally use class method to create factory methods. Factory\nmethods return class object ( similar to a constructor ) for different\nuse cases. We generally use static methods to create utility\nfunctions.\n\n如下是个比较形象的例子\n# Python program to demonstrate # use of class method and static method. from datetime import date class Person: \tdef __init__(self, name, age): \t\tself.name = name \t\tself.age = age \t# a class method to create a Person object by birth year. \t@classmethod\tdef fromBirthYear(cls, name, year): \t\treturn cls(name, date.today().year - year) \t# a static method to check if a Person is adult or not. \t@staticmethod\tdef isAdult(age): \t\treturn age &gt; 18person1 = Person('mayank', 21) person2 = Person.fromBirthYear('mayank', 1996) print person1.age print person2.age # print the result print Person.isAdult(22) \n魔法方法\n以双下划线 __ 包裹起来的方法，比如最常见的\n__init__，这些方法被称为魔法方法（magic\nmethod）或特殊方法（special method）, 这些方法可以给 Python\n的类提供特殊功能，方便我们定制一个类。\n__new__\n在 Python 中，当我们创建一个类的实例时，类会先调用\n__new__(cls[, ...]) 来创建并返回实例，然后\n__init__\n方法再对该实例（self）中的变量进行初始化。\n关于 __new__ 和 __init__\n有以下几点需要注意：\n\n__new__ 是在 __init__ 之前被调用的\n__new__ 是类方法，__init__ 是实例方法\n重载 __new__ 方法，需要返回类的实例\n\n一般情况下，我们不需要重载 __new__\n方法。但在某些情况下，我们想控制实例的创建过程，这时可以通过重载\n__new__ 方法来实现。 比如说，下面的例子通过了\n__new__ 来实现单例模式\nclass Singleton(object):    _instance = None    def __new__(cls, *args, **kw):        if not cls._instance:            cls._instance = super(Singleton, cls).__new__(cls, *args, **kw)          return cls._instance  class MyClass(Singleton):      a = 1\n__str__ &amp;\n__repr__\n这两个方法主要是在直接打印类时候调用的，通过下面两个例子可以比较直观地看到如何使用\nclass Foo(object):    def __init__(self, name):        self.name = name    def __str__(self):        return 'Foo object (name: %s)' % self.name&gt;&gt;&gt; print Foo('ethan')      # 使用 printFoo object (name: ethan)&gt;&gt;&gt;&gt;&gt;&gt; str(Foo('ethan'))       # 使用 str'Foo object (name: ethan)'&gt;&gt;&gt;&gt;&gt;&gt; Foo('ethan')             # 直接显示&lt;__main__.Foo at 0x10c37a490&gt;\n可以看到，使用 print 和 str 输出的是 __str__\n方法返回的内容，但如果直接显示则不能，因为这个是\n__repr__ 方法负责的，如下：\nclass Foo(object):    def __init__(self, name):        self.name = name    def __repr__(self):        return 'Foo object (name: %s)' % self.name&gt;&gt;&gt; Foo('ethan')'Foo object (name: ethan)'\n__iter__\n在某些情况下，我们希望实例对象可被用于 for...in\n循环，这时我们需要在类中定义 __iter__ 和\nnext（在 Python3 中是\n__next__）方法，其中，__iter__\n返回一个迭代对象，next\n返回容器的下一个元素，在没有后续元素时抛出\nStopIteration 异常\n如下是一个斐波那契数列的例子：\nclass Fib(object):    def __init__(self):        self.a, self.b = 0, 1    def __iter__(self):  # 返回迭代器对象本身        return self          def next(self):      # 返回容器下一个元素        self.a, self.b = self.b, self.a + self.b        return self.a    &gt;&gt;&gt; fib = Fib()&gt;&gt;&gt; for i in fib:...     if i &gt; 10:...         break...     print i...112358\n__getitem__\n&amp; __setitem__ &amp; __delitem__\n有时，我们希望可以使用 obj[n]\n这种方式对实例对象进行取值，比如对斐波那契数列，我们希望可以取出其中的某一项，这时我们需要在类中实现\n__getitem__ 方法，比如下面的例子：\nclass Fib(object):    def __getitem__(self, n):        a, b = 1, 1        for x in xrange(n):            a, b = b, a + b        return a&gt;&gt;&gt; fib = Fib()&gt;&gt;&gt; fib[0], fib[1], fib[2], fib[3], fib[4], fib[5](1, 1, 2, 3, 5, 8)\n类似地，__setitem__ 用于设置值，__delitem__\n用于删除值，让我们看下面一个例子：\nclass Point(object):    def __init__(self):        self.coordinate = {}    def __getitem__(self, key):        return self.coordinate.get(key)    def __setitem__(self, key, value):        self.coordinate[key] = value    def __delitem__(self, key):        del self.coordinate[key]        print 'delete %s' % key    def __len__(self):        return len(self.coordinate)\n在上面，我们定义了一个 Point 类，它有一个属性\ncoordinate（坐标），是一个字典，让我们看看使用：\n&gt;&gt;&gt; p = Point()&gt;&gt;&gt; p['x'] = 2    # 对应于 p.__setitem__('x', 2)&gt;&gt;&gt; p['y'] = 5    # 对应于 p.__setitem__('y', 5)&gt;&gt;&gt; len(p)        # 对应于 p.__len__2&gt;&gt;&gt; p['x']        # 对应于 p.__getitem__('x')2&gt;&gt;&gt; del p['x']    # 对应于 p.__delitem__('x')&gt;&gt;&gt; len(p)1\n__call__\n我们一般使用 obj.method ()\n来调用对象的方法，那能不能直接在实例本身上调用呢？在 Python\n中，只要我们在类中定义 __call__\n方法，就可以对实例进行调用，比如下面的例子：\nclass Point(object):    def __init__(self, x, y):        self.x, self.y = x, y    def __call__(self, z):        return self.x + self.y + z\n使用如下：\n&gt;&gt;&gt; p = Point(3, 4)&gt;&gt;&gt; callable(p)     # 使用 callable 判断对象是否能被调用True&gt;&gt;&gt; p(6)            # 传入参数，对实例进行调用，对应 p.__call__(6)13                  # 3+4+6\n__slots__\n__slots__\n跟前面的方法不太一样，因为这是一个类的属性，当我们创建了一个类的实例后，我们还可以给该实例绑定任意新的属性和方法，如下\nclass Point(object):        def __init__(self, x=0, y=0):        self.x = x        self.y = y&gt;&gt;&gt; p = Point(3, 4)&gt;&gt;&gt; p.z = 5    # 绑定了一个新的属性&gt;&gt;&gt; p.z5&gt;&gt;&gt; p.__dict__{'x': 3, 'y': 4, 'z': 5}\n这样其实是违背了 OOP\n的封装性的理念，而且会消耗更多的内存，为了禁止这一属性，可以使用\n__slots__ 来告诉 Python\n只给一个固定集合的属性分配空间，对上面的代码做一点改进，如下：\nclass Point(object):    __slots__ = ('x', 'y')       # 只允许使用 x 和 y    def __init__(self, x=0, y=0):        self.x = x        self.y = y\n我们给 __slots__\n设置了一个元组，来限制类能添加的属性。现在，如果想绑定一个新的属性，就会出错了\n常量类\n在 Python 中使用常量一般来说有以下两种方式：\n\n通过命名风格来提醒使用者该变量代表的意义为常量，如常量名所有字母大写，用下划线连接各个单词，PEP8\n给出的编程风格就是这样的\n通过自定义的类实现常量功能。这要求符合命名全部为大写和值一旦绑定便不可再修改\n这两个条件。下面是一种较为常见的解决办法，将常量放到同一个文件中 \n\n# FileName：constant.pyclass _const:    class ConstError(TypeError):        pass    class ConstCaseError(ConstError):        pass    def __setattr__(self, name, value):        if name in self.__dict__:            raise self.ConstError, \"Can't change const value!\"        if not name.isupper():            raise self.ConstCaseError, 'const \"%s\" is not all letters are capitalized' %name        self.__dict__[name] = valueimport syssys.modules[__name__] = _const()import constantconstant.MAX_COUNT = 10constant.JOBS = 5constant.PROCESSES = 8\n简单解释一下，对象的所有属性及属性的值都存储在 __dict__\n中， 上面的 __setattr__\n方法在对象每次创建新常量的时候会判断常量是否已经被定义过，如果已经定义过则\nraise error，从而确保了已经创建的常量不可修改。\nsys.modules[__name__] = _const() 则确保了当上面的文件被\nimport 时，其 module 名称 (也就是 __name__\n的值，当文件被运行时 __name__ 的值为 __main__,\n被 import 时 __name__ 的值则是 module 名称) 对应的是一个\n_const()\n对象，从而可以直接通过其创建常量。因此，使用的方法如下\nimport constantprint(constant.MAX_COUNT)\n\n参考：\nclass\nmethod vs static method in Python Data model\nPython 之旅\n","categories":["python","语法"],"tags":["python"]},{"title":"python 中的模块与包","url":"/2015/12/19/python%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97%E4%B8%8E%E5%8C%85/","content":"文章为转载，原文见这里，侵删\npython 中的 Module 是比较重要的概念。常见的情况是，事先写好一个.py 文\n件，在另一个文件中需要 import 时，将事先写好的.py 文件拷贝\n到当前目录，或者是在 sys.path 中增加事先写好的.py 文件所在的目录，然后 import。这样的做法，对于少数文件是可行的，但如果程序数目很\n多，层级很复杂，就很吃力了。\n有没有办法，像 Java 的 Package 一样，将多个.py 文件组织起来，以便在外部统一调用，和在内部互相调用呢？答案是有的。\n\n主要是用到 python 的包的概念，python\n__init__.py 在包里起一个比较重要的作用\n要弄明白这个问题，首先要知道，python 在执行 import 语句时，到底进行了什么操作，按照 python 的文档，它执行了如下操作：\n第 1 步，创建一个新的，空的 module 对象（它可能包含多个 module）；\n第 2 步，把这个 module 对象插入 sys.module 中\n第 3 步，装载 module 的代码（如果需要，首先必须编译）\n第 4 步，执行新的 module 中对应的代码。\n在执行第 3 步时，首先要找到 module 程序所在的位置，搜索的顺序是：当前路径\n（以及从当前目录指定的 sys.path）-&gt; 然后是 PYTHONPATH-&gt; 然后是 python 的安装设置相关的默认路径。\n正因为存在这样的顺序，如果当前\n路径或 PYTHONPATH 中存在与标准 module 同样的 module，则会覆盖标准 module。也就是说，如果当前目录下存在 xml.py，那么执\n行 import xml 时，导入的是当前目录下的 module，而不是系统标准的 xml。\n了解了这些，我们就可以先构建一个 package，以普通 module 的方式导入，就可以直接访问此 package 中的各个 module 了。\nPython 中的 package 定义很简单，其层次结构与程序所在目录的层次结构相同，这一点与 Java 类似，唯一不同的地方在于，python 中的 package 必须包含一个__init__.py 的文件。\n例如，我们可以这样组织一个 package:\npackage1/      __init__.py      subPack1/          __init__.py          module_11.py          module_12.py          module_13.py      subPack2/          __init__.py          module_21.py          module_22.py      ……  \n__init__.py 可以为空，但是必须要存在，只要它存在，就表明此目录应被作为一个 package 处理。当然，init.py 中也可以设置相应的内容，下文详细介绍。\n好了，现在我们在 module_11.py 中定义一个函数：\ndef funA():      print \"funcA in module_11\"      return  \n在顶层目录（也就是 package1 所在的目录，当然也参考上面的介绍，将 package1 放在解释器能够搜索到的地方）运行 python:\n&gt;&gt;&gt;from package1.subPack1.module_11 import funcA  &gt;&gt;&gt;funcA()  funcA in module_11  ```  这样，我们就按照package的层次关系，正确调用了module_11中的函数。有时在import语句中会出现通配符\\*，**导入某个module中的所有元素**，这是怎么实现的呢？答案就在__init__.py中。我们在subPack1的__init__.py文件中写`__all__ = ['module_13', 'module_12']`然后进入python  ```py  &gt;&gt;&gt;from package1.subPack1 import *  &gt;&gt;&gt;module_11.funcA()  Traceback (most recent call last):    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;  ImportError: No module named module_11  \n也就是说，** 以 * 导入时，package 内的 module 是受__init__.py 中的__all__列表限制的 **。\n为了避免 import 后面跟的层级过长，可以在__init__.py 中先导入所需的 module。比如上面的例子可以改为下面所示\n#package1的 __init__.py  from subPack1 import *  __all__=['module_11','module_13']&gt;&gt;&gt;from package1 import *  &gt;&gt;&gt;module_11.funcA()  funcA in module_11  &gt;&gt;&gt;module_12.funcA()  Traceback (most recent call last):    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;  ImportError: No module named module_12  \n下面看一下 package 内部互相调用\n如果希望调用同一个 package 中的 module，则直接 import 即可。也就是说，在 module_12.py 中，可以直接使用\nimport module_11\n如果不在同一个 package 中，例如我们希望在 module_21.py 中调用 module_11.py 中的 FuncA，则应该这样：\nfrom module_11的包名.module_11 import funcA\n","categories":["python","语法"],"tags":["转载","python"]},{"title":"python 中的多线程","url":"/2016/03/26/python%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B/","content":"本文主要讲述了 python 中多线程的使用、线程锁以及多线程在 python 中是否能够提高效率。\n多线程的概念\n进程的相信大家都听说过，而线程可以理解为比进程更小一级的概念，一个进程内至少有一个线程，如果有多个线程，那么他们就共享进程的资源，共同完成进程的任务。\n使用多线程一般有两个不同的目的：\n一是把程序细分成几个功能相对独立的模块，防止其中一个功能模块阻塞导致整个程序假死（GUI 程序是典型）\n另一个就是提高运行效率，比如多个核同时跑，或者单核里面，某个线程进行 IO 操作时，另一个线程可以同时执行。具体可以参考这篇文章\n\n相比进程，线程有以下优点\n\n创建和销毁的代价比进程要小得多，尤其是在 windows 下，可以参考这个回答。而且线程间彼此切换所需的时间也远远小于进程间切换所需要的时间\n\n线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行。而由于同一进程下的线程之间共享数据空间，降低了通信的开销。\n\n除了优点，\n线程间方便的通信机制源于线程间数据的共享，同时也带来了其他问题，如需要保护变量不能同时被两个线程所修改，这也需要一定的开销，而且需要开发者处理好这个调度。\npython 中的多线程\npython 中提供了两个模块实现多线程，分别是 thread 和 threading，thread 是比较低级的模块，而 threading 在其基础上封装了其他许多高级特性，故本文主要讲述 threading 模块的使用，若要了解 thread 模块的使用，请参考官方文档。\n创建进程有两种方式，分别是继承 threading.Thread 类创建自己的线程子类和将需要线程执行的函数传入线程构造函数中。下面分别讲述\n继承 threading.Thread 类\n继承 threading.Thread 类只能重写（override）__init__函数和 run() 函数，__init__函数就是构造函数，run() 函数就是创建线程后线程需要执行的任务。下面是一个简单的 demo\n# encoding:utf-8  import threading  import time  import random  class sleepThread(threading.Thread):      def __init__(self):          threading.Thread.__init__(self)          print self.name+ ' is created!'    def run(self):          randomTime = random.randint(1,9) # 生成 1~9的随机整数          time.sleep(randomTime)          print self.name+ ' slept for '+str(randomTime)+' seconds' if __name__ == '__main__':      threads = []      for i in range(5):  # 创建5个进程          th = sleepThread()          threads.append(th)          th.start()    for t in threads:          t.join()     print 'all threads finished'\n在上面的例子中，我们编写了自己的线程类 sleepThread, 然后创建了 5 个线程，用 start() 启动了各个线程，start() 实际上是执行了线程类的 run() 函数。这时输出如下所示：\n\n其中，默认线程的名称是 Thread-i，i 就是创建的第 i 个线程。join() 函数的作用是等待线程执行完成再执行下面任务，实际的应用场景比如说进程要合并多个线程的处理结果，那么这时候 join() 函数就必不可少了。假如没有 join() 函数，即主函数改成下面的样子。\nif __name__ == '__main__':    for i in range(5):  # 创建5个进程          th = sleepThread()          th.start()      print 'all threads finished'  ```  那么输出就像下面所示：![][5]那为什么不在`thread.start()`后执行join()呢？即主函数改成以下样子。  ```pyif __name__ == '__main__':      for i in range(5):  # 创建5个进程          th = sleepThread()          th.start()          th.join()      print 'all threads finished'  ```  这样输出的结果就像下面一样：![][6]原因是线程join()后会阻塞后面线程的创建，导致线程无法并行，这样多线程就没有意义了。### 将需要线程执行的函数传入线程构造函数中上面是线程的一种创建方式，实现上面相同功能的另外一种创建方式如下：  ```py  def sleepThread(threadName):      randomTime = random.randint(1,9) # 生成 1~9的随机整数      time.sleep(randomTime)      print threadName+ ' slept for '+str(randomTime)+' seconds' if __name__ == '__main__':      threads = []      for i in range(5):          th = threading.Thread(target=sleepThread,args=('Thread-'+str(i),))          threads.append(th)          th.start()    for t in threads:          t.join()      print 'all threads finished'  ```  利用了`threading.Thread`自身的构造函数，传入的target参数作为线程的`run`函数,args参数则为传入的run函数的参数。输出结果如下所示：![][7]线程还有比较常用的方法比如说setdaemon(True),字面上的意思是设为守护线程，但是这个守护线程跟守护进程有很大的区别，**实际上setdaemon(True)的作用是保证主线程（就是任何进程最开始的那个线程）退出时，派生出来的线程也必须退出。**详细例子见http://stackoverflow.com/questions/5127401/setdaemon-function-in-thread## 线程锁  因为多线程共享一个进程内的资源，所以**多个线程同时修改同一个变量时会发生冲突。这时候就需要线程锁**了。比如说下面这段代码;  ```py  count = 10def modifyThread(num):      global count      for i in range(1000):          count -= num          count += numif __name__ == '__main__':      threads = []      print 'before modifying, count=%s '%count      for i in range(5):          th = threading.Thread(target=modifyThread,args=(i,))          threads.append(th)          th.start()    for t in threads:          t.join()    print 'after modifying, count=%s '%count```  执行的的时候每次输出结果都不一样，例如下图：![][8]这是因为count是被多个线程同时修改了，解决方法就是利用线程锁`threading.Lock()`,每次需要修改count时先获取线程锁，修改完再释放。实例代码如下所示：  ```py  count = 10def modifyThread(num):      global count      threadLock.acquire()      try:          for i in range(1000):              count -= num              count += num      finally:          threadLock.release()if __name__ == '__main__':      threads = []      threadLock = threading.Lock()      print 'before modifying, count=%s '%count      for i in range(5):          th = threading.Thread(target=modifyThread,args=(i,))          threads.append(th)          th.start()    for t in threads:          t.join()    print 'after modifying, count=%s '%count  ```  当多个线程同时执行threadLock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。在其中一个线程获取了线程锁（threadLock.acquire()）后，其他线程便无法修改count，但是修改完后一定要记得释放线程锁（threadLock.release()），否则其他线程会一直处于blocked的状态，上面采用了`try-finally `保证锁一定被释放。除了`try-finally `,还可通过 `with` 语句实现锁的自动获取和释放, 也就是说上面的 `modifyThread` 函数可以写成下面的形式```py  def modifyThread(num):      global count      with threadLock:          for i in range(1000):              count -= num              count += num  \n通过加锁的方法修改 count, 最终得到的 count 的值不变。\n线程锁 (Lock) 是线程同步的一种方式，除此之外，还有 RLocks, Semaphores,\nCondition, Events 和 Queues，具体可参考官方文档和 Python\nthreads synchronization: Locks, RLocks, Semaphores, Conditions, Events\nand Queues\n多线程是否提高了效率\n常常会听到有人说，因为 python 多线程只能使用一个核，所以多线程实际上并没有提高效率。这句话可以说一半正确，一半不正确。原因如下：\npython 多线程只能使用一个核这句话针对部分 python 解析器如 CPython 等是正确的，而且是相对与 Java、C++ 那些一个线程就可以占一个核的程序而言。python 的官方文档描述如下：\n&gt;In CPython, the global interpreter lock, or GIL, is a mutex that\nprevents multiple native threads from executing Python bytecodes at\nonce. This lock is necessary mainly because CPython's memory management\nis not thread-safe\n原因是 python 的解析器（如 CPython）因为内存管理问题设计了一个 GIL（全局解析锁），GIL 保证了任何时候都只能有一个线程执行其字节码。这就限制了同一进程内同一时间只能有一个线程在执行其字节码，也就是说无论一个进程无论创建多少线程都只能使用一个核。\n而且，这个 GIL 也只在 CPython 等解释器有，其他的如 Jython 或\nIronPython\n中没有 GIL，多线程可以利用多个核。另外，即使是 CPython 解释器，也可通过多进程来达到利用多个核的目的。\n那第二句话多线程实际上并没有提高效率是否正确？可以说也是部分正确，实际上针对 CPU 密集型的\npython 进程，多线程没有提高效率，而针对 IO 密集型的 python\n进程会提高效率。\n从上面的解释我们知道，GIL 是限制了多线程并发执行的一个关键因素，而 GIL 仅仅是限制了同一时间同一进程只能有一个线程执行字节码，执行字节码是在 CPU 中的，对于 CPU 密集型的多线程，会一直占据着 CPU 导致其效果跟单线程一样。\n而对于 IO 密集型的多线程，线程的执行时间会较多地消耗在 IO 上，因而 CPU 可供多线程轮流使用。比如说我曾用 python 爬取几个输入法的词库的，多线程比单线程要快了好几倍，原因就是爬虫属于 IO 密集型的任务，线程执行字节码所需的时间很短，而把大部分时间放在了下载和存储在本地上，线程执行完字节码后会释放 GIL，从而其他线程也能够执行其字节码。从而在总体上提高了下载效率。\n文章为博主个人理解总结，如有错误，欢迎指出交流。\n参考：\nthreading\n— Higher-level threading interface\nGlobalInterpreterLock\n多线程\n","categories":["python","并行编程"],"tags":["python","操作系统"]},{"title":"python 中的多进程","url":"/2015/12/15/python%E4%B8%AD%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B/","content":"本文提到了 python 中实现多进程的几种方法（fork、multiprocessing、pool）以及进程间的简单通信。\n\nfork () 函数\nUnix/Linux 操作系统提供了一个 fork() 系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是 fork () 调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。\n子进程永远返回 0，而父进程返回子进程的 ID。这样做的理由是，一个父进程可以 fork 出很多子进程，所以，父进程要记下每个子进程的 ID，而子进程只需要调用 getppid () 就可以拿到父进程的 ID。\nPython 的 os 模块封装了常见的系统调用，其中就包括 fork，可以在 Python 程序中轻松创建子进程：\n# multiprocessing.py  import osprint 'Parent Process (%s) start...' % os.getpid()  pid = os.fork()  if pid==0:      print 'I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid())  else:      print 'I (%s) just created a child process (%s).' % (os.getpid(), pid)  ```  运行结果如下：```  Parent Process (876) start...  I (876) just created a child process (877).  I am child process (877) and my parent is 876.  \n由于 Windows 没有 fork 调用，上面的代码在 Windows 上无法运行。\n有了 fork 调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的 Apache 服务器就是由父进程监听端口，每当有新的 http 请求时，就 fork 出子进程来处理新的 http 请求。\nmultiprocessing 模块\n如果你打算编写多进程的服务程序，Unix/Linux 无疑是正确的选择。由于 Windows 没有 fork 调用，难道在 Windows 上无法用 Python 编写多进程的程序？\n由于 Python 是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing 模块就是跨平台版本的多进程模块。\nmultiprocessing 模块提供了一个 Process 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：\n#coding=utf-8  #多进程  from multiprocessing import Process  from time import  ctime,sleep  import os# 子进程要执行的代码  def run_proc(name):  \tprint 'Run child process %s (%s),parent pid is (%s),start at %s...' % (name, os.getpid(),os.getppid(),ctime())  \tsleep(2)processes=[]  p1 = Process(target=run_proc, args=('p1',))  processes.append(p1)  p2=Process(target=run_proc,args=('p2',))  processes.append(p2)if __name__ == '__main__':      print 'Parent Process pid(%s),start at %s...' %(os.getpid(),ctime())      for p in processes:      \tp.start()      \tp.join()      print \"all over at %s\" %ctime()  \n上面的代码也是在 Linux 上执行（如果要在 Windows 下执行，可以去掉 os 的 getppid () 方法）执行结果如下：\nParent Process pid(30994),start at Tue Jan 12 10:37:48 2016...  Run child process p1 (30995),parent pid is (30994),start at Tue Jan 12 10:37:48 2016...  Run child process p2 (31013),parent pid is (30994),start at Tue Jan 12 10:37:50 2016...  all over at Tue Jan 12 10:37:52 2016  \n创建子进程时，只需要传入一个执行函数和函数的参数，创建一个 Process 实例，用 start () 方法启动，这样创建进程比 fork () 还要简单。\njoin () 方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。\n如何理解上面这句话？将上面的代码中的 p.join() 去掉，执行结果如下：\nParent Process pid(31038),start at Tue Jan 12 10:39:27 2016...  all over at Tue Jan 12 10:39:27 2016  Run child process p1 (31039),parent pid is (31038),start at Tue Jan 12 10:39:27 2016...  Run child process p2 (31040),parent pid is (31038),start at Tue Jan 12 10:39:27 2016...  ```  从输出可以看到父进程还没等子进程结束就执行了最后的`print \"all over at %s\" %ctime()`语句，而且两个子进程的开始执行时间也相同。## Pool模块如果要启动大量的子进程，可以用**进程池的方式**批量创建子进程：```py  #coding:utf-8  #进程池  from multiprocessing import Pool  import os, time, randomdef child_task(name):      print 'Child Process %s (%s) starts' % (name, os.getpid())      start = time.time()      time.sleep(random.random() * 2)      end = time.time()      print 'Child Process %s runs %0.2f seconds.' % (name, (end - start))if __name__=='__main__':      print 'Parent Process %s starts' % os.getpid()      p = Pool()      for i in range(5):          p.apply_async(child_task, args=(i,))      print 'Waiting for all subprocesses done...'      p.close()      p.join()      print 'All subprocesses done.'  \n执行结果如下：\nParent Process 6028 starts  Waiting for all subprocesses done...  Child Process 0 (6448) starts  Child Process 1 (6912) starts  Child Process 2 (5176) starts  Child Process 3 (7676) starts  Child Process 2 runs 0.86 seconds.  Child Process 4 (5176) starts  Child Process 3 runs 0.98 seconds.  Child Process 0 runs 1.80 seconds.  Child Process 4 runs 1.97 seconds.  All subprocesses done.  ```  代码解读：对Pool对象调用join()方法会等待所有子进程执行完毕，**调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了**。请注意输出的结果，Child Process 0，1，2，3是立刻执行的，而Child Process要等待前面某个Child Process完成后才执行，这是因为**Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。**我的笔记本是四核的，所以创建五个进程的时候有一个需要等待。但是如果只有一核，输出会如下所示：```  Parent Process 32075 starts  Waiting for all subprocesses done...  Child Process 0 (32076) starts  Child Process 0 runs 1.99 seconds.  Child Process 1 (32076) starts  Child Process 1 runs 0.39 seconds.  Child Process 2 (32076) starts  Child Process 2 runs 0.75 seconds.  Child Process 3 (32076) starts  Child Process 3 runs 0.98 seconds.  Child Process 4 (32076) starts  Child Process 4 runs 1.97 seconds.  All subprocesses done.  \n这是 Pool 有意设计的限制，并不是操作系统的限制。如果改成：p = Pool(5) 就可以同时跑 5 个进程。\n进程间通信\nProcess 之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python 的 multiprocessing 模块包装了底层的机制，提供了 Queue、Pipes 等多种方式来交换数据。\n我们以 Queue 为例，在父进程中创建两个子进程，一个往 Queue 里写数据，一个从 Queue 里读数据：\n# coding:utf-8  #进程间的通信s  from multiprocessing import Process, Queue  import os, time, random# 写数据进程执行的代码:  def write(q):      for value in ['A', 'B', 'C']:          print 'Put %s to queue...' % value          q.put(value)          time.sleep(random.random()*3)# 读数据进程执行的代码:  def read(q):      while True:          value = q.get(True)          print 'Get %s from queue.' % valueif __name__=='__main__':      # 父进程创建Queue，并传给各个子进程：      q = Queue()      pw = Process(target=write, args=(q,))      pr = Process(target=read, args=(q,))      # 启动子进程pw，写入:      pw.start()      # 启动子进程pr，读取:      pr.start()      # 等待pw结束:      pw.join()      # pr进程里是死循环，无法等待其结束，只能强行终止:      pr.terminate()  \n运行结果如下：\nPut A to queue...  Get A from queue.  Put B to queue...  Get B from queue.  Put C to queue...  Get C from queue.  \n在 Unix/Linux 下，multiprocessing 模块封装了 fork () 调用，使我们不需要关注 fork () 的细节。由于 Windows 没有 fork 调用，因此，multiprocessing 需要 “模拟” 出 fork 的效果，父进程所有 Python 对象都必须通过 pickle 序列化再传到子进程去，所有，如果 multiprocessing 在 Windows 下调用失败了，要先考虑是不是 pickle 失败了。\n小结\n\n在 Unix/Linux 下，可以使用 fork () 调用实现多进程。\n要实现跨平台的多进程，可以使用 multiprocessing 模块。\n进程间通信是通过 Queue、Pipes 等实现的。\n\n参考：多进程\n","categories":["python","并行编程"],"tags":["python","操作系统"]},{"title":"python 批量下载文件","url":"/2015/12/02/python%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/","content":"　最近感觉要练练口语了，所以就上普特逛逛有哪些资源比较好的，发现美国惯用语板块挺好的，但是点进去想下载音频时，发现浏览器已经解释了这个 MP3 格式的音频文件；要下载就只能右键另存为，总共有 900 多个，这么点来点去岂不是要点一天才能下完。\n\n　　正当蛋疼之时，忽然发现了播放 MP3 文件的 url 格式比较统一，比如说下面这几个\n　　\n　　\n　　\n　　前面的前缀都是相同的，可以推测在服务器上这些音频文件都是放到同一个文件夹下，而且会依据数字来加上前缀来命名，这就为脚本自动下载提供了一个很好的条件。\n　　首先，从最简单的入手，就是下载给出 url 所代表的资源，代码如下\nimport urllib2  url=\"http://down02.putclub.com/homepage/courses/middle/oftenused/wi_01.mp3\"  f=urllib2.urlopen(url)  data=f.read()  with open(\"1.mp3\",\"wb\") as file:      file.write(data)  \n　　如果资源 url 的确存在，那执行脚本后会在当前目录生成跟一个名为 1.mp3 的音频文件，当然名称也可以自己取，这里为了简便就直接用数字来作为文件名了。如果资源 url 不存在则在 urlopen 时就会抛出一个 URLError(实际上也的确有某几期不提供音频文件)。根据测试，发现音频文件的前缀有 wi、wi_、putclub_mgxgy、wi_0 这几种。所以总体思路就是先得到合法的 url，再将其下载；因为基于数字命名，所以可以通过循环来批量下载。是不是很简单，这里给出完整的代码\n#encoding:utf-8  #下载普特英语文音频件的脚本  import urllib2#得到合法的资源url  def getLegalUrl(i):      base_url=\"http://down02.putclub.com/homepage/courses/middle/oftenused/\"      url_preletter_list=['wi_','wi','putclub_mgxgy','wi_0']      for j in url_preletter_list:          try:              url=base_url+j+str(i)+'.mp3'              f=urllib2.urlopen(url)              return url   #不合法的url会抛出URLError的错误，不抛出则说明url存在          except urllib2.URLError:              continue      return \"\"#下载给定的合法的url的资源  def download(url,i):      f=urllib2.urlopen(url)      data=f.read()      with open(str(i)+'.mp3','wb') as file:          file.write(data) if __name__ == '__main__':      for i in range(0,600):          url=getLegalUrl(i)          if url == \"\":              #记录无法下载的那几期，以便验证是否资源本来就没有              with open(\"download.log\",'a') as log:                  log.write(str(i)+' not found\\n')          else:              download(url,i)\n等到脚本执行完，在文件夹下就能得到下面这些文件，密集恐惧者可忽略\n\n日志如下，经检查，这些资源本来就不存在\n\n这个方法只限于那些资源名称有规律的文件下载，但是好像普特上的文件都是这么存储的，所以以后就能愉快的下载了。\n对于我这种小白而言只能想出这种方法，如果你有更好的方法，欢迎交流。\n","categories":["python","爬虫"],"tags":["python","爬虫"]},{"title":"python 内置的排序函数","url":"/2016/07/23/python%E5%86%85%E7%BD%AE%E7%9A%84%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0/","content":"排序是非常常见的操作，常见的排序算法的时间复杂度一般为 \\(O(n^2)\\)（冒泡、选择、插入）或 \\(O(nlogn)\\)(快排、归并等)。虽然这些算法对于编程人员来说是基础，但是在实际工程中往往会使用语言内置的排序函数，主要是考虑到编程效率和自己编写排序函数时涵盖情况不全的问题。因此本文主要讲述 python 中的内置函数。\n\npython 内置的排序函数主要有两个：sort 和 sorted，两者主要以下几点区别\n（1）针对的数据类型不同，sort 只能对 list 类型排序，sorted 可对 list、tuple、dictionary 以及自定义的类等数据类型排序\n（2）sort 会修改原来的 list，sorted 不会修改原来的数据结构，而是将排好序的结果以 list 形式返回，因此 sorted 才能对不可变的数据类型 tuple 进行排序\n（3）语法不同，sort 的使用方法是 list.sort (),sorted 的使用方法是 sorted (list|tuple|dict)\n除此之外，两者的参数完全一致，都含有 reverse，key，cmp 这几个参数，这几个参数的用法在两个函数中一致，下面以 sorted 为例分别讲述这几个参数的作用。\nreverse 参数\nreverse 参数的作用是决定从小到大还是从大到小排列结果，默认情况下 reverse=False，从小到大排列结果。如果要从大到小排列结果，添加 reverse=True 参数即可。示例如下所示：\n&gt;&gt;&gt; a = ['a','A','b','B']&gt;&gt;&gt; sorted(a)['A', 'B', 'a', 'b']&gt;&gt;&gt; a['a', 'A', 'b', 'B']&gt;&gt;&gt; a = ['abundunt','Array','bunch','&gt;&gt;&gt; sorted(a, reverse = True)['bunch', 'abundunt', 'But', 'Array']\n数字大小的判断很容易理解，这里比较的是字符串的大小，比较时会根据字符串首字符的 ASCII 码的大小进行排序。\nkey 参数\n上面的例子是根据单个元素来排序的，假如需要对多个元素组合而成的元素来排序（如 b = [(2,1),(1,3),(4,2)]），就需要用 key 来指定以哪个元素作为排序的依据。\n下面是对 tuple 组成的 tuple 的排序 &gt;&gt;&gt; b = ((2,1),(1,3),(4,2))# 不用key参数时也不会报错，这时排序依据默认是组合元素中的第一个元素，但是为了程序的清晰性，还是建议使用key参数&gt;&gt;&gt; sorted(b) [(1, 3), (2, 1), (4, 2)]&gt;&gt;&gt; sorted(b, key = lambda x:x[0])[(1, 3), (2, 1), (4, 2)]&gt;&gt;&gt; sorted(b, key = lambda x:x[1])[(2, 1), (4, 2), (1, 3)]\n给key参数传进去的是一个函数，上面使用 lambda 实现，传入的是需要排序的组合元素，返回的是根据组合元素中哪个元素排序，从下标 0 开始计算。\n对于字典的 key 或 value 排序也是如此，但此时的下标就只有 0 和 1 了，分别代表 key 和 value。\n&gt;&gt;&gt; d = {2:1, 4:3, 1:6}&gt;&gt;&gt; sorted(d.items(), key = lambda x:x[0])[(1, 6), (2, 1), (4, 3)]&gt;&gt;&gt; d{1: 6, 2: 1, 4: 3}&gt;&gt;&gt; sorted(d.items(), key = lambda x:x[1])[(2, 1), (4, 3), (1, 6)]&gt;&gt;&gt; d{1: 6, 2: 1, 4: 3}\n除了内部的 list、tuple 等数据类型，key 还可以针对自定义的数据类型进行排序。实例如下\n&gt;&gt;&gt; class Student:        def __init__(self, name, grade, age):                self.name = name                self.grade = grade                self.age = age        def __repr__(self):                return repr((self.name, self.grade, self.age))        def weighted_grade(self):                return 'CBA'.index(self.grade) / float(self.age)&gt;&gt;&gt; student_objects = [        Student('john', 'A', 15),        Student('jane', 'B', 12),        Student('dave', 'B', 10),]&gt;&gt;&gt; sorted(student_objects, key=lambda student: student.age)   # sort by age[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]\ncmp 参数\n利用上面两个参数已经能解决大部分的问题了，还有一些较特殊的排序需要编写特定的排序函数，这时就需要利用 cmp 参数。\n如 LeetCode 上的这道题目 179. Largest\nNumber，就是一道排序的题目，且排序的要求是对于两个字符串 s1，s2\n排序规则如下：(1)假如 s1+s2 &gt; s2+s1,则 s1 &gt; s2(2)假如 s1+s2 &lt; s2+s1,则 s1 &lt; s2(3)以上两种情况均不符合，则s1 = s2\n这种情况下上面所提到的两种方法都无法实现，因为上面的两个参数都是针对单个元素的特性的，而这种方法则是针对两个元素之间的关系。因此需要定义自己的排序函数。\n不指定 cmp 参数的时候，cmp = lambda x,y: cmp(x, y)，这里需要注意第一个 cmp 是 sort 函数的参数，第二个 cmp 则是 python 的一个内置函数。其定义如下：\n&gt;&gt;&gt; cmp(1,2)-1&gt;&gt;&gt; cmp(2,1)1&gt;&gt;&gt; cmp(2,2)0 对于 cmp(x, y),当 x 大于、等于、小于 y\n时，分别会返回\n1,0,-1。这个默认的cmp函数也可以写成 cmp = lambda x,y: x-y,这种情况下是从小到大排序的，那么从大到小的排序可以写成 cmp = lambda x,y: y-x。这等价于 reverse = True。\n回到题目 179.\nLargest\nNumber上，利用内置的 sorted 函数，我们可以写出下面较为简洁的代码\nclass Solution:    # @param {integer[]} nums    # @return {string}    def largestNumber(self, nums):        snums = [str(num) for num in nums]        snums.sort(cmp=lambda x,y: cmp(y+x,x+y))        return ''.join(snums).lstrip('0') or '0'\n关键点在 snums.sort(cmp=lambda x,y: cmp(y+x,x+y)) 这句话，首先是 cmp(y+x,x+y), 当\ny+x &gt; x+y 时，y&gt;x,\n此时 cmp(y+x,x+y) 返回 1，而由上面的知识可知这是从大到小的排序。\n上面可以说是比较抽象的代码，下面是通过自己实现的快排解决上面的题目，跟上面的答案一样能够 AC。\nclass Solution:    # @param {integer[]} nums    # @return {string}    def largestNumber(self, nums):        snums = [str(num) for num in nums]        self.quick_sort(0, len(snums)-1, snums)        tmp = ''.join(snums).lstrip('0')        return tmp if tmp else '0'    def quick_sort(self, left, right, nums):        if left &gt; right:            return        pivot, start, end = left, left, right        while left &lt; right:            while left &lt; right and nums[right]+nums[pivot] &lt;= nums[pivot]+nums[right]:                right -= 1            while left &lt; right and nums[left]+nums[pivot] &gt;= nums[pivot]+nums[left]:                left += 1            if left &lt; right:                nums[left], nums[right] =nums[right], nums[left]        if left == right:            nums[pivot], nums[left] = nums[left], nums[pivot]        self.quick_sort(start, left-1, nums)        self.quick_sort(left+1, end, nums)\n","categories":["python","语法"],"tags":["python"]},{"title":"python 中的类属性和实例属性","url":"/2016/02/14/python%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%B1%9E%E6%80%A7/","content":"面向对象语言中，一般会有 “静态变量”，也就是给整个类共享的变量，如 C++，java 中 static 修饰的变量。但是在\npython 中并没有 static\n这个关键字，实现类似功能需要依靠 python 中的类属性和实例属性的语法特点。本文主要就是讲述这两种属性的区别。\n\n在讲述之前，需要清楚下面两个事实：\n1）python 中类创建的对象叫实例\n2）类和实例均是对象，均有自己的对象属性，可通过__dict__查看\n下面先看一个例子：\nclass TestAttribute:      content = []      def addContent(self,x):          self.content.append(x)if __name__ == '__main__':      t1 = TestAttribute()      t2 = TestAttribute()      t1.addContent('t1')      print 'object t1:', t1.content, t1.__dict__      print 'object t2:', t2.content, t2.__dict__  \n输出结果：\nobject t1: ['t1'] {}  object t2: ['t1'] {}  ```  从例子可以看到，这时的 content 就相当于一个static 变量，被所有实例共享。**注意后面那个花括号{}表示实例的所有对象属性，只是当前实例没有自己的属性**。那假如各个实例要有自己独立的变量的？也很简单，**只需要在类的构造函数（也就是__init__函数)为变量赋值即可.**```py  class TestAttribute:      content = []    def  __init__(self):          self.content=[]    def addContent(self,x):          self.content.append(x)if __name__ == '__main__':      t1 = TestAttribute()      t2 = TestAttribute()      t1.addContent('t1')      print 'object t1:', t1.content ,t1.__dict__      print 'object t2:', t2.content ,t2.__dict__  ```  输出的结果如下所示：  ```  object t1: ['t1'] {'content':['t1']}  object t2: [] {'content':[]}  \n从结果可知，现在的 t1,t2 的变量独立了。那原因是什么呢？\n原因是 Python 中对象属性的获取是按照从下到上的顺序来查找属性\n怎么理解上面这句话呢？以上面代码为例，类和实例的关系如下所示：\n    TestAttribute       ____|____      |         |     t1         t2  ```  **输出`t1.content`时，python 解析器会先查看对象 t1 中是否有content这个属性，有的话输出这个属性的值，没有的话就往上查找 TestAttribute 中是否有这个属性并输出。**在上面的例一中，因为**t1和t2的属性均为空，所以输出 t1.content 和  t2.content时实际上是输出 TestAttribute 的属性**。又因为 TestAttribute 的 content 属性被修改了 t1 修改了，所以最后输出的的 t1.content 和 t2.content 内容一致。而在例二中，因为**在构造函数中的为content复制的操作使得每个被创建的实例均有自己的content属性**，所以 t1 修改 content 时查到自己有content的属性，就只会修改自己的 content。不影响t2 的 content 和 TestAttribute 的 content。这个可以从下面的例子看出，假如将例二的代码修改成如下所示：  ```py  class TestAttribute:      content = []    def  __init__(self):          self.content=[]    def addContent(self,x):          self.content.append(x)if __name__ == '__main__':      t1 = TestAttribute()      t2 = TestAttribute()      t1.addContent('t1')      TestAttribute.content.append('tt')      print 'object t1:', t1.content ,t1.__dict__      print 'object t2:', t2.content ,t2.__dict__      print 'class TestAttribute:', TestAttribute.content  ```  那输出结果是：  ```  object t1: ['t1'] {'content':['t1']}  object t2: [] {'content':[]}  class TestAttribute: ['tt']  ```  可以看到这三个对象的属性均独立。那么如何为一个实例添加属性呢？**答案是通过赋值号 = 给实例所需添加的属性赋值。** 通过赋值号 = 给实例所需添加的属性赋值实际上是将这个属性指向了新的引用，也就是新的内存空间。如在例一中没有通过赋值号为 content 赋值，所以这个属性并没有成为 t1 自己的属性，输出t1.__dict__ 为空。而在例二中的构造函数里面为每个实例的content均赋值，所以例二中的三个对象的content属性独立。通过下面的例子可以更深入说明这点：```py  class TestAttribute:      num = 0if __name__ == '__main__':      t1 = TestAttribute()      t2 = TestAttribute()    t1.num = 1      print 'object t1:', t1.num, t1.__dict__      print 'object t2:', t2.num, t2.__dict__  ```  输出结果：  ```  object t1: 1 {'num': 1}  object t2: 0 {}  \n最后，小结如下：\n1.Python 中的类和实例是两个完全独立的对象；\n2.Python 中属性的获取是按照从下到上的顺序来查找属性；\n3. 为实例添加属性的方法：通过赋值号 =\n给实例所需添加的属性赋值\n","categories":["python","语法"],"tags":["python"]},{"title":"python 网络编程","url":"/2016/06/03/python%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","content":"网络编程根据协议划分可以划分为 TCP 编程和 UDP 编程。两者的主要区别在于效率和可靠性，下面分别讲述两者在 python 中的实现\n\nTCP 编程\n客户端\n要创建一个基于 TCP 连接的 Socket，可以这样做： import sockets = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n创建Socket时，AF_INET 指定使用IPv4协议，如果要用更先进的IPv6，就指定为 AF_INET6。SOCK_STREAM 指定使用面向流的TCP协议，这样，一个Socket对象就创建成功，但是还没有建立连接。\n客户端要主动发起 TCP 连接，必须知道服务器的 IP 地址和端口号。接着上面的代码，通过创建的 socket 连接到本地服务器上。\nhost = ('127.0.0.1', 80) # tuple类型s.connect(host)s.send('GET / HTTP/1.1\\r\\nHost: 127.0.0.1\\r\\nConnection: close\\r\\n\\r\\n')\n建立 TCP 连接后，我们就可以向服务器发送请求。但是由于 TCP 连接创建的是双向通道，双方都可以同时给对方发数据。但是谁先发谁后发，怎么协调，要根据具体的协议来决定。例如，HTTP 协议规定客户端必须先发请求给服务器，服务器收到后才发数据给客户端。\n发送的文本格式必须符合 HTTP 标准，如果格式没问题，接下来就可以接收服务器返回的数据了：\n# 接收数据:buffer = []while True:    # 每次最多接收1k字节:    d = s.recv(1024)    if d:        buffer.append(d)    else:        breakdata = ''.join(buffer)s.close()\n接收数据时，调用 recv(max) 方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到recv()返回空数据，表示接收完毕，退出循环。当我们接收完数据后，调用 close() 方法关闭Socket，这样，一次完整的网络通信就结束了：\n服务器端\n服务器进程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立 Socket 连接，随后的通信就靠这个 Socket 连接了\n服务器需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。\n首先，创建一个基于 IPv4 和 TCP 协议的 Socket：\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n然后，我们要绑定监听的地址和端口。服务器可能有多块网卡，可以绑定到某一块网卡的 IP 地址上，也可以用 0.0.0.0 绑定到所有的网络地址，还可以用 127.0.0.1 绑定到本机地址。127.0.0.1 是一个特殊的 IP 地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。\n端口号需要预先指定。因为我们写的这个服务不是标准服务，所以用 9999 这个端口号。请注意，小于 1024 的端口号必须要有管理员权限才能绑定 , 紧接着，调用 listen() 方法开始监听端口，传入的参数指定等待连接的最大数量：\n# 绑定并监听端口:s.bind(('127.0.0.1', 9999))s.listen(5)print 'Waiting for connection...'\n接下来，服务器程序通过一个永久循环来接受来自客户端的连接，accept() 会等待并返回一个客户端的连接:\nwhile True:    # 接受一个新连接:    sock, addr = s.accept()    # 创建新线程来处理TCP连接:    t = threading.Thread(target=tcplink, args=(sock, addr))    t.start()\n每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接：\ndef tcplink(sock, addr):    print 'Accept new connection from %s:%s...' % addr    sock.send('Welcome!')    while True:        data = sock.recv(1024)        time.sleep(1)        if data == 'exit' or not data:            break        sock.send('Hello, %s!' % data)    sock.close()    print 'Connection from %s:%s closed.' % addr\n要测试这个服务器程序，示例的客户端程序如下所示： s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 建立连接:s.connect(('127.0.0.1', 9999))# 接收欢迎消息:print s.recv(1024)for data in ['Michael', 'Tracy', 'Sarah']:    # 发送数据:    s.send(data)    print s.recv(1024)s.send('exit')s.close() ##\nUDP编程\n客户端\n客户端使用 UDP 时，首先要创建基于 UDP 的 Socket，然后，不需要调用 connect()，直接通过 sendto() 给服务器发数据：\ns = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)for data in ['Michael', 'Tracy', 'Sarah']:    # 发送数据:    s.sendto(data, ('127.0.0.1', 9999))    # 接收数据:    print s.recv(1024)s.close()\n创建 Socket 时，SOCK_DGRAM 指定了这个 Socket 的类型是 UDP,TCP 则是 SOCK_STREAM。\n从服务器接收数据仍然调用 recv() 方法。\n服务器端\nTCP 是建立可靠连接，相对 TCP，UDP 则是面向无连接的协议。\n使用 UDP 协议时，不需要建立连接，只需要知道对方的 IP 地址和端口号，就可以直接发数据包。但是，能不能到达就不知道了。\n虽然用 UDP 传输数据不可靠，但它的优点是和 TCP 比，速度快，对于不要求可靠到达的数据，就可以使用 UDP 协议。\n和 TCP 类似，使用 UDP 的通信双方也分为客户端和服务器。服务器首先需要绑定端口：\ns = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# 绑定端口:s.bind(('127.0.0.1', 9999))\n创建Socket时，SOCK_DGRAM 指定了这个Socket的类型是UDP,TCP则是 SOCK_STREAM。绑定端口和TCP一样，但是不需要调用 listen() 方法，而是直接接收来自任何客户端的数据：\nprint 'Bind UDP on 9999...'while True:    # 接收数据:    data, addr = s.recvfrom(1024)    print 'Received from %s:%s.' % addr    s.sendto('Hello, %s!' % data, addr)\nrecvfrom() 方法返回数据和客户端的地址与端口，这样，服务器收到数据后，直接调用 sendto() 就可以把数据用 UDP 发给客户端。\n\n参考：\nhttp://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/00138683226192949cd41410a6d4f1ebfa9ba40bbd1399d000\n","categories":["python"],"tags":["python","计算机网络"]},{"title":"python 连接 mysql 及其注意事项","url":"/2016/02/24/python%E8%BF%9E%E6%8E%A5mysql%E5%8F%8A%E5%85%B6%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","content":"本文主要讲述利用 python\n连接数据库的过程和部分注意事项。文章不会涉及到连接的原理，只是简单介绍连接的步骤，以及\nmysql 不同的引擎连接的过程的细微区别。\n\n基本上任何语言与数据库进行交互都需要引入外部的数据库驱动，在 python\n操作 mysql 数据库中常用的就是 MySQLdb 这个驱动，后面也会以这个为例子进行讲解。\n数据库的最常见的操作就是 “增删查改”，实现这几个功能需要对应的 SQL 语句。而通过程序连接数据库实际上就是获得与数据库的连接，通过这个连接执行 SQL 命令，得到返回结果（如果有返回结果的话）。\n因此，通过 python 操作 mysql 的步骤可以概括为下面 3 个步骤：\n1. 获得连接\n2. 获取游标\n3. 通过游标执行 SQL 语句，获取返回结果（如果有）\n** 获得连接 **\n获得连接的代码如下\nimport MySQLdbHOST = XXXX  PORT = XXXX  USER = XXXX  PASSWD = XXXX  DB = XXXX  CHARSET = XXXXconn = MySQLdb.connect(host=HOST,port=PORT,user=USER,passwd=PASSWD,db=DB,charset=CHARSET)```  一开始声明的几个常量表示要连接到哪台机器的哪个数据库以及采用的编码，**注意除了PORT为整数类型，其余都为字符类型。**假如在某个工程中有多个地方需要操作数据库，可以将这个写成一个函数。如下所示：```py  def connectDB():      try:          conn = MySQLdb.connect(passwd=PASSWD, host=HOST, user=USER, port=PORT, db=DB, charset='utf8')          return conn      except:          return 0  ```  可通过返回值是不是0判断是否建立了正常连接。## **获取游标**  获得连接后，我们希望做的就是执行我们的 SQL 语句，但是在MySQLdb中，conn并不能执行此操作。**需要通过游标（cursor）来执行命令并保存执行的结果，而游标可通过第一步得到的连接获取**。代码如下：```  cursor = conn.cursor()  \n执行 SQL 语句，获得返回结果\n可以通过 cursor.execute(SQL) 来执行 SQL 语句，通过 cursor.fetchall() 获取返回的结果（针对 select 语句）\n这里获取返回结果针对的是查询（select）语句。\n查询的代码代码如下\nSQL = \"select * from table\"  try:      cursor.execute(SQL)      resultSet = cursor.fetchall()      for row in resultSet:         for i in row:             print i  except:      print 'error while querying'      return 0  finally:  ```  执行的SQL语句返回的结果集可以认为是一个嵌套的两级列表，数据库中每一条记录是一级列表中的一个元素。**插入新纪录**的代码如下：  ```py  SQL = 'insert into table(field) values(%s) ' %record  try:      cursor.execute(SQL)  except:      print 'error while querying'      return 0  \n另外删除和更新的代码也类似，只是 SQL 语句不同而已。\n这里需要注意的一点就是上面的代码有可能执行成功，但是数据库中不会更新记录。\n原因是 mysql 的引擎问题，假如 mysql 的引擎是 MyISAM，那么上面的代码就没问题，但是假如 mysql 的引擎是 InnoDB，那么上面的插入新纪录的代码将不会执行成功。因为 InnoDB 的是支持事务处理的，执行更新的操作会在 mysql 事先分配的缓存中进行，只有提交后，修改才能生效。提交的操作也很简单，就是在 cursor.execute () 后加上这句：\nconn.commit()  ```  关于这个问题，[MySQLdb官网说明][1]如下：&gt;commit()  If the database and the tables support transactions, this commits the current transaction; otherwise this method successfully does nothing.  rollback()  If the database and tables support transactions, this rolls back (cancels) the current transaction; otherwise a NotSupportedError is raised.从说明中可以看到，commit()函数对于支持事务的引擎生效，对于不支持事务的引擎也不会报错，所以**建议在代码中均使用**。除了commit()函数外，对于支持事务的引擎还有一个rollback()函数用于执行失败后的回滚，但是这个只能在支持事务的引擎上使用。最后，上个完整的代码  ```py  # encoding:utf-8  # 将数据文件的数据导入到mysql数据库中import MySQLdb  import ReadDirFilesHOST = XXXX  PORT = XXXX  USER = XXXX  PASSWD = XXXX  DB = XXXX  CHARSET = XXXXdef connectDB():      try:          conn = MySQLdb.connect(host=HOST, user=USER, passwd=PASSWD, port=PORT, db=DB, charset='utf8')          return conn      except:          print 'error connecting the database'          return 0def importData():      conn = connectDB()      if conn == 0:          return 0      cursor = conn.cursor()      SQL = 'insert into `ad_log` values(\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\",\"%s\") ' %tuple      # print SQL # 检查SQL语句是否正确      # 下面的代码是一个事务      try:          cursor.execute(SQL)          conn.commit()          print 'successfully insert the record'      except:          conn.rollback() #引擎不支持事务时会报错      finally:          cursor.close()          conn.close()  \n","categories":["python"],"tags":["python","数据库"]},{"title":"sae 上通过 python 获取访问网站 ip 及其来源","url":"/2015/12/22/sae%E4%B8%8A%E9%80%9A%E8%BF%87python%E8%8E%B7%E5%8F%96%E8%AE%BF%E9%97%AE%E7%BD%91%E7%AB%99ip%E5%8F%8A%E5%85%B6%E6%9D%A5%E6%BA%90/","content":"这篇文章是当时在新浪云上搭建博客时写的，后来因为新浪云开始各种收费了，所以就把博客转到了 github 上。这里还是把文章贴出来，做个记录\n常常看到有些网站会显示访问过该网站的所有人数及其分布地点，所以就琢磨着这个怎么实现，好给自己的网站也添加上去；在 google 上搜了一下发现大都是通过分析日志得出的，而新浪云上也提供了日志访问的 API，所以下面就说说怎么通过这个 API 获取访问的 IP 及其来源地。\n\n大致的步骤就是先通过身份校验获取访问日志的权限，然后通过 HTTP 请求摘取日志中表示访问 ip 和访问次数的段记录。剔除其中的私网 IP，再获取 IP 所在地，存入数据库。\n下面为具体的实施步骤\n获取访问的 IP 及其访问次数\n身份验证\n这个是新浪提供的用于校验身份的一个 api，校验身份是通过应用的 ACESSKEY 和 SECRETKEY 来实现的。\n代码下载链接地址：https://raw.githubusercontent.com/sinacloud/sae-python-dev-guide/master/examples/apibus/apibus_handler.py\n也可复制下面的代码创建一个 python 源文件，命名为 apibus_handler.py\n#-*-coding: utf8 -*-\"\"\"  SAE API auth handler for urllib2 and requestsurllib2:&gt;&gt;&gt; import urllib2  &gt;&gt;&gt; apibus_handler = SaeApibusAuthHandler(ACCESSKEY, SECRETKEY)  &gt;&gt;&gt; opener = urllib2.build_opener(apibus_handler)  &gt;&gt;&gt; print opener.open('http://g.sae.sina.com.cn/log/http/2015-06-18/1-access.log').read()requests:&gt;&gt;&gt; import requests  &gt;&gt;&gt; print requests.get('http://g.sae.sina.com.cn/log/http/2015-06-18/1-access.log?head/0/10|fields/ /1/2/3/4', auth=SaeApibusAuth(ACCESSKEY, SECRETKEY)).content  \"\"\"import hmac  import base64  import hashlib  import time  import urllib  from urllib2 import BaseHandler, Request_APIBUS_URL_PREFIX = 'http://g.sae.sina.com.cn/'class SaeApibusAuthHandler(BaseHandler):      # apibus handler must be in front      handler_order = 100    def __init__(self, accesskey, secretkey):          self.accesskey = accesskey          self.secretkey = secretkey    def http_request(self, req):          orig_url = req.get_full_url()          if not orig_url.startswith(_APIBUS_URL_PREFIX):              return req        timestamp = str(int(time.time()))          headers = [              ('x-sae-timestamp', timestamp),              ('x-sae-accesskey', self.accesskey),          ]          req.headers.update(headers)        method = req.get_method()          resource = urllib.unquote(req.get_full_url()[len(_APIBUS_URL_PREFIX)-1:])          sae_headers = [(k.lower(), v.lower()) for k, v in req.headers.items() if k.lower().startswith('x-sae-')]          req.add_header('Authorization', _signature(self.secretkey, method, resource, sae_headers))          return req    https_request = http_requesttry:      from requests.auth import AuthBase    class SaeApibusAuth(AuthBase):          \"\"\"Attaches HTTP Basic Authentication to the given Request object.\"\"\"          def __init__(self, accesskey, secretkey):              self.accesskey = accesskey              self.secretkey = secretkey        def __call__(self, r):              timestamp = str(int(time.time()))              r.headers['x-sae-timestamp'] = timestamp              r.headers['x-sae-accesskey'] = self.accesskey              resource = urllib.unquote(r.url[len(_APIBUS_URL_PREFIX)-1:])              #resource = r.url[len(_APIBUS_URL_PREFIX)-1:]              sae_headers = [(k.lower(), v.lower()) for k, v in r.headers.items() if k.lower().startswith('x-sae-')]              r.headers['Authorization'] = _signature(self.secretkey, r.method, resource, sae_headers)              return r  except ImportError:      # requests was not present!      passdef _signature(secret, method, resource, headers):      msgToSign = \"\\n\".join([          method, resource,          \"\\n\".join([(k + \":\" + v) for k, v in sorted(headers)]),      ])      return \"SAEV1_HMAC_SHA256 \" + base64.b64encode(hmac.new(secret, msgToSign, hashlib.sha256).digest())  \n通过 http 请求获取日志\n提供了通过 requests 包和 urllib 包两种方式，代码来源后面的参考文章，将下面代码保存成 sae_log_util.py 即可：\n#-*-coding: utf8 -*-#sae_log_util.py  #sae log utility based on sae apibus_handler  #author blog: http://bookshadow.com  #src date: 2015-09-17status_code_dict = {200 : 'OK', 206 : 'Partial Content', 400 : 'Bad Request', \\                                500 : 'Internal Server Error' , 404 : 'Not Found'}service_ident_dict = {'http': ['access', 'error', 'alert', 'debug', 'warning', 'notice'], \\      'taskqueue' : ['error'], \\      'cron' : ['error'], \\      'mail': ['access', 'error'], \\      'rdc' : ['error', 'warning'], \\      'storage' : ['access'], \\      'push' : ['access'], \\      'fetchurl' : ['access']  }_URL_PREFIX = 'http://g.sae.sina.com.cn/log/'class SaeLogFetcher(object):    def __init__(self, access_key, secret_key):          self.access_key = access_key          self.secret_key = secret_key    def fetch_log(self, service, date, ident, fop = '', version = 1):          assert self.access_key, 'access_key should not be empty'          assert self.secret_key, 'secret_key should not be empty'          assert service in service_ident_dict, 'invalid service parameter'          assert ident in service_ident_dict[service], 'invalid ident parameter'        url = _URL_PREFIX + service + '/' + date + '/' + str(version) + '-' + ident + '.log'          content = None        try:              import requests              from apibus_handler import SaeApibusAuth              r = requests.get(url + ('?' + fop if fop else ''), \\                       auth=SaeApibusAuth(self.access_key, self.secret_key))              status_code, status = r.status_code, status_code_dict.get(r.status_code, 'Unknown')              if status_code == 200:                  content = r.content          except ImportError:              # requests was not present!              from apibus_handler import SaeApibusAuthHandler              import urllib, urllib2              apibus_handler = SaeApibusAuthHandler(self.access_key, self.secret_key)              opener = urllib2.build_opener(apibus_handler)              if fop:                  url += '?' + urllib.quote(fop, safe='')              content = opener.open(url).read()          return content  \n调用上面的代码\n下面通过代码获取访问过的 ip 及次数，代码也是来源于参考链接，可将代码复制后另存为 ip_counter.py：\n#-*-coding: utf8 -*-  #ip_counter.py  #ip counter based on  sae_log_util  #author blog: http://bookshadow.com  #src date: 2015-09-17from collections import Counter  from sae_log_util import SaeLogFetcherdate = '2015-09-16'  service = 'http'  ident = 'access'  fop = 'fields/ /2' #fetch ip only  version = 1ACCESSKEY = '&lt;&lt;ACCESSKEY&gt;&gt;'  SECRETKEY = '&lt;&lt;SECRETKEY&gt;&gt;'log_fetcher = SaeLogFetcher(ACCESSKEY, SECRETKEY)result = log_fetcher.fetch_log(service, date, ident, fop, version)content = result.split('\\n')[:-1]for e, c in Counter(content).most_common():      print e, c  ```  将代码内的`&lt;&lt;ACCESSKEY&gt;&gt;`与`&lt;&lt;SECRETKEY&gt;&gt;`替换为你的sae应用具体的值。然后将上面的代码放到同一个工作目录，执行`ip_counter.py`这个文件，即可获取访问的ip，## 剔除私网IP 上面显示出出来的结果会显示出有私网ip，猜测是sae内部一些服务器间的通信，比如说`memcached`、`mysql`等服务与应用不在同一台服务器等，但是无论如何，这些私网ip都是我们不希望看到的，所以下面是剔除私网IP的过程。私网IP总共有A、B、C三类，而每一类IP的nei-id均是固定的，详见下面所示：  ```  A类：10.0.0.0/8： 10.0.0.0～10.255.255.255  B类：172.16.0.0/12： 172.16.0.0～172.31.255.255  C类：192.168.0.0/16： 192.168.0.0～192.168.255.255  ```  这样便可将IP移位后与三类私网IP的net-id比较，从而判断该IP是否属于私网IP。实现代码如下：```py  def ip_into_int(ip):      #以192.168.1.13为例，先把 192.168.1.13 变成16进制的 c0.a8.01.0d ，再去了“.”后转成10进制的 3232235789 即可。      #(((((192 * 256) + 168) * 256) + 1) * 256) + 13      return reduce(lambda x,y:(x&lt;&lt;8)+y,map(int,ip.split('.')))def is_internal_ip(ip):      ip = ip_into_int(ip)      net_a = ip_into_int('10.255.255.255') &gt;&gt; 24      net_b = ip_into_int('172.31.255.255') &gt;&gt; 20      net_c = ip_into_int('192.168.255.255') &gt;&gt; 16      return ip &gt;&gt; 24 == net_a or ip &gt;&gt;20 == net_b or ip &gt;&gt; 16 == net_c  \n查询 IP 所在地\n可以通过淘宝提供的 API 来查询 IP 所在地，查询代码如下所示：\n#encoding:utf-8  import requests#输入的ip的数据结构为字典:{'ip':具体的ip地址}  def find_ip_place(ip):      URL = 'http://ip.taobao.com/service/getIpInfo.php'      try:          r = requests.get(URL, params=ip, timeout=3)      except requests.RequestException as e:          print(e)      else:          json_data = r.json()          if json_data['code'] == 0:              print u'所在国家:',json_data[u'data'][u'country']              print u'所在地区:',json_data[u'data'][u'area']              print u'所在省份:',json_data[u'data'][u'region']              print u'所在城市:',json_data[u'data'][u'city']              print u'所属运营商:',json_data[u'data'][u'isp']          else:              print '查询失败,请稍后再试！'  \n然后就可以将获取到的关于 ip 的信息存入数据库，同时存入更新时间，这样便在数据库中有了访问网站的记录，便于后续的可视化分析。\n为了方便，还是利用了 sae 的 cron 服务每天定时将昨天的访问记录存入数据库。\n参考：\n使用 SAE 实时日志 API 统计 IP 来访次数\nSAE 实时日志 API\nPython 使用小记\nPython 判断内网 IP\n","categories":["python"],"tags":["python"]},{"title":"python 读取文件夹下所有文件的一种方法","url":"/2016/02/10/python%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/","content":"在数据挖掘中需要大量的数据，这些数据往往存储在数据库中或者文件中。存储数据库中比较好理解，可通过\n程序数据库接口+SQL语句\n获取。存储在文件中则往往有多个按日期命名的文件夹，数据以文本格式存储，且有特定的分割符。本文主要就是讲述如何通过 python 读取后一类的数据。\n\n总体思路就是先获取给定目录下所有文件的绝对路径，包括给定目录下的子目录；然后再读取每个文件的内容。\n首先要获取给定目录下的所有文件的绝对路径，python 的 os.listdir(dirPath) 方法可以列出 dirPath 下的所有文件和文件夹。为了处理 dirPath 下的子目录，需要判断读出的一个对象 a 是否为文件夹，可以通过 os.path.isdir(a) 来判断读出的 a 是否为一个文件夹，如果是，则递归读出下面的文件。\n实现代码如下：\n# encoding:utf-8  # 功能：读取传入的目录下所有的文件（包括该目录下的所有子目录）的绝对路径，并以列表形式返回所有文件的绝对路径  # 要求传入的路径参数最后不能有斜杠,目的是为了递归时格式统一import osdef readDir(dirPath):      if dirPath[-1] == '/':          print u'文件夹路径末尾不能加/'          return      allFiles = []      if os.path.isdir(dirPath):          fileList = os.listdir(dirPath)          for f in fileList:              f = dirPath+'/'+f              if os.path.isdir(f):                  subFiles = readDir(f)                  allFiles = subFiles + allFiles #合并当前目录与子目录的所有文件路径              else:                  allFiles.append(f)          return allFiles      else:          return 'Error,not a dir'  \n将上面的代码命名为 ReadDirFiles.py, 便于被下面调用。现在已经可以得到某一目录下所有文件的绝对路径，下面只需要读出这些文件里面的内容即可。\n为了存储的便利性，用文件存储数据时往往一行存储一条记录，一条记录中不同字段以特定分隔符分开。下面的代码就是解决这类型的数据的\n# encoding:utf-8  # 功能：解析文件中按行存放的数据，行内数据以SOH（\\001）分割import ReadDirFilesdef readFile (filePath):      with open(filePath) as f:          lines = f.readlines()          for line in lines:              data = line.split('\\001') #以列表形式返回分割了的行数据                  for i in data:                  print iif __name__ == '__main__':      dirPath = 'G:/20160107'      fileList=ReadDirFiles.readDir(dirPath)      for f in fileList:          readFile(f)  \n上面有两个需要注意的地方：\n1）建议采用\nwith open(filePath) as f 方法打开文件，因为这种方法不需要显示调用 f.close() 来关闭文件。open 函数可在第二个形参位置决定打开文件的模式，有读（a）、写（w）、追加（a）等，省略时默认为读。\n2）读取文件的行时，有 readline 和 readlines 两种方法，前者每次读一行，后者一次把文件全部行读入内存，显然后者的效率比前者要高。只有当内存太小或文件过大，无法一次全部读入内存才建议采用第一种方法。\n","categories":["python"],"tags":["python"]},{"title":"vim 中 SuperTab 的安装与使用","url":"/2015/12/01/vim%E4%B8%ADSuperTab%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/","content":"vim 是 Linux 下常用的编辑器，但是默认是没有补全功能的，所以插件 SuperTab 就是实现这个功能的。\n下载链接：http://www.vim.org/scripts/script.php?script_id=1643\n下载.vmb 文件即可，下载后可通过 rz 命令上传（需要安装 lrzsz）\n安装步骤也非常简单\n1. 先用 vim 打开下载的文件，vim supertab.vmb\n2. 在命令模式下输入:source %\n至此就可以使用 SuperTab 的功能了，在 vim 编辑模式时，输入文件中已经有的字符串的前几个字母，再按 Tab 键即可补全这个字符串，只能补全文件中已经出现的字符串。\nSuperTab 的 github 地址：https://github.com/ervandew/supertab\n","categories":["Linux"],"tags":["Linux","工具使用"]},{"title":"stacking 的基本思想及代码实现","url":"/2018/01/21/stacking%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","content":"本文主要介绍机器学习中的一种集成学习的方法 stacking，本文首先介绍\nstacking 这种方法的思想，然后提供一种实现 stacking\n的思路，能够简单地拓展 stacking 中的基本模型。\n\nstacking 的基本思想\nstacking\n就是将一系列模型（也称基模型）的输出结果作为新特征输入到其他模型，这种方法由于实现了模型的层叠，即第一层的模型输出作为第二层模型的输入，第二层模型的输出作为第三层模型的输入，依次类推，最后一层模型输出的结果作为最终结果。本文会以两层的\nstacking 为例进行说明。\nstacking\n的思想也很好理解，这里以论文审稿为例，首先是三个审稿人分别对论文进行审稿，然后分别返回审稿意见给总编辑，总编辑会结合审稿人的意见给出最终的判断，即是否录用。对应于 stacking，这里的三个审稿人就是第一层的模型，其输出（审稿人意见）会作为第二层模型（总编辑）的输入，然后第二层模型会给出最终的结果。\nstacking\n的思想很好理解，但是在实现时需要注意不能有泄漏（leak）的情况，也就是说对于训练样本中的每一条数据，基模型输出其结果时并不能用这条数据来训练。否则就是用这条数据来训练，同时用这条数据来测试，这样会造成最终预测时的过拟合现象，即经过 stacking 后在训练集上进行验证时效果很好，但是在测试集上效果很差。\n为了解决这个泄漏的问题，需要通过 K-Fold\n方法分别输出各部分样本的结果，这里以 5-Fold 为例，具体步骤如下\n\n将数据划分为 5 部分，每次用其中 1 部分做验证集，其余 4\n部分做训练集，则共可训练出 5 个模型\n对于训练集，每次训练出一个模型时，通过该模型对没有用来训练的验证集进行预测，将预测结果作为验证集对应的样本的第二层输入，则依次遍历 5 次后，每个训练样本都可得到其输出结果作为第二层模型的输入\n对于测试集，每次训练出一个模型时，都用这个模型对其进行预测，则最终测试集的每个样本都会有 5 个输出结果，对这些结果取平均作为该样本的第二层输入\n\n上述过程图示如下\n\n\nstacking\n\n除此之外，用 stacking 或者说 ensemble\n这一类方法时还需要注意以下两点：\n\nBase Model 之间的相关性要尽可能的小，从而能够互补模型间的优势\n Base Model 之间的性能表现不能差距太大，太差的模型会拖后腿\n\n代码实现\n由于需要 stacking\n中每个基模型都需要对数据集进行划分后进行交叉训练，如果为每个模型都写这部分的代码会显得非常冗余，因此这里提供一种简便实现\nstacking 的思路。\n具体做法就是先实现一个父类，父类中实现了交叉训练的方法，因为这个方法对所有模型都是一致的，然后声明两个方法：train\n和\npredict，由于采用的基模型不同，这两个方法的具体实现也不同，因此需要在子类中实现。下面以\npython 为例进行讲解\nimport numpy as npfrom sklearn.model_selection import KFoldclass BasicModel(object):    \"\"\"Parent class of basic models\"\"\"    def train(self, x_train, y_train, x_val, y_val):        \"\"\"return a trained model and eval metric of validation data\"\"\"        pass    def predict(self, model, x_test):        \"\"\"return the predicted result of test data\"\"\"        pass    def get_oof(self, x_train, y_train, x_test, n_folds = 5):        \"\"\"K-fold stacking\"\"\"        num_train, num_test = x_train.shape[0], x_test.shape[0]        oof_train = np.zeros((num_train,))         oof_test = np.zeros((num_test,))        oof_test_all_fold = np.zeros((num_test, n_folds))        aucs = []        KF = KFold(n_splits = n_folds, random_state=2017)        for i, (train_index, val_index) in enumerate(KF.split(x_train)):            print('{0} fold, train {1}, val {2}'.format(i,                                                         len(train_index),                                                        len(val_index)))            x_tra, y_tra = x_train[train_index], y_train[train_index]            x_val, y_val = x_train[val_index], y_train[val_index]            model, auc = self.train(x_tra, y_tra, x_val, y_val)            aucs.append(auc)            oof_train[val_index] = self.predict(model, x_val)            oof_test_all_fold[:, i] = self.predict(model, x_test)        oof_test = np.mean(oof_test_all_fold, axis=1)        print('all aucs {0}, average {1}'.format(aucs, np.mean(aucs)))        return oof_train, oof_test\n上面最重要的就是进行 K-fold 训练的 get_oof\n方法，该方法最终返回训练集和测试集在基模型上的预测结果，也就是两个一维向量，长度分别是训练集和测试集的样本数。\n下面以两个基模型为例进行 stacking，分别是 xgboost 和\nlightgbm，这两个模型都只需要实现 BasicModel 中的\ntrain 和 predict 方法\n第一个基模型 import xgboost as xgbclass XGBClassifier(BasicModel):    def __init__(self):        \"\"\"set parameters\"\"\"        self.num_rounds=1000        self.early_stopping_rounds = 15        self.params = {            'objective': 'binary:logistic',            'eta': 0.1,            'max_depth': 8,            'eval_metric': 'auc',            'seed': 0,            'silent' : 0         }    def train(self, x_train, y_train, x_val, y_val):        print('train with xgb model')        xgbtrain = xgb.DMatrix(x_train, y_train)        xgbval = xgb.DMatrix(x_val, y_val)        watchlist = [(xgbtrain,'train'), (xgbval, 'val')]        model = xgb.train(self.params,                           xgbtrain,                           self.num_rounds)                          watchlist,                          early_stopping_rounds = self.early_stopping_rounds)        return model, float(model.eval(xgbval).split()[1].split(':')[1])    def predict(self, model, x_test):        print('test with xgb model')        xgbtest = xgb.DMatrix(x_test)        return model.predict(xgbtest)\n第二个基模型\nimport lightgbm as lgbclass LGBClassifier(BasicModel):    def __init__(self):        self.num_boost_round = 2000        self.early_stopping_rounds = 15        self.params = {            'task': 'train',            'boosting_type': 'dart',            'objective': 'binary',            'metric': {'auc', 'binary_logloss'},            'num_leaves': 80,            'learning_rate': 0.05,            # 'scale_pos_weight': 1.5,            'feature_fraction': 0.5,            'bagging_fraction': 1,            'bagging_freq': 5,            'max_bin': 300,            'is_unbalance': True,            'lambda_l2': 5.0,            'verbose' : -1            }    def train(self, x_train, y_train, x_val, y_val):        print('train with lgb model')        lgbtrain = lgb.Dataset(x_train, y_train)        lgbval = lgb.Dataset(x_val, y_val)        model = lgb.train(self.params,                           lgbtrain,                          valid_sets = lgbval,                          verbose_eval = self.num_boost_round,                          num_boost_round = self.num_boost_round)                          early_stopping_rounds = self.early_stopping_rounds)        return model, model.best_score['valid_0']['auc']    def predict(self, model, x_test):        print('test with lgb model')        return model.predict(x_test, num_iteration=model.best_iteration)\n下一个步骤就是将这两个基模型的输出作为第二层模型的输入，这里选用的第二层模型是\nLogisticsRegression，\n首先需要将各个基模型的输出 reshape 和\nconcatenate 成合适的大小\nlgb_classifier = LGBClassifier()lgb_oof_train, lgb_oof_test = lgb_classifier.get_oof(x_train, y_train, x_test)xgb_classifier = XGBClassifier()xgb_oof_train, xgb_oof_test = xgb_classifier.get_oof(x_train, y_train, x_test)input_train = [xgb_oof_train, lgb_oof_train] input_test = [xgb_oof_test, lgb_oof_test]stacked_train = np.concatenate([f.reshape(-1, 1) for f in input_train], axis=1)stacked_test = np.concatenate([f.reshape(-1, 1) for f in input_test], axis=1)\n然后用第二层模型进行训练和预测 from sklearn.linear_model import LinearRegressionfinal_model = LinearRegression()final_model.fit(stacked_train, y_train)test_prediction = final_model.predict(stacked_test)\n上述实现的完整代码见下面的链接\nhttps://github.com/WuLC/MachineLearningAlgorithm/blob/master/python/Stacking.py\n如有错漏，欢迎交流指正\n\n参考\nIntroduction\nto Ensembling/Stacking in Python 如何在\nKaggle 首战中进入前 10%\n","categories":["机器学习"],"tags":["机器学习","python"]},{"title":"vim 编辑器使用","url":"/2015/11/30/vim%E7%BC%96%E8%BE%91%E5%99%A8/","content":"vim 是 Linux 下非常常用的一个编辑工具，所以有必要了解一下 vim 的一下使用\n技巧。\n\n这里将 vim 的状态分成两大类：命令模式和编辑模式。当你输入命令 vim filename 时就进入了命令模式，而此时按下 i 即可进入编辑模式，进入编辑模式后可同过按 ESC 退回命令模式\n跳到某一行\n\n跳到第一行：在命令模式下，连续两次按下 g\n\n跳到最后一行：在命令模式下，shift+g 即可\n\n跳到某一行：命令行模式下，通过冒号加行号即可实现，如:15 即可跳到第 15 行。\n\n跳到当前行往上或往下的 n 行：命令模式下，先按数字 n，然后按往上或往下的按钮（就是键盘上的上下左右的按钮）\n\n跳到一行的开头：可以使用键盘上的编辑键 Home, 也可以在命令模式中使用快捷键 \"^\"（即Shift+6）或 0（数字 0)\n\n跳到一行的末尾：以使用编辑键 End，也可以在命令模式中使用快捷键 \"$\"（Shift+4）。快捷键 \"$\" 前可以加上数字表示移动的行数。例如使用 \"1$\" 表示当前行的行尾，\"2$\" 表示当前行的下一行的行尾。\n\n查找字符串\n\n往前查找：命令模式下输入 ?word, 按 n 往前查找下一个，shift+n 往后查找下一个\n\n往后查找：命令模式下输入 /word, 按 n 往后查找下一个，shift+n 往前查找下一个\n注：这里的往前和往后指的是文本的顺序，其实只需要记住按 n 是顺序查找，shift+n 是逆序查找即可\n\n删除\n\n删除一行：命令模式下，光标定位到要删除的那一行，连续按下两次 d 即可\n\n删除多行：命令模式下，光标定位到要删除的那一行，先按下数字 n，然后连续按下两次 d，表示删除 n 行（包括当前行）\n\n修改\n\n替换字符串：命令模式下输入:n1,n2s/w1/w2/g, 表示将 n1 到 n2 行的 w1 转为 w2（1 代表第一行，$ 代表最后一行，没数字为整个文本），注意前面有冒号\n\n复制：命令模式下，将光标移到需要复制的那行，然后连续两次按下 y，即可复制当前行，如果要复制多行，在按下 yy 前需要按下数字 n，表示复制包括当前光标一下的 n 行，原理同删除操作\n\n块复制：命令模式下，通过 ctrl+v 进入块复制模式，选择高亮后按 y 复制\n\n剪切：实际上上面提到的删除命令除了将所选的内容删除掉，还将其复制到了剪切板上，按下 p 即可粘贴；所以剪切实际上就是 dd+p。\n\n粘贴：命令模式下，按下 p 即可将已经复制或剪切的内容复制到光标所在行下面的的一行或多行\n\n撤销与恢复前一步的操作\n\n撤销前一步的工作：命令模式下按 u\n\n撤消后恢复前一步的工作：命令模式下按 Ctrl+u\n\n另存为\n\n命令模式下输入:n1,n2 w new_file_name 可将修改文本 n1 到 n2 行（不加数字为整个文本）另存为其他文件\n\n","categories":["Linux"],"tags":["Linux","工具使用"]},{"title":"《Advanced Web Metrics with Google Analytics》读书笔记 (1)","url":"/2016/01/20/%E3%80%8AAdvanced%20Web%20Metrics%20with%20Google%20Analytics%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89/","content":"《Advanced Web Metrics with Google Analytics》是 Google\n一位数据分析专家 Brian Clifton\n出版的书，主要介绍了涉及网站分析的一些概念和方法以及如何利用 Google\nAnalytics 进行网站分析。Google Analytics 是 Google\n免费提供的一个用于网站分析的工具。\n\n本系列文章是笔者在阅读本书过程中总结整理的一些笔记。\n本文是本书中 Chapter\n1 的阅读笔记，主要介绍了网站分析的一些概念以及为什么要进行网站分析。\n为什么要进行网站分析\n在进行网站分析前需要考虑的一个问题是通过网站分析能够给你带来的价值，这种价值可以表现在商业上，如某家公司的网站是否能够给实际销量带来提升，也可以表现在个人上，如个人网站的访问量、影响力如何。\n网站分析能够获取的信息\n一般的网站分析均能够获取下面的信息：\n\n每天的访客人数\n\n平均转化率（conversion rate），就是销量、注册量和下载量等\n\n网站上访问最多的页面\n\n访客的每次平均访问时间和访问的频率\n\n访客的地理分布\n\n假如你的网站是商业网站，还能够获取以下信息：\n\n网站产生的盈利\n\n网站的顾客来自哪里\n\n网站最畅销的商品是那些\n\n随着挖掘的深入还可以获取到以下的信息：\n\n一个顾客对于网站的价值\n\n如何量化一个网页的价值\n\n老顾客和新顾客在使用网站的方式上是否有区别\n\n访问量以及转化率如何被将顾客引导到本站的外站的影响\n\n页面跳出率受何影响\n\n站内搜索是否对转化率有利\n\n平均需要多长时间才能让一位访客成为一位顾客\n\n如何利用网站分析得到的结果\n　　从前面的分析可知，网站的分析能够提供很多信息，这些繁杂的信息会让许多初学者感到无从下手。因此，在进行网站分析前一定要确定希望通过网站分析达到的目标。\n　　目标根据网站具体类型有所区别，总的来说，目标就是希望访客在离开网页之前要完成的内容。如对于购物网站，目标可能是希望顾客购买了产品，对于个人网站而言，可能希望顾客能够对网站留言给出建议等。更进一步说，目标就是在访客好你的网站之间建立起的任何一种关系，而不是仅仅一个 PV 量，如留言，订阅网站 RSS，下载了一个 PDF 文件等。\n　　有了目标，便可以从复杂繁多的数据中找到所需要的数据。进而分析这些数据，并采取相应的措施。如最简单的购物网站需要判断那种物品销量较好，哪种较差，进而修改相应的进货量；便可商品的购买量、PV 量等进行分析。\n上面只是对本书的内容做了一个很笼统的总结，具体的实现方法、原理及注意事项会在后续文章介绍。\n因为这本书主要就是讲述通过 Google Analytics\n进行网站分析，所以有必要了解一下 Google Analytics\n中的一些专用术语。下面是 Google Analytics 中一些常用的术语，更详细的可见\nhttp://   www.google.com/support/googleanalytics/bin/topic.py?topic=11285\n\n\n\n\n\n\n\n术语\n含义\n\n\n\n\n Bounced Visitor\n 只在你的网页上浏览过一次的访客，这个数值当然越小越好\n\n\n Google Analytics Tracking\nCode（GATC）\nGoogle Analytics\n工具提供的一段 js 代码，用来追踪网站的访问情况\n\n\n Goal Conversion\n 也可简称为 goal 或\nconversion，表示希望网站达到的一个目标，如购买页面的一次访问量或一次下载\n\n\n Funnel\n 表示达到 Goal\nConversion 所需要经过的一个流程\n\n\n Landing Page\n 网站的首页\n\n\n Referrer\n 含有你的网站超链接的页面\n\n\n Return On Investment(ROI)\n 检验网站分析效果的一种指标，计算公式：纯利润 / 支出\n\n\n Session\n 也叫会话，指一个访客在网站停留的时间，一个会话在访客关闭页面后结束，也会在访客在一段时间内对网站无任何操作情况下结束\n\n\n Site Search\n 网站的内部搜索功能\n\n\n\n","categories":["Google Analytics"],"tags":["Google Analytics"]},{"title":"《Bid Optimization by Multivariable Control in Display Advertising》阅读笔记","url":"/2020/07/19/%E3%80%8ABid%20Optimization%20by%20Multivariable%20Control%20in%20Display%20Advertising%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"推荐与广告可以说是很多互联网公司的两个重要业务，其中推荐是为了 DAU\n的增长，或者说流量的增长，而广告则是利用这些流量进行变现。两者的要解决的问题也很相似，都是在每条流量到来的时候，要从一个庞大的候选集中选出\ntopk 个候选返回，基本都采用 召回 + 精排\n的架构，中间还可能插入粗排，本质上都是在效果与工程之间做 trade-off。\n如果说两者技术上最大的\ndiff，笔者认为是出价，因为在广告场景中引入了广告主 (advertiser) 这一角色，因此我们除了考虑用户体验，还需要满足金主爸爸们的诉求（如跑量、成本等），才能带来持续的收入增长，而金主爸爸们表达其诉求的最直接的手段就是出价，其含义就是愿意为每个\nclick/convert 付出多少钱 (truthful telling)。这带出来的就是 bidding\n这一研究领域，关于这个领域在 rtb-papers\n中有很多相关的 paper。\n本文主要讲的是 2019 KDD 阿里的 Bid Optimization by\nMultivariable Control in Display Advertising，这篇 paper\n解决了出价的两个的核心问题：出价公式和调价策略，从最优的出价公式的推导到出价控制器的构建，文章的总体的建模思路非常值得学习，整个推导的\nparadigm 能够推广到更一般的出价场景，实践性也较强，推荐读原文。\n\n出价从技术上可认为主要由两大部分组成：出价公式和控制器，比如说常见的\ncpc 出价公式是 bid * ctr, 最常见的控制器则是 PID。出价公式我们能比较好理解，那为什么要控制器来调价而不是按照广告主给的出价来投呢？笔者认为主要有以下两个原因\n\n为了满足广告主的各种诉求；如需要匀速投放时，即在一天内均匀地花完预算，这样就需要通过控制器来控制运算花费曲线趋势与大盘流量曲线的趋势保持一致；如需要保成本时，需要通过控制器在成本高了时压价，\n低了时提价；需要跑量同时允许在成本有略微上涨时，可以在成本可控的情况下更激进一点出出价\n\nctr/cvr 的预估不是完全准确的；常见的 ctr/cvr\n高估时，容易导致超成本，因为这时候计算出来的 ecpm = bid×ctr×cvr\n也相当于是高估了；其根本原因是在 ctr/cvr 预估中没有一个绝对的 ground\ntruth, 我们能拿到的是点击 / 转化与否，但是要预估的则是点击 / 转化的概率\n\n前面提到，paper\n中主要讲了两部分内容，最优出价公式的推导和控制器，下面描述的内容也会主要从这两方面进行描述\n最优出价公式\npaper\n中要解决的场景是在保住点击成本和预算时，最大化广告主的在所有参竞价值 (value)，因此这个优化问题可写成如下形式\n\n\nLP1\n\n上式中的各个符号含义如下\n\n\\(N\\),\n广告计划的总的可参竞次数 (opportunities)\n\n\\(x_i\\), 第 i\n次竞价获胜的概率\n\n\\(wp_i\\), 第 i 次竞价的 winning\nprice，即 bid price 要大于等于这个值才能获胜\n\n\\(B\\), 计划的总预算\n\n\\(C\\), 计划设置的点击成本\n\n值得注意的是，我们不需要直接求解出上面的最优化问题的解，而只是需要求出取值为最优时的的解形式，然后作为最优出价公式，也就是说我们并不关心上面的\n\\(x_i\\)\n的最优解取值，而关心的是其他变量满足什么样的形式时，\\(x_i\\) 的解是最优的\npaper 原始推导\npaper 通过最优化中的对偶理论将上面的原问题 (primal\nproblem) 转为对偶问题 (dual problem)，如下图所示；对偶理论的详细描述可参考\n最优化计算课程总结\n中对偶理论这一节内容或 Duality\nTheory 这个讲义\n\n\nLP2\n\n上图中的 \\(p,q,r\\)\n都是对偶问题中变量，对应于原问题中的三类约束：预算，成本和对 \\(x\\)\n的范围的约束，根据互补松弛定理 (Complementary Slackness)\n，可得到下面两个式子，互补松弛定理的详细描述同样可参考上面的两个链接\n\n\nComplementary Slackness\n\n上面的公式中 \\(x_i^{*}\\) 和 \\(r_i^{*}\\)\n分别表示原问题和对偶问题的最优解，后面带 *\n上标的符号均表示最优解，至此为止，上面都是基于最优化理论推导出来的一些公式，但是接下来这一步就有点跳了，paper\n中直接令最优出价公式为\n\n则上面那些公式中表示给广告主带来的价值 \\(v_i\\) 可写成 \\(v_{i} = bid_{i} - wp_{i}\\),\n将公式（6）代入这个式子，再将 \\(v_i\\)\n代入上图中的公式 (8)，可得到下面的公式（10）及其分类讨论的结果\n\n\n分类讨论结果\n\n上面的两个分类讨论的结果实际上表明了无论最优解 \\(x^*\\) 是赢得这次竞价 (\\(x=1\\)) 还是输掉这次竞价 (\\(x=0\\))，按照公式 (6) 进行出价时，总能保证解是最优的\n另一种推导思路\n之所以说上面最后的推导有点跳，是因为公式 (6) 所表示的那一坨最优出价公式是怎么给出来的？随便拍脑袋似乎不太可能，paper\n中并没有针对这一点详细描述，笔者在这里尝试提供另一个求解的思路，就是利用拉格朗日对偶来推导出最优出价公式，关于这部分内容可参考\n凸优化总结\n中的拉格朗日对偶部分\n对于原问题即上图中的 LP1， 可写出其増广拉格朗日函数为 (暂时忽略 \\(x\\) 的值的约束)\n\\[\\begin{align} L(x,p,q) = \\sum_{i=1\\dots\nN}-x_iCTR_iCVR_i + p(\\sum_{i=1\\dots N}x_iwp_i-B) + q(\\sum_{i=1\\dots\nN}x_i(wp_i-CTR_iC)) \\end{align}\\]\n上式中的 \\(p&gt;=0,q&gt;=0\\)，则原问题可写成如下形式，\n详细推导过程可参考上面的凸优化总结的链接\n\\[\\begin{align} \\max_{p,\nq}\\min_{x_i}L(x,p,q) \\end{align}\\]\n如果是要求出这个问题的解，还需要转为拉格朗日对偶问题，通过 SMO\n等算法进行求解，但是我们这里不需要确切的解，而是只需要最优解的表达式，因此可令\n\\(\\frac{\\partial{L(x,p,q)}}{\\partial{x}}=0\\),\n求解可得\n\\[\\begin{align} wp_i = \\frac{1}{p+q} ×\nCTR_{i} × CVR_{i} + \\frac{q}{p+q} × C × CTR_{i} \\end{align}\\]\n可以看到，winning price \\(wp_i\\) 推导出来的形式跟上面给的公式 (6)\n中的最优出价公式的形式是一样的！！！\n但是上面的推导中的仍有一些问题，上面求解的是最优的 \\(wp_i\\)，bid\n这个变量没有显示地出现在增广拉格朗日函数中，但是可近似认为两者是相等的；其次是，\\(x_i\\) 本身的约束 (即取值在 0~1\n之间) 没有显式的考虑进增广拉格朗日函数中，针对这一点，笔者的理解是在解决最优化问题时可以忽略某个条件，然后在求解出来之后基于这个条件做分类讨论，而这一部分实际上就是原始\npaper 在推导出公式 (10) 后做的分类讨论。\n小结\n综合考虑上面两部分的推导，可以得到最优的出价公式如上面公式 (6) 所示，公式中的\n\\(p\\) 和 \\(q\\)\n是两个超参，是对偶问题中需要求解的变量，如果需要求解，意味着需要拿到参竞的所有后验数据，但是实际中在参竞时就需要通过这些参数给出参竞的\nbid，这似乎就成了一个先有鸡还是先有蛋的问题了，后面会通过控制器描述如何解决这一问题，具体思想就是不直接求解原始的最优化问题，而是通过近似的方式来逐渐逼近最优解\n回到公式 (6) 的最优出价公式，如果将其写成 c_bid * ctr\n的形式，有如下公式\n\n\nclick bid\n\n如果更直观的画出来如下图所示，从图中可知，哪怕 CVR 为 0 时，bid\n也不一定为 0，这跟常见的 ecpm = bid×ctr×cvr 不太一样，这可理解为一些 cvr\n低但是 ctr 高的流量也是可以拿的；此外，c_bid 的直线会过两个定点： (-qC,\n0) 和 (pC, C), 后面在控制器中会详细描述这两个点的具体含义。\n\n\nbid_optimization_cbid_graph\n\n调价策略\n前面提到，最优出价公式中的 \\(p\\) 和\n\\(q\\)\n的最优解求解需要拿到参竞后的后验数据，但是 bid\n是要在参竞的时候就给出来，因此这就成了一个先有鸡还是先有蛋的问题。针对这个问题，最直观的一个想法是，我们 可不可以用历史数据来求出最优的\n\\(p\\) 和 \\(q\\), 然后应用到下一时刻的出价中？\npaper 中也提到了这一点，答案是\nno，因为这个方法假设了参竞流量的分布是基本不变的，但是竞价环境是一个受多个因素影响的动态变化环境 (包括参竞流量、ctr、cvr\n等)，即历史的最优不会是未来的最优。在实际的实践中，笔者的实践经验的确是这样的。\n由于竞价环境是实时变化的，因此需要动态调价，在调价策略中，需要先明确两个点：调控的目标和调控的变量。以最经典的\nPID 控制器为例，如下图所示，调控的目标是 \\(r(t)\\) 与 \\(y(t)\\) 尽可能接近，调控的变量是 \\(x(t)\\)。\n\n\npid\n\n每次调整变量时，需要通过公式 (13) 计算出当前的\nerror，然后通过公式 (14) 计算出控制型号 \\(u(t)\\)（其中 \\(k_p\\)、\\(k_i\\)、\\(k_d\\) 是三个拍定的超参），最后利用 \\(u(t)\\) 通过公式 (15) 的调控公式 (actuator\nmodel) \\(\\phi(x(0), u(t))\\)\n得到最终调整后的值\nPID\n对于单变量的控制可以说是比较常用和有效的方法，但是我们的上面提到的问题是一个多变量控制问题，其中控制变量是\n\\(p\\) 和 \\(q\\)，控制目标是控制好预算花费并保点击成本 ;\n对于这个问题，一个很直观的想法就是通过双 PID\n分别进行独立调控，但是这些控制变量之间往往不是相互独立的，因此双\nPID 不是最优的 , 后面会详细描述这一点。\n参数分析\n下面首先会分析控制变量 \\(p\\) 和 \\(q\\) 分别影响哪些控制目标\n如下图 4 是固定 \\(q\\)，改变 \\(p\\) 时，c_bid\n的变化，其中虚线表示改变前的出价，实线表示改变后的出价；从图中可知\n\n出价的直线始终通过 (-qC, 0) 这个点\n\n随着 \\(p\\)\n的减小，出价的直线的斜率逐渐增大，表示出价更高，同时消耗的\nbudget 也会更多；而随着 \\(p\\)\n增大导致的结果则是刚好相反\n\n当 \\(p\\) 取最小值即 0\n时，表示没有 budget 的限制，出价公式退化为 \\(C×CTR+\\frac{1}{q}CTR×CVR\\),\n公式第一项可以认为是只按照点击出价来保点击成本 \\(C\\)，第二项则是为了达到 \\(\\max v=CTR×CVR\\) 的目标\n\n\n\n q fixed\n\n同理可画出下图 5 中固定 \\(p\\)，改变\n\\(q\\) 时，c_bid 的变化；\n从图中可知\n\n出价的直线始终通过 (pC, C) 这个点\n\n随着 \\(q\\)\n的减小，出价的直线的斜率逐渐增大，表示对于 CVR 比 pC\n更高的流量出价更高，CVR 比 pC 更低的流量出价更低 , 而随着\n\\(q\\) 增大导致的结果则是刚好相反\n\n当 \\(q\\) 取最小值即 0\n时，表示没有点击成本的限制，此时的出价公式退化为 \\(\\frac{1}{p}CTR×CVR\\)，代表出价成本的符号\n\\(C\\)\n没有出现的出价公式中，总体表示要达到 \\(\\max\nv=CTR×CVR\\) 的目标，同时通过 \\(q\\) 来控制预算\n\n\n\n p fixed\n\n控制器\n通过上面的分析可知，参数 \\(p\\)\n是被用来控制预算的使用，而参数 \\(q\\)\n则是被用来控制点击成本；其实这与我们上面推导最优出价公式时对应的约束条件是一致的。\n因此，一种最简单的策略是用两个独立的 PID 来分别调控变量 \\(p\\) 和 \\(q\\)，调控的目标则是预算和点击成本。如下图所示\n\n\nindependent pid\n\n但是我们前面也提到，这两者并不是完全独立的，比如说为了保点击成本进行提价或降价也会影响到预算的使用，反之亦然。因此两个完全独立的\nPID 并不是最优的选择，而是在控制中要考虑两个变量相互的影响\nModel\nPredict Control(MPC) 中有关于这类问题的研究，但是 paper\n中并不直接采用这个方法，因为 paper 中认为 \"modelling the highly\nnon-linear RTB environment is costly and even impratical\"；\n而是通过一个线性模型去拟合。笔者认为其可行的原因是调控往往会分为多个时间片，然后在每个时间片内进行调控，而在每个时间片内用直线去拟合，理论上只要把时间切得足够小，最终总体也能拟合出非线性的曲线。\n主要的建模思想是通过两个 linear regression model 直接建模变量\n\\(p、q\\) 和目标 cost、CPC\n的关系 (即原文的 model the bidding environment with respect to\ncost and CPC）具体的做法如下\n\n\nmulti-var control\n\n上图中的公式 (16) 里的 \\(X\\) 和 \\(b\\) 分别表示 2×2 的矩阵和 2×1\n的矩阵，如果展开后其实就是两个 linear regression model\n进一步地，公式 (17) 表示的是给定需要控制的目标 \\(\\Delta cost\\) 和 \\(\\Delta\nCPC\\)（调价是分时间片进行调控的，在每次调控前都可以根据当前累积消耗和成本等后验数据，进而进行计算当前时间片需要调控得到的\n\\(\\Delta cost\\) 和 \\(\\Delta CPC\\)），可以对 p 和 q 分别进行\n\\(\\Delta p\\) 和 \\(\\Delta q\\)\n的调控达到目标，笔者这里有个疑问，为什么这里的 b\n可以被约去，\n虽然 paper\n中没有直接提到，但是笔者认为其实到了公式 (17) 已经可以进行调控了，只是调控的方式跟\npaper 中不太一样，这里简单描述一下。首先需要获取公式 (17) 中\n\\(X\\)，而 \\(X\\)\n中的参数其实是可通过训练数据获取，训练的数据集 从当前时间往前的若干个个时间片内的\n(\\(\\Delta p\\), \\(\\Delta q\\), \\(\\Delta p\\), \\(\\Delta q\\))，然后 \\(X\\) 就可以通过常规的训练方式获取；这样在每个时间片调价时，只需要计算好的\n\\(X\\) 和下一时间片的调控目标：\\(\\Delta cost\\) 、 \\(\\Delta CPC\\) ，便能通过 grid search\n得到最优的 \\(\\Delta q\\) 和 \\(\\Delta p\\)。\n公式 (18) 是在公式 (17) 基础上乘上矩阵 \\(X\\) 的逆便得到；公式 (19) 则是 paper\n中提出的调控方式：首先通过上面的 PID 调控公式 (14) 可以将公式 (18) 中的 \\(\\Delta cost\\) 、 \\(\\Delta CPC\\) 变为 \\(u_{p}(t)\\)、\\(u_{q}(t)\\); 同时只用两个变量 \\(\\alpha\\) 和 \\(\\beta\\) 来近似矩阵 \\(X\\) 的逆（理论上应该有 4 个的），并认为\n\\(p\\)、\\(q\\) 的 control signal (通过公式 (14) 获取)\n\\(u'_{p}(t)\\)、\\(u'_{q}(t)\\) 是 \\(u_{p}(t)\\)、\\(u_{q}(t)\\) 的线性组合；paper\n称这样做的好处是 makes the controller more robust and stable\nagainst the changing environment, paper 中称 \\(\\alpha\\) 和 \\(\\beta\\)\n是从训练集中获取的 (详见下面的实验效果环节)\n因此，根据公式 (19), 总体的调控系统变为\n\n\nrelated pid\n\n实验设置与效果评估\n基本设置\npaper 中并没有进行在线实验，而是通过离线方式进行评估\n采用的数据集是 taobao 40 个计划总共 20M 条 bid log，每条 bid log\n的关键信息是 \\(wp_i、\nCTR、CVR\\)；同时根据时间划分了训练集和测试集\n评估指标主要有两个\n\n\\(CPC_{ratio}\\):\n表示保住点击成本的计划的比例\n\n\\(Value_{ratio}\\)：表示按最优出价公式重新投放时，每个计划获取的 value 与其理论最优的比值 (\\(\\le 1\\))\n\n实现细节\n除此之外，paper 中通过 PID 计算 control signal 时，会对 err\n进行加权，而传统的 PID 中所有的 err\n的权值都是一样的，其加权方法如下公式 (21) 所示，而前一章提到的 \\(u_{q}(t)\\) 计算方式如公式 (22) 所示\n\n\nweighted_pid_err\n\n除了对 err 进行加权，上面的公式（20）也很有意思，就是每次通过 control\nsignal 调整目标 zhi 值 \\(x\\)\n时，不是基于上一次的值 \\(x(t)\\)，而是基于最开始的值 \\(x(0)\\)\n此外，调价的频率是每小时调整一次，PID 中涉及的超参 (\\(k_p、 k_i、 k_d\\)) 以及公式 (19) 中的 \\(\\alpha\\) 和 \\(\\beta\\) 都是在训练集中通过 grid serach\n找到的\n因此，总体的实验步骤如下\n\n基于训练数据集计算出最优的 \\(p\\) 、\\(q\\) 作为其初始值\n\n在测试数据集上用上面的最优出价公式和调价策略进行模拟竞价\n(3）当计划的 budget 消耗完或者所有的日志回放完成则终止\n\n效果对比\n实验效果如下图所示，其中图 8 是 budget 消耗的情况，图 9\n是成本情况，同时左边的图是实时指标，右边的图是累积指标；从图中可知，budget\n消耗的幅度无论是实时还是累积都拟合得比较好，而点击成本虽然在实时上由波动，但是累积的成本是比较稳定的。\n\n\nperformance\n\n除此之外，paper 还与其他的一些方法进行了比较，结果如下图所示，其中\nI-PID 和 M-PID 是本文提出的方法，分别表示 PID\n是否相互独立的，从结果可知，采用了 paper\n中的出价公式且考虑控制变量的关系的调价方式的效果是最好的，也印证了我们前面提到的两个完全独立的\nPID 并不是最优的选择，而是在控制中要考虑两个变量相互的影响\n\n\nevaluation result\n\n小结\n综上，这篇 paper\n首先将要求解的方法建模成一个最优化的问题，然后通过对偶理论求解出这个最优化问题的最优解的形式，而不是它的解；接着描述了多变量调控系统中一种考虑控制变量间相互关系的调控策略，并通过实验结果证明这种策略的效果要优于只通过两个独立的\nPID\n分别调控的效果；美中不足的是没有做在线实验，毕竟离线实验的环境变基本就定下来了，重新进行竞价也不会改变这个环境，不像线上变化那么剧烈；但是\npaper 中的总体建模思路还是非常值得学习的，可以作为一个 paradigm\n推广到更一般的出价场景。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"《Advanced Web Metrics with Google Analytics》读书笔记 (2)","url":"/2016/01/22/%E3%80%8AAdvanced%20Web%20Metrics%20with%20Google%20Analytics%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89/","content":"《 Advanced Web Metrics with Google Analytics 》是 Google\n一位数据分析专家 Brian Clifton\n出版的书，主要介绍了涉及网站分析的一些概念和方法以及如何利用 Google\nAnalytics 进行网站分析。Google Analytics 是 Google\n免费提供的一个用于网站分析的工具。\n\n本系列文章是笔者在阅读本书过程中总结整理的一些笔记。\n本文是本书中 Chapter 2\n的阅读笔记，主要介绍了网站分析常用的两种途径以及网站分析的数据的准确性。\n网站分析的两种手段\n页面标签（page\ntags）和日志文件（logfiles）是网站分析的两种常用手段。\n页面标签一般是在 html 页面中嵌入 js 代码段，浏览页面时 js 代码会将浏览记录发送到远端服务器，远端服务器将对浏览页面进行统计，并提供可视\nweb 界面，GoogleAnalytics 就是属于这种手段。\n日志文件则是利用 web\n服务器软件（如 Apache、Tomcat 等）产生的日志文件进行统计，日志文件能够记录访问的 ip 和页面等信息；然后在本地服务器上统计并生成所需结果。\n除了上面两种途径，还可通过网络流量情况、web 服务器软件提供的 api 等方式进行数据的采集和统计。但是最常用的手段就是上面提到的两种。所以下面详细列出这两种方法的优点点和不足。\n\n\n\n\n\n\n\n\n方法\n Page Tags\nLogfiles\n\n\n\n\nAdvantages\n(1) 不受代理服务器和缓存服务器的影响\n(2) 能够实时处理客户端产生的数据(3) 数据采集程序的升级以及数据的存储仅需服务商提供\n(1) 能够保留历史数据(2) 不会受到防火墙影响(3) 跟踪带宽及下载情况，区分完全下载和部分下载(4) 能够跟踪网络爬虫的来源\n\n\nDisadvantage\n(1) 部署的不恰当或导致数据的丢失且无法重新找到(2) 防火墙可能会拦截这种方法的流量(3) 不能够获取带宽或者下载具体情况，即不能确定是否下载成功(4) 不能跟踪网络爬虫的来源\n (1) 代理服务器和缓存服务器会影响数据准确性(2) 需要维护服务器的程序正确性以及数据的存储等(3) 网络爬虫会使得访问者的数量比实际的大\n\n\n\n从上面的比较分析可以看到，两种方法的优点和缺点几乎是互补的，所以在实际中不需要局限于一种方法，而可以混合多种方法进行分析。\nCookies 在网站分析中的作用\n页面标签（page tags）是通过 cookies 来追踪访客的。cookies\n是以文本方式存储在客户端本地的一系列 key-value\n对，用来存储客户端的一些信息，每次客户端请求服务器端的资源时，服务器端都可以获取客户端对应的 cookie，不同的\ncookie 根据不同的域名来区分。\ncookies 有几种分类：\n如根据生存时间可以分为永久 cookies （persistent\ncookies）和 会话 cookies （session cookies），永久 cookies\n指那些关闭浏览器后再重新打开时依然有效的 cookies（前提是客户端不会清理 cookies），而\n会话 cookies 则指那些只在浏览网站期间有效的 cookies，关闭浏览器后 cookies\n会自动失效。\n根据来源可以分为第一方 cookie （first-party\ncookie）和第三方 cookie （third-party\ncookie），第一方 cookie 指你访问的 url\n所属的域名在你的电脑上留下的 cookie，我们之前已经提到过：不同的 cookie 通过不同的域名进行识别。第三方\ncookie\n则刚好相反，是你访问一个 url 后所获得的 cookies 中，那些不属于这个 url 所属域名的 cookie，如一些网页中嵌入的一些广告可能会留下 cookies，这些 cookies 不属于所访问的页面所属的域名，这个就可以算为第三方\ncookie。两者的区别在于第一方 cookie\n只能由设定这个 cookie 的域名所获取；而第三方 cookie\n则允许列出可以获取这个 cookie 的所有域名。\ncookies\n在网站分析中的一些常见作用包括：能够判断访客是否第一次访问这个网站，每隔一定时间会有多少访客再次访问，每位访客具体的访问间隔时间等。\n数据准确性\n无论是网页标签（page\ntags），还是日志文件（logfiles），任何一种手段均存在采集数据的不准确性问题。下面分别讲述这两种方法存在一些问题以及解决方法\n日志文件中的数据不准确性\n动态分配 IP\n动态分配 IP 会使得统计出来的访客人数比实际要大。\n现今为家庭提供网络服务的 ISP\n一般都是分配动态 IP, 美国曾统计过（http://www.comscore.com/Press_Events/Presentations_Whitepapers/2007/Cookie_Deletion_Whitepape）一个家庭平均每个月会使用 10.5 个 IP 地址，这会导致通过日志文件统计出来的访客数量要大于实际的。因为日志文件是通过不同 IP 来区分不同的访客的，按照上面的情况，会将一个访客统计为 10 个访客，因为这个人在访问网站时 IP 会变化。\n这类问题可通过 cookie 来解决。\n缓存\n缓存会使得统计出来的访客人数比实际要小。\n缓存又可以分为客户端的缓存和服务器端的缓存。\n客户端的缓存指的是用户在访问网站后，用户使用的浏览器会缓存其访问过的某些页面，使得用户再次访问这个页面时浏览器可以从本地获取，从而加快其访问速度。但是这样服务器的日志文件就无法记录这次的访问信息了。\n服务器端的缓存的作用与客户算的类似，也是将一些常被访问的页面缓存起来，加快客户端的访问速度，我们常听到的 CDN 就是其中的一种。\n对于这种问题，貌似目前还没有比较好的解决方法。\n网络爬虫\n网络爬虫会使得统计出来的访客人数比实际要大。\n网络爬虫在搜索引擎等领域使用得非常广泛，可以理解为通过程序获取页面信息。这会产生大量非实际访客访问的 PV 量，这就导致了统计出来的 PV 量比实际的要大。\n可以通过追踪爬虫的来源从而在日志中过滤掉这个爬虫的访问记录。但是因为爬虫的数量很多，往往难以完全过滤掉所有爬虫的访问记录。\n网页标签中的数据不准确性\n代码部署不全\n因为网页标签（page tags）的方法是通过在网站的每个网页上嵌入一段 JS\n代码实现，所以在一开始部署这段代码的时候有可能会存在部署不全的情况，就是没有在每个网页上部署这段代码。这种情况在一些较大的网站上普遍存在。\nJS 代码发生错误\n除了采集访问信息的 js 代码外，网页中不可避免会有完成其他功能的 js 代码，这些 js 代码假如发生了错误并且在网页源码中的位置处于采集访问信息的 js 代码前，会导致浏览器解析脚本引擎停滞工作，从而在下面的采集访问信息的 js 代码段将没有执行。\n防火墙的阻挡\n因为网页标签（page\ntags）方法会将数据发送给指定的数据采集服务器，所以防火墙能够阻挡这一动作。除此之外，防火墙还能够阻挡或者自动删除 cookies。\nCookies 中的数据不准确性\n访客拒绝或删除 cookies\n因为 cookies 是存储在访客本地的电脑的，故可以将已有的 cookies 删除掉，也可以在浏览器设置中距拒绝 cookies。据调查显示，第一方 cookies 的接受率可达 95%，而第三方 cookies 则常被防火墙或反病毒软件拦截。\n访客有多台设备或共享一台设备\n这里的设备可以指电脑、平板、手机等，现今同一个人同时拥有这几种设备是很常见的事情，同时有些家庭也共用一台电脑，这就导致了下面可能存在的问题。\n同一个访客有多台设备：用这些设备访问统一网页时均会生成 cookies，这样统计时会将同一用户产生的三个 cookies 当做是不同用户产生的。\n多个访客共享同一台设备：这样访问一个同一个网站只会产生一个 cookies，但是分析 cookies 时只会将这个当做一个用户，显然分析结果不合理。\n解决这类问题可以设置用户登录这一步骤，从而在 cookies 中标记不同的用户区分 cookies 的来源。\n","categories":["Google Analytics"],"tags":["Google Analytics"]},{"title":"《Budget Pacing for Targeted Online Advertisements at LinkedIn》 阅读笔记","url":"/2018/10/25/%E3%80%8ABudget%20Pacing%20for%20Targeted%20Online%20Advertisements%20at%20LinkedIn%E3%80%8B%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"《Budget\nPacing for Targeted Online Advertisements at LinkedIn》 是 LinkedIn\n在 2014\n年发表的一篇关于预算控制的论文，里面的预算控制的策略并不复杂，并且具有很强的实践性和工程性。本文主要是根据论文总结了这个方法的基本原理、工程实现以及实验效果。\n\n顾名思义，预算控制（budget\ncontrol）在广告系统中的作用就是该如何合理花掉广告主的预算。在实际中经常会出现广告主预算消耗过快的问题，这会导致广告主早早退出竞价，不仅会影响广告主体验，也会导致整个广告生态的竞争力下降（因为那些有竞争力的广告主消耗早早就花光了），在二价的机制下，直接影响了平台的收入。论文为了解决这个问题，提出了一个\nbudget pacing 的算法。\n原理\n算法的主要思想就是令每个\ncampaign（推广计划）的消耗趋势与其曝光变化趋势基本保持一致，以天为时间单位，campaign\n为预算控制单位，首先为每个 campaign\n预测出其在当天的曝光情况；然后基于其曝光情况，在当前时间片，假如\n已消耗/当天预算 的比例大于 已曝光/预测的总曝光\n的比例，则说明预算已经消耗过快，需要减小消耗的速度，反之则要加快消耗的速度。\n下面详细讲述这个算法的原理\n对于某个 campaign \\(i\\)，记其出价为\n\\(b_i\\)，当天预算为 \\(d_i\\)。一天的时间被划分为 \\(T\\) 个时间窗口， \\(s_{i,t}(0 \\le t \\lt T)\\) 表示截止到第 \\(t\\) 个时间窗口开始时的累积预算，\\(f_{i,t}\\) 与 \\(s_{i,t}\\) 对应，表示截止到第 \\(t\\) 个时间窗口开始时的累积曝光 (\\(f_{i,t}\\) 是预测出来的，下式的 \\(f_{i,T}\\) 表示预测出来 campaign \\(i\\) 在当天的总曝光)。则在时间窗口 \\(t\\) 开始时，有\n\\[\\begin{align} a_{it} := \\frac{f_{i,\nt}}{f_{i, T}}d_i \\end{align}\\]\n根据上面的比例，在每次竞价开始时，为 campaign \\(i\\) 算出其参与这次竞价的概率 \\(p_{i,t}\\)，论文称这个概率为\nPTR（pass through rate）, 计算方式如下\n\\[\\begin{align}p_{i,t} =\n\\begin{cases} p_{i, t-1}*(1 + r_t)&amp; s_{i, t} \\le a_{i, t}\\\\\\\np_{i, t-1}*(1 - r_t)&amp; s_{i, t} \\gt a_{i, t}\n\\end{cases}\\]\n上式中的 \\(r_t(0 &lt; r_t &lt; 1)\\)\n称作调整速率（adjustment rate）。\n对于 campaign \\(i\\) , \\(a_{it}\\) 在当天开始已经确定，因为 \\(f_{i,t}\\) 是预测出来的，\n因此控制预算就完全是针对 \\(s_{i,t}\\)\n的变化进行。\n除了这种调整 campaign\n参与竞价概率的控制方式，某些文献也建议通过调整出价的方式进行干预，如下所示是论文\n[1] 提出的调价方式\n\\[\\begin{align} b_i^* = b_i \\psi(s_{i,\nt}/d_i) \\end{align}\\]\n其中 \\(\\psi(x) = 1 - e^{x-1}\\)，但是\nLinkedIn\n这篇论文的作者不建议采用这种方式，原因是对于那些快耗尽预算的 campaign，bid\n修改的幅度很小，且出价一般存在着保留价（reserve\nprice），因此可调整的幅度很小。论文也对这种方式做了实验，结果显示该方式对提升\ncampaign 的预算消耗时长无帮助。\n整个算法就是这么简单，下面主要说一下具体的实现细节。\n实现细节\n更新 PTR 频率\n更新 PTR 频率设置成每分钟一次，也就是说时间窗口的大小为 1\nmin，实验证明这个更新频率使得整个系统更快达到一个稳定状态。\n预估曝光量\n上面预估某个 campaign\n的曝光量可以说是整个算法至为关键的地方，论文并没有针对这一点提出自己的方法，而是采用了论文\n[2] 里面的方法，这篇论文是 Yahoo\n在做保量的合约广告时提出的预估流量的方法。这里不详细展开了，会另外写一篇博客加以阐释。\n调整速率的设置\n调整速率也就是上面的 \\(r_t\\)，目的是控制 PTR\n变化的快慢，论文将这个值设置为固定的\n10%，这不仅实现简单，鲁棒性也很强。另外一种更复杂的设置方法就是将这个值设置成\n\\(s_{i,t}\\) 的变化率，也就是 \\(\\partial s_{i,t}/\\partial t\\),\n表示消耗过快的 campaign\n其对应的调整速率也应该较大。然后论文还是选择了固定的 10%\n的值，原因有两个\n1）\\(s_{i,t}\\)\n表示的曲线并不光滑（一系列离散的点），尤其是对于 CPC\n这种有了点击才会扣费的广告，这时候的 \\(s_{i,t}\\) 波动会比较大，从而使得计算出来的\n\\(\\partial s_{i,t}/\\partial t\\) 会比较\nnoisy 2）PTR 的更新频率比较频繁，因此即使当前 PTR\n不在最合适的位置（\\(\\partial s_{i,t}/\\partial\nt\\)），也能够很快更新到理想位置\n设置 PTR 初始值（Slow Start）\n论文将每个 campaign 的 PTR 初始值设置为 10%，并将这种方式称为 slow\nstart，因为这个初始值较小。设置较小的初始值给予系统以时间来调整每个\ncampaign 的 PTR，反之若 PTR\n一开始就设置得很高，会导致预算很快被花光。\n同样，更合理的方式是为每个 campaign 设置一个\nPTR，但是论文并没有针对这一点进行深度的探讨。\nFast Finish\n由于系统存在统计偏差，使得 PTR\n的值偏低，这会导致当天预算没法完全花出去，而 fast finish\n就是针对这个问题的一种解决方法。具体的做法就是修改上面预测的 \\(f_{i,t}\\) (allocation\ncurve)，令最后两个小时的曝光量为 0，这样会导致 budget pacing\n这个算法每天会尝试在 22 小时内花光预算。\n工程上的设计\n下图是 LinkedIn 的广告系统概览图，advertiser action\n是指广告主的行为，包括创建 campaign、修改 bid 或 budget 等；ad requests\n则是指用户浏览而触发的广告请求。在 ad sever 中的 campaign index\n记录着每个 campaign 当前的状态（曝光，消耗等情况），pacing module\n会根据预设的更新频率从 database 中获取最新的数据来更新 campaign\nindex。\n\n\nengineering design\n\n需要注意的是，pacing module\n的更新并不是一次立刻完成，而是采用了较为平缓的方式，上文提到了 pacing\n的更新频率为每分钟一次，因此在实际更新是大概每 7s 更新 12%\n的 campaign；这种更新频率能够让系统的负载较为均匀，也能够较快达到一个稳定状态。\n实验的设计与效果\n由于以上的 pacing 方式是以 campaign 为单位的，因此实验会将所有的\ncampaign\n等分为两部分，分别作为实验组和对照组（当然，也可以采用灰度而不是全量的方式）。\n为了避免时间因素（weekly，seasonality）的影响，论文认为设计的实验至少要持续两周，如下图所示是一个\ncampaign 的实验设置情况，其中 On 表示采用上述的 pacing 算法，Off\n表示不采用。\n\n\ndesign of experiment\n\n则采用 pacing 的效果是标为 On\n的那些天的效果的均值，那么该采用哪些指标来评估效果？\n首先我们要认识到，在线广告是一个广告主、平台和用户的三方博弈过程，因此在考虑任意机制带来的收益或损失时都要同时考虑到这三方的利益；论文也是同时考虑了这三方的利益，设置了以下指标\n\n广告主的利益\n\n Campain life time：预算消耗 95% 所消耗的时间\n Unique impressions per spend：单位消耗给广告主带来的 unique\nuser 数量，计算方式 number of unique user/total spend\nNumber of campaigns：表示当天平台服务的 campaign 的数量\n\n平台的收益\n\n Cost per request：每次请求的平均收益，计算方式\ntotal revenue/number of requests\nOver delivery: 超扣的金额占预算的比例\n\n用户体验\n\n Unique campaings served: 用户看到的所有广告中有几个 unique\ncampaign，表示用户看到的广告的多样性，论文认为这个值越大越好\n\n\n论文在 LinkedIn 的两种广告（Direct Ads 和 Sponsored Status\nUpdates）上分别做了这个实验，结果如下，带加号 + 的指标表示\nOn 在 oFF 的基础上的变化比例；根据下表，在各个指标上均有较高提升\n\n\neffect 1\n\n\n\neffect 2\n\n且根据 cost per click\n指标，可在一定程度上了解目前系统的竞争力情况，该值越大，表示系统竞争越激烈，论文也根据这个指标对比了采用这个策略前后系统的竞争力，如下图所示，可以看到，采用\npacing 后前期的竞争有所缓和（slow\nstart 导致的），而后期的竞争力比原来有所提升，原因是 campaign life time\n变长了，因此有竞争力的 campaign 不会早早就退出了竞价环境。\n\n\ncompetition\n\n\n论文参考文献\n[1] A. Mehta, A. Saberi, U. Vazirani, and V. Vazirani.Adwords and\ngeneralized on-line matching. Journal of the ACM, 54(5):Article no. 22,\nOctober 2007 [2] D. Agarwal, D. Chen, L.-j. Lin, J. Shanmugasundaram,\nand E. Vee. Forecasting high-dimensional data. In Proceedings of the\n2010 ACM SIGMOD International Conference on Management of Data, SIGMOD\n’10, pages 1003–1012, New York, NY, USA, 2010. ACM.\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"《Embedding-based Retrieval in Facebook Search》阅读笔记","url":"/2020/08/30/%E3%80%8AEmbedding-based%20Retrieval%20in%20Facebook%20Search%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"Embedding-based Retrieval\nin Facebook Search 是 FB 在 2020\n年发表的一篇搜索场景下如何做向量化召回的\npaper，整篇文章读下来，就像是一个奋战在一线的工程师向你娓娓道来他们是怎么从\n0 到 1 构建一个召回系统，从训练数据与特征的选取，到模型的 training 与\nserving、再到把新的召回策略融入现有的 ranking system, 整篇 paper\n并没有太多的公式与推导，但是却有很多在实战中总结出来的经验，而且这些经验相信也可以推广搜索以外的推荐 / 广告领域。本文主要是根据笔者对这篇\npaper 的理解做一些提炼，推荐读原文。\n\n笔者认为这篇 paper 值得关注的点如下\n\n召回模型的负样本的选取（为什么不能只选取曝光未点击的样本作为负样本，easy\nnegative 与 hard negative）\n\n新的召回策略如何克服当前 ranking system 的 bias\n\n构建一个召回系统的常规流程及每个流程中的一些经验\n\n后面介绍的内容与 paper 中的基本保持一致，且主要介绍其中一些关键点\nSystem Overview\n在推荐、广告和搜索的场景下基本的架构都是召回 (Retrival)+ 精排（Ranking），因为这三者其实都是要在每条请求到来的时候从一个庞大的候选集中选取出 topk 个返回给用户，而召回作为这个流程的入口，面对的几乎是整个候选集，为了在延迟上满足要求，召回不会采用太复杂的模型和特征，且往往会对\nitem 做倒排索引 (Inverted Index)。\npaper 中的系统总体的架构如下，在每条请求到来的时候会实时计算用户的\nembedding，然后利用构建好的 document embedding 倒排索引做\nretrival，为了加速，在向量化召回中还会采用 Quantization 技术 (在后面的\nserving 步骤中介绍)\n\n\nsys overview\n\nModel\n这里的 model 主要指上图中的 Query Embedding Model 与 Document\nEmbedding Model，即生成 embedding\n的模型，采用的也是很经典的双塔模型，如下图所示；这里的 unified embedding\n主要是指这个 embedding 输入的原始 feature 不仅仅包含 query 和 document\n本身的文本信息，还有对应的上下文信息 (context) 信息，这种做法其实在 google\n2016 发表的那篇 Deep Neural Networks for YouTube Recommendations\n已经有了，而这也是 NN 模型比起 Matrix Factorization 等方法生成 embedding\n的优点；可以添加更多的 feature 到模型中\n\n\nmodel overview\n\n损失函数\n模型采用的损失函数是 triple loss，最早是在人脸识别中提出的一个\nloss，假设每条训练样本是 \\((q^{(i)},d_+^{(i)},\nd_-^{(i)})\\), 式子中的 \\(d_+^{(i)}\\) 表示与 query \\(q^{(i)}\\) 相关的 document，而 \\(d_-^{(i)}\\) 表示与 query \\(q^{(i)}\\) 不相关的 document, 则 paper 中的\nloss 定义如下\n\\[\\begin{align} L=\\sum_{i=1}^{N} \\max(0,\nD(q^{(i)},d_+^{(i)}) - D(q^{(i)},d_-^{(i)}) + m)\n\\end{align}\\]\n上式中的 \\(m\\)\n是一个超参，表示正样本与负样本的 enforced margin，表示正负样本的\ndistance 假如大于 m，则认为这个是一个 easy example\n不需要模型进一步学习去区分了；paper\n中提到了这个超参对结果影响较大，因为不同的任务的最优的 m\n往往不一样。\n上面的 D 表示距离函数 (越小表示越相似)，paper 中采用的 distance 函数是\n\\(D(q,d) = 1-cos(q, d)\\),\n此外，针对 paper 中的训练样本对，其实也可以采用经典的 LTR\n中的 pariwise loss，具体可参考 From\nRankNet to LambdaRank to LambdaMART: An Overview\n此外，paper 中的\n采用的指标是离线评估的召回率（recall），采用的验证集是 10000 个\nsearch session 中每个 query 及其 target result(paper\n中并无明确给出这一标准，只是提到按照点击或人工评估方式来获取均可以)\n训练样本\n训练样本的选取是 paper\n中的一个着重强调的一个点，且关键点在于负样本的选取，paper\n中选取的正样本是点击样本，而负样本则做了下面的两组对比\n\n随机选取负样本\n\n选取曝光未点击的样本作为负样本\n\n实验结果显示选取曝光未点击的样本作为负样本时，其效果比随机选取负样本要差很多；paper\n中对这一现象的解释是\n\nWe believe it is because these negatives bias towards hard cases\nwhich might match the query in one or multiple factors, while the\nmajority of documents in index are easy cases which do not match the\nquery at alll. Having all negatives being such hard negatives will\nchange the representativeness of the training data to the real retrieval\ntask, which might impose non-trivial bias to the learned embeddings.\n\n笔者理解采用曝光未点击的样本作为负例，其实就是造成了 training\n与 serving 的不一致性，因为曝光未点击的样本大部分是 hard\ncases，即使最终未被点击，但是与 query\n也还是有一定相关性的，但是线上召回时面对的候选集是全部的候选，其中有绝大部分与本次\nquery 无关的 easy cases。当负例全部采用 hard cases，实际上与最终的\nserving 就是不一致的，而 paper 中共则说这种行为 “might impose\nnon-trivial bias to the learned embeddings”\n除了负例，paper\n中也探索了正例的选择，关于正例的选择做了下面两组的对比\n\n选取点击作为正例\n\n选取曝光作为正例\n\n实验结果显示在数据量相同的情况下，两者效果基本一致，即使是在曝光的正例基础上叠加点击正例，结果也没有进一步的提升。\n关于正例的选择，虽然 paper\n中称两类正样本差别不大，但是笔者认为在实际中采用曝光样本作为正例更合适，原因如下\n\n点击一般是比较稀疏的，数据量较少\n\n未点击的样本不应就不是好的样本，有可能只是位置等原因导致的，相比于点击样本，采样这样的样本一定程度上相当于做了\nExplore\n\nhard mining\n这部分内容是 paper\n中第六节的内容，但也是训练样本选取的一个关键点，因此放在这里一起说明。\n前面提到，选取负样本的时候不能选择曝光未点击的 hard\ncases，但是凡事有多个度，当负例中的样本都是很容易就能跟正例区分开的\neasy cases，模型也不一定能学得好，paper\n中对这一现象描述如下\n\nThis motivated us believe that the model was not able to utilize\nsocial features properly yet, and it’s very likely because the negative\ntraining data were too easy as they were random samples which are\nusually with different names. To make the model better at\ndifferentiating between similar results, we can use samples that are\ncloser to the positive examples in the embedding space as hard negatives\nin training.\n\n这里的 hard nagative\n指的就是那些与正例相似性较高的负例 (现对于随机选取的负例)，但是这里的\nhard nagative 并不是那些曝光未点击的负例，paper\n中提出了两种方法来挖掘 hard nagative：online hard negative mining 和\noffline hard nagative mining，两种方法的基本流程如下\nonline hard negative mining\n在每个 batch 的训练中，假设正样本对为 \\(\\lbrace(q^{(i)},d_+^{(i)})\\rbrace_{i=1}^{n}\\),\n则对于每个 query \\(q^{(i)}\\), 会从 \\(\\lbrace d_+^{(1)} \\dots d_+^{(j)} \\dots d_+^{(n)}\n| j \\ne i\\rbrace\\) 中随机选出 k 个 document 作为 hard\nnagative，paper 中称其场景下 k=2\n是最优的，如果多了会导致模型的效果下降\npaper 中的实验数据表示加入这样的 hard nagative 后，在不同类型的 iterm\n的搜索上的召回率均有提升。\n但是实际上以这种方式选取出来的负样本还不够\nhard，原因也很简单，因为这些 negative 是属于不同的 query\n的，不同 query 的相关性不高，因此这些样本的相似性也不高，因此有了\noffline hard negative mining\noffline hard nagative mining\noffline hard nagative 的做法更像 LTR 的 pairwise 样本构造了，其选取\nnegative 的方式是在每个 query 的所有 document 中，选择那些排序在\n101-500 的位置的样本作为 hard nagative；值得注意的是，选择那些\nhardest 的 negative 的效果并不是最优的（如排序在第二名的那些）\n上面提到的是负样本的 mining，但是同样也可以针对正样本做这样的 hard\nmining，但是 paper 这一块篇幅较少，细节说的也不是非常清晰，只是提到了\n\"we mined potential target results for failed search sessions from\nsearchers’ activity log\", 大意就是从那些失败的 search session\n的日志中找到那些没被系统召回的但是 positive\n的样本，笔者猜测这些可能是因为工程上的失败导致没被 send\n出来但是日志里面已经显示排序的第一名的样本\n综上，在召回样本的选取上，paper 强调了负样本中的要同时包含\neasy nagative 和 hard nagative，paper 的观点是 hard nagative 更关注\nnon-text 的特征（如 social 特征等），而 easy nagative 则更关注 text\n的特征，因此需要混合两者使用，而混合的方式有两种，分别是\n(1)blending, 即混合两者一起来训练，paper\n中给出两者的最优比例大概是 easy:hard ≈ 100:1\n(2)transfer learning from \"hard\" model to \"easy\" model,\n即先用 hard nagative 训练模型，然后用 easy nagative 训练模型（但是 paper\n提到从 \"easy\" model 到 \"hard\" model 并不能达到相同的效果）\nFeature Engineering\n这里的 Feature Engineering 着重强调的是在 query 和 document 的 text\nfeature 基础上加入一些其他的 context feature (paper\n中主要提出了两种，location feature 和 social embedding feature)\n能取得取得较大提升，下面简单介绍一些这几种 feature\ntext feature\n对于文本特征的构建，paper 中采用的是 character n-gram 而不是 word\nn-gram，这里的 n-gram 其实就是把连续的 n 个 character 或 word 作为一个\nitem 输入到 embedding table 中做 embedding lookup，paper\n中通过实验证明了采用 character n-gram 比起 word n-gram\n效果要更优，分析其优点如下\n\nembedding lookup table 的 size 更小，能更好的学习到 embedding table\n中的参数，其实就是降低了 model size\n\n对于出现在训练集以外的单词有更好的鲁棒性，因为 embedding 的粒度是\ncharacter\n\nlocation feature\npaper 在 query 和 document 中均添加了了 localtion feature；对于\nquery，添加的 feature 包括 searcher's\ncity/region/country/language, 对于 document，则采用一些\npublicly available information 如一些 explicit group location 的 tag\n之类的\n下面是加入了 localtion feature 前后同一个 query 返回的 document\n的对比，可以看到加入 location feature 后，返回的搜索结果中的 document 的\nlocation 信息与 searcher 的更加相似\n\n\nlocation feature\n\nsocial embedding feature\n关于这个 feature paper 中并无详细说明，猜测是通过类似 graph embedding\n的方式来得到这个 embedding，然后作为 feature 输入给召回模型\n下面是加了 location feature 和 social embedding feature\n后得效果提升情况\n\n\nfeature effectiveness\n\nServing\nserving 采用的是 ANN（Approximate Near Neighbor），且通过\nquantization 来进一步缩短向量间相似性的计算时间，quantization\n相当于是一种向量压缩的技术；关于 product quantization 可参考 理解\nproduct quantization 算法 这篇文章\n实际中的向量化召回系统往往会包含两个步骤，indexing 和\nscoring，indexing 是为了过滤大部分基本无关的候选，而 scoring\n则是在相关的候选中进行计算与排序，indexing 常用的技术有\nK-means、HNSW、LSH 等，而 scoring 则主要是各种 quantization\n及其变种方法\n上面这两个步骤在不同的地方的叫法可能也不一样，如在 paper\n中这两个步骤就被称为 coarse quantization 和 product quantization；coarse\nquantization 其实就是通过聚类的方法将整个候选库中的分为若干个\nclsuter，在 query 到来的时候，选出 topk 个 cluster，并通过 product\nquantization 来计算出分数进行排序。具体工程实现上采用的是 facebook\n开源的 faiss\n库，关于上面的步骤细节可参考 PQ 和 IVF 介绍\n而在这个过程中 paper 总结的一些经验如下\n\n对比 coarse quantization 的算法时，需要固定条数对比召回率 (如 paper\n中采用的是 1-Recall@10)，其实这种做法也比较常见，因为不同的 coarse\nquantization 算法聚类的结果差别比较大，因此在固定 cluster 数量和召回时\ntopk 中 k 的数值，得到的结果差异也会比较大\n\n当模型有较大 (non-trivial) 变动时，ann\n的一些超参也需要相应进行调整来适应新的模型\n\nPQ 算法中的 OPQ(Optimized\nProduct Quantization) 效果一般较好，值得进行相应的尝试\n\nPQ 算法中对 embedding\n进行子空间划分时，划分的空间大小是个超参，理论上这个值越大，计算结果越精确，同时资源开销也越大；paper\n中的建议值是 4, paper 中表示 “From empirical results, we found that the\naccuracy improvement is limited after x &gt; d/4.”\n\n实际 serving 的时候，只会实时计算 query 塔的 embedding，而\ndocument 塔的 embedding\n则会离线计算好并构建倒排索引，且在实际的系统中，新的 document\n会不断生成，因此还会计算新 document\n作为增量索引，而间隔一段时间好需要重新计算全量的倒排索引。\nLater-stage Optimization\n这一部分主要描述了所有推荐系统现在存在的一个\nbias，就是训练数据都是由当前系统产生的并反哺给系统的，因此很可能会造成 “马太效应”，即强者约强，弱者越弱；更广义来说，这也属于一个\nE&amp;E 问题.\n在 paper\n中，这一点体现在新的 ANN 召回的结果可能并不会被精排认可，paper\n中描述如下\n\nsince the current ranking stages are designed for existing retrieval\nscenarios, this could result in new results returned from embedding\nbased retrieval to be ranked sub-optimally by the existing rankers\n\n为了克服这个问题，paper 中提出了两种方法\n\n将召回的 embedding 作为精排模型的特征，paper 中称这样做的 motivation\n是能更快让精排学到新召回的特性；embedding\n加入精排作为特征的方式有：embedding 作为 feature 直接加入精排模型、基于\nembedding 计算出的值加入精排模型 (如 query embedding 与 document\nembedding 的 cosine similarity）等，其中效果最好是通过 cosine similarity\n计算出 feature 加入精排模型。\n\n人为干预加入新召回后训练数据的分布。为了避免新召回的结果不被 ANN\n认可，paper\n中并不仅仅依赖系统自身产生的数据作为训练数据，而是通过人工的方式对被召回的结果重新打上\nlabel，但是在实际中感觉这个操作成本会比较高～\n\n小结\n这篇 paper 主要讲了 FB 在搜索场景下构建一个 ANN\n召回系统的基本步骤，包括训练数据的选择、模型的训练、\nserving 等，每一部分都给出了一些实战经验，值得参考；而其中比较值得关注的是负样本的选择，paper\n将负样本分为 easy negative 与 hard nagative 两大类，并通过实验证明只用\neasy nagative 或 hard nagative\n训练出来的都不是最优的，而是需要联合两者共同训练。\n此外，paper 最后还提到了 embedding ensemble 方法，基本的方法就是\nweighted concatentation (parallel，类似 bagging) 和 cascade\nmodel (cascade, 类似\nboosting)，但是笔者觉得召回阶段本来的候选就非常大，对耗时要求严格，如果再加上\nensemble，耗时会更严重，在实际中是否具有可行性？ensemble\n做在精排是否更合理？\n最后，知乎上也有一篇针对这篇 paper 的解读，也值得看一下 负样本为王：评 Facebook 的向量化召回算法，而关于\nembedding 在推荐 / 广告上的各种应用推荐看 推荐系统 embedding\n技术实践总结\n","categories":["机器学习"],"tags":["计算广告","机器学习"]},{"title":"《Forecasting High-Dimensional Data》阅读笔记","url":"/2018/11/15/%E3%80%8AForecasting%20High-Dimensional%20Data%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"《Forecasting\nHigh-Dimensional Data》 是 Yahoo!\n一篇关于流量预估的论文。在合约广告中，需要提前预估某个定向下的流量情况，从而进行合理的售卖和分配。但是由于定向的组合非常多（广告主的多样的需求导致的），而工程上不允许为每个可能的定向预估其流量，因此这篇论文提出了先预估一些基本定向的流量，然后通过\ncorrelation model\n从基本定向的流量计算出各种定向下的流量情况，具有较强的工程性，也是之前提到的文章\n《Budget\nPacing for Targeted Online Advertisements at LinkedIn》\n中采用的流量预估方法。\n\n问题定义\n前面简单提到了文章解决的问题的背景，文章将一个最基本的定向（如性别为男）称为一个\nattribute，而每个广告主的需要的流量就是若干个 attribute 的\ncombination，因此 combination\n的数量是非常庞大的，文章要求的就是如何有效地预估所有 combination\n的流量情况。\n一个最直观的思路就是统计出某个 combination\n的历史数据，然后训练模型并进行预估。但是这个方法最致命的地方在于统计\ncombination\n的历史数据以及模型训练所需的耗时在实际中是无法容忍的，需要数小时（文章要求需要在数百毫秒内返回某个\ncombination\n的流量预估值）。那可以将这些放到离线来做么？答案也是不可以的，因为我们不知道哪些\ncombination 在未来会用到，因此所有的 combination\n都需要进行预估，而这个数量过于庞大了。\n解决思路\n文章提出的解决思路是先对一小部分有代表性的 combination\n进行上面的历史统计和预估操作，这里的有代表性的流量可以从统计的历史数据中得到（如访问量最多的 combination\n等），也可以手工选择等；然后结合文章提到的 correlation\nmodel，可以从这一小部分的 combination 中预估出所有的 combination\n的流量情况。\n下面具体详细介绍整体的解决思路\n系统总览\n文章提出的方法系统图如下，每个的 Historical Data Point\n实际上就是一个历史 query，包含了某些 attribute\n的组合，这些数据主要有两个用途\n（1）送入到 Historical Data Aggregator 中对某些有代表性的 Selected\nAttribute Combinations\n进行历史统计，并通过模型进行时间序列的预估，这里的时间序列预估使用了 SARIMA\n模型 （2）用于构建 Correlation Model，并与上面若干 Selected Attribute\nCombinations 的预估结果共同对在线的 query 返回预估值\n\n\nsystem\n\n可以看到文章的重点在于 correlation model 的构建以及\ncorrelation model 在 online 部分是如何工作的。\nCorrelation Model\n上文提到了对一部分有代表性的 combination（也就是上图中的 Selected\nAttribute Combinations）进行时间序列预估，这里记为 \\(Q = \\lbrace Q_1, Q_2,... Q_m \\rbrace\\),\n称其为 base queries。则对于一个在线的 query \\(q\\), 其预估步骤如下\n\n选择 \\(Q\\) 中某个 base query \\(Q_k\\), 使得 \\(q\n\\subseteq Q_k\\)\n 计算 \\(Q_k\\) 在未来时间 \\(T\\) 内的流量，记为 \\(B(Q_k, T)\\)\n 计算 \\(q\\) 在 \\(Q_k\\) 中出现的概率，记为 \\(R(q|Q_k)\\)\n 返回 \\(q\\) 的预估结果为 \\(B(Q_k, T) \\times R(q|Q_k)\\)\n\nCorrelation Model 所做的事情就是第 3 步：计算比例。\n而在第一步中，必然会存在某个 \\(Q_k\\)，使得 \\(q\n\\subseteq Q_k\\) , 为什么呢？\n假设总共有 n 个 attribute，那么只要将每个 attribute 作为一个 base\nquery（这样 \\(Q\\) 中便有了 n 个 base\nqueries），则 attribute 的任意 combination 肯定会属于 Q 中的某几个 base\nqueries 的。那如果不止一个 \\(Q_k\\)，使得 \\(q\n\\subseteq Q_k\\) 时该怎么办？这时候就需要选择一个最小的\n\\(Q_k\\), 这里的最小指的是不存在某个\n\\(Q_l\\) 同时满足 \\(q \\subseteq Q_l\\) 且 \\(Q_l \\subseteq\nQ_k\\)。举个简单的例子，假如有两个 base queries \\(Q1 = a_1 , Q_2 = a1 \\wedge a_2\\),\n则对于某个 query \\(q = a1 \\wedge a_2  \\wedge\na_3\\)，应该选择 \\(Q_2\\) 作为\n\\(Q_k\\)。\n下面分别讲述文中提到的三种 correlation model\nFull Independence Model (FIM)\nFIM 实际上就是个 Naive Bayes，假设了每个 attribute\n是相互独立的，也就是\n\\[\\begin{align} R(Gender = male \\wedge Age\n&lt; 30|Q_k) = R(Gender = male|Q_k) \\times R(Age &lt; 30|Q_k)\n\\end{align}\\]\n也就是只要为 \\(Q_k\\) 每个单独的\nattribute 计算其比例 \\(R(.| Q_k)\\)\n即可。具体计算方法如下\n令 \\(P_t\\) 是截止到时间 \\(t\\) 时所有的访问量，\\(|Q_k \\wedge P_t|\\) 为这些访问量中满足 \\(|Q_k|\\) 的那些访问量，\\(|a_i \\in (Q_k \\wedge P_t)|\\) 为满足 \\(|Q_k|\\) 的那些访问量中同时满足 attribute\n\\(a_i\\) 的，则对于\nattribute，其计算公式如下\n\\[\\begin{align} R(a_i|Q_k) = \\frac{|a_i\n\\in (Q_k \\wedge P_t)|}{|Q_k \\wedge P_t|} \\end{align}\\]\nPartwise Independence Model\n(PIM)\n将所有的 attribute 都认为是相互独立的显然是不合理的，因为有某些\nattribute 之间是相互关联的，比如说年龄和收入一般是存在关联的，因为 PIM\n实际上就是将某些可能有关联的 attribute 的比例一起计算。即\n\\[R(Gender = male \\wedge Age &lt; 30\n\\wedge Incom &gt; 10000|Q_k) = R(Gender = male|Q_k) \\times R(Age &lt; 30\n\\wedge Incom &gt; 10000|Q_k)\\],\n而比例的计算公式也跟上面的类似\nSampling-based Joint\nModel(SJM)\n上面的两种方法均假设了 attribute\n之间的的相互独立性，这依然会存在一定的局限性，能够完全避免独立性的假设呢？这篇文章提出的\nSampling-based Joint Model (SJM) 就避免了独立性的假设。\n虽然说是 model ，但是方法还是统计，只是为了避免数量太大，首先做了\nsampling，选出经过 sample 后的数据并记为 \\(S\\)，然后计算 base query \\(|Q_k|\\) 在 \\(S\\) 中的数量 \\(|Q_k \\cap S|\\),\n这部分会在离线做，然后在线来了一个 query \\(q\\) 后，会计算在 \\(S\\) 中满足 \\(q\\) 的数量并记为 \\(n\\)。则比例计算公式就很简单了\n\\[\\begin{align} R(q|Q_k) = \\frac{n}{|Q_k\n\\cap S|} \\end{align}\\]\n整个过程思路非常简单，没有涉及到 attribute，因此也没有 attribute\nindependent\n的假设。但是关键的问题在于如何高效地算出上面的分子和分母的那些计数。论文使用的是 bitmap\nindex, 且使用了论文 Optimizing bitmap\nindices with efficient compression 中提出的关于 bitmap index\n的一种改进方法。\n实验效果\n数据\n实验采用的数据是\nYahoo！部分页面过去一年的历史访问数据，其中一半用于进行 time-series\nforecasting，另一半用于验证效果（按时间划分），且对于某些 combination\n用了过去四年累积的数据进行 forecasting。Correlation Model\n则用了过去一周的数据进行训练，且 SJM 采样得到了 20 million 的数据。\n评估指标\n工程上的评估指标有 speed 和 space，就是时间和空间的评估。\n效果上的评估指标主要就是 accuracy，采用的是 absolute percentage error\n(APE), 假设预估值为 \\(F\\), 真实值为\n\\(A\\), 则 APE 的定义如下\n\\[\\begin{align} APE = \\frac{|F-A|}{A}\n\\end{align}\\]\nAPE 针对的是单个 query，但是往往希望的是验证一系列 query\n的效果，其指标为 Root Mean Square Error (RMSE)，定义如下，\\(w_q\\) 表示每个 query 的权重，这个值被设为\nquery 在过去两年的合约中出现的次数\n\\[\\begin{align} RMSE = \\sqrt{\\frac{\\sum_{q\n\\in Q}w_qAPE^2(q)}{\\sum_{q \\in Q}w_q}} \\end{align}\\]\n效果对比\n下面是三个模型的 RMSE（取了对数） 效果对比，横轴的 forecast horizon\n表示预估未来多少天的时的效果。可以看到假设 attribute independent\n会降低最后的效果。\n除此之外，每个模型在进行 online 是返回结果的耗时都要小于 50 毫秒；且\nFIM 和 PIM 模型的内存占比大概是 500\nMB，因为这两个模型只需要存储比例值和预估的趋势曲线，但是 SJM\n的内存大概到了 20GB（20 million 的 data points），空间主要由 bit-map index\n消耗。\n\n\n对比\n\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"《Programming Collective Intelligence》读书笔记 (3)-- 聚类","url":"/2017/01/25/%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(3)--%E8%81%9A%E7%B1%BB/","content":"《Programming Collective\nIntelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。\n本文是本书的第三章 Discovering Groups 的读书笔记\n, 主要介绍了对文本进行聚类以及对聚类后的结果进行可视化。\n\n聚类就是将相似的数据聚合在一起，形成一类，与分类比较相似，但是又有不同。一般来说聚类没有明确的类别数目，而分类根据具体的问题在一开始就确定了所有分类的可能，而且一般用于聚类的数据不需要标签，\n属于无监督算法，而用于分类的数据则需要，属于有监督的算法。常见的聚类算法有 KMeans，SOM（Self-Organized\nFeature\nMap）等，常见的分类算法则更多，如逻辑回归，朴素贝叶斯，SVM 等。\n本文主要讲逐层聚类 (Hierarchical\nClustering) 和 KMeans 聚类这两个聚类算法，依然会通过一个例子来阐述算法的具体实现。文中所有的数据和代码可从这里获取。\n获取数据\n本文使用的数据为博客文本数据，通过分析博客文本之间的相似性，从而将相似的博客聚类在一起。\n通过博客的 RSS\n源可以获取博客所有的文本数据，python 中提供了 feedparser\n这个第三方 package 来实现这个功能，如下代码实现的功能就是获取一个博客的\nRSS 中的所有文章的 summary\n并进行分词统计，注意本文使用的均是英文文本。\nimport reimport feedparserfrom collections import defaultdictdef parse_rss(target_url):    rss = feedparser.parse(target_url)    word_count = defaultdict(int)    # traverse all passages of the blog    for entry in rss.entries:          if 'summary' in entry:            summary = entry.summary # su        else:            summary = entry.description        words = extract_words(entry.title+' '+summary)        for word in words:            word_count[word] += 1    return rss.feed.get('title', 'empty title'), word_count  # title can be empty sometimesdef extract_words(content):    # remove tag in the form of &lt;XXXX&gt;    txt = re.compile(r'&lt;[^&gt;]+&gt;').sub('',content)      # split words by all non-alpha characters    words = re.compile(r'[^A-Z^a-z]+').split(content)     # turn all words into lowercase    return [word.lower() for word in words if word != ''] \n上面的 parse_rss 函数实现的功能就是从 RSS\n的 url 中提取出所有文本的 summary（或 description），然后通过 extract_words 函数剔除 html 标签并分词，进而统计出该\nRSS 源中各个词语所出现的次数。\n由于聚类需要多个博客的数据，本文使用了原书提供的 100 个博客的 RSS 作为原始数据，并通过原始数据构造一个\nblog-word\n矩阵，行表示博客，列表示各个具体的词语，矩阵中的值表示某个词语在某个博客中出现的总次数。由于词语数目过多，因此这里会限制出现在矩阵的列的词语必须要在原始数据中出现的频率在一定的百分比。得到\nblog-word\n矩阵需要一定的运算量，因此将其写入到文件中进行持久化，方便下次的读取。下面就是实现上面功能的代码。\ndef get_content_from_feedlist(feed_list, data_file):    word_appear_count = defaultdict(int) # count thow many blogs does a word appear in    blog_word_count = {} # words of each blog    empty_title_count = 0    for rss_url in file(feed_list):        title, wc = parse_rss(rss_url.strip())        if title == 'empty title':  # cannot get title of some rss            empty_title_count += 1            title = title+' %s'%empty_title_count        blog_word_count[title] = wc        for word, count in wc.items():            word_appear_count[word] += 1    # caculate the appearing percentage of each word    # record those words that appear within maximum and minimum percentage     minimum, maximum = 0.1, 0.5    word_list = []    total_blog = len(blog_word_count)    for word, count in word_appear_count.items():        if minimum &lt;= count*1.0/total_blog &lt;= maximum:            word_list.append(word)    # write data into data_file     with io.open(data_file, mode = 'w', encoding = 'utf8') as wf:        wf.write('Blog'.decode('utf8'))        for word in word_list:            wf.write(('\\t%s'%word).decode('utf8'))        wf.write('\\n'.decode('utf8'))        # words of each blog        for blog_title, blog_words in blog_word_count.items():            wf.write(blog_title.decode('utf8'))            for word in word_list:                if word in blog_words:                    wf.write(('\\t%s'%blog_words[word]).decode('utf8'))                else:                    wf.write(('\\t'+'0').decode('utf8'))            wf.write('\\n'.decode('utf8'))\n上面的 get_content_from_feedlist 中通过获取了\nfeed_list(每行一个 rss 源的 url) 中所有博客的文本，并将那些出现频率在\n0.1 到 0.5 间的词语作为 blog-word\n矩阵的列，出现频率的计算方法为\n词语出现的博客数/总的博客数。然后将 blog-word\n矩阵写入到 data_file 文件中。至此完成了获取数据的步骤。\n聚类\n下面将讲述两种方法对上面获取的博客数据进行聚类：逐层聚类（Hierarchical\nClustering） 和 KMeans 聚类（KMeans Clustering）。\n逐层聚类（Hierarchical\nClustering）\n逐层聚类的思想很简单，每次将距离最近的两个 cluster 进行聚类，组成一个新的类，然后重复此过程直到只剩下一个最大的 cluster。整个过程如下图所示：\n\n\n逐层聚类\n\n这种聚类方法的具体过程如同构造一棵树，其中每个叶子节点表示一个单一的实例，而其他节点表示由多个实例聚成的 cluster。\n\n\n逐层聚类树状图\n\n算法的思路比较简单，但是有个关键问题就是如何判断 cluster 与\ncluster 之间的距离。这里采用皮尔逊系数，皮尔逊系数在这篇文章中有比较详细的描述，这里就不详细展开了，实际上皮尔逊系数就是概率论中常用的相关系数，用于表示两者的相关性，范围在\n[-1,\n1] 之间，其中正值表示正相关，负值表示负相关，且绝对值越大，相关性越强，这里的距离越短，表示两个的正相关性越强，因此采用\n1-皮尔逊系数 作为距离的值。实现的代码如下所示：\ndef pearson(v1,v2):\t# Simple sums\tsum1=sum(v1)\tsum2=sum(v2)\t# Sums of the squares\tsum1Sq=sum([pow(v,2) for v in v1])\tsum2Sq=sum([pow(v,2) for v in v2])\t# Sum of the products\tpSum=sum([v1[i]*v2[i] for i in xrange(len(v1))])\t# Calculate r (Pearson score)\tnum=pSum-(sum1*sum2/len(v1))\tden=sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\tif den==0: return 0\treturn 1.0-num/den\n另外一个问题就是如何在 python\n中表示一个 cluster，从上面的算法过程可知，一个 cluster 包括了它的两个子 cluster（最开始的单实例没有），cluster 的中心值等，在\npython\n并没有符合这种要求的数据结构，因此可以创建一个 python 类这个 cluster。\n创建的 cluster 类如下所示： class hcluster:\t\"\"\"describe a cluster as a node in a tree\"\"\"\tdef __init__(self, id, vector, distance=0, left = None, right = None):\t\tself.id = id\t\tself.vector = vector\t\tself.distance = distance\t\tself.left = left\t\tself.right = right\n各个变量及其含义如下所示\n\n\n\n变量\n含义\n\n\n\n\n id\ncluster 的唯一标示号\n\n\n vector\ncluster 的值，即 cluster 的中心点\n\n\n distance\n 组成 cluster 的两个子 clusters 的距离，叶子节点为 0\n\n\nleft，right\n 组成 cluster 的两个子 clusters，叶子节点为 None\n\n\n\n通过各个 cluster 的唯一标示号 id 可以方便进行 cluster\n的合并和删除，而存储 distance\n以及子 clusters 的便于后面对聚类结果的可视化。下面是逐层聚类的过程的代码\ndef hierarchicalClustering(blog_data, distance = pearson):\t# initi clusters, each node is a cluster\tclusters = [hcluster(id = i, vector = blog_data[i]) for i in xrange(len(blog_data))] \t# use negativ number to represent cluster with more than one node\tclust_id = -1\t# use distance to store caculated results\tdistances = {}\twhile len(clusters) &gt; 1:\t\tsimilar_pairs = (0,1)\t\tclosest_distance = distance(clusters[0].vector, clusters[1].vector)\t\tfor i in xrange(len(clusters)):\t\t\tfor j in xrange(i+1, len(clusters)):\t\t\t\tif (clusters[i].id, clusters[j].id) not in distances:\t\t\t\t\tdistances[(clusters[i].id, clusters[j].id)] = distance(clusters[i].vector, clusters[j].vector)\t\t\t\td = distances[(clusters[i].id, clusters[j].id)]\t\t\t\tif closest_distance &gt; d:\t\t\t\t\tclosest_distance = d\t\t\t\t\tsimilar_pairs = (i, j)\t\tmerged_vector = [(clusters[similar_pairs[0]].vector[i] + clusters[similar_pairs[1]].vector[i])/2.0 \t\t\t\t\t\t   for i in xrange(len(clusters[similar_pairs[0]].vector))]\t\tnew_cluster = hcluster(id = clust_id, vector = merged_vector, distance = closest_distance, \t\t\t\t\t\t\t\tleft = clusters[similar_pairs[0]], right = clusters[similar_pairs[1]])\t\t# must delete elements from higher index to lower index\t\tdel clusters[similar_pairs[1]]\t\tdel clusters[similar_pairs[0]]\t\tclusters.append(new_cluster)\t\tclust_id -= 1\treturn clusters[0]\n上面的代码重复\n合并两个最邻近cluster为新cluster-&gt;调整新cluster的位置的操作，直到只剩下一个 cluster 并将该 cluster 返回。\nKMeans 聚类\nKMeans 聚类的思想是一开始就确认了最终需要聚类成 k 个\nclusters，然后在训练数据的范围内随机选择 k 个初始点作为初始 k 个 cluster\n的中心，并将每个实例聚类到离其最近的一个 cluster，所有的点都分到离其最近的 cluster 后，根据各个 cluster 中的点重新调整这个 cluster 的中心。重复这个过程直到每个 cluster 中的点不变为止。如下图为 KMeans 的过程\n\n\nKMeans 聚类过程\n\n这里度量距离的方式仍采用皮尔逊系数，下面是实现以上功能的代码：\ndef kMeans(blog_data, distance = pearson, k = 5):\tm, n = len(blog_data), len(blog_data[0])\tmax_value = [0 for i in xrange(n)]\tmin_value = [0 for i in xrange(n)]\tfor i in xrange(m):\t\tfor j in xrange(n):\t\t\tmax_value[j] = max(max_value[j], blog_data[i][j])\t\t\tmin_value[j] = min(min_value[j], blog_data[i][j])    # initial random clusters\tclusters = []\tfor i in xrange(k):\t\tclusters.append([min_value[j] + random.random()*(max_value[j] - min_value[j]) for j in xrange(n)])\tcount = 0\tprevious_cluster_nodes = None\twhile True:\t\tcount += 1\t\tprint 'iteration count %s'%count\t\tcurr_cluster_nodes = [[] for i in xrange(k)]\t\tfor i in xrange(m):\t\t\tclosest_distance = distance(blog_data[i], clusters[0])\t\t\tcluster = 0\t\t\tfor j in xrange(1, k):\t\t\t\td = distance(blog_data[i], clusters[j])\t\t\t\tif closest_distance &gt; d:\t\t\t\t\tclosest_distance = d\t\t\t\t\tcluster = j\t\t\tcurr_cluster_nodes[cluster].append(i)\t\tif curr_cluster_nodes == previous_cluster_nodes:\t\t\tbreak\t\tprevious_cluster_nodes = curr_cluster_nodes\t\t# modify the core of each cluster\t\tfor i in xrange(k):\t\t\ttmp = [0 for _ in xrange(n)]\t\t\tfor node in curr_cluster_nodes[i]:\t\t\t\tfor j in xrange(n):\t\t\t\t\ttmp[j] += blog_data[node][j] \t\t\tclusters[i] = [float(tmp[j])/len(curr_cluster_nodes) for j in xrange(n)]\treturn clusters, curr_cluster_nodes\n上面的代码最终返回 k 个 cluster 的中心点的值，以及各个 cluster 中所包含的点（实例）。需要注意的是由于每次选择的初始点是随机的，因此每次运行所获得的结果不一定相同。\n可视化\n下面介绍如何将上面两种聚类算法得到的结果进行可视化，可视化通过 python 中的\npython image library (PIL) 实现。\n逐层聚类结果可视化\n从上面的算法描述可知，逐层距离每次将两个 cluster 聚合在一起，从构造过程来看，实际上最终是构造了一颗二叉树，因此这里会以二叉树的形式将聚类的结果可视化。由于二叉树叶子节点过多，为了便于展示，将二叉树横着放，如下图就是二叉树的放置方式\n\n\n逐层聚类树状图\n\n要将这课二叉树可视化，首先需要知道这这棵树以上图放置时的高度和宽度 (深度)，然后根据图片的大小进行相应的缩放。\n回想上面在逐层聚类中定义的下面的类表示一个 cluster class hcluster:\t\"\"\"describe a cluster as a node in a tree\"\"\"\tdef __init__(self, id, vector, distance=0, left = None, right = None):\t\tself.id = id\t\tself.vector = vector\t\tself.distance = distance\t\tself.left = left\t\tself.right = right\n通过 self.left 和 self.right\n属性可以访问当前 cluster 的两个子 cluster，而 self.distance\n则表示两个子 cluster 的距离的大小，该属性在图中表现为当前 cluster 到两个子 cluster 的线段的长短，假如 self.distance 的值越大，那么这个 cluster 到两个子 cluster 的线段也越长。因此可以通过 distance 的值来获取二叉树的最长深度来作为图片的宽度。获取二叉树深度代码如下所示\ndef get_depth(cluster):    # The distance of an endpoint is 0.0    if cluster.left==None and cluster.right==None: return 0    return max(get_depth(cluster.left),get_depth(cluster.right))+cluster.distance\n通过递归的方式将两棵字数中最长的深度加上当前节点到两棵子树的长度，便是以当前节点为根节点的树的深度。\n同样地，树的高度也可以通过递归方式获取，由上面的二叉树的放置方法可知，整棵树的高度就是其两棵子树的高度之和。下面便是获取高度的代码\ndef get_height(cluster):    if cluster.left==None and cluster.right==None:  return 1    return get_height(cluster.left)+get_height(cluster.right)\n整个二叉树中有两种节点：一种是叶子节点，表示一个单一实例，另外的非叶子结点的表示有多个叶子节点组成的 cluster。对于叶子节点，只需要在图片上显示其内容即可，而对于非叶子节点则需要画出其两棵子树的分支，通过\nself.id\n属性可以区分节点是否为叶子节点。下面便是具体的实现代码，（注意这里需要用到 PIL 了，关于 PIL 的具体用法可参考这里）\nfrom PIL import Image,ImageDrawdef draw_node(draw,cluster,x,y,scaling,blog_names):    if cluster.id &lt; 0:        h1=get_height(cluster.left)*20        h2=get_height(cluster.right)*20        top=y-(h1+h2)/2        bottom=y+(h1+h2)/2        # Line length        ll=cluster.distance*scaling        # Vertical line from this cluster to children            draw.line((x,top+h1/2,x,bottom-h2/2),fill=(255,0,0))            # Horizontal line to left item        draw.line((x,top+h1/2,x+ll,top+h1/2),fill=(255,0,0))            # Horizontal line to right item        draw.line((x,bottom-h2/2,x+ll,bottom-h2/2),fill=(255,0,0))                # Call the function to draw the left and right nodes            draw_node(draw,cluster.left,x+ll,top+h1/2,scaling,blog_names)        draw_node(draw,cluster.right,x+ll,bottom-h2/2,scaling,blog_names)    else:           # If this is an endpoint, draw the item label        draw.text((x+5,y-7),blog_names[cluster.id],(0,0,0))\n上面的 scaling\n参数是根据树的深度和图片的宽度的比例得到的缩放因子，在开始画图前需要通过树的深度和图片预定义的宽度获取。下面的代码便是在作图前的需要准备的参数以及调用上面定义好的函数进行作图的过程。\ndef draw_cluster(cluster, blog_names, jpeg_path):    # height and width    h=get_height(cluster)*20    w=1200    depth=get_depth(cluster)    # width is fixed, so scale distances accordingly    scaling=float(w-150)/depth    # Create a new image with a white background    img=Image.new('RGB',(w,h),(255,255,255))    draw=ImageDraw.Draw(img)    draw.line((0,h/2,10,h/2),fill=(255,0,0))        # Draw the first node    draw_node(draw,cluster,10,h/2,scaling,blog_names)    img.save(jpeg_path,'JPEG') 上面完整的代码可从这里获取\n作图的结果如下所示：\n\n\n逐层层聚类的可视化\n\nKMeans 聚类结果可视化\n原书并没有给出 KMeans 聚类结果的可视化操作，只是提供了一种多维缩放 (Multidimensional\nscaling, MDS)的技术，\n用于将高维的数据转换为二维，同时保留数据间的距离关系，这样便可通过图片对其进行可视化。实际上，\nMDS 与 PCA 都是一种基于线性变换而进行降维的方法。\nMDS 的思想是通过点的原始距离矩阵 \\(D\\),\n计算出变换到新的维度空间中的点的距离矩阵 \\(B\\), 然后对 \\(B\\) 做特征值分解，选取前 \\(n\\) 个特征值 (\\(n\\)\n为所变换到的维度空间的维度值) 及其对应的特征向量矩阵，便可得到新的维度空间中各点的坐标。具体的推导过程可以参考周志华的机器学习中第十章的内容或这篇博客，这里不详细展开论述了。\n但是该书给出的方案并不是上面讲述的方法，而是先在二维空间中随机初始化\n\\(m\\)\n个点作为 m 个实例的初始点，然后根据其在二维空间的距离与高维空间中实际的距离的误差来调整点的位置，是一种迭代的方法。其具体实现如下所示：\ndef scale_dowm(blog_data,distance=pearson,rate=0.01):    n=len(blog_data)    # The real distances between every pair of items    real_list=[[distance(blog_data[i],blog_data[j]) for j in xrange(n)]              for i in xrange(n)]    # Randomly initialize the starting points of the locations in 2D    loc=[[random.random(), random.random()] for i in xrange(n)]    fake_list=[[0.0 for j in xrange(n)] for i in xrange(n)]    lasterror=None    for m in range(0,1000):        # Find projected distances        for i in range(n):          for j in range(n):            fake_list[i][j]=sqrt(sum([pow(loc[i][x]-loc[j][x],2)                                      for x in xrange(len(loc[i]))]))        # Move points        grad=[[0.0,0.0] for i in range(n)]        totalerror=0        for k in range(n):          for j in range(n):            if j==k or real_list[j][k] == 0: continue  # acoid the case when real_list[j][k] == 0.0            # The error is percent difference between the distances            error_term=(fake_list[j][k]-real_list[j][k])/real_list[j][k]            # Each point needs to be moved away from or towards the other            # point in proportion to how much error it has            grad[k][0] += ((loc[k][0]-loc[j][0])/fake_list[j][k])*error_term            grad[k][14] += ((loc[k][15]-loc[j][16])/fake_list[j][k])*error_term            # Keep track of the total error            totalerror+=abs(error_term)        # print 'curr error {0}'.format(totalerror)        # If the answer got worse by moving the points, we are done        if lasterror and lasterror&lt;totalerror: break        lasterror=totalerror        # Move each of the points by the learning rate times the gradient        for k in range(n):          loc[k][0] -= rate*grad[k][0]          loc[k][17] -= rate*grad[k][18]    return loc\n上面的 scale_dowm 函数传入原始的博客数据\nblog_data, 然后进行迭代计算，最终返回这些数据在二维空间中对应的向量。每次的迭代时，根据每个点与其他各个点的距离误差调整该点的距离，并且计算出一个总体误差，当本次调整后的误差比上一次要大或者迭代次数达到最大，就跳出循环。\n上面返回了各个高纬向量在二维空间中的坐标，因此通过 PIL\n可以很自然地作出图。原书讲到这里就结束了，但是结合我们 KMeans 聚类的结果可知，每个聚类中心的向量长度与博客数据的长度一样，因此可以将 KMeans 聚类得到的聚类中心一并传入到 scale_dowm 函数中，然后得到聚类中心在二维空间中的坐标，然后以其为中心，连线到其聚类中的各个点。\n作图的实现的代码如下所示： def draw_clusters(blog_data, clusters, cluster_nodes, blog_names, jpeg_path = 'Clustering_data/mds2d.jpg'):    img=Image.new('RGB',(2000,2000),(255,255,255))    draw=ImageDraw.Draw(img)    for i in xrange(len(clusters)):        for node in cluster_nodes[i]:            c_x,c_y = (clusters[i][0] + 0.5)*1000, (clusters[i][19] + 0.5)*1000            x, y =(blog_data[node][0]+0.5)*1000, (blog_data[node][20]+0.5)*1000            draw.line((c_x, c_y, x, y),fill=(255,0,0))            draw.text((x,y),blog_names[node],(0,0,0))       img.save(jpeg_path ,'JPEG') \n完整的代码可参见 这里\n当类别数为 3 时，上面可视化得到的结果为：\n\n\nKMeans 可视化结果\n\n除了博客数据，原书还用了 Zebo\n网站上的数据进行了聚类，但是采用的聚类算法仍是我们上面提到的两个聚类算法。只是采用度量距离的标准不同，上面博客数据采用的是皮尔逊系数，而从 Zebo\n网站上获取的数据的值仅仅是 0 和 1，不宜采用皮尔逊系数，而是采用了 Tanimoto\ncoefficient, 该系数用于表示两者的重合程度。对于两个长度为 \\(m\\) , 值为 0 或 1 的向量 \\(A,B\\)，其 Tanimoto\ncoefficient 计算公式如下：\n\\[\\begin{align}\n\\frac{\\sum_{i=1}^{m}a_ib_i}{\\sum_{i=1}^{m}(a_i + b_i + a_ib_i)}\n\\end{align}\\]\n计算出来的值得范围为 [0.0, 1.0],\n且值越大，表示两者相似性越强。下面是求解 Tanimoto coefficient\n的代码，注意为了用 Tanimoto coefficient 表示距离，最后返回的是 1 -\nTanimoto coefficient，表示距离值越小，两者越相似。\ndef tanamoto(v1,v2):        c1,c2,shr=0,0,0        for i in range(len(v1)):        if v1[i]!=0: c1+=1 # in v1        if v2[i]!=0: c2+=1 # in v2        if v1[i]!=0 and v2[i]!=0: shr+=1 # in both    return 1.0-(float(shr)/(c1+c2-shr))\n","categories":["集体智慧编程"],"tags":["机器学习","集体智慧编程"]},{"title":"《Programming Collective Intelligence》读书笔记 (1)-- 梗概","url":"/2016/01/24/%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(1)/","content":"《Programming Collective\nIntelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。\n本文是关于本书的第一章 Introduction to Collective\nIntelligence , 主要介绍了 collective intelligence 以及 machine learning\n的一些概念。\n\n什么是 collective intelligence\n？\n根据单词直译过来就是 “集体智慧”, 引用原文的解释如下\n\nCollecting answers from a large group of people lets you draw\nstatistical conclusions about the group that no individual member would\nhave known by themselves. Building new conclusions from independent\ncontributors is really what collective intelligence is all about.\n\n可以简单认为就是从一个群体中获取每个个体的信息，经过算法处理后得出能够描述这个群体的一些结论。常见的比如说问卷调查可以认为是一种集体智慧，超市从顾客的购物清单得出顾客的喜好进而调整货物的摆放也可以认为是一种集体智慧。\n什么是 machine learning？\n机器学习，顾名思义，就是让机器具有学习的能力。具体的做法就是先用一些历史数据来训练一个模型，再利用模型去预测新的数据或趋势。训练模型的方法就是机器学习算法，根据实际的应用场景也可以分为多种，如直观的决策树算法、较为抽象的神经网络等。\n机器学习也有其的局限性，主要体现在两个方面：\n第一个方面是机器学习只能凭借其 “见过的数据”（也就是用来训练这个模型的数据）来进行预测归纳，这导致了遇到了新的情况可能出现误判的情况。因此在机器学习中用来训练模型的数据集对模型的效果有很大影响。\n第二个方面是大部分机器学习存在笼统归纳（overgeneralize）的问题，例如你收到朋友的一份邮件，里面很可能出现 “购买” 的字眼，而如果垃圾邮件过滤算法认为出现这个字眼即为垃圾邮件，那么便会把这封邮件归为垃圾邮件过滤掉。但是这种情况也存在解决方法，就是在拉结邮件过滤系统中将这位朋友的邮件均标记为合法邮件。这也说明了只要给机器学习算法更详细的信息进行学习，机器学习算法便能够变得更加精准。\n","categories":["集体智慧编程"],"tags":["集体智慧编程"]},{"title":"《Programming Collective Intelligence》读书笔记 (2)-- 协同过滤","url":"/2016/02/22/%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(2)--%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/","content":"《Programming Collective\nIntelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。\n本文是关于本书的第二章 Making Recommendations\n的前半部分。主要讲述了寻找用户相似性和物品相似性的方法，并在这个基础上讲述如何为用户推荐物品。\n\n推荐这个功能在很多网站或软件都有实现，如淘宝，当当，网易云音乐等。实现推荐的算法也许有很多。本文主要讲的是协同过滤。\n下面分为这几部分来讲述：\n1. 寻找用户相似性的几种方法\n2. 基于用户相似性为用户推荐物品\n3. 寻找物品相似性的方法\n在讲述之前假定一下的数据集：\n# 电影评分数据集,用于后面的的测试  # critics 字典里面每一项是一个用户对若干部电影的评分  critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,'The Night Listener': 3.0},  'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,'You, Me and Dupree': 3.5},  'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,'Superman Returns': 3.5, 'The Night Listener': 4.0},  'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,   'The Night Listener': 4.5, 'Superman Returns': 4.0,'You, Me and Dupree': 2.5},  'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,'You, Me and Dupree': 2.0},  'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},  'Toby': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0,'Superman Returns': 4.0}}  \n寻找用户相似性的几种方法\n欧几里得距离\n基于上面的数据集，最容易想到的第一种方法就是衡量他们所评的分数的距离。这个怎么理解呢？比如说 A 对两部电影的评分是 1 和 2，B 对两部电影的评分书 3 和 4。那么他们评分的距离就是 \\(\\sqrt{(1-3)^2+(2-4)^2\n}\\)。如果有多部电影则以此类推。这相当与把一个用户所有评分用多维空间中的一个点来表示，用户间的相似性用点间的距离来衡量，距离越小，相似度越高。原文称这种距离为欧几里得距离分数（Euclidean\nDistance score）。\n通过 python 实现也很简单：\n# 通过欧几里得距离找用户相似性，为了使返回数值越大，表示相似性越高，对得出的距离取倒数即可，范围为（0,1）  import math  def similarUserWithEuclidean(scores,user1,user2):      commom = [movie for movie in scores[user1] if movie in scores[user2]]      if len(commom) == 0:  #没有相同喜爱的电影          return 0      total = sum([math.pow(scores[user1][movie] - scores[user2][movie], 2) for movie in commom])      similarity=math.sqrt(total)      return 1/(total+1)  \n上面实现的方法虽然简单，但是存在着分数膨胀 (grade\ninflation) 的问题，比如说 A 和 B 两个人的评判标准不一样，A 比较苛刻，给三部电影打的分数是 1、2、1，但是 B 要求不会那么高，给两部电影打分为 4、5、4，如果用第一种方法那他们在二维空间中的距离将会比较大，从而被判为无相似性。那么他们真的是没有相似性吗？\n虽然他们在分数值上存在区别，但是两个人在每部电影的评分差上保持一致性，也可认为他们是相似用户。因为在对于好和不好的标准上每个人都会有自己的度量，也许对于 A 来说 2 就算是好的了，但是对于 B 来说好的标准是 5。假如对于多部电影两者的评分趋势一致，那么可认为两者的品味也是差不多的，可以认为他们有相似性。通过皮尔逊相关系数（Pearson\nCorrelation Coefficient）可以实现上面的功能。\n皮尔逊相关系数\n皮尔逊相关系数取值范围为 [-1,1], 数值为正表示正相关，且越大表示相关性越强；数值为负则为负相关，越小则负相关性越强。计算公式如下：\n\\[\\begin{align} sim = \\frac{\\sum_{c \\in\nI_{ij}}(R_{i,c} - \\overline{R_i})(R_{j,c} -\n\\overline{R_j})}{\\sqrt{\\sum_{c \\in I_{ij}} (R_{i,c} - \\overline{R_i})^2}\n\\sqrt{\\sum_{c \\in I_{ij}} (R_{j,c} - \\overline{R_j})^2} }\n\\end{align}\\]\n公式中的符号意义如下：\n\n\n\n\n\n\n\n符号\n含义\n\n\n\n\n \\(I_{ij}\\)\n 用户 \\(i\\) 和用户 \\(j\\)\n的公共评分集，也就是两者都有评分的物品的集合\n\n\n \\(R_{i,c}\\)\n 用户 \\(i\\) 对物品 c 的评分\n\n\n \\(R_{j,c}\\)\n 用户 \\(j\\) 对物品 c 的评分\n\n\n \\(\\overline {R_i}\\)\n 用户 \\(i\\) 对 \\(I_{ij}\\) 中物品评分的均值\n\n\n \\(\\overline {R_j}\\)\n 用户 \\(j\\) 对 \\(I_{ij}\\) 中物品评分的均值\n\n\n\n皮尔逊系数的公式初看有点长，但是如果对概率论了解的同学可知，对于变量\n\\(X\\) 和 \\(Y\\) ，上面的皮尔逊系数的表达式其实就是 \\[\\frac {X 和 Y 的协方差}{X 的标准差 \\*Y 的标准差}\\]，而这就是概率论中对相关系数的定义。实际上概率论中的相关系数就是皮尔逊提出的这个皮尔逊相关系数率 (这里需要注意的是，对于不同测量尺度的变数，有不同的相关系数可用，而我们接触到的概率论教材里，大多是都是讲皮尔逊相关系数，实际上还有其他的相关系数)。\n用图像直观表示皮尔逊相关系数如下所示，假设 r 为互相关系数：\n\n且一般认为 r 值范围和相关性的对应关系如下，0 表示两者无关：\n\n\n\n相关性强弱\n对应的 r 值\n\n\n\n\n High correlation\n0.5~1.0 or -0.5~ -1.0\n\n\nMedium correlation\n0.3 ~ 0.5 or -0.3~ -0.5\n\n\nLow correlation\n0.1~0.3 or -0.1~ -0.3\n\n\n\n关于皮尔逊系数更详细的资料可参考该链接：http://www.statisticshowto.com/what-is-the-pearson-correlation-coefficient\n实现该功能的代码如下：\n# 通过 Pearson Correlation Coefficient 计算两个用户的相似性,数值绝对值越大相关性，正负表示正相关或负相关  def similarUserWithPearson(scores,user1,user2):      commom = [movie for movie in scores[user1] if movie in scores[user2]]      if len(commom) == 0:  #no common item of the two users          return 0      average1 = float(sum(scores[user1][movie] for movie in scores[user1]))/len(scores[user1])      average2 = float(sum(scores[user2][movie] for movie in scores[user2]))/len(scores[user2])      # denominator      multiply_sum = sum( (scores[user1][movie]-average1) * (scores[user2][movie]-average2) for movie in commom )      # member      pow_sum_1 = sum( math.pow(scores[user1][movie]-average1, 2) for movie in commom )      pow_sum_2 = sum( math.pow(scores[user2][movie]-average2, 2) for movie in commom )    modified_cosine_similarity = float(multiply_sum)/math.sqrt(pow_sum_1*pow_sum_2)      return modified_cosine_similarity  \n上面两种方法是书中提到的，并且计算皮尔逊相关系数的方法与原书有区别，但是结果是一样的。只是原书上对公式进行了进一步的简化。除此之外，还有一类比较常用的方法是通过余弦相似性（Cosine\nSimilarity）判断用户相似性。原理也是用空间中的一个多维向量表示用户对所有电影的评分，用户间的相似性可以通过向量间的夹角表示，取值范围也是（-1,1）。但是因为这种方法也存在上面提到的分数膨胀 (grade\ninflation)，在此基础上提出了修正的余弦相似度（Adjusted Cosine\nSimilarity）。\n修正的余弦相似性\n修正的余弦相似度跟上面提到的皮尔逊系数很相似，包括计算公式，区别在于皮尔逊系数是依据双方共同评分项进行计算，而修正的余弦相似度则对所有项都进行运算。比如说一部电影 A 有评分，B 没有评分，那么计算皮尔逊系数时就不能采用这不电影作为计算的数据集，但是计算修正的余弦相似性则可以利用这个数据。\n修正的余弦相似性的计算公式如下\n\\[\\begin{align} sim = \\frac{\\sum_{c \\in\nI_{ij}}(R_{i,c} - \\overline{R_i})(R_{j,c} -\n\\overline{R_j})}{\\sqrt{\\sum_{c \\in I_i} (R_{i,c} - \\overline{R_i})^2}\n\\sqrt{\\sum_{c \\in I_j} (R_{j,c} - \\overline{R_j})^2} }\n\\end{align}\\]\n公式中的符号意义如下：\n\n\n\n\n\n\n\n符号\n含义\n\n\n\n\n \\(I_{ij}\\)\n 用户 \\(i\\) 和用户 \\(j\\)\n的公共评分集，也就是均被两者评分的物品的集合\n\n\n \\(I_i\\)\n 被用户 \\(i\\)\n评分了的物品的集合\n\n\n \\(I_j\\)\n 被用户 \\(j\\)\n评分了的物品的集合\n\n\n \\(R_{i,c}\\)\n 用户 \\(i\\) 对物品 c 的评分\n\n\n \\(R_{j,c}\\)\n 用户 \\(j\\) 对物品 c 的评分\n\n\n \\(\\overline {R_i}\\)\n 用户 \\(i\\) 所有评分的均值\n\n\n \\(\\overline {R_j}\\)\n 用户 \\(j\\) 所有评分的均值\n\n\n\n这条公式跟上面的皮尔逊相关系数很相似，皮尔逊相关系数的公式如下：\n\\[\\begin{align} sim = \\frac{\\sum_{c \\in\nI_{ij}}(R_{i,c} - \\overline{R_i})(R_{j,c} -\n\\overline{R_j})}{\\sqrt{\\sum_{c \\in I_{ij}} (R_{i,c} - \\overline{R_i})^2}\n\\sqrt{\\sum_{c \\in I_{ij}} (R_{j,c} - \\overline{R_j})^2} }\n\\end{align}\\]\n两者的区别在于分母上，皮尔逊系数的分母采用的评分集是两个用户的共同评分集（就是两个用户都对这个物品有评价），而修正的余弦系数则采用两个用户各自的评分集。\n采用修正的余弦系数的实现代码如下：\ndef user_similarity_on_modified_cosine(scores, user1, user2):      commom = [movie for movie in scores[user1] if movie in scores[user2]]      if len(commom) == 0:  #no common item of the two users          return 0      average1 = float(sum(scores[user1][movie] for movie in scores[user1]))/len(scores[user1])      average2 = float(sum(scores[user2][movie] for movie in scores[user2]))/len(scores[user2])      # denominator      multiply_sum = sum( (scores[user1][movie]-average1) * (scores[user2][movie]-average2) for movie in commom )      # member      pow_sum_1 = sum( math.pow(scores[user1][movie]-average1, 2) for movie in scores[user1] )      pow_sum_2 = sum( math.pow(scores[user2][movie]-average2, 2) for movie in scores[user2] )    modified_cosine_similarity = float(multiply_sum)/math.sqrt(pow_sum_1*pow_sum_2)      return modified_cosine_similarity  \n计算最相似的前 n 个用户\n通过上面任意一种方法找出用户相似度后，可以根据相似度的大小排序，找出前 n 个最相似的用户，实现代码如下：\ndef findSimilarUsers(scores,user,similarFunction = similarUserWithPearson):      similarUser = [(similarFunction(critics, user, otherUser), otherUser) for otherUser in scores if otherUser!=user]      # 使用了dict会将含有kv对的列表封装成一个字典,无法利用列表自带的排序函数sort      # similarDict = dict([(similarFunction(user,otherUser),otherUser) for otherUser in scores if otherUser!=user])      similarUser.sort()      similarUser.reverse()      #也可将上面两行改成:similarUser.sort(reverse = True)      return similarUser # 返回相似度从高到低排序的一个列表  ```  ### 小结传统的计算相似度的方法就是上面提到的三种：**余弦相似度、修正的余弦相似度和相关相似性（也就是计算皮尔逊系数）**。下面是分别采用上面三种方法为测试数据集中的用户`Lisa Rose`找到前6个最相似用户的结果。完整的代码见文末```  # 欧几里得距离，第一个值是分数，第二个值是具体的用户  (0.4444444444444444, 'Michael Phillips')  (0.3333333333333333, 'Mick LaSalle')  (0.2857142857142857, 'Claudia Puig')  (0.2222222222222222, 'Toby')  (0.21052631578947367, 'Jack Matthews')  (0.14814814814814814, 'Gene Seymour')# 皮尔逊系数，第一个值是分数，第二个值是具体的用户  (0.9345507010964664, 'Toby')  (0.747017880833996, 'Jack Matthews')  (0.5940885257860046, 'Mick LaSalle')  (0.5477225575051661, 'Claudia Puig')  (0.39605901719066977, 'Gene Seymour')  (0.3872983346207417, 'Michael Phillips')# 修正的余弦相似度，第一个值是分数，第二个值是具体的用户  (0.8093446482740976, 'Toby')  (0.747017880833996, 'Jack Matthews')  (0.5940885257860046, 'Mick LaSalle')  (0.4743416490252569, 'Claudia Puig')  (0.39605901719066977, 'Gene Seymour')  (0.33541019662496846, 'Michael Phillips')  \n从结果可以看到，修正的余弦相似度和皮尔逊系数大小虽然不一样，但是预测的整体用户相似度分布一致。\n为用户推荐物品\n经过第一步找出相似用户后，可以直接对相似用户之间进行物品推荐，如 A 和 B 相似，且 B 有部分喜欢的电影 A 没看过，就可以为 A 推荐这部分电影。\n为了使得推荐结果更具有代表性，需要考虑多几位相似用户，同时也是为了有更多可推荐的选择，因为可能 B 看过而 A 没看过的电影也不多。\n实现思路如下：为 A 找出了若干位相似用户，每个用户都有一个相似度系数（就是上面的皮尔森系数），找出他们看过而 A 没看过电影集合 T。将每个用户的相似度乘上用户对集合 T 中某部电影的评分便可作为这部电影的得分，得分越高，代表推荐性越强。这乍一看还比较合理。但是一分析也存在一个问题，就是假如一部电影实际上都符合这一批用户的，但是看过的人很少，其他一些不太符合的电影因为看过的人多而的得分高。这就导致了误判。怎么解决呢？\n这里需要引入一个因子来调和这种不平衡性，使得结果能够合理，既然这种不平衡是由于评分的人数不同而引起的，那么就可以从这里着手。将上面相加得到的总分除以对这部电影有评分的所有用户的相似性。\n例如下图就是为上面 Toby 推荐的结果：\n\n实现的代码如下：\n# 给用户推荐物品，物品的评分采用与其他用户的相似度乘上其他用户的实际评分  def recommendItem(scores,user):      userSimilarity = findSimilarUsers(scores, user)      swapUserSimilarity = {v:k for k, v in userSimilarity} # 交换键值，将存储kv对的列表转换为字典，交换后为无序      allMovies = []      for (k,v) in critics.items():          for movie in v:              if movie not in allMovies:                  allMovies.append(movie)      itemScore = []      for movie in allMovies:          scoreSum = 0          similaritySum = 0          for similarity, otherUser in userSimilarity:              if critics[otherUser].has_key(movie):                  scoreSum += critics[otherUser][movie] * similarity                  similaritySum += swapUserSimilarity[otherUser]          itemScore.append((scoreSum/similaritySum, movie))    itemScore.sort(reverse=True)      recommend_movies = [] # 为user推荐的电影      print 'all movies ranking:'      for i,j in itemScore:          print i,j          if j not in critics[user]:              recommend_movies.append(j)      print 'recommended movies for %s:%s' %(user, recommend_movies)  \n寻找物品间的相似性\n上面的推荐方法是通过寻找相似用户进行推荐，是一种 “A 喜欢这个物品，跟他相似的 B 也可能喜欢这件物品” 思想；除此之外，还有一种思想就是 “A 喜欢物品 1，也可能喜欢相似的物品 2”。在这里判断 1 和 2 的相似性就成了关键，也就是如何寻找物品间的相似性。\n乍一看会没有思路，但是实际上很简单，只需要对原始数据做个简单的转换。上面一开始给出的数据集是一个用户对多部电影的评价，现在把换成同一部电影不同用户的评价即可.\n将  'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5},  'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5}}  转换为  {'Lady in the Water':{'Lisa Rose':2.5,'Gene Seymour':3.0},  'Snakes on a Plane':{'Lisa Rose':3.5,'Gene Seymour':3.5}} etc..  \n这样上面求用户相似性的方法是不是同样可以用来求物品间的相似性了？这种基于物品相似性的方法称为基于物品的协同过滤，而上面基于用户相似性的方法则称为基于用户的协同过滤。\n文中完整代码见这里\n文章为笔者总结，如有错误，烦请指出\n","categories":["集体智慧编程"],"tags":["机器学习","集体智慧编程"]},{"title":"《Real-time Personalization using Embeddings for Search Ranking at Airbnb》 阅读笔记","url":"/2020/06/20/%E3%80%8AReal-time%20Personalization%20using%20Embeddings%20for%20Search%20Ranking%20at%20Airbnb%E3%80%8B%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"Real-time\nPersonalization using Embeddings for Search Ranking at Airbnb 是 KDD\n2018 的 best paper, 整篇文章读下来，初看好像只是套了 word2vec 来生成\nuser embedding 和 item\nembedding；但是细读下来，会发现其中有不少细节值得考究，这种风格跟\nyoutube 在 2016 年发表的那篇 Deep\nNeural Networks for YouTube Recommendations\n很像，两篇都是实践性很强的 paper，非常值得看。而且两篇文章分别代表着\ndeep learning 中生成 embedding\n的两大流派：无监督和有监督。本文主要讲的是 Airbnb 的这篇 paper\n的基本做法和一些细节。\n\nAirbnb 的这项工作的业务背景是：用户在其租房 app\n上搜索时，需要返回一个 list 的推荐结果，在 paper 中提到了 Airbnb\n用的是业界比较主流的 Learning To Rank 技术，关于 LTR\n的技术细节可参考微软的 From\nRankNet to LambdaRank to LambdaMART: An Overview，Airbnb 用的也是\npairwise 模式下的 LambdaRank，而这篇 paper 主要描述的是如何通过 word2vec\n的方式生成 user embedding 和 item embedding 作为 feature\n供排序模型使用（文章中也将 item 称为\nlisting，所以后文中这两个词的含义是一样的）。\n这篇文章的主要亮点如下\n1. 将正反馈（用户最终下单，作为 global\ncontext）和负反馈（商家拒接接单，作为额外的一个 data set）的信号加入到\nskip-gram 中进行训练\n2. 做 negative sampling\n时，考虑到具体业务不仅仅在全集上做负采样\n3. 同一个 uid 的样本较为稀疏，因此 user embedding 从 uid\n粒度变为 user type 粒度，避免样本过于稀疏 embedding\n学习不充分\n这 3\n点其实都是作者在考虑到具体业务场景下，所做出的改进，也是笔者认为这篇 paper 最值得学习的地方：不要盲目地套算法，而是要充分考虑到当前具体业务然后做出相应的适配和改进\n离线训练\npaper 中的离线训练可分为两大部分：short-term interest 和 long-term\ninterest\n对于短期 (short-term) 兴趣，利用了 800 billion 个 click session\n来训练了 listing embedding\n对于长期 (long-term) 兴趣，利用了 50 million 个用户的 booked\nlistings 来训练 user type embedding 和 listing type\nembedding\n具体的训练方法基本就是 word2vec 中的\nskip-gram，下面会分别描述这两部分的具体训练细节\nlisting embedding\n在 short-term insterest 中只生成 item embedding，训练的样本就是 click\nsession（类比 nlp 中的一个 sentence），在 paper\n中共的定义就是用户一系列的点击事件；在构建这个训练数据集时，核心就是如何定义 session\n的长度，在 paper 中定义当用户的两次点击超过 30\n分钟时，那这两次点击应该属于两个 session。\n假设总体训练集为 \\(S\\), 每个 session\n为 \\(s=(l_1,l_2...l_M) \\in S\\), 则通过\nword2vec 中的 skip-gram 可写出 loss 如下\n\\[\\begin{align} L=\\sum_{s \\in S}\\sum_{l_i\n\\in s}(\\sum_{-m \\le j \\le m,j \\ne 0}^M \\log p(l_{i+j}|l_i))\\tag{1}\n\\end{align}\\]\n上面的 m 是个超参，表示 skip-gram\n训练时窗口的大小，且上式的概率可通过 embedding 向量和 softmax\n表示成如下形式\n\\[\\begin{align} p(l_{i+j}|l_i) =\n\\frac{\\exp(v_{l_j}^T\nv_{l_{i+j}}^{'})}{\\sum_{l=1}^{|V|}\\exp(v_{l_i}^Tv_{l}^{'})}\\tag{2}\n\\end{align}\\]\n上式中的 \\(|V|\\) 是所有 item\n的集合， \\(v\\) 和 \\(v'\\) 分表表示 input vector 和 output\nvetor，其实就是同一个 item 在 skip gram\n模型中两个参数矩阵的表示，即如下图所示的参数矩阵\nW 和 W'；具体可参考讲义 CS224n:\nNatural Language Processing with Deep Learning\n\n\nskip gram\n\n类似 word2vec\n由于这个集合一般很大，算一次公式（2）的概率时分母的累加的耗时会非常长，为了加速训练，word2vec\n中提出了 negative sampling\n的策略，简单来说，就是对于每个 session 内的每个 clicked item，从其窗口 m\n内选择一些 positive pairs \\((l,\nc)\\) 作为 positive set（记为 \\(D_p\\)）, 同时从整个 item 集合 \\(|V|\\) 中随机选一些 negative pairs \\((l, c)\\) 取一些 item 作为 negative\nset (记为 \\(D_n\\)), 不用 softmax 而是用\nsigmoid 的方式将要求解的参数写成如下形式\n\\[\\begin{align}  \\arg\\max_{\\theta}\n\\sum_{(l,c) \\in D_p} \\log\\frac{1}{1+e^{-v_{c}^{'}v_l}} + \\sum_{(l,c)\n\\in D_n} \\log\\frac{1}{1+e^{v_{c}^{'}v_l}} \\tag{3}\n\\end{align}\\]\n上式中要求解的参数 \\(\\theta\\) 就是\n每个 item 的 embedding \\(v_l\\)，基本就是原始 word2vec 中的 skip-gram\n算法了，下面文章中的几个点可以认为是作者根据业务对 word2vec\n的一些改动，也是非常值得借鉴的\n(1) 利用监督信号作为 global context\n在 airbnb 的业务中，上面的 session\n实际上可分为两大类，分别是最终有下单的 session 和没有下单的\nsession，paper 中分别称其为 booked session 和 exploratory\nsessions，这两个实际上都是监督信号，而在 paper\n中只使用了第一个监督信号，则公式 (3）可写成如下形式\n\\[\\begin{align} \\arg\\max_{\\theta}\n\\sum_{(l,c) \\in D_p} \\log\\frac{1}{1+e^{-v_{c}^{'}v_l}} + \\sum_{(l,c)\n\\in D_n} \\log\\frac{1}{1+e^{v_{c}^{'}v_l}} + \\\\\\  \n\\log \\frac{1}{1+e^{-v_{l_b}v_l}}\\tag{4}\\end{align}\\]\n公式（4）中的 \\(v_{l_b}\\)\n表示的是被下单的 item \\(l_b\\) 的\nembedding，下图则是更直观地显示了这个监督信号是如何起作用的，\n可以看到，每个 centeral listing/item 更新时都会与 booked\nlisting/item 一起算一次概率，因此称这个 listing 为 global\ncontext，其效果相当于加强用户在购买前点击的所有 item\n和最终下单的 item 的联系\n\n\nglobal context\n\n(2) 控制 negative sampling 的采样空间\n上面提到 negative sampling 会随机从全集中抽取 n 个 item\n作为负例，但是在 airbnb 的业务中，一个用户往往只会在一个\nmarket (也就是旅游地点) 中下单，而上面做 negative sampling\n得到的结果往往是， \\(D_p\\)\n里的候选都是同一个 market 的，\\(D_n\\)\n里的候选往往都不是同一个 market 的，paper 称这样的 imbalanced\n并不是最优的，笔者猜测原始是在做预估时候候选的 item\n基本都是同一个 market 的，而这样 \\(D_n\\) 的随机性使得同一个 market 内的正负\nitem 区分度并不高，因此，在原始的 nagative sampling\n基础上增加了一项在同一 market 下随机选取负样本的 \\(D_{m_n}\\), 因此，最终的 loss 形式如下\n\\[\\begin{align}  \n\\arg\\max_{\\theta} \\sum_{(l,c) \\in D_p}\n\\log\\frac{1}{1+e^{-v_{c}^{'}v_l}} + \\sum_{(l,c) \\in D_p}\n\\log\\frac{1}{1+e^{v_{c}^{'}v_l}} + \\\\\\  \n\\log \\frac{1}{1+e^{-v_{l_b}v_l}}+\\sum_{(l,m_n) \\in D_{m_n}}\n\\log\\frac{1}{1+e^{v_{m_n}^{'}v_l}}\\tag{5}  \n\\end{align}\\]\n(3) 冷启动的 item embedding\n在推荐系统中，每天都会有很多新的 item 加入，还有一些 item 压根没有\nclick session，无法通过上面的 word2vec 训练出对应的\nembedding；对于这些处于冷启动的 item，paper\n中用了一种比较常见且有效的手段来处理：根据冷启动的 item\n的属性选取 k 个与其最相似且有 embedding 的 item 然后做 mean\npooling，选取的方法则是根据 item 的一些 meta-data，如\nprice，listing type 等\nuser_type/listing_type\nembeddings\n上面的训练方法中只用了 click session，且只生成了 listing\nembedding，paper 中认为这个是 short-term\ninterest，其实这从上面对 session\n的划分规则就体现了这一点，即两次点击间隔超过 30 min 就认为是两个不同的\nsession，在这种划分规则下，捕获的就是用户的短期兴趣，且一个 session\n内所有的 item 基本上都是同一个 market 的了\n为了（1）捕获用户更长周期的兴趣（2）捕获 cross-market 的\nlisting embedding 的关联，paper 中通过 skip-gram 训练出 user\nembedding + listing embedding, 且相对于前面只训练出 listing embedding 的\nshort-term interest，paper 称这个为 long-term interest。\n前面的训练数据是 800 billion 个 click\nsession，而这里捕获 long-term interest 的训练则 利用了\n50 million 个用户的 booked listings\n假设总体训练集为 \\(S_b\\), 每个\nbooked listing 为 \\(s_b=(l_{b1},l_{b2}...l_{bM}) \\in S\\),\n表示某个用户的历史下单的所有 lisitng（按时间排序）\n比起上面较为丰富的 click\nsession，这个问题更为严峻，主要体现在下面几个方面\n\n数据更为稀疏，前面的是点击事件，这里则是转化事件\n\n某些用户的 booked listing 的长度可能只有 1，这样没法直接用 skip-gram\n来训练\n\n对于每个 entity, 如果需要较充分地学习出其 embedding，每个\nentity 至少要出现 5-10 次，而实际上不少 listing\n被下单的次数是没有 5 次的\n\n其中，问题 2 和 3 实际上都是问题 1 的具体表现，因此，paper\n中解决上面的三个问题方法就是将 id 转为 type，本质上就是将 id\n特征转为一个更泛化的特征，基本的装换方法就是根据 user/listing\n的一些 meta 信息映射成一个 type，然后用这个 type 来代替 user/listing 的\nid，映射表如下图所示\n\n\nid2type\n\n解决上面的数据稀疏问题后，还剩下的一个问题是如何通过 word2vec\n训练出处于同一向量空间的 user_type embedding 和 listing_type\nembedding？因为最原始的 word2vec 中的 sequence\n往往都是同一属性的 item 的\npaper 中解决这个问题的方法也和直接，就是将前面提到的每个 user\n的 booked listing \\(s_b=(l_{b1},l_{b2}...l_{bM}) \\in S\\) 变为\n\\(s_b=(u_{type1},l_{type1}...u_{typeM},\nl_{typeM}) \\in S\\), 值得注意的是，虽然每个 session 的\nsequence 表示的是同一个 user_id，但是其 user_type\n是会随时间变化而变化的，但是 paper 中并没有提到改变 user_type\n的时间窗口，猜测是一旦发生变化就改变，然后插入到上面的 booked listing\n对应的时间位置上，（这里有个问题，这样做似乎是没法解决那些\nbooked listing 长度为 1 的数据）\n\n\ntype-skipgram\n\n做了 negative sampling 后，loss 跟公式 3 是一样的，如下是 central\nitem 是 user type 的情况（listing type 同理）\n\\[\\begin{align}  \\arg\\max_{\\theta}\n\\sum_{(u_t,c) \\in D_{book}} \\log\\frac{1}{1+e^{-v_{c}^{'}v_{u_t}}} +\n\\sum_{(u_t,c) \\in D_{neg}} \\log\\frac{1}{1+e^{v_{c}^{'}v_{u_t}}}\n\\tag{6} \\end{align}\\]\n相比于前面在训练 listing embedding 在做 negative sampling\n对同一个 market 中的 item 做额外的负采样，但是这里并没有做，原因是这里的\nbooked session 中对应的 item 已经是包含了不同 market\n的，因此没必要做额外的处理\n此外，airbnb\n的这个业务背景中也有商家的反馈数据，即可能会存在某些用户下单但是商家不接单的情况（可能的原因是用户信息不完整、评分低等），这些负反馈信息对于\nuser_type embedding 的学习是有好处的，对于这些负反馈信号，airbnb\n是按照如下方法加入到模型中的\n对于每个 central item，训练的数据除了上面的做 negative sampling\n得到的 \\(D_{book}\\) 和 \\(D_{neg}\\)，还定义了一个 \\(D_{rej}\\), 其中包含了 user 历史上被拒绝的\npair \\((u_t, l_t)\\),\n则当前的 central item 是 user（listing 同理） 时， loss\n可写成如下形式，\n\\[\\begin{align} \\arg\\max_{\\theta}\n\\sum_{(u_t,c) \\in D_{book}} \\log\\frac{1}{1+e^{-v_{c}^{'}v_{u_t}}} +\n\\sum_{(u_t,c) \\in D_{neg}} \\log\\frac{1}{1+e^{v_{c}^{'}v_{u_t}}} +\n\\\\\\  \n\\sum_{(u_t,l_t) \\in D_{neg}} \\log\\frac{1}{1+e^{v_{l_t}v_{u_t}}}\n\\tag{7}\\end{align}\\]\n则加入了这个监督信号后的 skip gram 则如下图所示\n\n\nreject skip-gram\n\n训练细节\n上面讲的只是大致的训练过程，但是其中的一些训练细节也是非常重要的，尤其是这种有具体业务背景的工作，\npaper 中只讲了训练 listing embedding 的一些细节\n训练数据细节\n\n如前文所说，用了 800 million 的 click\nsession，划分的标准就是同一个用户的连续两次点击如果超过 30min\n就划分为两个 session，\n\n去掉了一些无效点击（定义为点击后在页面停留时长小于 30s\n的）\n\n对于 booked session 做了 5 倍的 upsampling\n\n训练细节\n\n天级更新，按照时间滑动窗口从当前时间往前取几个月的训练数据\n\n每天都重新训练 listing\nembedding（随机初始化），而不是在之前训练好的基础上做\nfinetune，paper 称这样的效果比做 finetune 的要好\n\nembedding 的维度是 32（做了效果和资源的 trade-off），skipgram\n的时间窗口是 5，训练 10 个 iteration 的效果是最好的\n\n值得注意的是，上面之所以每天能重新训练 listing embedding，\n是因为 Airbnb 没有直接把 embedding 作为一个 feature 输入到模型中，而是将\nembedding 计算出来的 similarity 作为 feature 输入模型；而如果需要把\nuser/listing embedding 作为 feature 落入样本中，重新训练 listing\nembedding 应该是不行的， 原因分析如下\n因为通过 embedding 计算出来的 user/item similarity 是由\nuser/item\n在训练数据集中的相对位置决定了，这个信息在每天的训练数据中的变动其实是比较小的；但是每次重新训练\nembedding 的时候是随机初始化的，这很容易导致同一个训练集两次训练出来的\nembedding 所处的向量空间是不一样的，而如果将 embedding 直接作为 feature\n输入一个 nn 模型，容易导致同一个 user/item\n在昨天和今天的向量差别很大，对于一个 feature\n来说这样来讲显然是不合理的\nembedding 有效性校验\nembedding\n训练出来后，怎么判断其有效性是个值得探讨的问题，尤其是在深度学习缺乏可解释性的情况下，paper\n中用到了以下几种方法，都比较值得借鉴\n\n可视化\n\n计算 listing embedding 之间的 cosine similarity\n\n根据 embedding similarity 对历史日志中的 item 重新排序，看 booked\nitem 的位置（越小越好）\n\n方法（1）是可视化，也是评估 embedding\n有效性常用的方法；通常做法是聚类然后看每个类里面的 item\n的相关性，对于上面训练的 listing embedding，通过 k-means 聚成 100\n个类，且由于 airbnb 中每个 item 可以对应于地图上的某个点，因此 paper\n中做了两种可视化，第一种是将不同类的点用不同的颜色在地图上进行可视化，如下图所示，paper\n称这样聚类的结果对于重新为每个 travel market\n定义其范围也是有好处的，\n\n\nvisulization\n\n第二种可视化则是对于特定的 listing embedding，选取 k-nearest listing\nembedding，直观地看这些 listing 对应的房屋的相似性，如下图所示，\n可以看到房屋的相似性还是比较高的\n\n\nitem visulization\n\n方法（2）是计算 embedding 的 cosine 相似性，如下面两个表中，根据 item\n的一些 meta 信息来划分 listing embedding，然后计算这些划分好的各个类中的\nitem embedding 与其他各个 item embedding 的 cos\n相似性的均值，从结果可知，价格、房屋类型越相似的 item，其对应的\nlisting embedding 的 cos 相似性越高 , 因此这些信息是可以被学习到\nlisting embedding 中的\n\n\nsimilarity\n\n方法（3）是 paper 根据业务去设计的一种评估 embedding 优劣的方法，其\nmotivation 是：利用 embedding 信息来对历史的 search session\n重新排序，并与 search rank model 的历史排序效果做比较。\n基本的做法就是：选取用户某个 booked item 所出现过的所有历史 serach\nsession, 每个 search session 会包含其他的 candidate\nitems；根据用户历史的 clicked items 对应的 embedding，计算每个\ncandidate item 的 embedding 与这些 clicked items 的 embedding 的\nsimilarity，并根据 similarity 重新排序，观察 booked item\n的排序（越小越靠前）\n因为是历史的 session，所以对于 candidate items 已经被 Airbnb 的\nsearch rank 排过序了，因此可作为 baseline，paper 中选取了每个 user 最近\n17 次的点击来计算相似性，效果如图所示，\n\n\nembedding evaluation\n\n上图中各项含义如下\n\nsearch ranking：排序的后验，即历史 search session 的真正排序\n\nd32: 公式 (3) 训练出来的 embedding, 即原始的 word2vec 方法\n\nd32 book: 公式 (4) 训练出来的 embedding，即加上 booked item 作为\nglobal context 训练出来的 embedding\n\nd32 book+neg: 公式 (5) 训练出来的 embedding，即做 nagative sampling\n的时候专门考虑与 item 处于同一个 market 的 nagative samples\n\n从上图可知，d32 book+neg 的效果是最好的\n在线 serving\n上面训练好的 embedding，在 serving 的时候怎么用？paper\n中主要用到了两个地方\n\nSearch Ranking: 用户搜索的排序模型\n\nSimilar Listing Recommemdation：物品页面中相似物品动态的推荐\n\nSearch Ranking\n对于 NN 模型，可以直接把 embedding 当做一个 feature\n输进去，但是 airbnb 的 ranking model 并不是 NN 模型，而是基于树模型的\nLambdaRank，这个模型的细节就不在这里展开了，感兴趣的同学可参考 paper\n中的 4.4 节。\n使用树模型意味着 embedding\n不能直接输入模型，需要构造一些连续值特征。下面主要讲的是这些特征是如何构建以及线上效果\npaper 中主要基于 embedding\n间计算出来的相似性作为连续值特征，构造的特征列表如下图所示\n\n\nfeatureList\n\n上图中的一些符号含义如下\n\n\\(H_c\\): 用户过去两周点击过的\nlisting ids\n\n\\(H_{lc}\\):\n用户点击过且停留时长超过 60s 的 listing ids\n\n\\(H_s\\): 用户点击的某个 listing\n时跳过的其他的 listing ids（如点击了排序第 4 的，就跳过了前 3 个\nlisting）\n\n\\(H_w\\): 用户过去两周添加到心愿单的\nlisting ids\n\n\\(H_i\\):\n用户过去两周联系过但是没有下单的 listing ids\n\n\\(H_b\\): 用户过去两周下单过的\nlisting ids\n\n因此，上面的列表中的前 6 个 Feature 表示的都是当前 candidate\n与用户在历史发生过交互的 lisitng 的一些相似性。除此之外，paper\n还对 feature 做了 market 的划分，比如对于 \\(H_c\\), 将其划分为 \\(H_c(market1)\\), \\(H_c(market2)\\)..., 然后对同一个 market\n内的所有 listing id 的 embedding 做一个 mean-pooling, 称为 market-level\nembedding，然后计算 candidate listing 跟各个 market-level embedding 的\nsimilarity 并取最大那个，即对于 candidate \\(l_c\\), 上面第一个 feature 的计算如下\n\\[\\begin{align} EmbClickSim(l_c, H_c) =\n\\max_{m \\in M} cos(v_{l_i}, \\sum_{l_h \\in m, l_h \\in H_c}v_{l_h})\n\\end{align}\\]\n笔者这里有个疑惑，为什么要取 max\n操作而不是把所有的特征都加进去？因为上面的做法相当于将 market\n和 simlarity 这两个 feature 做了交叉，如果把所有 feature\n输进去也能扩大原始的 feature 的数量。paper\n中并没有针对这一点作出解释，可能是都交叉的时候很多交叉特征的值都为空？\n上面 table 后的这两个 feature 计算方法类似，只是没有再分 market\n计算，因为这里的并不像前面提到的 listing 有明确的 market 概念，\nEmbLastLongClickSim 这个 feature 是为了捕获用户最近的兴趣了，而\nUserTypeListingTypeSim 这个 feature\n则是为了捕获用户长期更泛化的一些兴趣\n上面的特征都攒了 30\n天，下面是特征的覆盖度和重要性（通过树模型训练结果获取），airbnb\n中的模型有 104 个特征，从图中可知，有 5 个 feature 进入了 top20 的重要\nfeature；其中有几个点值得关注\n\nEmbClickSum 的重要性是这些 embedding 构造出来的 feature\n中最重要的，反映了用户短期内的兴趣\n\n描述下单的特征中，用户的长期兴趣比用户的短期兴趣更好（UserTypeListingTypeSim 重要性高于\nEmbBookSim），笔者猜测另一个原因可能是计算 EmbBookSim\n数据会更为稀疏，因为一般对于一个用户来说两周内下单一次的频率是很低的\n\n\n\n featureimportance\n\n此外，标题中的 Real-time 体现在哪里，其实就是 Table 6 中的几个\nfeature 会随着用户的行为的实时变化而变化。\nSimilar Listing\nRecommemdation\nSimilar Listing Recommemdation 做的事情就是在每个 listing\n详情页里推荐相似物品，这里主要利用的就是 listing embeddings 的 cos\nsimilarity 来选出 top-k 个最相似的物品， paper 将当前 airbnb\n的相似物品推荐策略与这个基于 embedding 的方法作比较，线上的 AB\n实验显示这个方法相比与当前的方法 ctr 提升了约 21%，cvr 提升了约 4.9%\n小结\n笔者认为这篇 paper 介绍了三个方面的内容\n\n如何训练生成 embedding\n\n怎么评估训练出来的 embedding 的有效性\n\n线上怎么使用这些 embedding\n\n训练方法是用了 word2vec 中的 skip-gram 方法，训练数据用了用户历史的\nclick session 和 book session，且根据具体业务在 skip-gram\n做了一些改动，如做 market-level 的 negative sampling、将用户预定的\nlisting 作为 global\ncontext、将被商家拒绝的订单作为负例等；同时在转化数据系数情况下，将 id\nembedding 转为 type embedding，缓解了数据系数情况下 embedding\n无法学得很好的问题。除此之外，一些训练细节也值得参考\n评估生成的 embedding 的有效性时，除了最常用的可视化方法，paper\n还用了另外两种值得参考方法，第一种是确定某些维度后，比较这些不同维度上的\nlisting embedding 的相似性，第二种则是根据 listing embedding\n相似性对历史排序结果重新排序，同时看最终被下单的 listing\n在重新排序后的结果\n线上使用 embedding 时，由于 Airbnb 使用的是树模型，不能直接将\nembedding 作为 feature 输入模型，paper 中是通过计算出 embedding\n间的相似性作为连续值特征输入模型的\n最后，知乎上如何评价 Airbnb 的 Real-time\nPersonalization 获得 2018 kdd 最佳论文？也有很多值对这篇 paper\n的解读，都值得一看，推荐看下这个回答\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"《Python 简明教程》学习笔记","url":"/2015/11/19/%E3%80%8APython%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","content":"《python 简明教程》python 入门的一个非常好的文档。如需下载，请点击这里(密码：grjj)。最近又看了一遍，把里面一些容易忽略的知识点记录下来。\n\n运算符\n\n** 乘方，2**3=8\n\n// 取商的整数部分，10//3=3\n\n&amp; 按位与，5&amp;3=1\n\n| 按位或，5|3=7\n\n^ 按位异或，5^3=6\n\n~\n按位取反，~5=-6，取反后数的正负反转，且正数要比负数绝对值小 1\n\nand、or、not 就是 Java 里面的 &amp;&amp;、||、！，但是在 python 里面 &amp;&amp;、||、！是非法字符\n\n数据结构\n\npython 里面常用的数据结构有列表（list）、元组（tuple）、字典（dic) 和集合 (set)\n\n\n\nlist, 用方括号 [] 括起来，里面元素用逗号分隔，元素有序可变\n\n增加元素用 append() 方法，删除元素用 del 方法\n\n\ntuple，用圆括号 () 括起来，里面元素用逗号分隔，元素有序不可变\n\n仅有一个元素时也要加 ,，避免与运算符优先级混淆\n\n常用于打印语句，如 print \"name:%s,age: %d\" %(name,age)\n\n\ndic，用花括号 {} 括起来，里面元素用逗号分隔，元素无序可变，每个元素是用冒号: 分隔的键值对\n\n通过 dic[k] 访问元素，dic[newK]=newV\n增加元素，del dic[k] 删除元素\n\n遍历的方法为\nfor k,v in  dic.items():, 实际上 dic.items() 会返回一个元组的列表\n\n判断某一个 k 是否在 dic if k in dic:\n\n序列，序列不是一种具体的数据结构，而是一类数据结构，字符串，列表和元组均属于序列，序列有一些通用的方法\n\n通过 len() 获取序列的长度\n\n表示序列的范围：[a:b] 表示下表 a 到 b-1，[a:] 表示从下标 a 到最后一个，[:b] 表示从第一个 (也就是 0) 到下标 b-1,[:] 表示所有元素，默认步长为 1，也可以添加多一个参数变为 [a:b:c], 这时步长为 c，与 range () 函数相似\n\n对于一个序列，复制这个序列与给这个序列使用一个别名不同，详见下面代码 \n\n\nlist1=[1,2,3]  list2=list1     #别名  list3=list1[:]  #复制  del list1[1]  print list2  print list3\n控制语句\n\npython 的控制语句有 if，while，for，continue，break，但是没有 switch\n\nif、while 判断条件的判断条件均没有括号\n\nwhile、for 语句均有一个 可选的\nelse 语句，while 语句条件不成立时退出 while 并执行 else 语句；for 语句中的 else 语句则在 for 循环后执行一次；但是如果在 while 和 for 循环里面遇到 break 退出程序，则不会执行 else 语句\n\nfor i in k: 语句里，k 可以是任何序列，包括字符串、列表、元组、字典等\n\nfor 里面常用的 range 函数的范围不包括第二个参数，默认步长为 1，可通过第三个参数选择步长 \n\nrange(1,5) --&gt; 1,2,3,4  range(1,5,2)--&gt;1,3  \n函数\n\n函数没有 return 语句时默认返回一个 None（没有任何东西的特殊类）\n\npass 语句表示一个空的语法句\n\n文档字符串：用来描述函数或者类的功能，一般格式：首行以大写字母开始，句号结尾。第二行是空行，从第三行开始是详细的描述 \n\n        def printMax(x, y):          '''Prints the maximum of two numbers.        The two values must be integers.'''```  文档字符串也可以通过函数或类的`__doc__`属性获取- 一些常见的函数      - `range()`，一般用于for循环      - `dir(module)`,返回module的名称列表，不提供module值时默认是当前module      - `len(arr)`,用于获取序列arr的长度，arr可以是list、tuple、dic或者是string      - `os.system(command)`，执行系统命令`command`      - `time.strftime()`，按指定格式输出当前系统时间,如`time.strftime(\"%Y-%m-%d %H:%M:%S\")`      - `time.sleep(t)`,一般用于循环，让系统休眠t秒      - `string.startswith(str)`,字符串对象的方法，判断字符串string是否以 str开头      - `string.find(str)`,字符串对象的方法，判断 str是否在字符串string里面      - `string.join(list)`,字符串对象的方法，用字符串 string 连接list列表## 模块- 可以导入的模块有两种：（1）标准模块,常见的有os、sys（2）自定义模块，就是.py结尾的python文件  - `import `操作会在`sys.path`列出的目录列表里面查找需要import的模块  - 为了使导入模块的操作更快，会在第一次导入模块时创建模块的pyc文件（字节编译文件）  - 每个模块都有一个`__name__`,当模块被直接执行时，该值为__main__## 类- 语法 `class 类名:`  - 类方法与普通函数形式上最大区别在于类方法的第一个形参必须为`self`,且该形参不需要实参，`self`类似于Java中的`this`指针  - `__init__`为构造函数，`__del__`为析构函数  - 所有的类成员都是公开的，而以双下划线开头的成员是属于类的，是私有的，作为惯例，一般属于类的成员都以单下划线开头  - 继承,语法 `class son(father):`,注意**子类不会自动调用父类的构造函数，因此必须显示调用**，但是不调用时也没有报错。## 异常捕获- 语法```  try：      可能抛出异常的方法  except (错误或异常的元组)： #没有指定异常时捕获所有的错误异常      处理  finally：      无论是否有异常均要执行\n其他一些用法\n\npython 程序第一行的 #！\n目的是确定使用哪个解释器，一般在 Linux 平台下写的是\n#!/usr/bin/env python\n而不是直接写 python 解析器的位置，目的是即使程序在其他机子上跑时也能够找得到 python 解析器；而且程序迁移到 windows 下也不会报错问题，但是\n#!/usr/bin/env python\n并不适用于 Windows 默认的 cmd 窗口，假如直接在 dos/powershell 下输入 python 的文件名，是会用文件关联的程序打开（关联程序可以通过右键设置打开方式来设定）。也可在 Windows 下安装一个 git shell，在 git shell\n下输入 python 的文件名即可执行。\n可以通过内置的 help ( ) 函数来找到函数和类的帮助信息，实际上 help (\n) 是通过抽取文档字符串（DocStrings，也就是函数和类的__doc__属性）并打印输出的\n字符串或者程序在一行放不下时通过反斜杠 \\ 可以在下一行继续写，\n字符串前加上 u 或者 U 表示采用 Unicode 编码，字符串的连接可以使用加号 +\nprint 语句的一些技巧：在 for 循环的 print 最后加逗号可以避免分行；print\nstr*i 可以重复输出 i 个 str\n 强制类型转换一般形式是被转换的内容用圆括号括起来，类型不用扩。如 str(5)\n是从整数转为字符串 ，int(raw_input('input an integer'))\n是从字符串转为整形\n列表综合：从一个已有的列表导出一个新的列表。实例如下：\n\nlist1=[1,2,3,4,5]  list2=[i*2 for i in list1 if i&gt;2] #list2=[6,8,10]  ```  也可从一个列表导出一个字典(dict只能接受一个参数，这里为一个列表)  ```py  l1=[1,2,3]  timesten=dict([(v,v*10) for v in l1 if i &gt;1])#timesten={2:20,3:30}  \n\n函数形参为元组和字典：可用 * 和 ** 加在形参前，代表这是元组和字典，但是不加也能够正常使用。加上的目的是为了能让函数接受不定参数，例子如下：\n\ndef add(x, *args):      total = x      for arg in args:          total += arg      return total  \n这里的 *args\n表示参数数目不定，可以看成一个元组，把第一个参数后面的参数当作元组中的元素。运行\n&gt;&gt;&gt;print add(1, 2, 3, 4)  10  &gt;&gt;&gt;print add(1, 2)  3  \n这样定义的函数不能使用关键词传入参数，要使用关键词，可以这样：\ndef add(x, **kwargs):      total = x      for arg, value in kwargs.items():          print \"adding \", arg          total += value      return total  \n这里的 **kwargs\n表示参数数目不定，相当于一个字典，关键词和值对应于键值对。运行如下\n&gt;&gt;&gt;print add(10, y=11, z=12, w=13)  adding  y  adding  z  adding  w  46  \n","categories":["python","语法"],"tags":["python"]},{"title":"《Smart Pacing for Effective Online Ad Campaign Optimization》阅读笔记","url":"/2019/10/03/%E3%80%8ASmart%20Pacing%20for%20Effective%20Online%20Ad%20Campaign%20Optimization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"《Smart Pacing for\nEffective Online Ad Campaign Optimization》是 Yahoo 在 2015\n发表的一篇关于 budget pacing 的论文，与之前写过的 Budget\nPacing for Targeted Online Advertisements at LinkedIn\n相似，目标也是把预算均匀花完，但是除了这个目标，这篇论文还提出了在预算均匀花完的基础上如何保成本的方法，算是一个多目标优化了。在离线环境和真实环境验证了方法的有效性，是实践性较强的一篇文章，值得一看。\n\n背景\n预算控制的做法目前可分为两大流派：Probabilistic throttling 和 Bid\nmodification，其中 Probabilistic throttling 通过一个概率值 (下图中的\nPacing Rate) 来决定是否计划参竞来控制预算花费速率，而 Bid modification\n则通过直接改价的方式来控制花费速率。两种方式的简单区别如下图所示，\n\n\ntwo methods\n\n本文使用的方法属于第一种方式即 Probabilistic\nthrottling，文章说这么做主要原因有以下三个\n\nProbabilistic throttling 是直接影响 budget 花费的速度，而 Bid\nmodification 则是通过改变 win rate 间接达到影响 budget\n花费的速度的，这种方式有可能导致 budget\n花费波动较大\n bid\nlandscape (即广告投放中的一些统计信息，能够反映投放的竞争情况，更详细可参考\nBid\nLandscapes 和 DSP 的 bidding 算法)\n一般会随着时间变化而变化；以上两点使得通过 Bid modification 来达到\nbudget pacing 比较困难\n使用 Probabilistic throttling 时能够将 pacing control 与 bid\noptimization 解耦，能够分别进行优化\n\n虽然文章是这么列出这三点优点，但是笔者觉得其中有个很大的问题：就是不进行\nbid modification 时，假如当前的 bid 太小，即使 pacing rate 调到了 1\n也拿不到任何的 impression 时 预算岂不是花不出去？\n文章提到说 bid 有单独的一个 bid optimization module\n也许能解决这个额问题，但是并没有详细展开这个 module\n是如何工作的，是否能够解决上面的问题。\n这篇 paper 以及 Budget Pacing for Targeted Online Advertisements at\nLinkedIn 都是通过 Probabilistic throttling 进行 budget pacing 的，而通过\nbid modification 进行的 budget pacing 的 paper\n也不少，可以参考以下两篇\n\nOptimal\ndelivery of sponsored search advertisements subject to budget\nconstraints\nDynamics of\nbid optimization in online advertisement auctions\n\n算法流程\n这部分主要描述文章是如何对问题进行建模和近似求解，主要思想是对一个计划的不同\nrequest 分层（根据 ctr，cvr 等），在不同的分层采用不同的 pacing\nrate；并且根据实时的花费效果调整 pacing\nrate，根据是否需要优化效果，算法又分成了 Campaigns without Performance\nGoals 和 Campaigns with Performance Goals\n问题建模\n文章里主要解决两大类问题，第一类是只让计划均匀花完预算即可，第二类则需要在预算均匀花完基础上保住点击成本 (eCPC),\n对应的方法在文章中称为 Campaigns without Performance Goals 和 Campaigns\nwith Performance Goals，下面会分别介绍。\n文章约定了一些符号以便对问题进行建模\n\n\\(r_i\\): 在第 \\(i\\) 个请求参竞的概率，就是本文描述的 pacing\nrate, \\(s_i \\sim Bern(r_i)\\)\n则表示是否参竞这个事件\n \\(w_i\\)：赢得第 \\(i\\) 个请求的概率\n \\(c_i\\): 赢得第 \\(i\\) 个请求的 cost\n\\(p_i\\): 在第 \\(i\\) 个请求能带来用户点击等行为的概率，\\(q_i \\sim Bern(p_i)\\)\n则表示是否带来点击这个事件\n\n通过上述符合可定义\n\n\\(C = \\sum_i s_iw_ic_i\\)\n为计划的总 cost\n\\(P = \\frac{C}{\\sum_i s_i w_i\nq_i}\\) 为计划的 performance,\n即成本（如点击成本，转化成本等）, 文中描述为 eCPC 即点击成本\n\n此外，计划在一天的总预算 \\(B\\) 和总\ncost \\(C\\) 都会被划分到 K\n个时间片内，以此来度量 budget pacing 的效果；即 \\(B = \\sum_{t=1}^{K}B^{(t)}\\)， \\(C = \\sum_{t=1}^{K}C^{(t)}\\)，则 budget\npacing 的 error 定义如下\n\\[\\begin{align} \\Omega(B, C) =\n\\sqrt{\\frac{1}{K}\\sum_{t=1}^{K}(C^{(t)}-B^{(t)})^2}\n\\end{align}\\]\n则对第一类问题即只需要 pacing 预算的问题文章建模如下\n\\[\\begin{align}\n\\min_{r_i} P \\\\\\\ns.t\\quad C = B, \\Omega(C, B) \\le \\epsilon\n\\end{align}\\]\n上面的 \\(\\epsilon\\) 表示可容忍的\npacing 误差，总体表示在满足预算均匀花完时最小化成本即 \\(P\\)\n上面建模中一个值得注意问题是最小化成本是否合理？由于文章是\nDSP\n视角出发进行建模的，从仅考虑广告主的利益角度来说，最小化成本是合理的；但是对于一个完整的广告系统即\nSSP，ADX 和 DSP 都在同一个平台时，这并不适用；如微信、抖音、微博等 app\n的广告，SSP、ADX 和 DSP 往往都由这个 app\n所在的公司运作，如果一味的最小化成本会损害平台的利益，因此更合理的做法是尽可能让广告主的真实成本与其预期成本接近\n而对于第二类不仅需要 pacing 预算同时需要保证 performance\n的问题，文章建模如下\n\\[\\begin{align}\n\\min_{r_i} \\Omega(C, B) \\\\\\\ns.t\\quad P \\le G,  B-C \\le \\epsilon\n\\end{align}\\]\n上面的 \\(G\\)\n表示广告主的目标成本，其他符号含义与前面描述一致，表示在广告主成本不超和预算花费不超的情况下，让预算更\npacing 地花完。\n问题求解\n文章对两种问题提出了两种上面建模，但是在实际求解时并没有对上面这两个最优化问题进行求解，而是通过启发式方法 (heuristics) 近似求解这个问题，\n这个启发式方法其实就是一个控制算法，\n而至于为什么启发式方法能够近似求解上面的最优化问题，文章并没有说明。\n文章的一个亮点是对一个广告计划的所有 requests\n做分层，分层的依据是 request 在这个计划的 ctr，而不同的 pacing\nrate 是不一样的，当一个 request 到来的时候，先判断这个 request\n属于哪一个层，然后再取出这个层对应的 pacing rate 作为最终的 pacing\nrate；ctr 越高的层，其对应的 pacing rate 就越大\n而每个层的 pacing rate\n会在固定时间间隔进行调整，调整的方法就是下文要介绍的两个控制算法。\nCampaigns without\nPerformance Goals\n这部分的控制算法针对的问题是第一类问题即只需要保证 budget 被\npacin'g\n根据上面的分层方法，假设每个广告计划的所有 requests 会被分成 \\(L\\) 层，则在第 t-1 个时间片内各层的 pacing\nrate 记为 \\(r^{(t-1)} = (r_1^{(t-1)},\nr_2^{(t-1)}...r_L^{(t-1)})\\), 且各层的 cost 记为 \\(c^{(t-1)} = (c_1^{(t-1)},\nc_2^{(t-1)}...c_L^{(t-1)})\\)\n前面提到一天的总预算 \\(B\\)\n会根据时间片划分成 \\(K\\) 份小预算，即\n\\(B = (B^{(1)}, B^{(2)}....B^{(K)})\\),\n但是实际花费中不能保证每个时间片的 cost\n都刚好达到分配的预算值，因此如果前面的时间片中出现了少投或超投情况，就要把少投或超投那些部分均摊到后面的时间片中，所以每个时刻的预算需要根据前面的花费来调整，记的经过调整的预算为\n\\(\\hat{C}^{(t)}\\)，则 \\(\\hat{C}^{(t)}\\) 可表示为\n\\[\\begin{align} \\hat{C}^{(t)} = B^{(t)} +\n\\frac{B_m - \\sum_{i=t}^KB^{(i)}}{K-m} \\end{align}\\]\n上式中的 \\(B_m\\) 表示经过了 \\(m\\) 个时间片后剩余的实际预算，分母 \\(B_m - \\sum_{i=t}^KB^{(i)}\\)\n表示当前预算花费是否超过了预期 (&lt;0) 或者不满足预期 (&gt;0)，并通过分母均摊到后面的\n\\(K-m\\) 个时间片中。\n则调整 \\(L\\) 个的 pacing rate\n的控制算法如下图所示，图中的 \\(R =\n\\hat{C}^{(t)} - C^{(t-1)}\\), 表示如果按照前一时间片的\npacing rate 即 \\(r^{(t-1)}\\)\n投放时，当前时间片的预算 \\(\\hat{C}^{(t)}\\) 能否花完。则当\n\\(R &gt; 0\\) 时，需要提高当前时间片的\npacing rate，反之需要降低 这一时间片的 pacing rate\n\n\nAdjustWithoutPerformanceGoal\n\n上面的算法认为 pacing rate 的大小即 \\(r^{(t)}\\) 与实际的 cost 即 \\(c^{(t)}\\) 是成正比的，因此调整 pacing rate\n也会根据 cost 的变化比例来调整\n除此之外，上面算法还有几点细节需要注意\n\n\\(l'\\) 层表示 pacing rate\n为非 0 的最小的层\n当需要提升 pacing rate 时，是从第 \\(L\\) 层到 \\(l'\\) 层进行的；而需要降低 pacing rate\n时，是从第 \\(l'\\) 层到第 \\(L\\) 层进行的，其目的都是优先提升\nctr 高的层的 pacing rate，优先降低 ctr 低的层的 pacing\nrate，从而达到上面最小化成本的目的\n trial rate 的目的令 pacing rate 非 0\n的最小层的下一层以一个很小的 pacing rate 进行参竞（pacing rate\n会随着层数增加而增加），目的是后面预算花费加速做准备， trail\nrate 在这里是一个超参，是一个很小的值，后面会介绍该如何选取\n\nCampaigns with Performance\nGoals\n这部分的控制算法除了要保证 budget 被 pacing\n花完，还要保证成本小于预设的成本；而每一层的成本在这里记为 \\(e = (e_1, e_2...e_L)\\), 其中 \\(e_i = \\frac{CPM}{1000*p_i}\\)\n考虑了成本的算法在主要在上面的算法上增加了一个 ExpPerf\n函数，用于计算当前所有层的联合期望成本，该函数的定义如下\n\\[\\begin{align} ExpPerf(c^{(t-1)},\nr^{(t-1)}, r^{(t)}, e, i) = \\frac{\\sum_{j=i}^{L} \\frac{c_j^{(t-1)}\nr_j^{(t)}}{r_j^{(t-1)}}}{\\sum_{j=i}^{L} \\frac{c_j^{(t-1)}\nr_j^{(t)}}{r_j^{(t-1)}e_j}} \\end{align}\\]\n上面的式子中的分子可理解为各层在当前时间片的 cost\n之和，分母可理解为各层在当前时间片的点击数之和，因此总体就是当前的时间片所有层的联合期望成本\n则成本约束的 budget pacing 控制算法过程如下所示\n\n\nAdjustWithPerformanceGoal\n\n如果计算出当前的联合期望成本大于目标成本时，会从第 1 层到第 L\n层逐渐减少每层的 pacing rate，而这分为两种情况\n\n如果当前层到第 L 层的联合期望成本大于目标成本，则当前层的 pacing\nrate 会直接置为 0；\n如果当前层到第 L 层的联合期望成本不大于目标成本，当前层的 pacing\nrate 会根据上面算法第 7\n行来调整，然后算法就终止了，根据文章描述其含义是调整当前的层使得总体的期望成本达到目标的期望成本；\n\n但是文章并没具体解释第七行的公式的含义，比如说如分母为 0 时，\\(r_{l}^{(t)}\\) 岂不是变为无穷大？\n超参选取\n上面提过的算法的流程中涉及到多个超参数，如层数 \\(L\\) 以及 trial\nrate，下面介绍如何选取这两个超参\n对于一个新计划，决定其层数 \\(L\\)\n的方法如下，首先找到与这个新计划最相似的老计划 \\(a\\), 并找到这个老计划最合适的 pacing rate\n\\(r_G\\), 则新计划的层数可计算为 \\(L = \\lceil \\frac{1}{r_G} \\rceil\\),\n计算的逻辑其实就是要找到这个计划最合适 pacing rate 粒度对应的层数。\n一旦层数决定后，在第一个时间片内会每个层都会通过 \\(r_G\\)\n初始化，在第一个时间片积累了数据后，后面会根据上面提到的算法调整各层的\npacing rate。\n前面提到 trail rate 的目的令 pacing rate 非 0\n的最小层的下一层以一个很小的 pacing rate 进行参竞，因为 pacing rate\n会随着层数增加而增加，本来这一层的 pacing rate 应该是 0\n的，但是这里给了一个很小的 trail\nrate，目的是为了让后面加快预算花费做准备。其设置方法如下\n将当前时间片的预算 \\(\\hat{C}^{(t)}\\)\n划分一小部分，记为 \\(\\lambda\\)(e.g.\n\\(\\lambda\\) = 1%), 假设当前层为第 \\(l\\) 层，则其 trail rate = \\(r_l^{(\\*)} ×\\frac{\\lambda\n×\\hat{C}^{(t)}}{c_l^{(\\*)}}\\) ，而 \\(r_l^{(\\*)}\\) 和 \\(c_l^{(\\*)}\\) 则是 \\(l\\) 层的历史 pacing rate 和 cost。\n实验效果\n文章对提出的方法进行了 AB 实验评估，并与论文 Budget Pacing for Targeted\nOnline Advertisements at LinkedIn\n中的方法进行了比较，预算的花费时间定义为一天，而时间片定义为 15 分钟；超参的值为\n\\(r_G = 0.01\\) 以及 \\(\\lambda = 1\\%\\)\n考察的指标主要有三个 1）performance 2）budget spending 3）spending\npattern\n而作为对比的 baseline\n方法也是使用本文的方法，只是只有一层，下面是选取的三个计划进行 AB\n测试的效果，实验组的层数为 8；其中计划 1 和 3 是没有 performance\n目标也就是成本约束的，而计划 2 的 ecpc\n为 2.5，从结果来看也能较好地满足成本约束\n\n\nexp1\n\n除此之外，还对比了两种方法在保证成本约束的效果，结果如下图所示，从结果来看，分层处理后能够有效降低成本，同时花费不变\n\n\nexp2\n\n同时文章与另外一篇论文 Budget Pacing for Targeted Online\nAdvertisements at LinkedIn 中的 budget pacing\n方法进行了对比，结果如下所示\n\n\nexp3\n\n从结果来看，相比于 Budget Pacing for Targeted Online Advertisements\nat LinkedIn 中的方法，这篇文章调节 budget\n的花费得更为平缓，同时成本的控制也更好，因为 Budget Pacing for Targeted\nOnline Advertisements at LinkedIn 论文中的方法没有考虑成本问题。\n工程实现\n本文的方法的工程实现架构如下图所示，架构不复杂，左边的部分通过\nmessage queue (如 kafka) 搜集后验数据，用于更新 in-memory data source\n中各层的 pacing rate；右边的 Controller 则会利用 in-memory data source\n中的 pacing rate 进行 budget pacing\n\n\narchitecture\n\n因为 pacing rate 是存储在内存中，存在丢失问题，因此会根据 message\nqueue 中的日志进行恢复，其原理看起来跟 redis 的 AOF 模式恢复类似。\n小结\n这篇文章主要提出了两个基于 Probabilistic throttling 的算法用于解决\nbudget pacing 问题，\n第一个算法仅解决了如何让预算更平缓地花完，第二个算法在第一个算法的基础上，不仅让预算平缓花完，还能保成本。文章中的方法的一个亮点在于根据\nctr 等指标将 request 分层，不同的层采用不同的 pacing rate，从而能够针对\nctr 高的 request 给予更高的 pacing rate，而这也是文章中的算法能够优化\nperformance 的原因。\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"《Reducing the Dimensionality of Data with Neural Networks》阅读笔记","url":"/2016/11/28/%E3%80%8AReducing%20the%20Dimensionality%20of%20Data%20with%20Neural%20Networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"Reducing\nthe Dimensionality of Data with Neural\nNetworks是对深度学习有重要影响的一篇论文，可以说是拉开了深度学习的帷幕，该论文出自\nHinton\n大神之手。本文是读了论文后结合其他一些参考资料整理成的读书笔记。\n\n简介\n该论文主要讲述了通过神经网络对数据进行降维，并通过多项实验结果证明通过神经网络进行降维的效果要优于传统的降维方法\nPCA (Principal component analysis,\n主成成分分析)。但是要达到这种效果，需要有一个前提，那就是神经网络中的参数在初始化的时候不能随机初始化，而是要有一个预训练的过程，论文中通过\nRBM (Restricted Boltzmann\nmachine，受限玻尔兹曼机) 来实现这个预训练的过程，利用 RBM\n对神经网络中的参数进行逐层预训练，然后将训练出来的参数作为神经网络的初始化参数。\n数据的降维\n在机器学习中，原始数据往往会存在着各种各样的问题，样本的特征数目过多是其中之一，当样本的特征过多的时候往往会存在冗余的信息和噪声；而当特征数目原大于样本数目的时候容易导致过拟合，使得模型的泛化能力弱；除此之外，特征数目过多的样本也需要更长的训练时间，训练的成本较高。\n基于上述的原因，在训练模型之前往往需要对数据进行一个降维的操作，常见的降维方法有\nPCA\n等。降维直观的反映就是样本特征数目的减少，同时原始的信息（包括有用的信息和噪声）也会有损失；从另外一个角度来看降维就是提取原始数据的主要特征，而神经网络的结构特点恰恰为其进行特征提取提供了可能性，下面就讲述如何通过神经网络进行特征的提取，也就是论文的主要工作。\n自编码器与逐层预训练\n自编码器 (autoencoder) 是论文提出的一种特殊的神经网络，由\nEncoder 和 Decoder 两部分构成，其中\nEncoder 的作用是降维，而 Decoder\n的作用是从降维的后的特征中恢复出原始特征。其结构如下所示：\n\n上图主要展示了通过自编码器对图像进行特征压缩并复原的过程。其中左边部分是初始训练时候的状态，Encoder\n将原图像 2000 维的特征压缩到了 30 维， 而 Decoder\n将压缩后得到的 30 维的图像恢复成原来的 2000 维，由于还没对网络进行训练，所以此时的图像会比较模糊。右边部分则是通过经典的\nBP (Backpropagation, 反向传播)\n算法对网络进行训练后的恢复效果，得到的效果与原图像已经非常接近了。\n从上面的描述看来，自编码器的训练方法与传统的神经网络的训练没有差别。但是论文中指出了网络的初始化参数要足够好，才能利用这种训练方法得到比较好的效果。\n网络的初始化参数就是上图中的 \\(W_1,W_2,W_3,W_4\\),\n我们知道，神经网络的主要是通过前向传播和反向传播这两个过程来训练网络中层与层之间的参数，通过这些网络间的参数来拟合数据的内在特性。但是在开始训练前，必须要给网络中的参数赋一个初始值，由于对数据没有任何的先验知识，这种初始化赋值往往是随机的，在多层网络中随机初始化参数存在着以下问题：当随机初始化的值过大时容易陷入局部最优，当随机初始化的值过小时训练会比较困难（在反向传播的时候梯度很快趋于 0，错误信息传不到前面的层）。\n而论文中所说的 “网络的初始化参数足够好” 其实是要通过逐层训练的方法先训练出一批参数值作为初始值赋给\n\\(W_1,W_2,W_3,W_4\\)，然后再进行后面的前向传播和反向传播来训练整个网络。\n下面主要讲述如何训练出网络的初始化参数，而这也是本文的最重要的工作。\n逐层预训练过程\n在逐层预训练中采用的模型是 RBM, RBM 的结构图如下所示\n\n从上面的的结构图可知， RBM\n是一个二层全连接的双向网络，二层指的是隐藏层 (h 节点) 和可视层 (v 节点)，其中各层节点的大小关系没有要求（也就是\nm 可以大于 n 也可以小于 n），双向指数据既可从可视层传播到隐藏层，也可从隐藏层传输到可视层。\nRBM 包含的参数有\n\n权重矩阵 \\(W_{nm}\\)\n 隐藏层偏置量 \\(c = (c_1, c_2, c_3, ...\nc_n)\\)\n 可视层偏置量 \\(b = (b_1, b_2, b_3, ...\nb_m)\\)\n\n其中偏置量的取值为 0 或 1。\n由于传播方向是双向的，这里先不加证明给出两个方向的传播的公式，具体的证明看下一节的原理与推导。\n从可视层传到隐藏层 \\[P(h_i=1|v) =\n\\sigma(\\sum_{j=1}^mw_{ij}v_j+c_i)\\tag{3-1}\\]\n从隐藏层传到可视层 \\[P(v_j=1|h) =\n\\sigma(\\sum_{i=1}^nw_{ij}h_i+b_j)\\tag{3-2}\\] 其中 \\[\\begin{align}  \\sigma(x) = 1/(1+e^{-x})\\tag{3-3}\n\\end{align}\\]\n由上面的传播公式可知，两个传播过程计算出来的都是一个概率值，就是传播的目标点取 1 的概率，实际中赋值时按照均匀分布产生一个 0 到 1 之间的随机浮点数，如果它小于 \\(P(v_j=1|h)，v_j\\) 的取值就是 1，否则就是\n0。\n有了上面关于 RBM 的基础知识，下面就是逐层预训练的具体过程\n\n正向过程：样本 \\(v\\) 通过公式 (3-1)\n从可视层输入得到 \\(h\\)\n 反向过程：隐藏层 \\(h\\) 通过公式\n(3-2) 回传到可视层得到 \\(v'\\), 利用\n\\(v'\\)\n再进行一次正向传播得到隐藏层的 \\(h'\\)\n 权重更新过程：更新公式为 (其中 \\(\\alpha\\) 为学习率) \\[\\begin{align} W(t+1) = W(t) +\n\\alpha(vh^T-v'h'^T)\\tag{3-4} \\end{align}\\]\n 迭代上面过程直至权重 \\(W\\)\n收敛\n\n上面的公式中的 \\(v,h,v',h'\\)\n均为向量，且公式 (3-4) 在原文表述为 \\(\\Delta\nW_{ij} = \\varepsilon ( (v_jh_j)_{data} -\n(v_jh_j)_{recon})\\)，但是含义是一致的，就是利用被压缩后再恢复的数据与原始数据的误差来调整二层网络间的参数，使得恢复出来的数据尽可能与原始数据接近，也就是要让被压缩后的数据尽可能的保留着原始数据的特征。\n上面的训练过程中只是训练了相邻两层网络间的参数，而神经网络一般是有多层的，所以需要利用这种方法逐层进行训练，这也是逐层预训练说法的来由。所以上面通过自编码器对图像进行特征压缩并复原的完整过程如下图所示：\n\n首先利用 RBM\n逐层训练出网络的初始化参数，后面就是传统神经网络的训练过程了，通过前向传播和反向传播来调整网络间的参数，从而达到收敛。\n原理与推导\n上面主要讲述了参数逐层预训练的具体过程，下面主要讲述这种方法的思想以及推导对上面不加证明给出的传播过程的公式。\n在自编码器预训练的过程中， RBM\n的主要作用是在隐藏层尽可能保留从可视层输入的数据的主要特征（因为特征维度的压缩会导致数据的损失），而度量其保留程度的指标就是利用压缩了的特征恢复出来的图像与原图像的概率分布的差别，差别越小，保留的特征就越好。利用这个差别，可以调整\nRBM 中的参数，从而使得误差逐步减小。\n因此，上面的正向过程 ( \\(v \\rightarrow\nh\\) ) 是一个特征压缩过程，影响了真实数据的特征，而反向过程 ( \\(h \\rightarrow v', v'\\ \\rightarrow\nh'\\) ) 就是利用压缩后的特征 (\\(h\\)) 重现真实数据的特征（\\(v'\\)）, 权重更新过程则是利用他们的误差来更新权重矩阵，误差在这里表示为\n\\((vh^T-v'h'^T)\\)。\n上面的正向过程和反向过程中的两个关键公式 (3-1) 和 (3-2)\n我们是不加证明的使用的，下面对其进行简单推导，推导的思路是从 RBM\n的能量函数推导出概率模型，再从概率模型推导极大似然估计。\n首先， RBM 诞生于统计力学，统计力学中为其定义的一个能量函数, 针对下图的 RBM\n\n其能量函数表示如下：\\[E(v,h) =\n-\\sum_{i=1}^n\\sum_{j=1}^m\nw_{ij}h_iv_j-\\sum_{j=1}^mb_jv_j-\\sum_{i=1}^nc_ih_i\n\\tag{3-5}\\]\n定义出这个能量函数又有什么作用呢？根据参考资料，原因有以下几个\n&gt; 第一、RBM 网络是一种无监督学习的方法，无监督学习的目的是最大可能的拟合输入数据，所以学习 RBM 网络的目的是让 RBM 网络最大可能地拟合输入数据。\n\n第二、对于一组输入数据来说，现在还不知道它符合那个分布，那是非常难学的。例如，知道它符合高斯分布，那就可以写出似然函数，然后求解，就能求出这个是一个什么样个高斯分布；但是要是不知道它符合一个什么分布，那可是连似然函数都没法写的，问题都没有，根本就无从下手。\n\n\n第三，统计力学的结论表明，任何概率分布都可以转变成基于能量的模型，而且很多的分布都可以利用能量模型的特有的性质和学习过程，有些甚至从能量模型中找到了通用的学习方法。换句话说，就是使用能量模型使得学习一个数据的分布变得容易可行了。\n\n因此，基于上面的能量函数，可以定义出一个可视节点与隐藏节点间的联合概率\n\\[\\begin{align} P(v,h) =\n\\frac{e^{-E(v,h)}}{\\sum_{v,h}e^{-E(v,h)}} \\tag{3-6}\n\\end{align}\\]\n该公式也是根据统计热力学给出的，具体参看参考文献。有了联合概率，就可以求出其条件概率如下所示：\n\\[\\begin{align} P(v) =\n\\frac{\\sum_he^{-E(v,h)}}{\\sum_{v,h}e^{-E(v,h)}} \\tag{3-7}\n\\end{align}\\] \\[\\begin{align} P(h) =\n\\frac{\\sum_ve^{-E(v,h)}}{\\sum_{v,h}e^{-E(v,h)}} \\tag{3-8}\n\\end{align}\\] \\[\\begin{align} P(v|h) =\n\\frac{e^{-E(v,h)}}{\\sum_ve^{-E(v,h)}} \\tag{3-9} \\end{align}\\]\n\\[\\begin{align} P(h|v) =\n\\frac{e^{-E(v,h)}}{\\sum_he^{-E(v,h)}} \\tag{3-10}\n\\end{align}\\]\n上面的这些概率分布也叫 Gibbs 分布，这样就完成了从能量模型到概率模型的推导，下面是从概率模型推导出极大似然估计。\n现在回到求解的目标:\n让 RBM 网络的表示 Gibbs 分布最大可能的拟合输入数据的分布。那么这两个分布的 KL 散度如下所示，KL 散度主要用于表示两个分布的一个相似度，其值越小，表示两个分布越相似：\n\\[\\begin{align} KL(q||p) = \\sum_{x \\epsilon\n\\Omega} q(x) ln\\frac{q(x)}{p(x)} =\\sum_{x \\epsilon \\Omega}\nq(x)lnq(x)-\\sum_{x \\epsilon \\Omega} q(x)lnp(x) \\tag{3-11}\n\\end{align}\\]\n上式中的 \\(q(x)\\)\n是输入样本的分布，样本确定的时候，该分布也确定了下来，而 \\(p(x)\\) 表示通过 RBM\n后输出样本的分布，也就是公式 (3-8)\n表示的隐藏层的分布。当输入样本确定的时候，要最小化公式\n(3-9) 的 KL 距离，实际上就是要最大化公式 (3-9) 中的 \\(lnp(x)\\), 而 \\(lnp(x)\\) 与\nRBM 网络中的参数相关，实际上就是进行一个极大似然估计求出网络中的参数。具体的数学推导过程参见这里。通过求解便可以得到公式 (3-1)\n和 (3-2), 也就完成了从概率模型到最大似然的推导。\n实验效果对比\n论文通过若干的实验证明了通过自编码器对图像进行压缩后再恢复的效果要优于 PCA，需要注意的是图像的压缩实际上就是特征的压缩，也就是一个特征提取或者说降维的过程，下面是具体的实验结果。\n### 实验一：曲线图像的压缩与恢复 下图是实验数据 \n上图中从上到下每一行对应于下表中从上到下的每一行\n\n\n\n方法\n特征数\n均方误差\n\n\n\n\n原图像\n 786\n\n\n\n 自编码器\n 6\n1.44\n\n\nPCA\n6\n7.64\n\n\nLogistic PCA\n18\n2.45\n\n\nPCA\n18\n5.90\n\n\n\n实验二：手写数字图片的压缩、恢复与分类\n该实验采用 MNIST 数据集，下图是实验数据 \n上图中从上到下每一行对应于下表中从上到下的每一行\n\n\n\n方法\n特征数\n均方误差\n\n\n\n\n原图像\n 786\n\n\n\n 自编码器\n 30\n3.00\n\n\nLogistic PCA\n30\n8.01\n\n\nPCA\n30\n13.87\n\n\n\n上面是对图像进行压缩与恢复的实验，下面是提取每个手写数字两维的特征（原始维度为 786 维）进行分类的结果，图 A 是原始数据进行分类后的结果，图 B 是通过自编码器中的\nEncoder\n压缩到两维后再分类的结果。可以看到，通过自编码器得到的两维特征已经能够将各个数字较好分离开。\n\n实验三：人脸图像的压缩与恢复\n实验数据如下所示 \n上图中从上到下每一行对应于下表中从上到下的每一行\n\n\n\n方法\n特征数\n均方误差\n\n\n\n\n原图像\n 625\n\n\n\n 自编码器\n 30\n126\n\n\nPCA\n30\n135\n\n\n\n实验四：词向量的降维与分类\n上面的实验均是针对图像的，但是实际上通过自编码器中的\nEncoder\n对原始数据进行提取特征后，可利用这些特征进行分类和回归。这个实验就是对词向量进行降维后并进行分类，主要比较自编码器和 LSA(\nLatent Semantic Analysis, 隐性语义分析) 对词向量降维后分类的效果。\n\n上图中 A\n是文档相似性判断的准确率，通过 LSA 分类要提取文档向量的 50 维才能达到自编码器提取前 10 维进行分类的效果，图\nB\n是采用 LSA 对提取了 2 维的词向量 (原始为 2000 维) 进行分类的结果，可以看到完全无法分开，而图 c 是自编码器提取 2 维后的分类结果，可以看到分类结果要大大优于图 B 的效果。\n总结\n这篇论文由深度学习的开山鼻祖 Geoffrey E. Hinton 2006\n年发表在 science\n上，论文虽然只有短短的四页，但是做了两个非常重要的工作\n(1)\n多层的神经网络具有优秀的特征学习能力，能够学习到数据更本质的特征 (2)\n多层神经网络的初始化参数可通过逐层预训练获得\n从上面的四个实验结果中可以看到自编码器提取的特征均要优于传统的 PCA 和 LSA，也就是上面说的第 (1) 点；但是多层的神经网络很早就已经提出了，只是因为一直存在着初始化参数赋值的困难（过大陷入局部最优，过小梯度消失）而无法应用到实际中，本论文通过\nRBM\n逐层预训练得到多层神经网络的初始化参数，从而解决了这个问题，也就是上面说的第 (2) 点，也正是这个工作为多层神经网络或者说深度学习在实际中的应用拉开了帷幕。\n参考文献： Reducing\nthe Dimensionality of Data with Neural Networks 深度学习读书笔记之 RBM（限制波尔兹曼机）\n能量模型 (EBM)、限制波尔兹曼机 (RBM)\n深度学习方法：受限玻尔兹曼机 RBM（一）基本概念\n深度学习方法：受限玻尔兹曼机 RBM（二）网络模型\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"《认知红利》阅读笔记 (1)- 概念重塑","url":"/2021/08/22/%E3%80%8A%E8%AE%A4%E7%9F%A5%E7%BA%A2%E5%88%A9%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(1)-%E6%A6%82%E5%BF%B5%E9%87%8D%E5%A1%91/","content":"最近在看《认知红利》，提到认知，笔者第一时间想到的是认知的四个阶段：不知道自己不知道\n-&gt; 知道自己不知道 -&gt; 知道自己知道 -&gt;\n不知道自己知道；以及《三体》里那句有名的\n“弱小和无知不是生存的障碍，傲慢才是”，而笔者觉得傲慢的原因正是因为 “不知道自己不知道” 且不愿意去了解，如同井底之蛙一样的看不到更先进的技术，一如闭关锁国的清末，三体里被水滴几乎团灭的人类太空舰队。\n回到这本书，可以说是在解决 “不知道自己不知道” 的问题，而且书里不仅提供了鸡汤，还提供了勺子 (即方法论)。虽然这本书在豆瓣上褒贬不一，诟病之处无非是这本 “借鉴” 了很多其他作品；但是对笔者这种看不懂那些晦涩的大部头、时间不多的 “打工人” 来说，也不失为一种较好的选择，因为这本书可以说是对其他一些作品里的内容的提炼。整本书可分为两大部分：概念重塑和大脑升级，前者主要是概念重构，后者则侧重方法论。本文主要与概念重塑相关，而且只记录了对笔者而言印象比较深刻的观点，推荐读一下原书。\n\n关于概念重塑这部分，书中主要分为了三个部分：重新理解财富、重新理解自我和重新理解世界。\n重新理解财富\n注意力是最宝贵的财富\n书里在这里的观点是注意力是与生俱来的、可以自主控制的并且还能拥有生产力的财富，且只有通过注意力，才能更好地利用身体、时间创造出金钱。(笔者认为虽然有些牵强，但是注意力的确很重要)\n注意力是宝贵的，但是往往被花费在以下地方（下面引用的例子摘自书里，可能不完全准确，但是核心观点还是需要有保护注意力，不被无意义地消耗的意识）\n1. 被浪费\n\n有些人，走在大街上看到一群人围在一起，他就会特别好奇，想过去看一下发生了什么，结果一看就是半小时......\n还有些人，特别喜欢关注一些明星的动态，比如某某歌手最近参加了一个大赛，竟然拿了一个冠军，你特别不开心，唱得那么差，为什么还能得奖呢？一定有黑幕！还比如，谁和谁最近爆出了地下恋情，你特别吃惊，心想他们是从什么时候开始的啊，我怎么不知道，然后就去百度疯狂地搜索......\n还有些人，整天喜欢琢磨各种国家大事，比如南海局势的下一步对策应该是什么...... 我国的对外贸易政策该如何改善...... 分分钟你会觉得这哥们儿在公司干销售，真的是太屈才了！\n\n2. 被收割\n\n在美国有一本畅销很多年的书，书名就叫作《注意力商人》，书中列举了一系列的方法论，目的就是要想尽一切办法把你吸引过来，并且留住你，让你看上瘾！比如越低俗的内容，越反常的谣言，越可怕的消息...... 越能吸引你的注意力！\n真不真不重要，对不对不重要，你看不看才重要！\n\n3. 被利用\n常见的推荐系统，\n广告系统本质上就是在做的就是这件事情，其实也可以算在被收割里面；因为一些大厂之间的竞争其实就是在抢占用户时间，收割用户注意力\n\n在互联网上的任何注意力投放，几乎都会被完整地记录下来。通过对你的注意力轨迹进行大量的分析，商家们就能够更了解你，知道你更愿意把注意力花费在什么样的内容上，那么商家就可以针对你投放更多这方面的内容，继续收割你更多的注意力！\n或者，它们还能把这个分析结果直接卖给其他商家，告诉它们你爱看这些内容，那么其他商家也\n可以用这些定向内容去更高效地收割你的注意力......\n\n那注意力应该被消耗在什么地方？\n1. 聚焦在能产生价值的事情上\n应该将所有精力投入到当前的事情上，进入心流模式，具体方法在书里的 “大脑升级” 部分有介绍。\n2. 人际关系，特别是亲密关系\n即工作之余，千万别忽略了对家人的关注。\n3. 寻找新的趋势\n趋势其实就是大环境，其重要性无需赘述，具体方法论会在本文重新理解世界中讲述\n4. 自我成长\n这是最重要的一点：除了上面说这些方向之外，你应该把所有的注意力，都花在自我成长上！\n时间商人的四种模式\n上面强调了注意力是宝贵的，但是对于绝大多数人来说，最终换取金钱的媒介还是时间，书里认为我们都是时间商人，而时间商人往往有以下四种模式。\n1. 零售时间\n上班族、专车司机、兼职快递员等绝大部分职业都可以算作这种模式，这种模式下提高个人收入的方法：提高你的时间单价。\n而提升时间单价，撇去市场调整的影响，最重要的还是自身的能力，对于上班族来讲，基本就是提升自身价值，卖个好价钱....\n2. 批发时间\n网红、明星、作家等属于这种模式；边际成本较低\n\n这些人的高收入，并不是因为他们在 “被买断的那个时间段内” 产生了这么多的价值，而是因为他们所创造的产品，比如歌曲、电视剧、电影、娱乐节目，甚至是自己的一张照片、一个微笑...... 都可以通过他们的影响力，被复制成无数多份，然后批发出售\n一个明星，有多少人喜欢，就意味着他的作品有多强的 “批发能力”。\n\n3. 买卖时间\n雇佣工人的老板属于这种模式\n企业为什么会花钱请你来工作，而不是老板自己去做，或者找外包？主要有 2\n个原因:\n（1）你在某方面更专业（2）你能称为他们的帮手；实际上，这也对应着公司两种类型的人才（1）创造价值的员工（2）提升效率的员工\n“买卖时间” 的这种方式并不是说就让你去办个公司，然后花点钱买一堆人的时间，再把它们卖出去，这就能赚差价了，如果这么简单，就不会有那么多创业失败的人了\n“买卖时间” 的本质其实是一个放大器，就是它得先看 “你有什么价值”,\n如下是书里的一个例子\n\n比如你文章写得不错，一篇文章能换来 1000 元的稿费。但是现在你只有一个人，除了写稿的时间，你还得花时间寻找甲方，沟通需求，还得自己配图、排版，收集各种案例素材...... 这些得用去你很多时间，所以你一个月真正能用来\n写稿子的时间并不多，因此总价值也并不高\n这个时候你就可以开始招人来帮助自己了，你可以先找效率型的人才，让他帮你节省时间，比如帮你去和甲方沟通需求，给你写好的文章去配图、排版，帮你收集各种需要的案例素材...... 你只需要专心写文章就可以了。虽然说多请了一个人，看似成本提高了，但是你创造价值的时间变多了，原来一个月只能写 10 篇文章，现在能写 20 篇了。那么减去新增的人员工资，总收入反而提高了\n你可能又会有新的想法，比如我写的内容既然有那么多人喜欢，为什么不直接运营一个公众号，经营自己的粉丝圈，让内容的价值变得更大呢？当然可以，可是你并不会经营公众号啊，也不知道如何让公众号拥有商业价值，你只会写文章，怎么办？\n这个时候你就需要招聘创造价值的人才进入团队了，你不需要自己去学如何运营一个公众号，或者学习如何将公众号变现，这些学习成本很高，你直接把这些领域里的牛人招募进团队就可以了，你还是只管写文章......\n\n4. 收时间税\n平台是这种模式；就是你并不需要自己去出售时间，而是创造出一个平台，让其他人到你这个平台上来自由交易他们的时间，而你只需要对他们的每一笔交易进行 “抽税” 即可。\n像淘宝、微信、滴滴专车，国外的亚马逊、Facebook、Uber、苹果的 AppStore，还有类似证券交易所、赌场都是这种模式\n这里需要注意的一点是：平台是结果，而不是原因；很多创业者一上来就搞一个大平台，但是即没买家又没卖家，最终可能导致这只是一个空壳子；当然，资本的力量也是非常强大的，烧钱来买用户，然后垄断的例子也是有的。。。。\n但书里认为平台真正的价值在于能对平台上的人 “赋能”；说人话就是需要为用户 (包括卖方和买方) 创造价值，切实解决用户问题，用户才会买单，这个价值可能是时间单价，可能是效率，可能是安全等\n打造复利效应\n复利是经济学上的一个基本概念，但是传达的思想是：当你做了事情 A，就会导致结果 B，而结果 B 又会加强 A，如此不断循环，循环次数越多，A 就越强大；因此针对生活中的很多事情，都可以打造复利效应，书里提出了\n3 个步骤\n1. 找到因果关系\n不要自己去摸索着写个因果关系，然后用自己的时间和金钱来验证它是否正确。因为你的顿悟，很可能只是别人的基本功，你要学会站在巨人的肩膀上。因此，最高效的方式是去相关领域中，找到那些已经被验证过的结论，或者是一些基本常识，甚至是数学定律来用作 “支点”。\n2. 设计增强循环\n说的其实就是 B 如何反过来增强 A；这里又可分为 2 种情况\n第一种情况，它们之间天生就有增强循环；如投资\n第二种情况，需要补充要素的增强闭环\n其中，第二种情况往往是生活中遇到的大部分情况，书里举了如下例子\n\n在淘宝上，流量越多，销量越高；销量越高排名就越高，因此获得的流量就会越多。可线下门店呢？没有平台给它排名，销量好并不能直接带来流量，怎么办\n这个时候，我们就需要在 “销售额” 和 “门店人流” 之间，增加一些环节，让它们之间连成一个 “增强闭环”:\n比如用赚到的钱，开更多的门店；把利润折换成优惠券的形式，发放给老客户\n\n如果回到更常见的个人问题，即专业能力与被雇佣获得报酬这两者的关系，又该如何增强这个循环呢？\n3. 重复与耐性\n复利效应天然有个缺陷，就是在初期很漫长的一个时间段里，增效都非常低，低到你几乎感觉不到它在增长。当你已经走到 50% 的位置的时候，甚至怀疑它的存在\n这个时候绝大多数人可能会放弃，因为大部分人对于世界的理解度是线性的，但更多情况下，事物却是以漫长的潜伏震荡后爆发突破的形式发展的，如果明确你做的事情是有复利效应时，当你想要放弃的时候，不妨回来看看这张图，找找自己所在的位置，然后你相信，你的未来一定是这样的一条曲线，只是现在还没走到 “里程碑” 这个位置而已\n\n重新理解自我\n角色化与去角色化\n人在社会中都会被角色化 (或者说标签化)，比如一个人在单位是员工的领导，在家里是孩子的父亲... 那么人为什么会被角色化？书里的观点是便于管理和协作，当每个角色都有其规则时，其行为可以被预期，而预判会让协作变得更加高效。\n但同时规则也意味着边界和约束，我们忘记了 “真实” 是什么：看不见真实的自己；看不见真实的\n对方；做产品时也看不见真实的用户。因此，有必要学会去角色化\n那么如何去角色化，首先要了解人，梳理将一个人划为了 5\n层：感知层、角色层、资源结构层、能力层、存在感知层；从外至内，逐步加深\n\n感知层：对一个人的第一印象\n角色层：对一个人的基本信息的了解（职业、婚否等）\n资源结构层：一个人的财富资源、人脉资源、精神资源等\n能力层:\n一个人的各种能力，如管理能力、商业能力、沟通能力、专业技能，等\n存在感知层：对自己存在感的定义，即通过什么 “刷存在感”\n\n这里有几个笔者比较认可的观点\n\n每个人在人生初期时的角色都差不多，一开始都是学生，毕业后都是小职员，但是每个人未来能去向哪，成为另一个什么角色，是由你的能力和资源结构决定的\n如果你的资源结构和能力不发生改变，那即便是更换了角色，也是在同层次的角色中跳来跳去，甚至还会变得更糟，因为你不进步别人会进步，而随着你年龄的增大，相对价值也就变小了\n要找到自身的存在感知层，需要抛开自身当前的角色，设想如果你身上没有这些职位、身份，你想要的到底是什么？\n当前是否已经满足了你的预期？当存在感的需求越高，没有被满足的落差越大，痛苦感也就越大\n如果你要想和一个优秀的人在一起，并和他建立深度关系，那你就既要懂他真实的快乐，更要懂他真实的痛苦，并且给予他能量，支持他的存在感\n\n至此，我们可以问自己一些问题，你与人相处是看到了他的哪个层次：是与他的角色层相处，还是存在感知层？你看自己是看到了哪一层？看到的是自己的角色，还是角色下自己的能力和资源？还是看到了自己的存在感知？\n我们要能够意识到，日常生活中你所接触到的大多数人，其实都是 “角色化” 的，你以为的他，并不一定是真正的他，他今天对你这么说、这么做，或许是完全基于角色化的考量，而非他真正的本意。你要学会在与人交往的过程中，把 “他” 和 “他的角色” 区别对待。这里作者举了一个产品的例子\n\n如果你在创业，或者从事着和产品相关的工作，那么更要对你的用户 “刮目相看”，你要知道自己到底在为 “他” 还是为 “他的角色” 提供产品或者服务。比如各种协同办公、CRM、任务清单等软件就是为用户\n的 “角色” 提供服务的，你就不能加入娱乐功能，不然用户玩开心了老板就不买单了......\n\n理解问题 6 个层级\n原书这部分本来的标题是 “你是第几流人才”，但其实也可看做看待问题的 6\n个角度，源自 NLP (神经语言程序学) 理论\n\n“NLP 理解层次” 把对一件事情的理解分成了六个不同的层次，而这\n些层次是有高低之分的。\n如果你用低维度的视角去看这个问题，可能会感觉它根本无法解决。但当你站在一个更高的维度去看它，它也许就变成了一个很简单的问题，甚至连问题本身也消失了。就像马车的时代，大家都在寻找更快的马，但是当汽车被发明出来之后，这个问题就不存在了，因为马的快慢已经变得无关紧要了。\n\n笔者在这里简单将这六类人才概括成如下表格，值得注意的是，这里说的每提高一个层次，并不是说就不要下一个层次了，比如有了方法就不需要努力了，而是在原来的基础上，上升了一个思考层次\n\n\n\n层级\n别名\n所处理解层次\n典型思考模式\n\n\n\n\n第五流人才\n怨妇\n环境\n都是你们的错！\n\n\n第四流人才\n行动派\n行动\n我还不够努力！\n\n\n第三流人才\n战术家\n能力\n方法总比问题多！\n\n\n第二流人才\n战略家\n BVR (信念 / 价值观 / 规条)\n 什么才是更重要的？\n\n\n第一流人才\n觉醒者\n身份\n因为我是 ×××，所以我会 ×××\n\n\n 顶级人才\n领袖 / 伟人\n精神 / 使命\n人活着就是为了改变世界！\n\n\n\n第五流的人才在问题发生时，首先会把问题归结到 “因为环境的不好”，这类型的人只要一与他们接触，就会感受到这 “满满的负能量”，感觉这人世间的不幸都被他们碰巧遇上了，命运多舛得不行；书里给出的一些典型的思考模式如下\n\n工作不顺利，是因为领导是个蠢蛋......\n没有晋升机会，是因为公司的办公室政治严重，没有好的晋升机制......\n房子太贵买不起，都是因为那些黑心炒房团、政府调控不到位、没有一个富爸爸......\n....\n\n第四流人才在问题发生的时候，首先会把问题归结到 “因为我的努力还不够”, 这类人往往没有意识到努力是成功的一个必要条件，但远远不是充分条件，也就是说用力的方向要正确，书里给出的一些典型的思考方式如下\n\n都一年没涨工资了，今晚开始多加 1 个小时的班！\n女朋友为什么最近对我变得冷淡了？我要多发些消息，多打些电话去关心她！\n公司业绩变差了？那一定是我睡觉睡得太多了，明天开始不睡觉了！....\n\n第三流人在问题发生的时候，首先会把问题归结到 “因为我的能力不足”，然后在 “能力” 这个层次里去寻找更好的 “方法” 来解决问题。\n这类人有非常强大的学习能力和应用能力，能把学习到的知识，转化为可操作的方法，进而改善效率，解决问题。他们明白，任何问题都不是孤立存在的，一定有人曾经遇到过，并且已经有更好的解决办法了，只是我还不知道；我不应该在黑暗中独自前行，去重新发明轮子，也许我的顿悟，只是别人的基本功！\n我应该要站在巨人的肩膀上，学习更成熟的经验和方法，然后再来解决这个问题。一些典型的思考方式如下\n\n线下门店生意不好，是因为我的经营模式太陈旧，我需要学习新的方法...... 比如，可以通过社群经济的方式来降低我的获客成本，提高客户复购率......\n和男朋友关系处理不好，一定是我的沟通能力有问题，我要去学习能改善亲密关系的沟通技巧，比如先看两本沟通方面的畅销书:《关键对话》《幸福的婚姻》......\n以前我是做业务的，现在刚成为部门经理，团队业绩下滑，一定是我的管理能力有问题，我以前根本没有系统学习过管理的方法，我得去报个 MBA，从 “古狄逊定理” 开始学起......\n...\n\n第二流人才在问题发生的时候，可能并不会马上给出解决方案，而是会先去思考：这些很可能是表面问题，还有没有藏在这些问题下面的更重要的问题？如果说 “能力层” 是做解答题的能力，“BVR 层” 就是做选择题的能力，什么可以做，什么不可以做，什么更重要，什么可以忽略不管\n那什么是 BVR？\n\nB (Believe): 信念，即你相信什么是对的？\n你相信这个世界应该是怎么样的？往大了说可以是世界观，往小了说就是一个个概念。\nV (Value): 价值观，即当出现 A/B 选择的时候，你认为 A 和 B 哪个更重要？\nR (Rule): 规条，你做人做事的原则是什么？\n\n书里还是举了线下门店业绩出现下滑的例子\n\n当门店业绩出现下滑，可能的原因有很多，可以先做个简单的分类，比如可以分成:\n成本问题：房租涨价，库存积压，已投入的装修成本，进货成本高于淘宝售价......\n团队问题：员工士气低落，引发离职......\n市场问题：客流减少，人们现在喜欢在网上购物，网上商品的售价更便宜......\n营销问题：目前门店没什么营销方式，就是开门迎客；\n渠道问题：线下门店是目前唯一的渠道。\n如果是处在 “能力层” 的人，他就会胡子眉毛一把抓，针对这些问题提出各种解决办法，比如针对团队问题，他可能会给出一整套员工激励方案；面对营销问题，他可能会增加户外广告，再运营一个公众号；而面对渠道单一的问题，他就会建议也去开个淘宝店......\n如果是处在 “BVR 层” 的人会思考，那么多问题，其中到底哪个才是最关键的问题？经过一番思考，他画出了一个关系图\n\n原来，一切的罪魁祸首，都是因为互联网的连接效率变高，导致了原本市场上的交易结构发生了变化，淘宝店家短路掉了中间的总代、省代、区代...... 等等这些价值传递的环节，让商品可以用更短的距离来到消费者的面前，所以价格才能那么低。由此导致了后面的一连串反应......\n因此，可以制定出如下战略 (1) 短路经济:\n既然淘宝店家能短路掉中间的环节，我实体店为什么 不可以？(2) 体验经济:\n增强线下门店的体验感\n\n笔者觉得上面的例子虽然现实指导意义有限，但核心还是希望说明一个问题，那就是要抓住本质的问题去解决。到这里基本是笔者能够理解到的层级了，第一级人才和顶级人才的思维这里就只摘录书里的相关内容了，也许未来的笔者才能理解。\n第一流人才，其实就是明确清楚自己想成为什么样的人 (或者说身份) 的人才；因为不同的身份定位，会配套不同的 BVR，而 BVR 决定了你当下的每次选择；\n即你的身份层次决定了你的价值判断，进而决定了你的每一次选择。书里在这部分没有给出具体的案例，说的东西有点大而宽，也许是到了这个层级的就是这样？\n\n联系前面说的角色，可能我们会问：“角色” 和 “身份” 有什么不同？答案是：角色是被动的，是别人给你的；身份是主动的，是你自己想成为的。\n前面说的 “去角色化”，就是想让你突破角色的束缚，获得一个更 “主动” 的人生，找到自己的身份层次。因为你身上的 “角色” 太多，会阻碍你看见自己真实的 “身份”。\n而当你想清楚自己的身份定位后，就应该围绕它配套相应的 BVR，再构建你的能力圈，并做出相应的计划与行动，你就会成为第一流的人才！\n处在这个层级的人，能开创出一番自己的事业，设计出令人尖叫的产品，成为上市公司的领军人物。\n\n顶级人才，所有的思考都围绕着两个字 ——“利他”: 我如何选择能够让更多的人获益？如何才能够推动时代的进步？书里在这强调了理解层次的逐级上升，不能脱离低层次而单独存在高层次，比如只谈精神理想，而无视自己的身份，更没有相对应的能力和行动\n\n“精神层” 一定要有 “身份层” 的支撑，换句话说，如果你在身份层想不清楚自己要成为谁，可以试着来到精神层，想想你能为这个世界做些什么；可以不用那么大，哪怕只是在某一方面，能帮助到为数不多的人。\n也许这个就能成为你的人生使命，然后再去思考，什么样的身份能够更好地帮你完成这个使命？你就能想清楚身份层次的问题了。\n\n关于方法论部分，即如何成为顶级人才，书里给的方法不是一级级往上打怪升级，而是直接让自己成为一流人才或者顶级人才，对你的人生做个 “顶层设计”，从精神层开始往下规划，如下图所示\n\n元认知\n元认知就是你对自己思考过程的认知与理解。其一般模式是\n发生了事件A......→你有了反应B;→我为什么会有反应B?反应B是对的吗?→C好像是更适合的反应;→于是，你有了反应C。\n书里举了如下例子\n\n当你打开抖音，你发现好多有意思的视频，然后你不断地翻看，结果不知不觉 3 个小时过去了。这就是没有元认知的状态。\n如果你的元认知能力已经被激活了呢？\n你还是打开抖音，发现有好多有意思的视频，然后你不断地翻看，但是突然，一个声音进入大脑:“你为什么会不停地翻看？你的注意力\n是不是正在被别人收割？” 你浑身一颤，于是赶紧关掉抖音，回到了工作当中......\n\n上面的例子其实是为了说明大脑也是一个器官，是可以被控制的，而元认知则是大脑的纠错机制。\n因此，元认知有着以下三种重要作用：控制输入、控制大脑、控制输出\n1. 控制输入：即控制什么信息要进入大脑，是即时满足还是有价值的知识积累\n2. 控制大脑：即控制进入大脑的信息怎么处理，这里提供了三种方法:\n\n(1) 对无用的信息：丢弃\n知真识假，看多了，自然就知道什么是假的了\n先去主动学习大量的正确知识，等累积到一定量之后，你自然就会拥有这双火眼金睛，一眼就能看出某个信息的好坏了。\n(2) 对有益的信息：储存\n需要存储的有益的信息可分为以下四大类 a. 概念:\n你对事物的理解，其实就是这本书的上半部分，这个是你理解世界、思考问题的砖瓦。\nb. 价值观:\n对事物的正确价值判断，什么是对的，什么是错的，什么是更好的。 c.\n思维方式: 不同问题的思考方法。 d.\n方法论: 专项问题的已知最优解法。 结合前面说的理解问题的\n6 个层次，可画出下图\n\n(3) 对问题或者任务：处理\n你大脑里可能储存了很多套思考方式，当不同的任务、不同的问题进入大脑之后，你得先判断，它属于哪种问题、什么任务、目的是什么，然后启动不同的 “思考方式”“方法论” 来处理这些信息\n\n3. 控制输出：即知行合一，也许你学习过很多时间管理的方法，你也知道它很重要，但是如果你无法调用你的 “元认知”，你就无法控制你的大脑，想到却又无法做到，你的焦虑感越来越强\n那该如何提升元认知的能力？书里也提供了三种方法：刻意练习，经常反思和练习冥想\n1. 刻意练习\n即使是 10000\n小时定律的练习也需要借助套路，套路就是一些储存在能力层里的 “思维方式” 和 “方法论”，它们是已经被前人验证为做某件事更高效的方式方法，掌握了它们，你就掌握了做某件事情的 “诀窍”。比如帮助你提高思维能力的工具：金字塔原理、MECE 法则、5W2H、SCQA、二维四象限等；帮助你提高决策能力的工具：KT 法、概率决策树、麦穗理论等。\n但这些方法你仅仅知道是没用的，当新问题出现时，还是会习惯性地用原来的方式去思考；这时候要通过元认知强迫自己去使用这种方法，虽然会不习惯、有点别扭，但坚持用这种方式 “刻意练习”，你大脑中的某些特定区域就会不断被强化，久而久之，你某方面的技能就会甩开普通人一条大街，元认知能力也因此得到了加强。\n2. 经常反思\n每天晚上你可以花半个小时的时间，对今天遇到的事情，自己说过的话、做过的行为进行一次复盘，看看哪些事情做得好，哪些事情做得不好，下一次应该如何提高，哪些行为是被大脑 “绑架” 了？\n除了这种 “每日三省吾身” 的方式，还有第二种方式 ——“阅读”，这里的阅读不只是要理解作者想表达的意思，而是作者的思考方式；即他为什么会这样写？他的思考方式是怎么样的？用到了哪些概念和价值观？有没有错误？\n如果是自己来写这个内容的话会怎么写？他这种思维方式比自己好在哪里？这样不断地和自己做对比，就像和一个人在对话一样，久而久之，你会慢慢发现你阅读的速度竟然变得更快了，而且学习效率也提高了很多，甚至读了一半，你就能猜到作者之后会写什么，如何写，甚至知道怎么样写才会更好，你不再是被文字牵着走，而是陪着作者一起思考......（笔者觉得这部分跟这篇文章的观点很像，天空之城：拉马努金式思维训练法）\n3. 练习冥想\n冥想这部分在很多外部资料中都讨论的比较多，这里推荐知乎的一个问题：冥想的具体做法是怎样的？，有很多原理解释和方法指导，核心其实还是主动控制你的注意力\n打造稀缺性\n对于个人而言，打造稀缺性其实就是要打造有效的多维能力，这里有效体现在\n（1）每个能力都有价值，也是别人需要的 （2）每个能力都有关联\n比如对于互联网的从业人员，技术能力和管理能力就是有效多维能力，技术能力又可粗略分为工程能力和算法能力，其实就是常说的广度和深度，如果两者都做的足够好，那其实就是稀缺的。\n那该如何打造多维能力，书里给出了三个步骤 1. 先把一个能力打造成长板\n2. 让自己兴趣广泛，培养多种能力\n3. 确定一个目标，并把多种能力组合成多维能力\n笔者的实践经验就是先把本职工作的技能打磨好，然后去看同行业的其他人在做什么，选择自己感兴趣的部分去学习，而大目标基本就是你所在行业所要解决的问题了。\n重新理解世界\n趋势\n这里的趋势其实就是大环境，很多人应该都明白这个道理，要顺势而为；这里是更系统的描述趋势是如何产生的、如何判断趋势等\n假设平面上有一个静止的小球，小球只要运动了，这个运动方向就是趋势；那么趋势产生可归结为\n2 个原因：外力和势能差\n1. 外力\n现实世界中的热点、重大新闻、科技突破、黑天鹅事件等，都是突发性的 “外力”。一旦这种重大意外事件发生，你就得去了解一下它可能会产生什么样的趋势，然后跟进，赚取趋势红利，这也就是我们平时说的追热点。但这种方法有\n2 个问题\n（1）不知道什么时候来，从哪个位置来，这个很难预测，只能等\n（2）即使来了，你也不知道它未来究竟会如何发展\n由于这种外力的不可预知性，以及未来发展趋势难以捉摸的特点，书里并不推荐这种方式，因为这更像是一种投机。\n2. 势能差\n寻找势能差，其实就是主动寻找存在 “不公平竞争” 的地方，然后成为高势能的一方，顺势而为，用 “不公平” 的方式赢得胜利。就像站在山上和山下的人打仗一样，在万仞之巅推下千钧之石。\n书里将势能差分为了四大类：效率势能、规模势能、认知势能和引力势能\n（1）效率势能\n这部分可提炼为 9 个字：优打劣、快打慢、廉打贵\n优打劣：同样的产品，你提供的的品质更好\n快打慢：快一步就意味着抢先占领了市场，用户规模会成为新的势能高地\n廉打贵：产品价格更低会更受到消费者的青睐\n（2）规模势能\n大鱼吃小鱼，就是用规模去碾压对手。\n比如外卖大战、共享单车大战、打车软件大战都是这么一个剧本，一开始大家都势均力敌、打得不可开交，而一旦其中某一方通过资本注入，它就能迅速扩大规模，用规模优势蚕食对手；有一些大公司对与一些刚冒起苗头的小公司，要么收购，要么复制，直接通过用户规模去把小公司绞杀在襁褓里\n（3）认知势能\n这里类比《三体》就是高级文明打低级文明，用新模式、新科技进行降维打击；也可称之为:\n颠覆式创新\n书里举了 2 个例子：被数码相机打败的柯达，被智能手机打败的诺基亚\n大企业要做创新，面临着 “创新者的窘境”:\n你要干掉原有业务？想颠覆原有技术？那你面对的就是具有规模势能的既得利益者的压力。\n所以现在一般大企业都会通过下面 2 种方式来践行这部分\na. 从内部独立出去一个团队，脱离原公司，在创业环境下单独搞新业务；\nb. 看到好的公司直接买买买\n（4）引力势能\n这里指得是某个事物，能够像拥有引力一样地吸引周围的事物，让它们成为自己能力的一部分，类似一个黑洞的存在，笔者觉得这就是马太效应\n比如拥有网络效应的平台型公司，它拥有一个互相增益的双边市场，用户越多就会吸引更多的商家来到平台，而更多的商家来到平台，就会吸引更多的用户来平台购物\n上面提到了 4\n种势能，那该如何站上这些势能高点？书里借鉴了阿里巴巴的曾鸣教授提到过的一个概念:\n“点、线、面、体” 的战略分析框架，来分析这个问题\n\n点：可以指个人或者某个单一产品；\n线：一个小公司，也可以指大公司里的一条业务线；\n面：指平台型公司或者生态型企业，比如阿里巴巴、腾讯；\n体：指时代、行业、新的经济体。\n\n当我们有了这么一个框架，就可以问一下，当前自己到底在哪个位置上？拥有什么势能？怎样获取缺失的势能？因为点线面体上有不同的势能\n\n“点” 自身唯一能把握住的是认知势能，其他基本都不具备；\n“线” 可以拥有效率势能和认知势能，势能比 “面” 小一个数量级，\n比 “点” 高一个数量级\n规模势能和引力势能是 “面” 和 “体” 独有的，而 “体” 的势能比 “面” 高出一个数量级\n\n\n而站上势能高点的具体方法，书里给了 2\n个：成为和赋能\n成为：让自己成为高势能的一部分\n对于个人可以加入一家有势能的公司，对于小公司可以发挥效率势能和认知势能来执行战略，选择在大公司瞧不上的某条业务线上，通过集中的效率势能，在它还没有反应过来的时候，快速将效率势能转化成用户规模\n赋能：找到高势能的 “线、面、体” 来帮助自己提高势能\n书里举了 2 个例子\n\n一个点本身拥有的势能是很小的，单打独斗在这个时代成不了什么气候。但是一些自媒体、一些网红却单干而财富自由了；为什么呢？\n原因他们这个 “点” 在如今这个时代，可以脱离于 “线” 而直接附着在 “面” 上了，比如微信、淘宝、抖音、知乎；这些面又附着在一个正在快速崛起的 “体” 上，也就是移动互联网。他们看似在单干，但其实，是体和面一起在给这些 “点” 赋能，给他们提供了海量的客户资源，给他们提供了数据支持、技术支持、物流支持......\nCEO 最重要的工作就是为员工赋能。员工再好再不好，他们都是一个 “点”，资源和能力是有限的，势能就那么多，你不应该天天对自己的员工不满意，而应该去外部找有势能的线、面、体，为你的组织赋能，\n这才是一个好 CEO 该干的事。\n\n如何找到高势能的 “线、面、体”，书里虽然给出了一些例子，但是并没有给出具体的方法论，笔者认为也许具体的高势能 “线、面、体” 在每个时代都不同，都得靠那个时代的愿意折腾的人去发现吧\n书里留了两个思考题，笔者觉得值得每个人去认真想一下答案\n\n思考题 1: 你现在附着在哪些线、面、体上呢？它们是在上升还是在沉沦？\n思考题 2: 你可以通过什么方式，找到适合的线、面、体为自己赋能？它们又为什么愿意给你赋能呢？\n\n为想法估值\n这部分内容更多是针对创业者，但其分析的思路其实也能应用到日常工作中，主要观点是不仅要有想法，而是要考虑想法的可行性，即为想法估值，\n考虑想法的\nROI，书里提出了四个维度来进行这个估值：客户终生价值、获客成本、用户规模和风险成本\n1. 客户终生价值\n客户终生价值，就是一个客户一辈子在你这里花多少钱，其计算公式可简化如下\n客户终生价值 =(客单价 - 边际成本)× 购买次数\n客单价：指的是一个客户在你这里购买一次产品或者服务，平均需要花多少钱\n边际成本：指的是每多增加一个客户所增加的总成本\n有了这个公式，就可以估算一下产品上线后，客单价会有多高？边际成本是多少？客户可能会来购买几次？\n但是当产品没实际运营，会存在估算不准确的问题，书里说的是 “对多个要素一起进行估算，通过计算，误差就会彼此对冲掉”；笔者不太理解这句话，但是觉得更可行的方法是去调研当前类似产品的一些数据来做估算。\n2. 获客成本\n获客成本，就是获得一个付费客户所需花费的成本；\n为什么有些明星、网红出来创业，一开始就能有很高的估值？其实就是他们存在粉丝价值，很容易做到较低的获客成本。\n回到获客成本的估算：你有这方面的存量用户资源吗？你有开发市场的独门秘籍吗？没有资源，你就得花钱打广告；所以我们经常看到，某某创业公司在玩命地烧钱，其真正有效的逻辑是：客户终生价值 - 获客成本 &gt; 0\n3. 用户规模\n用户规模，就是项目最多可以获得多少个用户\n估算这部分往往会面临这以下两个问题\n（1）市场总容量，就是市场的天花板，书里提到这部分数据的获取方式是：“别问我该如何知道这些数据，这些数据你都没法搞到，也就不要创业了”\n（2）市场竞争 , 这里又有 2 点值得思考\n\n\n如果你发现你进入的这个市场，除了你之外就没有其他玩家了，或者玩家很少。有可能是你认为的这个需求不一定存在\n\nb. 假定需求是有的，但是真的就没有人做，那意味着你就得教育市场了，教育市场可是一件吃力不讨好的事情，你也许花了一年的时间，好不容易把用户从无到有地培养起来了，结果竞争对手看到你这里有肉吃便蜂拥而至\n\n4. 风险成本\n风险成本，就是你的项目如果失败了，一分钱没挣到，最多会损失多少？\n综合以上点，可对想法进行如下的估值，\n而最终的公式是：项目估值 =(客户终生价值 - 获客成本)× 用户规模 - 风险成本\n\n上面只是从四方面为想法估值，真正执行的时候还需要考虑趋势、合作的团队等隐私，书里对这部分只是简单概括，这里也不详细展开了。\n镜像世界\n笔者觉得这部分的内容跟我们这一代人的困惑里面的内容很像，只是这里更明确地将我们所做的事情划分为四个象限。\n书里根据左侧和右侧、光明与黑暗将事情划分为如下图所示的四个象限，其中左侧与右侧、光明与黑暗的含义是\n左侧与右侧：越靠近左侧，因果关系就越强，达成目标主要靠技术，运气的空间很小；越往右侧，因果关系就越弱，行为和结果之间存在很多不确定因素，也就是我们常说的运气成分偏多\n光明与黑暗：光明指那些你已知的事情，黑暗指那些你还不知道的事情，包括其他人已知，而你不知道，以及所有人都不知道的事情\n\n而对于每个领域的事情，都要有不同的策略\n1. 掌控域：已知、确定\n总结来说就是要积极努力，刻意练习，提高技能水平；因为技能在这个域内的占比很大，想要获得成功，你就得不断提升技能水平。\n2. 盲域：未知、确定\n总结来说就是要承认自己的无知，并开始学习与探索，书里以演讲为例\n\n你看到别人的演讲非常精彩，既有深度，又有广度，还风趣幽默，讲得你激情澎湃，你觉得这人的 “口才” 真好，真是天生的演说家。\n但其实 “演讲” 这件事，也是左侧世界的技能，背后是有一套理论框架的:\n开场怎么开？结尾怎么收？用案例库中的哪些案例？\n如何把结构化的知识，用线性的方式讲述出来？\n如何将理性的认知和感性的了解结合起来？如何设计演讲中的峰值与终值？\n如何与观众互动？如何应对挑事的观众？......\n\n3. 概率域：已知、不确定\n总结来说就是要不赌单次，赌整体，善用数据决策 ;\n这部分从概率论的角度思考，也许会更好理解，其实就是利用大数定律来判断事情的期望，而且是最好是借鉴他人构造出来的样本和数据\n4. 风险域：未知、不确定\n书里认为这是个恐怖的领域，给出应对策略是如下几种\n策略一：避免进入\n策略二：增加冗余备份，即多维能力；如个人学习多种能力，公司布局多个领域\n策略三:“买彩票” 思维，做风险小，收益大的事情，如风投\n最后，书里还提出了一条适用于这四个领域的生存法则：守株待兔，简单翻译就是守住不变的东西，等待机会的来临\n\n守株指的是：在这个充满变化和不确定的世界里，你首先要牢牢抓住那些正确的概念和不变的规律，\n关键在于你能否发现这些 “株”，并牢牢地 “守” 住它们。因此，你得保持初心，尊重未知，通过不断学习，减少盲域的面积，提高 “株” 的数量\n待兔指的是：等待不确定的机会。兔子可能会来，也可能不会来，也不知道什么时候会来。因此，你不能把你一天所有的口粮都寄托在这些不确定的兔子身上，那风险太大了，几天没抓到兔子，你可能会饿死。你得花大部分的时间，先把确定的事做好，保证自己的基本生存，如果能等来一个兔子，你就额外赚到了\n\n怎么赚到这些兔子，书里举了三种方法\n1. 发展有效人脉:\n所谓的 “人脉”，不是能帮到你的人，而是你能帮到他的人。你多帮助别人，就是在往你们之间的 “情感账户里存钱”\n2. 学习跨界知识:\n你遇到某个问题，也许会在其他领域里找到一个线头，抽出一个很好的解决方案，而这个 “好运” 就来自于你曾经的广泛学习\n3. 投资成长性资产: 投资股票、创业公司等\n提升运气\n这部分将 “运气” 归结到人、事、物这三个方面上，并且给出了每部分给出了一些方法论\n常说的运气，往往可归因为三类:\n1. 人：获得其他人的帮助。 2. 事:\n有些事情的发生，增加了你目标事件的成功概率。 3. 物:\n周围的物理环境，能使用的物品道具，天时地利的变化。比如钻木取火里那个干燥的环境。\n1. 人\n一个人得到另一个人的原因往往有三个：投资、扶弱、还债，因此针对这三个原因，我们要做一个什么样的人？书里给出了如下建议\n(1) 才华出众：你在寻找伯乐的同时，伯乐也在寻找千里马\n(2) 低调、谦逊、知恩图报：人们喜欢帮助弱者，其实是源于内心的一种存在感的满足。就是看到对方因为自己的帮助而变得更好，内心会有一种开心的满足感。但如果对方是傲慢自大的，是会恩将仇报的，那这份 “意义感” 就荡然无存了。\n(3) 广结善缘、吃亏是福:\n与别人交往、合作，也要想办法让对方多赚一点，这样，他们会带着一些 “亏欠你” 的心态与你交往，会更愿意帮助你，愿意介绍自己的朋友给你认识，介绍更多的生意与你合作\n2. 事\n这里的观点是要让你做的每一件事，都产生 “积累” 的效果，前一件事是后一件事的预动作，过去的经验是今天的铺路石，让时间成为你的朋友，产生复利效应。\n3. 物\n其实就是前面分析的势能类型，以及如何站到高势能的地方\n小结\n本文主要讲述了书里前半部分内容：概念重塑，包括重新理解财富、重新理解自我和重新理解世界三大部分。各部分核心观点如下\n1. 重新理解财富\n\n注意力是最宝贵的财富，要警惕被浪费、被收割\n时间商人有四种模式，你是哪一种模式，能不能找到更高效的模式\n复利不限于经济领域，3 步建立复利效应\n\n2. 重新理解自我\n\n人会被角色化，角色化是一把双刃剑，需要学会 “去角色化”\n 一个人有 5 个层级，你对自我、对他人理解到了哪个层级\n理解问题有 6 个层次，从底层的环境到顶层的精神，你在哪一层\n元认知有 3 个重要作用，提升元认知的 3 个手段\n打造稀缺性就是在打造有效的多维能力\n\n3. 重新理解世界\n\n趋势由外力和势能产生，势能相对外力更容易把握\n势能有四种类型，从 “点线面体” 理论出发，判断自身处于哪个位置，拥有哪些势能，怎么获取其他势能\n为想法估值的四个维度，量化 ROI\n 根据确定性和掌握的信息量，可以将所做的事情划分为四个领域，每个领域都有不同的生存规则；但其中不变的是 “守株待兔”\n 提升运气可以从人、事、物三个角度出发\n\n因为概念较多，所以这里也做了思维导图，如下图所示，原始的 xmind\n文件也可以从 github下载\n\n\nconcept rebuild\n\n","categories":["读书"],"tags":["读书","拾人牙慧"]},{"title":"《商业产品经理的实战修炼》学习笔记","url":"/2021/10/30/%E3%80%8A%E5%95%86%E4%B8%9A%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E7%9A%84%E5%AE%9E%E6%88%98%E4%BF%AE%E7%82%BC%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","content":"一直觉得从不同视角去看事情，往往能有更全面的了解，也能得出一些更有趣的结论。因此，习惯了从\nrd\n视角去看商业化广告，老早就想从产品 / 运营 / 销售的视角去看这个事情，最近正好找到关于商业化产品经理的一门课，即《商业产品经理的实战修炼》，虽然是几年前的课程，但是其中一些内容放到今天也是值得参考的，也让笔者对商业产品有了进一步的理解。\n课程里主要讲了四大部分内容 (其实是五章，但是因为对广告进阶这部分比较熟悉，所以略过了这部分)：商业产品的定义及演进、商业产品经理的职责与方法论、有效的商业产品需求、商业产品从 0\n到\n1，下文也是按照这四部分展开，内容经过梳理且只摘录了笔者比较关心的一些内容，推荐看原视频。\n\n商业产品定义及切入\n这部分介绍了商业产品的定义、启动时机、模式的选择以及切入点 (部分内容是第五讲的，但是笔者觉得放在这里更合适)\n定义\n商业产品需要满足以下三个要素\n\n收钱对象\n\n服务 / 产品\n\n定价 &amp; 分发机制\n\n将这三个元素串起来就是：找准收费对象，通过提供特定的服务 / 产品满足收钱对象的需求，同时需要指定一套定价 &amp; 分发机制，以此为准则向收钱对象进行收费\n常见的商业产品有两种收费模式：前向收费和后向收费\n前向收费面向的是用户，要思考能给用户的最优体验是什么；常见的模式如各种会员，\nVIP；收费的规则视频里没有细讲，但是其中的一些逻辑可以参考 付费会员体系分析\n和 VIP\n会员套餐的定价策略\n后向收费面向的是客户，要结合用户行为思考如何最大化客户效果；典型形态是广告；收费的规则就是典型的拍卖\n+\nGSP/VGG；视频里讲了广告演进的一些内容，包括广告的形态、参与者、收费模式等，但是比较基础这里就不展开了。\n启动时机与切入点\n教程里还给了商业化启动时机的参考，简单来说就是从我们能把用户留住开始，量化的指标有以下三个\n\n用户新增率 &gt; 用户流失率\n\n用户留存率 &gt; 40%\n\nLTV &gt; CPA (至少), LTV &gt; 3CPA (最好)\n\n商业化的模式选择就是上面说的前向收费和后向收费，两种方式需要注意的点如下\n\n前向收费：需要谨慎，因为用户习惯了免费的模式，可尝试 “基础功能免费 + 增值收费收费” 的模式\n\n后向收费：比较常见的是广告，但是也要注意用户体验，让广告更原生化\n\n书里举了美团的例子，讲述了美团变现的三个步骤：CPL -&gt; CPT -&gt;\n效果广告；后两者比较好理解，CPL\n指的是用户购买成功后平台收佣；但存在跳单问题，即商家可能会让用户绕过平台直接下单\n那该如何找到商业化的切入点？课程里主要关注如下 2 点\n\n客户 / 用户的需求分析\n\n平台资源的梳理\n\n第 1 步里的需求分析可从以下三个方面挖掘 (这里客户可认为是商业产品的使用者，而用户则是用户产品的使用者)\n\n客户打开 app / 网站 目的是什么\n\n客户达到目的的行为路径是什么\n\n挖掘客户的内在需求 (当前的路径存是否存在可优化的地方？)\n\n课程里以视频会员为例，介绍了这种模式下的需求挖掘，即用户为了看电影，其传统的行为路径是：先打开搜索引擎\n-&gt; 进行资源的搜索和下载 -&gt;\n使用电脑播放器打开；而现在常用的一些视频 app\n能够更好地满足用户的这些需求，其路径对比如下\n　\n平台资源的梳理部分，书里没有给明确的方法论，这部分在不同领域需要不同的背景知识，但基本的方法都是 “搜集 + 分类”,\n“搜集” 需要看个人对信息的敏感程度以及信息来源是否足够多，“分类” 有一些详细的方法论可参考，笔者之前写的《认知红利》阅读笔记 (2)- 大脑升级，里面的结构化思维部分可以参考下，里面提到了两个具体的方法：金字塔结构和平面切割\n商业产品经理\n角色定位\n商业产品经理往往有两个角色\n\n产品经理\n\nseller\n\n第一个角色的职责是设计并推动落地满足行业市场和用户 / 客户需求的产品，跟用户产品经理的职责基本一样，更详细可划分为:\n(1) 行业认知与分析、需求挖掘与分析 (2) 产品设计力 (3) 推动产品实现\n第二个角色的职责是把产品卖出去创造收益，这部分更多跟销售环节挂钩，更详细可划分为\n(1) 目标客户分析，销售、运营协同（2）业务流程规则设计\n以广告为例，商业产品经理的职责如下\n　\n具体的产品推广链路如下\n　\n课程里注重提到了售前环节，并将其分为以下 3 步，并概述了每个步骤中销售的职责\n\n销售线索搜集：地推、营销网站 (自主开户系统)\n\n售卖：销售线索的公海、私海、保护期 (CRM 系统) ，这部分更详细可参考漫谈 CRM 体系化建设 2 –\n如何开发客户？\n\n宣讲：演示 (提供相关的 demo、数据等)\n\n方法论\n教程里给了四点关于 “商业产品经理的修炼功法”\n\n明确产品定位\n\n会讲故事\n\n用户思维\n\n善用监控表\n\n1. 明确产品定位\n产品定位简单来说就是 “产品在客户心中第一反映”，如下是一些例子\n\n进一步地，可以从以下 6 个方面去明晰产品的定位\n\n属于什么行业 / 类型\n\n目标用户 / 客户是谁\n\n解决用户 / 客户什么问题\n\n给用户 / 客户带来什么价值\n\n与竞对的差异\n\n如何匹配和强化产品和用户 / 客户\n\n能较好回答出以上的 6\n个问题，才算对产品有一个较为清晰的定位，以上面的百度搜索移动推广为例，这\n6 个问题的答案如下\n\n2. 会讲故事\n会讲故事这个技能其实也不限于产品经理了，课程里提到的针对产品经理的方法是：(1) 站在客户角度思考\n(2) 形象描述客户使用后能带来什么收益\n如下是一个例子\n\n讲故事的详细方法论在课程里没有详细讲述，这部分可以参考知乎上的这个答案：怎么提高讲故事的能力？\n-\n废柴潇的回答，简单来说就是要具体，引起听众的共情\n3. 用户思维\n商业产品是为金主爸爸们服务的，但也要考虑用户体验，也就是站在用户的角度去评估产品的优劣\n后向收费进行变现的模式尤其需要注意这部分，以广告为例，出广告时需要注意\n(1) 让用户不反感：用户在什么场景会被打动\n(2) 增强互动：用户在媒体平台的行为路径\n(3) 提升效果：用户在不同客户行业产生转化的决策路径\n如下是一个例子\n\n那挖掘用户思维的方法有哪些？课程里简单概述了如下 3 个角度\n(1) 马斯洛需求层次（生理需求 -&gt; 精神需求），更简单来说就是饱暖思淫欲\n(2) 人性的弱点\n(3) 大数据分析（其实就是各种推荐 / 广告里的模型）\n4. 善用监控表\n使用监控报表是的目的主要有 2 个: (1) 了解收入业绩完成情况\n(2) 了解生意模式是否良性\n\n了解收入业绩完成情况：需要监控收入的进度，即当前的收入完成情况是否能跟上时间进度；进一步的细分可从行业、客户类型、广告类型等角度进行\n\n了解生意模式是否良性，即需要关注客户的以下指标\n\n流失率：进来的客户有多少不会再购买\n\n续费率：客户的复购情况\n\n新开户、留存、流失对比：产品引入新客情况\n\n客户数 = 新开 + 留存 - 流式，新开 &gt; 流失才能保证客户数是正增长的\n\n\n搭建监控报表需要\n(1) 指标服务于业务；需要明确具体业务和使用者是谁？根据使用者决定需要关注的点，\n如下面是在广告场景下，针对产品和销售需要提供的不同指标和维度\n\n\n(2) 设计整体框架；即将第一步列出来的点更加具象化，如下所示是五个值得关注的方面\n\n业务场景的划分：即监控平台需要支持哪些业务场景\n\n数据指标：用什么样的数据指标体系\n\n分析维度：要从哪些维度去看这些数据指标 (如时间、销售渠道)\n\n层次关系：分析维度有没有层次关系，\n是否需要下钻 (如行业分类，往下是否还有一级行业、二级行业)\n\n呈现形式：用什么图表分析更直观\n\n(3) 明确数据源；需要保证数据源的准确性、时效性、归一性 (数据在更大范围内也是可解读的)\n(4) 功能模设计；课程里列举了如下四个模块\n\n权限管理：即需要保证数据的机密性\n\n关键指标监控：即直接反映业务目标是否完成的数据\n\n实时收入数据：即除了定期 review\n关键数据外，还需要一个实时的监控来预警业务形态是否有问题\n\n总分式结构：即选从不同维度去分析，一般必备的是平台维度 (流量，\n即平台不同流量上的收入) 和行业维度 (客户，\n即在不同类型客户上的收入情况)\n\n(5) 设计原则，大原则是高效向使用者传递数据信息，可细分为功能设计和界面设计\n功能设计需要突出核心指标、突出对比、使用总分式结构 (提供细分及下钻)\n界面设计需要以简洁为上、避免过多颜色、选择正确的可视化形式\n确认有效的商业产品需求\n需求分析\n课程在这部分给了经典的福特汽车与马的案例，主要想表明客户存在着显性需求与隐形需求，而隐性需求才体现了客户的本质问题\n\n那怎么挖掘客户隐形的本质需求？课程给出了以下三点建议\n1. 找准分析对象 (目标客户群体)，要为哪些人解决问题\n2. 明确目标，解决眼前的问题能达到最终目标吗？\n3. 换位思考，站在客户角度去想问题\n这里注重讲一下换位思考这部分，即需要用客户的语言来描述产品或者需求，像客户一样去体验，理解客户显性需求背后的真正需求；\n即需要完成从平台思维到客户思维的转变，如下是一些常见的例子\n\n而挖掘这些需求实际中常用的一些方法包括\n(1) 问卷\n\n常用于已有产品，客观选择题\n\n判断客户的满意度 / 接受度\n\n方案优先级排序或方案 N 选 1，一般是 3% 回收率\n\n(2) 电话\n\n常用于需求深入挖掘\n\n问题不超过 3 个\n\n(3) 面谈\n\n常用于对预设问题没有答案 / 方案，做大而全的需求收集\n\n开放式问答，引导客户多说，但是说的有方向性\n\n根据客户回答抓住要点追问\n\n数据分析\n数据分析有三个关键的要素：找指标、读指标、指标拆解，\n1. 找指标\n指标需要具备简单性、可比较性，且与公司目标保持一致\n2. 读指标\n只有指标和数值无意义，需要参考值，参考值可从以下 2\n方面入手\n\n跟自己比：同比、环比看异常点，提升 / 下降的服务\n\n跟业内经验比：增长、活跃用户数、获客成本等\n\n3. 指标拆解\n这部分有三个具体步骤，确认主指标 -&gt; 确认计算公式 -&gt;\n多维度拆解\n\n确认主指标：明确当前阶段最有意义的指标\n\n确认计算公式：将目标拆解为可量化的公式，找到影响的相关指标\n\n多维度拆解：商业产品通常从以下两个维度入手：流量 (货源)、客户 (客源)；\n\n\n另外，比较值得关注的商业产品常常可通从 2\n个维度入手，即从流量角度看，收入 = 曝光量 ×cpm；而从客户角度看，收入 = 客户数量 ×arpu\n竞品调研\n竞品调研也有 3 个主要关注的要素：目的，手段，结果\n\n目的：选择好分析的目标\n\n手段：若干对比分析\n\n结果：讨论和提出解决思路\n\n在调研过程中需要常问自己以下四个问题\n1. 竞品是什么样的\n2. 什么要这么做\n3. 是否要学\n4. 如何学\n\n课程里以 facebook 为例讲了这个过程\n\n商业产品从 0 到 1\n商业产品通常的流程是：需求分析 -&gt; 需求筛选 -&gt; 产品设计 -&gt;\n发布验证 -&gt; 上线运营\n需求分析\n其实就是上面第三部分 “确认有效的商业产品需求” 讲的内容，可分为需求分析、数据分析和竞品分析几个部分\n需求筛选\n这部分就是根据\nroi，从第一部分得到的候选需求中筛选出重要的需求；这部分内容跟 《认知红利》阅读笔记 (1)- 概念重塑\n里面的 “为想法估值” 比较相似，推荐一起看\n需求筛选需要通常会从以下四个方面去评估，并提供了参考的综合评分\n(1) 商业收益评估\n(2) 实现成本\n(3) 客户接收程度\n(4) 法务风险\n需求的综合评分的计算公式如下\n\n但是上面的公式其实也只是一个参考值，实际中使用会更加灵活，但是以下的三个基本原则是需要遵守的\n1. 以综合评分排序\n2. 风险拥有一票否决权\n3. 不要挑战客户\n产品设计\n在产品设计环节，最常见的产出物就是需求文档，需求文档又常常分为三类:\nBRD (Business Requirement Document)、MRD (Market Requirement Document) 和\nPRD (Product Requirement Document)，对应着一个产品从调研到执行的过程，\n详细区别见下图\n课程里重点讲了 PRD 的一些撰写规范，可总结为 3+2+1 个基本要素\n\n此外，产出文档还需要遵循 “0123 原则”\n\n0: 不需要说明或帮助\n\n1: 一看就会\n\n2: 两秒等待时间\n\n3: 三步以内的操作\n\n产品研发\n这部分课程里没有详细展开，商业产品经理在这个过程里需要做的应该就是明确开发进度，确保开发进度正常\n发布前验证\n这里指的验证就是常见的 ab\n小流量实验，基本上有两种模式：圈流量与圈客户，也就是流量侧实验和用户侧实验\n用户侧实验也可以分为两种\n1. 随机圈客户，不需要客户参与\n2. 圈指定客户的实验，需要客户的强参与\n上线运营\n产品上线后需要和运营联动，扩大产品的覆盖面；常见的手段有以下三个\n1. 产品宣传\n2. 产品培训\n3. 运营激励\n产品宣传是将产品披露给用户和相关的运营人员，让他们知道有这么一个产品，\n常见有如下手段\n\n产品培训是教导客户使用这个产品，需要注意从客户视角去看进行这个过程，即要思考能给客户带来什么价值\n\n运营激励常见的手段有以下 2 种\n1. 业绩下发：即跟业绩强绑定\n2. 活动激励：申请奖金当做活动的激励\n小结\n综上，课程主要讲了以下四个部分的内容，各部分重点如下\n\n商业产品定义及切入\n\n商业产品定义的三个要素，前向收费和后向收费模式\n\n从能把用户留住后才启动商业化，三个量化条件\n\n从客户需求和平台资源梳理切入商业化\n\n商业产品经理\n\n肩负产品经理和 seller 两种职责\n\n修炼方法论：明确产品定位、会讲故事、用户思维、善用监控表\n\n有效的商业产品需求\n\n需求分析：明确需要为哪些人解决哪些问题，需要换位思考 (平台思维与客户思维)\n\n数据分析：找到核心指标，读指标需要有对比值，公式拆解指标，多角度分析 (货源和客源)\n\n竞品调研：4 个问题，what、why、whether、how\n\n 商业产品从 0 到 1\n\n需求分析 (见上)\n\n需求筛选：关注 roi、客户接收程度和法务风险；3 条基本原则\n\n产品设计：3 种类型的需求文档，0123 的产出原则\n\n产品研发：把控进度\n\n产品发布验证：即 ab 实验，可分为流量侧和客户侧\n\n产品上线后运营：产品宣传、产品培训和运营激励\n\n\n总体的脑图如下\n\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"《链接、装载与库》 阅读笔记 (2)- 可执行文件的装载","url":"/2020/06/13/%E3%80%8A%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93%E3%80%8B%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(2)-%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E7%9A%84%E8%A3%85%E8%BD%BD/","content":"本文是链接、装载与库中关于可执行文件装载的过程，主要描述了进程在被装载时虚拟空间是如何分布的，物理内存空间与虚拟地址空间是如何映射的，同时描述了\nLinux 系统下装载一个可执行文件的基本过程。\n\n基本概念\n可执行文件只有被装载到内存后才能被 CPU\n执行，因为程序执行时所需要的指令和数据必须在内存中才能正常运行，这部分细节涉及到\nCPU 的内部组成架构，具体可参考文章 程序的表示、转换与链接 - week1\n中 现代计算机的模型结构和工作原理部分\n而在每个程序被运行起来后都有自己独立的虚拟地址空间 (Virtual Address\nSpace), 这个虚拟地址空间大小由计算机的硬件平台决定 (CPU\n的位数)，如 32 位的 CPU 上寻址空间是 0-2^32-1 (4GB)\n但实际上这 4GB\n的虚拟空间并不能全部被程序使用，因为操作系统会占掉一部分内存。如果进程访问了未经允许的地址，在\nLinux 下会出现 segment fault 的错误\n在程序装入内存过程中，往往会出现某个程序需要的内存比当前物理内存大得多的情况，这种情况下就需要动态装载了，动态装载的基本原理：程序运行时是有局部性的，因此可以将程序最常用的部分放在内存中，不常用的放在磁盘中\n目前常用的方法是页映射 (paging),\n就是把内存切成小块（page）再分配，当有新的空间申请时，按照一定算法驱逐已分配内存的空间（如\nFIFO、LUR）, 如下图所示\n\n\npageload\n\n从操作系统看可执行文件的装载\n从操作系统的角度来看，一个进程最关键的特征是它拥有独立的虚拟地址空间，这使得该进程能跟其他进程区分开来\n从操作系统角度来看，创建一个进程，然后装载相应的可执行文件并执行，在有虚拟存储的情况下，需要做三件事\n\n创建一个独立的虚拟地址空间\n\n读取可执行文件头，建立虚拟空间与可执行文件的映射关系\n\n将 CPU 的指令寄存器设置成可执行文件的入口地址，启动运行\n\n宏观来说，步骤 1\n相当于是建立虚拟地址空间到物理地址空间的映射关系，步骤 2\n则是建立虚拟地址空间到可执行文件的映射关系，步骤 3\n则是跳转指至可执行文件的入口（保存在 ELF 文件头中）然后开始执行。\n步骤 2 是传统意义上 “装载” 的过程最重要的一步，因为步骤 1\n只分配了一个页目录，具体的映射交给了步骤\n2，而当程序执行发生页错误的时候，操作系统会从物理内存中分配一个物理页，然后将该 “缺页” 从磁盘读取到内存中，再设置缺页的虚拟页和物理页的映射关系\n因此，操作系统捕获到页错误的时候，需要知道程序当前所需要的页在可执行文件中的偏移位置，这就是虚拟空间与可执行文件的映射关系，这种关系会保存在一个数据结构中，在\nLinux\n中会保存在进程中，记录每个段对应的虚拟地址范围和所在可执行文件的位置，称为 VMA (Virtual\nMemory Area)\n如下图所示，在进程创建后，进程内部会有一个 .text 段的\nVMA ，属性为只读，在虚拟空间的地址为\n0x08048000-0x08049000，这个大小就是 32 位 IntelIA32 的一个页的大小，哪怕\n.text 的数据没这么多也会占用掉一个页的大小\n\n\nvirtual addr\n\n那上面提到的页错误（PageFault）指的是什么呢？其实在做完上面三个步骤后，只是建立了映射关系，可执行文件还没被装载到内存中执行。以上图为例，在进程开始执行时，会发现入口地址对应的页\n0x08048000-0x08049000 是个空页面，进而触发一个页错误，CPU\n把控制权交给操作系统，操作系统查询进程的\nVMA，计算出相应页面在可执行文件中的位置，进而在物理内存中分配一个物理页，将进程中改虚拟页与物理页简历映射关系，再把控制权交给进程。如下图所示\n\n\npageFault\n\n进程虚拟空间分布\nsection 与 segment\n对于前面提到的可执行文件中的每个段，假如都要在物理内存中分别分配一个\n页，这样会导致页面内部碎片情况严重，同时浪费内存，因为一个段的大小可能会远小于一个页的大小\n解决这个问题的方法是将相同权限的段合并到一起当做一个段处理，称为\nsegment，如下图所示是将两个 .text\n段合并成一个，从而使得原来需要分配三个页的物理内存变成只需要分配两个页\n\n\nmergeSection\n\n在最开始讨论 ELF 文件时也有段的概念，为了与这里的段区分开，在英文中\nELF 文件中原始的 “段” 被称为 section，虚拟空间地址后被称为\nsegment，实际上，这是看待 ELF 文件的两个视角，如下图所示，左边的 section\n会被合并成右边的 segment\n\n\nsegmentVSsection\n\n因此最终分配物理内存时是以 segment\n来映射的，通过 readelf -S 可看到 elf 文件的\nsection，通过 readelf -l 则可看到 elf 文件的 segment,\n如下图是一个简单例子，有 20 + 的 section，但是只有 5 个 segment，且\nsegment 中只有类型为 LOAD 的两个段才需要被映射到物理内存中\n\n\nsection memory\n\n\n\nsegment memory\n\n类似 section 有段表，ELF 可执行文件中也有\n一个专门的数据结构叫程序头表（program header table），用来保存 segment\n的信息，值得注意的是，因为 ELF\n目标文件不需要被装载，所以没有程序表头\n程序表头的结构体即各个字段的含义如下图所示\n\n\nprogramHeader\n\n堆与栈\n因为进程在运行过程中需要用到堆和栈，而堆和栈在虚拟空间中的表现也是以\nVMA 形式存在的，在 Linux 下，可通过查看 /proc\n来查看进程的虚拟空间分布，如下图所示\n\n\nproc\n\n上面结果中主要关注的是几列表示的含义如下：第一列是 VMA\n的地址范围，第二列是 VMA 的权限 (p 表示 COW,copy on write), 第三列是 VMA\n对应的 segment 在映像文件中的偏移，最后一列是映像文件的路径\n进程栈初始化\n在进程刚启动的时候，需要知道一些进程运行的环境，如系统环境变量和进程的运行参数；因此操作系统在进程启动前会将这些信息提前保存到进程的虚拟空间的栈中（即\nVMA 中的 stack VMA）\n假设系统中有两个环境变量: HOME=/home/usr 和\nPATH=/usr/bin, 如下图运行命令 prog 1234\n后，进程的栈分布如下图所示\n\n\nstackInitialize\n\n栈顶的 esp\n寄存器指向的位置是初始化后的堆栈地址，前面四个字节表示的是命令行参数的格式，这里所谓\n2 (即 prog 和 123)，然后就是指向这两个参数的指针，后面跟着一个\n0，紧着是指向两个环境变量的字符串的指针（即 HOME 和\nPATH 这两个环境变量）\n在进程启动后，程序的库部分会把堆栈的初始化信息中的参数信息传给\nmain() 函数，也就是熟知的 main () 函数的两个 argc 和 argv\n两个参数，分别对应于命令行参数数量和命令行参数字符串指针数组。\n小结\n通过上面的例子可知，进程中的虚拟地址空间可理解为操作系统给进程空间划分出一个个的\nVMA\n来管理进程的虚拟空间，基本原则是将属性相同、有相同映像文件的映射成同一个\nVMA，一个进程基本可映射成以下几个 VMA（segment）\n\n代码 VMA，权限只读、可执行；有映像文件 (即 elf 文件)\n\n数据 VMA，权限可读写、可执行；有映像文件\n\n堆 VMA，权限可读写、可执行；无映像文件，可向上拓展\n\n栈 VMA，权限可读写、不可执行；无映像文件，可向下拓展\n\n因此，一个进程的虚拟地址空间如下图所示\n\n\nexecutable2virtualaddress\n\nLinux 内核装载 ELF\n可执行文件过程\n下面会简单介绍在 Linux 系统的 bash 下输入一个命令执行某个 ELF\n程序时，Linux 系统是怎么装载这个 ELF 文件并执行它的。\n在用户层面，主要有三个步骤\n\nbash 进程会调用 fork() 系统调用来创建一个新的进程\n\n新的进程调用 execve() 系统调用来执行指定的 ELF\n文件\n\nbash\n进程返回并等待前面启动的进程结束，然后用户再输入新的命令（可以用\n&amp; 让程序在后台运行）\n\nexecve() 函数定义如下，\n其三个参数分别表示可执行文件名、执行参数和环境变量，其中执行参数和环境变量对应于前面提到的进程栈的初始化中存储的相关内容\nint execve(cosnt char* filename, char *const argv[], char *const envp[]);\nexecve() 在找到可执行文件后，首先会读取文件前 128\n个字节，其目的是为了判断文件的格式，因为 Linux 执行的可执行文件不知 ELF\n一种，还有 Java、以及以 #! 开始的脚本程序等\n每种可执行文件\n的格式的开头几个字节都是很特殊的，尤其是开头的四个字节（被称为\nmagic number），通过 magic number 可判断文件的格式和类型，如 ELF\n文件前四个字节是 0x7F、'e'、'l'、'f'；而 Java 可执行文件格式头 4\n个字节为 'c'、'a'、'f'、'e'；如果是 shell、python、perl\n这类解释型的语言，第一行往往是 #!/bin/bash、\n#!/usr/bin/python、 #!/user/bin/perl\nexecve() 读取了 128 个字节的文件头部后，会调用\nsearch_binary_handle()\n去搜索和匹配合适的可执行文件装载处理过程，不同类型的可执行文件格式都有相应的装载处理过程，如\nelf 可执行的装载处理过程叫 load_elf_binary(),\n装载可执行脚本程序的处理过程叫 load_scrip() ,\n这里主要描述load_elf_binary() 的基本过程\n\n检查 elf 可执行文件的有效性，比如说 magic number，program header 中\nsegment 的数量\n\n寻找动态链接的 .interp\n段，设置动态链接的路径（后面会有一篇文章专门描述动态链接）\n\n根据 elf 可执行文件的 program header 描述，对 elf\n文件进行映射，比如代码、数据、只读数据等\n\n初始化 elf 进程环境\n\n将系统调用的返回地址改成 elf\n文件可执行文件的入口点，这个入口点取决于程序的链接方式，如对于静态链接的\nELF 文件，这个入口就是 ELF 文件的文件头中 e_entry\n所指的地址；对于动态链接的 ELF 文件，这个入口就是动态链接器\n\n当 load_elf_binary() 执行完后，第五步会令 EIP\n寄存器直接跳转到 ELF 程序的入口地址，于是程序就开始执行，ELF\n可执行文件装载完成。\n总结\n这一章主要描述了程序运行时是如何使用内存空间的，即程序如何被装载到内存中（页映射模式）；然后详细介绍了进程虚拟地址空间的分布，即操作系统如何为程序的代码、数据、堆和栈在进程中分配虚拟地址空间 (VMA),\n最后介绍了 Linux 系统下是如何装载 ELF\n可执行文件的，且这一章中描述的都是在都是静态链接，即只有一个可执行文件，后面会描述动态链接，即一个可执行文件会被拆成若干个模块。\n","categories":["链接、装载与库"],"tags":["C++","读书","链接、装载与库"]},{"title":"《链接、装载与库》 阅读笔记 (1)- 基本概念与静态链接","url":"/2020/05/31/%E3%80%8A%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(1)-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5/","content":"一直对以 C/C++\n为代表的的编译型语言的编译、运行的原理了解不多，最近正好看到这本由国人写的书\n链接、装载与库，书名已经比较言简意赅的介绍了书里相关内容，而且写得挺通俗的，值得一看。这里是书里第一、二部分内容的一些笔记，主要讲了操作系统的一些基本概念，编译生成的目标文件格式和静态链接的过程；由于笔者只摘录一些不太了解的内容，因此总体内容可能不是非常成体系，建议读原书。\n\n操作系统基本概念\n这部分主要讲了一些与操作系统相关的基本概念，了解这些基本概念是了解书中后面内容的基础\n\n增加中间层解决问题\n\n避免直接访问磁盘，增加中间层：文件系统\n\n磁盘以扇区作为基本单位，一个文件会被存储在多个扇区上，文件系统保存了这些数据结构\n\n\n避免直接访问物理地址的方法，增加中间层：虚拟地址\n\n操作系统保留了从物理地址到虚拟地址的映射 (MMU)\n\nsegmentation：把一段物理地址映射到一段虚拟地址，解决了物理地址不隔离的问题\n\npaging：将物理 / 虚拟地址切成小块 (page) 再分配，避免程序占据了整块连续物理地址；\n某些小块可能当前用不着，先存到磁盘中，需要用的时候再取出来，这就是虚拟内存\n\n\n多线程\n\n线程共享代码段，进程数据，打开的文件等资源。但是有自己的私有空间：即寄存器和栈\n\n进程内部默认有一个主线程\n\n可抢占式线程 (preemption) 与不可抢占线程，\n即是否能强行中断当前线程分配资源给其他线程\n\n对 Linux 来说，线程不是一个通用概念\n\n所有执行实体都被称为一个 task，task 间可共享内存空间；如 fork 就是创建了一个任务（COW 机制，copy\non write，即只有在新的 task 改动内存时才为新的 task 创建内存\n\n\n线程不安全的原因：代码的一个操作在被编译成汇编指令后不止一条，因此可能还行一半就被打断，不会被打断的指令也称为原子（atomic）指令，为了达到线程安全，需要同步与锁\n\n信号量 - semaphore: 允许 N 个线程并发访问一个资源\n\n互斥锁 - mutex：仅允许一个进程访问，与 semaphore 不同点在于 mutex\n可由不同的线程来 release lock，但是 semaphore 只能由原来的线程释放\n\n\n编译器优化时可能会造成多线程的加了锁也有问题\n\n用户态 v.s\n内核态：用户实际使用的线程是用户态的，真正执行的线程是内核态的，这种情况下有几种模式（1）一对一（2）一对多（线程内可能会阻塞）\n（3）多对多\n\n堆栈内存区域（C 中还有静态区域，用来存储 static\n变量和全局变量)\n\n堆可以理解为当前可以使用的空闲内存，但是其申请和释放都要程序员自己写代码管理。如果只申请不释放就会造成内存泄露\n\n栈是程序运行时自动拥有的一小块内存，大小在编译期时由编译器参数决定，用于局部变量的存放或者函数调用栈的保存\n\n栈的另一个作用则是保存函数调用栈，这时和数据结构的栈就有关系了。在函数调用过程中，常常会多层甚至递归调用。每一个函数调用都有各自的局部变量值和返回值，每一次函数调用其实是先将当前函数的状态压栈，然后在栈顶开辟新空间用于保存新的函数状态，接下来才是函数执行。当函数执行完毕之后，栈先进后出的特性使得后调用的函数先返回，这样可以保证返回值的有序传递，也保证函数现场可以按顺序恢复\n\n操作系统的栈在内存中高地址向低地址增长，也即低地址为栈顶，高地址为栈底。这就导致了栈的空间有限制，一旦局部变量申请过多（例如开个超大数组），或者函数调用太深（例如递归太多次），那么就会导致栈溢出（Stack\nOverflow），操作系统这时候就会直接把你的程序杀掉。\n\n\n编译与链接概述\n编译的过程 (预编译 -&gt; 编译 -&gt; 汇编 -&gt; 链接)\n\n预编译（生成 .i 文件）：主要是处理 #\n开头的语句，如进行宏展开、将被 include\n的文件插入到对应的地方（递归执行）\n\n编译（生成 .s\n文件）：将代码编译成汇编代码，包括词法分析、语法分析、语义分析和生成汇编代码的优化；虽然不同语言都可通过\ngcc 来统一编译，但是 gcc\n对于对于不同的语言调用了不同的编译程序（如 c 是 cc1，c++ 是 cclplus，java 是 jc1）\n\n汇编（生成.o 文件）：将汇编代码逐条转换成机器指令（有查找表）\n\n链接（生成可执行文件）：静态链接与动态链接\n\n其中编译这个步骤常常涉及到以下几个过程\n\n词法分析：将源码分割成一系列的 token，利用\nlex，编译器开发者只需要定义词法规则\n\n语法分析：生成语法树，利用\nyacc，编译器开发者只需要定义语法规则\n\n语义分析：确定语句是否合法（编译器只能分析静态语义），为语法树中的每个节点标记出其类型\n\n编译器的前端和后端\n- 前端：生成与机器无关的中间代码（即前面的词法和语法分析步骤）\n- 后端：将中间代码转换成目标机器代码\n- 跨平台编译器，使用同一个前端和不同后端\n关于编译器的架构可参考这篇文章：LLVM 概述 —— 基础架构\n链接基本概念\n\n处理各模块间的相互引用（如引用了其他模块的函数，需要在运行时知道这个函数的地址）\n\n从原理上来说，链接就是把一些指令对于其他符号的地址的引用加以修正\n\n库是一些常用代码被编译成目标文件后打包存放\n\n目标文件的格式\n\n目标文件就是源代码编译后但是未进行链接的那些中间文件（windows 的\n.obj 和 linux 下的 .o,\n它跟可执行文件的内容基本一样，所以两类文件使用同一种格式存储（windows\n下的 PE-COFF 和 Linux 下的 ELF）\n\n除了目标文件，动态库 (windows 的 .dll 和 linux\n下的.so) 和静态库 (windows 的 .lib 和 linux\n下的.a) 都是按照可执行文件格式存储\n\nELF\n文件格式可归为如下四类（linux 下可通过 file 格式来显示文件格式）\n\n\n\n\n\n\n\n\n\nELF 文件类型\n说明\n实例\n\n\n\n\n可重定位文件 (Relocatable File)\n 包含代码和数据，可用来被链接为可执行文件或共享目标文件，静态链接库也可归为这一类\n Linux 的 .o, Windows 的 .obj\n\n\n可执行文件 (Executable File)\n 包含了可以直接执行的文件，在 Linux 下一般都没有拓展名\n Linux 的 /bin/bash, windows 下的 .exe\n\n\n共享目标文件 (shared object file)\n 包含代码和数据，可以（1）被链接器将其与可重定位文件和共享目标文件进行链接，产生新的目标文件\n（2）动态链接器可将几个共享目标文件与可执行文件结合，作为进程映像的一部分来运行\n Linux 的 .so, Windows 的 .dll\n\n\n核心存储文件 (Core Dump File)\n 进程意外终止是，系统将进程的地址空间和终止时的一些其他信息转储到核心存储文件下\n Linux 下的 core dump 文件\n\n\n\n\n目标文件组成主要分为两部分：程序指令和程序数据；顾名思义，程度指令就是存放代码，程序数据用来存放代码中的数据，分局数据中的类型又可分为几种段\n数据和指令代码分开存放的好处\n\n数据是可以读写的，但是代码指令是只读的\n\n系统中运行着该程序的多个副本运行时（多进程，多线程），只需要保存一份指令代码 / 只读数据即可，可节省大量内存\n\n可提高缓存命中率\n\nobjdump\n可被用来分析目标文件中所包含的各个段（代码段，数据段，bss 段）的一些信息 (如下图所示），size\n则可被用来查看对应的各个段的大小\n\n\n\n objdump\n\n从上图可知，\n目标文件中有好几个类型的数据段，但是其中某些段是不占空间如.bss\n段，用于表示未初始化的全局变量和局部静态变量 (更准确地说是为这些变量预留了空间)，已初始化的保存在\n.data；因此在内存中的占位如下所示\n\n\nobj file\n\n各个段的含义如下\n\n.text: 存储汇编后的机器指令，通过\nobjdump -d\n反汇编可看到代码段的机器指令机器和对应的汇编代码\n\n.data:\n存储已经初始化的全局变量和局部静态变量\n\n.bss:\n存储未初始化的局部变量和局部静态变量\n\n.rodata: 存储只读数据段，一般用来存储 const\n常量和字符常量\n\n可将一个二进制的文件通过 objcopy copy 到目标文件中\n除了上面介绍的各个段，ELF 文件还有下面几个比较重要的结构\n\n文件头：描述整个文件属性（如文件版本、目标机器型号、程序入口地址等）\n\n段表：描述上面提到的各个段的基本属性，编译器，链接器和装载器都是依赖段表来定位和访问各个段的属性\n\n符号表：管理代码中的符号（函数名和变量名），用于进行链接，因为链接的接口就是符号名\n\n实际上，无论是可执行文件、目标文件或库，实际上都是一样基于段的\nELF\n文件或者是这种文件的集合；源代码经过编译后，按照代码和数据分别存放到相应的段中；除此之外，编译器还会将一些辅助性的信息（符号表，重定位表等）放到目标文件中。下面就是怎么把这些目标文件组装起来了，这就是链接要解决的事情。\n静态链接的过程\n问题：合并目标文件时，对于多个目标文件中的多个段，链接器如何将它们各个段合并到输出文件；或者说链接器怎么为目标文件分配地址和空间\n先说结论，链接可简单地分为两步（two-pass linking)\n\n空间地址与分配：扫描输入文件，合并相同类型的段，输出合并后的段的长度与位置\n\n符号解析与重定位：利用第一步搜集到的信息，进行符号解析与重定位、调整代码中的地址等；这一步是链接过程的核心，尤其是重定位部分\n\n上面提到的 “链接器怎么为目标文件分配地址和空间” 中的 “地址和空间” 实际包括两方面:\n(1) 输出的可执行文件中的空间 (2) 装载后的虚拟地址中的虚拟地址空间；\n对于目标文件中像 .text 和 .data\n的段这两方面都要考虑，但是对于 .bss\n的段只需要考虑虚拟地址空间，因为 .bss\n不在可执行文件中占用空间，后面说的分配也着重于虚拟地址空间的分配\n这里以下面两个源文件 a.c 和 b.c 说明\n\n\nsource code\n\n空间地址与分配\n链接前后使用的虚拟地址（VMA，Virtual Memory Address）\n已经是程序在进程中的虚拟地址（通过 objdump 可看到目标文件中各个段的\nVMA）；如下图所示是将两个目标文件 a.o 和 b.o\n链接成一个可执行文件 ab（通过命令\nld a.o b.o -e main -o ab 可生成可执行文件 ab,\n其入口为函数 main【通过 -e 参数指定，ld 链接器默认的入口为\n_start】）\n通过 objdump 可分析出目标文件与可执行文件中 VMA\n的变化\n\n\nlink_ojbdump\n\n从上面可以看到，链接前目标文件的 VMA\n都是 0，因为虚拟空间还没有被分配，等链接后，可执行文件中的各个段都被分配到了相应的虚拟地址\n下图展示了将两个目标文件 a.o 和 b.o\n链接成可执行文件 ab\n后目标文件与可执行文件各个段的映射关系，以及可执行文件与虚拟地址的映射关系\n\n\nlink_rearrange\n\n在确定了各个段的地址后，由于符号地址在本来段内就是确定的（从前面的\nobjdump\n命令输出结构可知），因此可以根据各个段的地址偏移确定各个符号的地址\n符号解析与重定位\n完成空间地址分配后，进入到了符号的解析与重定位，这也是静态编译的重点\n首先，通过 objdump -d 可以看到 a.o\n代码的反汇编结果如下图所示\n\n\nobj Disassembly\n\n从上图输出可知，目标文件 a.o 中定义了一个函数\nmain, 其起始地址是\n0x00000000，这是因为在程序代码里使用的都是虚拟地址，且在空间地址与分配之前，目标文件中的起始地址是\n0x00000000，等到空间分配完成后，各个函数才会确定自己在虚拟地址空间的地址。\n此外，上图中命令的输出中的 main 函数下面每行都是一条指令，\n从左到右依次是指令的偏移量、指令的机器码，右边是指令对应的汇编代码。\n因此，上图中的 main 函数共占了 0x33 个字节， 17 条指令，其中 0x18\n的指令占了两行，加粗的两条指令分别对应于 a.c\n中引用的 shared 和 swap,\n对于 shared 的引用是偏移为 0x18 的 mov 指令，这条指令共 8 个字节，前\n4 个字节是指令吗，后面 4 个字节是 shared 的地址，从图中看到这个地址为全\n0，这是因为 a.c 被编译成目标文件时，并不知道 shared 和 swap\n的地址，因此暂时用全 0 来表示\n对于 swap 的引用则是偏移为 0x26 的 call 指令，这条指令共 5\n个字节，其作用就是调用另一条指令，后面四个字节就是要调用的下一条指令的偏移量（地址），同样地，在链接前这个地址是一个\nmock 的地址。\n由上面描述可知，目标文件中的地址都是临时的 mock\n地址，真实地址的分配与调整是由链接器来完成的，因为经过第一步的 “空间地址与分配”\n后，链接器已经能够确定所有符号的地址了。同样地通过\nobjdump -d 我们可以看到可执行文件 ab 的反汇编的结果如下\n\n\nexec Disassembly\n\n从上图可知，main 函数地址确定了下来，同时 shared 和 swap\n的地址也确定了，\n链接器是怎么知道哪些指令是要被调整的？在 ELF\n文件中有一个叫重定位表（Relocation\nTable）的结构来专门保存与重定位相关信息，重定位表实际上也是一个\nELF 文件中的一个段（重定位段），通过 objdump -r\n可以看到目标文件中的重定位段，如下图所示\n\n\nRelocationTable\n\n上图展示了 a.o 里面要重定位的地方，即 a.o\n所有引用到外部符号的地址，对照前面 objdump -d a.o\n的结果可知，上图中的偏移的值就是 shared 和 swap\n在目标文件中的机器指令的后四个字节所在地址\n那在上面重定位的过程中，链接器怎么知道 a.o\n里面需要的符号在哪去查找？这就涉及到链接器是如何进行符号解析的了，实际中，链接器会查找所有输入目标文件的符号表组成的全局符号表，找到相应的符号然后重定位。我们在编译过程中常碰到的找不到符号，或者符号冲突，其实都是发生在这个过程中\n通过 readelf -s 可以看到目标文件中的符号表，如下是\na.o 的符号表，可以看到 shared 和 swap 都是\nUND (undefined）的\n\n\ncharTable\n\nCOMMOM 块\n强符号与弱符号：在 C\n语言中，函数和初始化的全局变量是强符号，未初始化的全局变量是弱符号\n在链接中一个符号可能会出现在多个目标文件中，因此可能会出现下面几种情况\n\n两个或两个以上的强符号，类型不一致\n\n两个或两个以上弱符号，类型不一致\n\n有一个强符号，其他都是弱符号，类型不一致\n\n第一种情况是非法的，因为不能定义多个强符号，链接器会报多重定义错误，链接器要处理的就是后两种情况，处理的方式就是下面提到的\nCOMMON 块 (Commom block) 机制，简单来说，COMMON\n块记录了对应的符号的空间大小\n对于第二种情况，会从多个弱符号中选择一个空间最大的，比如说两个弱符号，一个是\nint 型，一个是 double 类型，则最终会选择 double 类型的\n对于第三种情况，会选择强符号，但是如果有弱符号的空间大于强符号的，最终会报如下的\nwarning:\nld: warning: alignment 4 of symbol global in a.o is smaller than 8 in b.o\n因此，使 COMMOM\n模块的原因是因为编译器和链接器允许不同类型的弱符号存在，同时链接器不支持符号类型，即连机器无法判断各个符号类型是否一致。\n静态库\n一种语言的开发环境往往会带有语言库（Language\nLibrary），这些库就是对操作系统的 API\n的包装；一个静态库可以看做是一组目标文件的集合\n通过 ar 可以看到一个静态库中有哪些目标文件\n\n\nlibrary\n\n值得注意的是，静态库里的一个目标文件只包含一个函数，这是是为了是的在链接过程中只链接那些使用到的函数对应的目标文件，如果很多函数都放在同一个目标文件中，会导致很多没用的函数都被遗弃链接进了输出结果中。\n小结\n综上，本文主要介绍了操作系统的一些基本概念，以及静态链接的基本过程，主要关注一下几个方面\n\n目标文件由各种段组成，基本可分为代码段和数据段两大类\n\n目标文件被链接成最终的可执行文件时，输入的目标文件中的各个段是如何被合并到输出文件中的\n\n链接器如何为合并后的段分配在空间和地址（包含输出文件和进程虚拟地址空间）\n\n地址确定后如何进行符号的解析与重定位，使得每个段中的指令和数据都指向正确的位置\n\n","categories":["链接、装载与库"],"tags":["C++","读书","链接、装载与库"]},{"title":"一些有意思的题目","url":"/2016/05/28/%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E9%A2%98%E7%9B%AE/","content":"从互联网搜集的一些比较有趣的题目，通过 python 实现，换成其他语言大多也能实现。题目会尽量保持更新，如果您有好的项目推荐，欢迎在评论区留言。\ngithub 地址:https://github.com/WuLC/show-me-the-code\n\nTalk is cheap. Show me the code.--Linus Torvalds\n\n\n\n第 0000 题：将你的 QQ\n头像（或者微博头像）右上角加上红色的数字，类似于微信未读信息数量那种提示效果。\n类似于图中效果\n\n\n头像\n\n第 0001 题：做为 Apple Store App\n独立开发者，你要搞限时促销，为你的应用生成激活码（或者优惠券），使用\nPython 如何生成 200 个激活码（或者优惠券）？\n第 0002 题：将 0001 题生成的 200\n个激活码（或者优惠券）保存到 MySQL 关系型数据库中。\n第 0003 题：将 0001 题生成的 200\n个激活码（或者优惠券）保存到 Redis\n非关系型数据库中。\n第 0004\n题：任一个英文的纯文本文件，统计其中的单词出现的个数。\n第 0005\n题：你有一个目录，装了很多照片，把它们的尺寸变成都不大于\niPhone5 分辨率的大小。\n第 0006 题：你有一个目录，放了你一个月的日记，都是\ntxt，为了避免分词的问题，假设内容都是英文，请统计出你认为每篇日记最重要的词。\n第 0007\n题：有个目录，里面是你自己写过的程序，统计一下你写过多少行代码。包括空行和注释，但是要分别列出来。\n第 0008\n题：一个 HTML 文件，找出里面的正文。\n第 0009\n题：一个 HTML 文件，找出里面的链接。\n第 0010 题：使用 Python\n生成类似于下图中的字母验证码图片 , 同时写出识别验证码的程序。\n\n\n字母验证码\n\n\n阅读资料\n\n第 0011 题： 敏感词文本文件\nfiltered_words.txt，里面的内容为以下内容，当用户输入敏感词语时，则打印出\nFreedom，否则打印出 Human Rights。\n北京  \n程序员  \n公务员  \n领导  \n牛比  \n牛逼  \n你娘  \n你妈  \nlove  \nsex  \njiangge\n第 0012 题： 敏感词文本文件\nfiltered_words.txt，里面的内容 和 0011 题一样，当用户输入敏感词语，则用\n星号 *\n替换，例如当用户输入「北京是个好城市」，则变成「** 是个好城市」。\n第 0013 题： 用 Python 写一个爬图片的程序，爬这个链接的壁纸\n第 0014 题： 纯文本文件 student.txt 为学生信息，\n里面的内容（包括花括号）如下所示：\n{  \n    \"1\":[\"张三\",150,120,100],  \n    \"2\":[\"李四\",90,99,95],  \n    \"3\":[\"王五\",60,66,68]  \n}\n请将上述内容写到 student.xls 文件中，如下图所示：\n\n\nstudent.xls\n\n\n 阅读资料\n腾讯游戏开发 XML 和 Excel 内容相互转换\n\n第 0015 题： 纯文本文件 city.txt 为城市信息，\n里面的内容（包括花括号）如下所示：\n{  \n    \"1\" : \"上海\",  \n    \"2\" : \"北京\",  \n    \"3\" : \"成都\"  \n}\n请将上述内容写到 city.xls 文件中，如下图所示：\n\n\ncity.xls\n\n第 0016 题： 纯文本文件 numbers.txt,\n里面的内容（包括方括号）如下所示：\n[  \n    [1, 82, 65535],  \n    [20, 90, 13],  \n    [26, 809, 1024]  \n]\n请将上述内容写到 numbers.xls 文件中，如下图所示：\n\n\nnumbers.xls\n\n第 0017 题： 将第 0014 题中的 student.xls\n文件中的内容写到 student.xml 文件中，如\n下所示：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  \n&lt;root&gt;  \n&lt;students&gt;  \n&lt;!--  \n    学生信息表  \n    \"id\" : [名字, 数学, 语文, 英文]  \n--&gt;  \n{  \n    \"1\" : [\"张三\", 150, 120, 100],  \n    \"2\" : [\"李四\", 90, 99, 95],  \n    \"3\" : [\"王五\", 60, 66, 68]  \n}  \n&lt;/students&gt;  \n&lt;/root&gt;\n第 0018 题： 将 第 0015 题中的 city.xls\n文件中的内容写到 city.xml 文件中，如下所示：\n&lt;?xmlversion=\"1.0\" encoding=\"UTF-8\"?&gt;  \n&lt;root&gt;  \n&lt;citys&gt;  \n&lt;!--  \n    城市信息  \n--&gt;  \n{  \n    \"1\" : \"上海\",  \n    \"2\" : \"北京\",  \n    \"3\" : \"成都\"  \n}  \n&lt;/citys&gt;  \n&lt;/root&gt;\n第 0019 题： 将 第 0016 题中的 numbers.xls\n文件中的内容写到 numbers.xml 文件中，如下\n所示：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  \n&lt;root&gt;  \n&lt;numbers&gt;  \n&lt;!--  \n    数字信息  \n--&gt;\n\n[  \n    [1, 82, 65535],  \n    [20, 90, 13],  \n    [26, 809, 1024]  \n]\n\n&lt;/numbers&gt;  \n&lt;/root&gt;\n第 0020 题： 使用 Python\n语言开发服务器端口扫描器，用来检测目标服务器上有哪些端口开放。\n第 0021 题： 通常，登陆某个网站或者\nAPP，需要使用用户名和密码。密码是如何加密后存储起来的呢？请使用 Python\n对密码加密。\n\n阅读资料 用户密码的存储与\nPython 示例\n阅读资料 Hashing\nStrings with Python\n阅读资料 Python's\nsafest method to store and retrieve passwords from a\ndatabase\n\n第 0022 题： 使用 Python 的 Web 框架，做一个 Web\n版本 留言簿 应用。\n阅读资料：Python 有哪些\nWeb 框架\n\n\n\n留言簿参考\n\n\n第 0023 题：\n通过有道翻译提供的 API 写一个支持命令行翻译单词的工具，效果如下图：\n\n","categories":["python"],"tags":["python"]},{"title":"《认知红利》阅读笔记 (2)- 大脑升级","url":"/2021/10/05/%E3%80%8A%E8%AE%A4%E7%9F%A5%E7%BA%A2%E5%88%A9%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0(2)-%E5%A4%A7%E8%84%91%E5%8D%87%E7%BA%A7/","content":"本文是《认知红利》下半部分内容的笔记 (书里的上半部分的内容可参考《认知红利》阅读笔记 (1)- 概念重塑)，这里的 “大脑升级” 采用的是原书的大标题，简单来说就是这部分内容侧重方法论，主要介绍了一些分析问题、解决问题的思维方式，具备一定的实践意义，本文记录了对笔者而言印象比较深刻的观点，推荐读一下原书。\n\n解开大脑的封印\n大脑的封印\n两道封印\n书里认为大脑的两道封印是：负面词语和负面情绪；主要观点是这些负面的词语和情绪会影响到人的思维方式，进而影响人的具体行动\n什么是负面词语？\n比如我不行、我做不到、我没有办法、这样行不通...... 在这些语句中的 “不、没有” 就是负面词语，书里的观点是 “当我们使用 “负面词语” 来思考问题的时候，我们大脑状态就是停滞的”，这个观点其实在《得意忘形》播客里的第 11 期有更详细的介绍和延伸：不如我们停止自我攻击，看看会发生什么？\n书里给出的方法路是：找到这些限制你思考的负面词语，并把它们从你的语言中删除，并用正面词语代替，笔者理解就是从思维上需要进行\n“描述 -&gt; 行动” 的转换，因为抱怨、愤懑是无法解决问题的，如下是一些例子\n\n我没有朋友 →我要多参加一些社交活动\n这个没办法 →我要换个新角度思考一下\n我不想那么穷 →我要想办法增加收入\n\n负面情绪则更常见的，通常是由外部引起的，如同书里提到的 “既然是外围世界导致了你的情绪，那么你要么改变世界，要么控制自己”；所以大多数人在面对负面情绪的时候，往往会选择如下几种方式处理：发泄、隐忍、转移...\n但核心问题是，如果负面情绪是由外部变化造成且这些外部变化是长期存在的，那么上面这些处理方法都是治标不治本的，那该如何做？书里从给了如下例子分析负面情绪产生的根本原因\n\n你正参加一场培训，很认真地在听讲，突然老师冲下讲台，抢走了你的手机，然后对你大吼一声:“你爸妈小时候怎么教你的？！上课玩\n手机，你懂不懂尊重人啊！你是不是有病啊！”\n然后当着众人的面，越骂越凶...... 请问，你心里是什么感受？\n一定是怒火中烧吧，是不是会立刻站起来怼回去？甚至握紧了双拳，随时准备挥舞？\n可是，如果你在上课之前，收到了这样一条消息:“今天给你们分享的这位老师，有点精神失常，今早出门的时候忘记吃药了，你要小心一点......”\n再面对刚才的一幕，请问你心里又会是什么感受？\n也许不是愤怒而是害怕了吧，不是想怼回去，而是想赶紧躲得远远的吧......\n为什么同样的场景，同样的人，做同样的事情，你会表现出截然不同的情绪反应？\n这里有两个原因:\n1. 你对外围世界的理解发生了变化。原来你觉得老师是个正常人，现在变成神经病了！正常人做出这种行为是不可理喻的，而神经病做出这种行为就是情理之中的......\n2. 你对眼前问题失去了掌控力。老师原来是个正常人，你这样羞辱我，我是有办法处理这种情况的，我要么怼回去，要么把你打趴下；可如果面对的是一个神经病，那我真不敢保证搞得过他，谁知道他会干嘛，吓死人了，还是走为上计\n\n上面的 2 个原因，可更抽象一层，记为如下两个原因\n1. 信念不匹配。即外在世界与内在的 BVR\n信念系统 (参考上书里第一部分内容) 冲突，与自己预期不符\n2. 能力不足够。类似常说的无能狂怒，负面情绪的出现，往往是有事情阻碍了我们，当我们无法很好解决时，负面情绪标会一直都在\n解开封印\n怎么解开封印？或者说怎么避免负面情绪？书里给出了长期和短期的两个方法\n长期方法是指在没有情绪的时候，好好调养自己的身心，让负面情绪的状态根本没有机会出现，书里主要提到了一下两点\n1. 提升解决问题的能力，这一点针对的是上面的 “能力不足够” 的原因，比较好理解，如果一个人的大部分问题都能够被很好处理，那自然不会有过多的负面情绪\n2. 建立正面的信念系统，这一点针对的是上面的 “信念不匹配” 的原因，书里给了\n12 条供参考的信念，其实就是给了我们另一种看待世界的方式\n\n1. 没有两个人是一样的\n每一个人都在不同的环境里长大，形成了不同的价值观和信念系统，因此对待同一个问题，自然会有不同的看法，你要学会接受这一事实\n2. 一个人不能控制另外一个人\n每个人的信念、价值观、行为习惯等，只对自己有效，不应该强加给另外一个人\n3. 有效果比有道理更重要\n听上去再有道理的道理，如果实际应用的时候没效果，那就是没道理。在不伤害其他人的基础之上，有利于目标的达成，就是好方法。\n4. 只有由感官经验塑造出来的世界，没有绝对的真实世界\n你永远不可能看完世界上所有的角落，了解每个人遇到的每件事，过程中也会缺失很多信息。就算是进入了大脑的信息，也会被你的信念系统给重新编码，被赋予新的意义。你遇到的所有事情，本身其实都是没有意义的，所有的意义都是我们根据自己的信念系统，人为给加上去的。\n5. 沟通的意义在于对方的回应\n在和对方沟通的时候，你不应该只关注自己说了什么，而是要关注对方听到了什么，理解的程度到哪里。对方的回应，才是你这次沟通的效果。\n6. 重复旧的做法，只会得到旧的结果\n你如果想要得到从未得到过的东西，就要去做从未做过的事情。\n7. 凡事必有至少三个解决方法\n当你感到无计可施的时候，只能说你已知的办法都行不通而已，并不能说问题无法解决。因此，你只要相信一定还有未知的、更好的方法存在，那么总有一天，问题会被你解决\n8. 每一个人都选择给自己最佳利益的行为\n每个人的行为背后，一定有他的正面动机。如果你了解和接受了他的正面动机，他就会觉得你\n接受他这个人，你就更容易引导他做出有效的改善\n9. 每个人都已经具备使自己成功快乐的资源\n你的快乐取决于怎么看待眼前发生的事情，而不是眼前的这件事决定你快不快乐。你遇到的每一件事里，正面和负面的意义都是同时存在的，至于你想看到事物的哪一面，赋予它什么意义，由你自己决定。\n10. 在任何一个系统里，最灵活的部分便是最能影响大局的部分\n能有一个以上的选择，便是灵活；能容纳别人的不同意见，便是灵活；灵活并不代表放弃自己的立场，而是寻求双赢、多赢的可能性；灵活也代表你足够地自信，自信度越低，越容易在某个角落认死理，态度强硬；而强硬的态度会让周围的人感到紧张，灵活却能让人放松\n11. 没有失败，只有反馈\n失败只有在事情画上句号的时候才能使用，失败只是一种反馈信号，告诉我们之前的尝试没有用而已\n12. 动机和情绪总不会错，只是行为没有效果而已\n接受自己的动机和情绪，同时改变自己的行为方式\n\n短期方法则是在情绪来袭的时候使用的方法，主要有三步\n1. 自觉\n察觉到自己出在情绪状态中，因为当你知道自己 “正在生气”，你的怒火便会减少一大半，这部分笔者觉得就是书里上半部分说的元认知\n2. 理解\n找到自己发火的动机，问一下自己这几个问题，“我为什么会发火？我的动机是什么？我想通过发火得到什么？有没有更好的方式来获得？”\n3. 转化\n实际的处理，如找到情绪表达里的负面词语并试着转为正面词语；前面提到的\n12\n条信念有没有哪一条可以解释当下状况；是不是自身能力不足导致了这个问题，有没有谁能帮你解决这个问题\n在这一章的小结里，作者写了如下的话，笔者也比较认可\n\n本章说的这些都是治本的方式，而治本的方式都有一个通病，就是耗时特别长。你不要期望读完本课就能马上不一样了，这是不可能的。\n删除负面词语，替换信念系统，提高解决问题的能力，这每一项都需要你反复训练、长期坚持，才能逐渐收效。\n有些人喜欢快，喜欢药到病除，喜欢掌握一套武功秘籍，然后马上小人物逆袭，而忘了真正重要的是强身健体和修炼内功。\n正因为那么多人喜欢快，喜欢疲于解决表面问题，才给我们这些喜欢慢，喜欢解决根本问题的人以机会。坚持练习，持续打磨，日拱一卒，做时间的朋友，期待一年后一个不一样的自己！\n\n知识的获取与应用\n这部分主要介绍了该控制什么信息进入大脑的信息，以及如何对进入大脑后的信息进行分类和整理\n三个过滤器\n过滤器是什么？其实就是要对输入大脑的东西有所警觉，而不是来者不拒，因为请神容易送神难，进入大脑的信息，是极难被清理干净的，特别是那些含有说服技巧、有煽动性的话语，会长期霸占你的大脑，影响你的思维方式，而你可能还不自知......\n书里提供了 3\n个过滤器：区分信息与知识、区分经验与规律和区分优质与劣质\n1. 区分信息与知识\n对于信息与知识，书里给的定义如下\n\n信息：一切听到的、看到的、闻到的、感觉到的都可以称之为信息；\n如马路上的大妈骂街、电视里的新闻联播、抖音上的美女热舞、微信里的表情斗图\n知识:\n那些被验证过的、正确的，被人们相信的概念、规律、方法论。如 “复利”“元认知”“注意力” 这些本书上篇所讲的内容就是概念；规律是事物背后的运行法则，比如用户需求不变，产品的供应量降低，价格就会升高，这就是规律；而方法论，就是我们俗称的 “套路”，是一套被验证过的，解决某一特定问题最有效率的执行流程。\n\n书里的观点是：信息有真假，有时效；而知识有积累，有迭代。你要学习的是知识，而不是信息。\n虽然这部分强调的是改学习什么内容，同时书里为了强调不被无用信息干扰，在上面阐述信息是给的例子基本都是负面的，但笔者认为在生活中一些有用的信息是必不可少的，尤其是在做决策的时候，比如说择业、成家等。\n2. 区分经验与规律\n一些成功人士的分享，往往会说他们如何通过艰难困苦最后创业成功的，这是经验；中间可能存在幸存者偏差，存在当时的环境红利，同样的事情重复做一遍，哪怕是他自己也不一定能再次成功。因此，我们要学习的是规律，而不是经验。\n而规律简单来说就是能够导致重复成功的因果关系，具体的方法是归纳与推演，从成功经验中推导出原因，而且只有在多次的推演下这个因果关系成立才是规律。\n3. 区分优质与劣质\n海量书籍、互联网资讯容易带来信息过载，让人无所适从，不知道从哪里学起，或者说分不清哪些该学，哪些不该学\n书里将这个原因归结为个人知道得太少了 , 就是个人的知识量还不足以拥有分辨内容优劣的能力，而如果要做到这一点关键是 “见真识假”，即好的看多了，自然就能分辨什么是差的了。书里给出了如下两个例子：\n（1）如果你想学习某个领域里的内容，那就先去找这个领域里最出名的经典书去阅读，可以从各类图书的畅销榜里去挑选，也可以看看业内牛人们的推荐，这样找到的书你会发现来来回回说的就是那几本，这些就是好书。\n（2）如果你是刚刚进入一个新的领域，建议先从经典入门级的书开始读，这有助于你快速掌握这个领域内的基础概念，基础概念的夯实对后期的学习帮助巨大。\n放入知识\n在过滤器辨别了哪些知识值得学习后，下一步就要把知识放入大脑了，书里在这里给出了四个步骤 (这里在第\n2、3 点上笔者的理解跟原书不太一样，所以这里按照笔者的理解来写)\n1. 给大脑外接一个硬盘\n就是借助一些工具来记录这些知识，如各种云笔记软件；对于笔者，这个 blog\n的内容也是一个外接的硬盘\n2. 把知识分类归档\n就是设置一个分类体系，将凌乱的知识分门别类，比如笔者会对每篇文章打上标签；\n分类的体系怎么建立？书里给出 2 中方法 (书里将这部分放在了第 3\n点即结构化的部分，但是笔者觉得放在这里更合适)\n\nMECE 法则：就是遵循 “不遗漏，不重复” 的原则建立分类体系，详细可参考这篇文章\n\n站在巨人的肩膀上：有些领域知识的结构化已经非常成熟了，你可以直接拿来使用\n\n3. 把知识结构化\n当某一个类别超过 20 篇知识点文章，你就可以试着把它们 “结构化”；这样知识就成了一个体系，比如笔者前不久写的\nAn\nOverview Of An Ad System,\n里面就是笔者看过或写过的与 “计算广告” 这一类别相关的内容的一个结构化的过程\n4. 建立知识之间的链接\n这里想表达的观点是：每一次新知识的加入，都需要与原有知识做一次链接，也可以认为是重做步骤\n3；而大多数人的情况是收藏后就不再看了；\n这个过程也可以想象成你当前了解的知识点组成了一张\nDAG (有向无环图)，当有新知识到来时，应该把这个点拼到哪里？\n应用知识\n学习知识不是目的，解决问题才是；学习知识只是为了能更高效地解决问题\n书里认为应用知识去高效解决问题的关键点在于提高思考质量；因为 “知识” 并不能直接解决问题的，而是提高了你解决这个问题的 “思考质量”，因此这部分的重点是如何提升思考质量\n思考的过程可以总结为 2\n步：链接背景知识、梳理背景知识\n1. 链接背景知识\n即在自己的知识库中搜索相关的背景知识，可以是概念、方法论或者别人的经验，或者是自己所见所闻的信息，也可以是其他行业的知识......\n当你掌握的背景知识越多，可用于思考的要素就越多，最终给出的方案也会越全面。当别人还在理解问题的时候，你可能已经链接到一个方法论，并开始侃侃而谈了\n2. 梳理背景知识\n背景知识可能会很零碎，你需要结合问题，把它们重新排列组合一下，梳理成一条完整的信息，形成最终的结论。这个过程包括筛选、整理、重组、缩放等\n针对上面这两点，书里给出的提升思考质量的方法论有如下 4 点\n1. 增加背景知识量\n思考问题中的大部分时间其实是在回忆；如果你掌握的背景知识量太少，你的思考就会比较片面，以偏概全；\n或者所有问题，都链接到一个方法论，比如用供需理论解释一切，这就像拿着一把锤子，眼里都是钉子..\n2. 提高链接强度\n链接强度指的是熟悉程度，那如何提高与背景知识的链接强度，这部分跟上面放入知识中的链接内容有点相似，书里给了如下\n2 步\n\n建立初次链接\n\n\n学习的过程是链接，而不是记忆；\n每次学习了一个新概念、新方法，并不是把它背出来，或者\n存入收藏夹，而是让它和你的旧知识发生链接，用旧的知识来理解这个新概念，让这个新概念从你的原有知识里 “长” 出来。\n不要死记硬背定义，那样很快就会忘记，因为没有发生链接。你可以用其他熟悉的知识来理解它，比如电脑里的硬盘和内存:“背景知识” 就相当于 “硬盘” 里储存的信息，平时一般不用，等有个程序需要用\n到这个信息的时候，这个信息就会从 “硬盘” 进到 “内存” 里进行工作，这个内存就是 “思考区域”\n\n\n重复再重复，形成条件反射级的链接\n\n其实就是要不断应用这些知识，只有当强度到达一定程度后，才会呈现出条件反射级的链接。\n3. 增强知识的结构性\n增加知识的结构性的好处是当你联想到某个背景知识的时候，不是一个个想到的，而是一整片一整片的，甚至是一整套完整的方案；书里给了如下例子\n\n产品卖不出去怎么办？\n别人能链接到的背景知识是：激励销售员，降价促销，增加广告投放渠道这些零碎的点；而你就可以直接链接到 “企业能量模型” 这个结构化的知识，然后分别从 “产品、营销、渠道” 这三个方向，九个常用解决方案里挑选几个适合的，几乎在瞬间给出一套完整的优化方案......\n\n4. 提高对背景知识的梳理能力\n这部分要解决的是从大脑搜索出了背景知识后，该如何做筛选、整理、重组、缩放等操作，梳理给了\n2 个方法\n(1) 随意搭配：没有什么规则，根据自己的喜好来搭配\n(2) 按套路搭配:\n就是一些公开的套路，如整理背景知识的使用 MECE 法则；提升沟通效果的使用 SCQA 结构化表达；\n策略选择使用 SWOT 分析；正向演绎推理的使用三段论....\n提高效率\n专注才能提高效率，因为在一段时间内，输入同类信息，\n就可以同一套背景知识去理解，不需要来回切换，即思考就省去了不断寻找背景知识的过程，只剩下梳理了，思考的效率自然会提高不少；不知不觉便会进入所谓的 “心流” 状态\n无法专注的原因，主要可分为 2 个：外在的和内在的\n1. 外在原因：总结为 “他人需要你、他事勾引你”，即不断有人来找你，各类 app\n不断吸引你的注意力\n2. 内在原因：人类天生容易分心，从进化论的角度来解释就是：时刻关注身边所有的信息是刻在我们基因里的习惯\n获得专注的具体方法，书里列了三点\n1. 破除内部干扰：选择能专注的事情\n如果说天生爱分心、爱主动去捕获各类信息，同时\n内心又如万马奔腾是我们的天性使然，我们无法改变的，那么要解决天性的问题，我们只能用另一种天性去对抗\n而有两类事情，当它们出现时，我们的天性就会自然切换到专注的状态\n(1) 对恐惧的逃避：即时间紧迫，不得不做\n(2) 对愉悦的追求：即做能让自己感到愉悦的事情\n第二点告诉我们我们要找到我们自己的天赋，自己感兴趣，能给自己带来成就感和满足感的事情，并将这些事情作为自己的事业\n第一点则是应用在哪些重要但你不感兴趣的事情上，就是要给事情一个威胁对象和时间期限；这其实也是笔者比较有感触的一点：恐惧让人成长，但在《得意忘形》第\n10 期 「快乐」到底是个什么东西？中，提到了人的三种驱动模式：欲望驱动、恐惧驱动与创造驱动，而恐惧驱动具备一定的缺陷，可以去听一下。\n2. 阻断外部干扰：创造纯净的环境\n这部分应该绝大多数人都懂，就是要营造一个有利于让自己进入专注状态的整体环境，具体是如下\n3 点：隔离噪声、调控信道、摒除杂念\n(1) 隔离噪声\n当你需要专注工作的时候，找一个相对独立或者周围都是陌生人的环境，然后关闭各种 App 的提醒功能，最后，如果环境声音对你有影响，带上降噪耳机\n(2) 调控信道\n为什么你在电影院能非常专心地看完一部电影？因为电影霸占了你两个 “信道”: 视觉信道和听觉信道\n为什么电子游戏比电影更容易让人专注？因为它霸占了你三个 “信道”: 视觉、听觉还有触觉。\n那当你想要专注一件事情的时候，能否调用多个信道来一起完成它 ?\n比如说晨读，其实就是听觉、视觉、触觉都围绕一件事情展开，理解和记忆的效果就会明显提升\n那假如某些工作用不到所有的信道？剩余没用到的信道最好用一些信息量小的内容填充，如写作的时候，一般只用到视觉和触觉，听觉是用不到的，这个时候可以用白噪声去填充；但笔者觉得这条成立的前提是这些没用到的信道受到了干扰，否则在一个安静的环境没必要这么做\n(3) 摒除杂念\n针对摒除杂念给了三点建议:\n主动进入受拘束的环境、少看不相关的内容、不轻易中断思维\na. 主动进入受拘束的环境\n在家很容易分心，因为在家我们可以想干嘛就干嘛，这是一个极度自由的环境。而太自由的环境，会让想法和行动离得太近，你可以任性而为，这不利于专心，你需要主动进入办公室、咖啡厅等公共场合，从一定程度上限制自己的自由。（笔者对这一点深有体会，所以周末如果需要学习写文章之类的，都会回公司）\nb. 少看不相关的内容\n任何信息进入你的大脑，都会留下记忆的灰烬。看到的内容，都会留存在大脑中，时不时地冒出来，叨扰你一下。因此最好的方法就是不看；\n和自我成长没有关系的内容，对知识增长没有帮助的信息，尽量远离\nc. 不轻易中断思维\n在做事情的时候，大脑很容易会冒出一个不相关的想法，如果是无益的想法可以直接摈弃，而好的想法可以先临时记录一下，然后回过头再来思考，就是不要轻易中断自己当前的思维，可以利用番茄工作法帮助自己达到这一目标\n3. 注意休息\n专注需要生理上的良好状态，所以要休息好，书里给出三类的休息和一些参考方法如下\n\n(1) 短休息\n专注了一段时间后，我们需要让大脑进行一次短暂的放松，休息 5~10 分钟，可以闭目养神，或者站起来随意走动一下\n(2) 中度休息\n每天午饭后半小时，人会感觉昏昏欲睡，如果你强行工作，效率就会很低。你可以在这个时候进行一次 20 分钟左右的冥想，既能达到休息的目的，又能锻炼元认知\n开始之前，你可以先喝一杯咖啡，15~20 分钟的时间正好可以让咖啡因进入血液，等睁开双眼的时候，你就会感到精神饱满，思维清晰，充满活力，比脉动还管用。\n(3) 夜晚睡眠质量的保证\n每天至少需要 7 小时的充足睡眠时间，晚上尽量不要超过 12 点入睡，同时给自己配个好枕头、好床垫，投资自己的睡眠质量\n\n关于最后的睡眠，《睡眠革命》里提出的\nR90 睡眠法值得去实践一下，大致的意思就是每个睡眠周期是 1.5h, 每晚需要睡\n4-5 个完整周期\n思维力的提升\n找到本质问题\n描述问题\n“问题” 是什么？\n书里给出的定义是期望与现状的落差部分\n为什么常说 “没有问题，就是最大的问题”?\n因为没有问题，就意味着你不知道目标在哪里，也不知道现状是什么，自然就不知道有什么问题，只是当一天和尚撞一天钟，随波逐流，一脸迷茫，书里举了如下例子\n\n比如你刚对一群人讲完一大段话，然后问:“大家还有没有问题？”\n大家回答你:“没有问题......”\n你千万别天真地以为大家都听懂了，更大的可能是：他不知道什么算真正听懂了，以及为什么要听你说这一大段，他没有一个期望值；也不知道自己听懂了什么，没听懂什么，处在游离状态，找不到自己的现状\n因此，不是他完全听懂了，没有问题，而是他不知道自己有没有听懂，不知道什么算完全听懂，因而找不到这个 “落差” 在哪里，没有发现落差也就没有发现问题，所以只能回答:“没有问题......”\n\n将期望记为 B, 现状记为 B', 则解决问题的办法都应该围绕 B'→B\n来展开思考的，但大部分人的的做法是是还没有理清 B 和 B'\n的实际情况，就急于给出自己的建议，那该如何描述一个问题，书里给出了如下三步法\n1. 明确期望值 (B)\n\n你的目标是什么？\n\n目标是否现实，或者说正常的情况应该是怎样的？\n\n这个目标可衡量吗？\n\n2. 精准定位现状 (B')\n清晰地描述目前所处的位置，并不是一件容易的事。因为现状往往不是单一维度的，需要牵涉到许多方面，在这个过程中需要注意的一点是要区分事实与观点，其实就是要给出量化的指标，用数据说话\n3. 用 B'→B 这个落差，精准描述问题\n不要问模糊的问题，而是要用数字描述问题，书里举了如下的例子\n\n下一次，请记得不要再问出类似于 “你的业绩那么差，打算怎么办” 这样模糊的问题，因为你认为的差，和他认为的差，也许并不一样。在他眼里 20% 的下跌，也许算正常波动，而你却已忧心忡忡。所以，你想让他给出方案，而听到的却感觉他在不断寻找借口......\n你们在讨论的，其实并不是该如何提升业绩的方法，而是到底什么\n才算 “差”......\n那应该怎么问？\n你应该问:“你之前三个月的业绩分别是 100 万元、110 万元、105 万元，而这个月变成了 80 万元，我们来讨论一下，下个月如何能做到 120 万元？”\n\n解决问题\n怎么找到本质问题，而不是头痛医头、脚痛医脚？书里针对这部分，给出了一个叫 “透析三棱镜” 的方法，这个方法认为问题并不会平白无故地出现，它是由 “目标、方法、变量” 这三个因素共同影响产生的，而所谓的三棱镜也对应着这三个要素。书里举了如下例子\n\n比如我们设定，公司本月业绩的期望值 B=100 万元。\n然后怎么办？每月的业绩又不会自己完成。所以，我们要同时制定一个实现它的方法，我们假定这个方法是 (A)。那么，理想的状态应该是：做了 A 就能完成 B (见图 17-5)。\n可现在做了 A 之后，并没有达到预期结果 B，而是达到了 B′，这就产生了 B′→B 的这个落差。也就是我们看到的表面问题，或者称之为症状 (见图 17-6)。\n然后我们就开始分析，B′为什么会产生？\n结果发现有一家讨厌的竞争对手降价了！\n这个因素，在我们当时制定 A 的时候，没有考虑进去，是一个在过程中突发的变量，我们称之为 C。现状 B′的出现，它脱不了干系 (见图\n17-7)\n总体如下图所示\n\n\n因此，现状 B′并不是凭空出现的，而是在三个因素的共同影响下导致的\n\nA:\n为了实现 B 的结果所使用的方法。如果方法是错误的，目标自然无法达到\n\nB:\n期望值。目标设置不当，或者目标设定过高，那么即便完美做到了 A，这个目标也是无法达成的\n\nC:\n过程中出现的变量。方法和目标都没有问题，可是出现了意料之外的事，也有可能导致目标无法达成\n\n因此，要解决问题，不能盯着 B'→B 看，而是要透过 B' 去看 ABC，即透析三棱镜 ,\n具体就是如下的三步：校准目标、重构方法、消除变量\n(1) 校准目标 B\n目标要符合 SMART 原则，SMART 含义如下\n\nS: Specific，明确的，具体的\n\nM: Measurable，可衡量的\n\nA: Achievable，可独力完成的\n\nR: Rewarding，完成后有满足感的\n\nT: Time-bou，有时间限制的\n\nSpecific\n指的是目标要明确，不能太过虚无，如 “幸福” 这个目标定义就不明确，需要把这个目标用清晰明确的行动指引来替代，如 ““我的目标是有一份稳定的工作，有一个爱自己的老公，每周能一起去看次电影，每年去旅行一次”\nMeasurable\n指的是目标是否达成，需要可衡量的标准，比如 “我们的目标就是让客户满意” 就没有可衡量的标准，而是要改成说 “用户好评分，在 9.5 分以上”\nAchievable\n指的是可独力完成的，即目标的达成，一定是自己的力量可以控制的过程，而不能把目标达成与否寄托在他人或者你不可控的事情上。比如，目标定为 “下半年能够升职”，或者是 “他能更喜欢我”，这些你不能控制，因为决定权在对方，你可以改成 “连续三个月业绩达到 100 万元”、“提升自己知识量和气质，增加自己的魅力” 等\nRewarding\n指的是完成后有满足感的，即当你完成这个目标的时候，是否能满足你的存在感知层？太近、太容易的目标，即便完成，你也不会有愉悦感和满足感\nTime-bound 指的是有时间限制即 deadline，否则任何目标都没有意义\n制定目标除了要遵循 “SMART 原则” 外，还得注意区分目标和手段，即在使用方法 A 来达成目标 B 时，不要把\nA 本身当成了目标，书里给了如下例子\n\n就比如读书这件事，你定了一个目标：一年要读 50 本书\n然后呢，具体要看什么书？历史、商业还是文学？找不到方向......\n年中的时候，发现才读了 10 本书，下半年就开始奋起直追\n到了年底，终于读完了 50 本书了，然后呢？\n好像也没学到什么，读完这 50 本书能干嘛呢？\n还是一脸迷茫......\n为什么会这样？\n那是因为，读书是手段，并不是目的。\n你不应该问:“读书是为了什么？” 而是要问:“为了什么，我需要读书？”\n读书是让你达成某个目标的手段，但我们却常常把它当成了目标本身\n\n(2) 重构方法 A\nA 是你为了实现 B 所用的方法，\n当出现了现状 B' 时，我们会习惯性地再找一条从 B' 到 B\n的途径，但这其实是治标不治本的方法，结果常常让同样的问题重复出现，书里举了如下例子\n\n王小锤的团队离开了两名重要伙伴，不应该马上给出解决方案：再补两名销售，而是要回过头，去看看王小锤平时是用什么方式经营团队的，也就是原来的 A 是怎么样的。\n管理方法是什么？责权利有没有对等？是否应用了情境管理？\n团队结构是怎么样的？合适的人有没有放到合适的位置？\n激励机制能否激励到所有人？是资源分配不公平，还是保健因素没给到？\n......\n是其中的哪个原因导致了员工的离开？\n如果这些 “导致员工离开的原有系统” 不改变，只是单纯地再补两个新人，那么依然还会有新的员工继续离职。\n\n调整 A 需要大量的背景知识和正确的思维方式，才能找到适合的解决办法。每个问题都有其独特性和不同的时空背景，需要具体问题具体分析。这一部分书里也没给出相应的方法论，你平时看的大量方法论书籍都是在讲各种 A，这些就像是不同的药丸，当你能够快速抓住问题本质，就能在这些药丸中找到适合的来对症下药了\n所以这部分主要是强调了这点:\n现状是由原来的方法导致的，因此想要改变现状，不是从现状出发，添加一个新的解决方案，而是要回过头，重构原来的方法系统。\n(3) 消除变量 C\n如果 A 和 B 都没有问题，问题依然存在，那一定存在着变量\nC，书里给出了 “象、数、理” 这个框架来寻找 C，意思是:\n任何一个 “现象” 背后一定有 “数据”，任何 “数据” 的变动，背后一定有 “道理”\n面对一个客观问题，要避免使用 “我感觉......” 这样的表述方式，这样的反馈没有任何意义，因为这只是你的 “观点”，不是 “事实”，你要用 “数据” 来说明，数据就是事实。书里举了如下例子\n\n比如，上个月我们的销量是 1000 单，共接到 2 个投诉电话，投诉率为 2%; 这个月我们卖了 3000 单，却接到了 20 次投诉电话，投诉率为 6.7‰，比上个月足足提高了 3 倍多，这个问题需要引起我们的重视！\n但有了这个数据够不够，你要继续挖掘更细的数据，比如这 20 个投诉电话，分别投\n诉了哪些内容。然后你发现，其中有 19 个投诉了产品质量问题，有 1 个投诉了物流问题。\n当然，你还可以继续追问下去，比如具体是哪些部位的质量问题，占比各是多少？这些产品分别是什么时间生产的？等等。\n有了数据后，通过不断问 “为什么” 来找到问题的根源\n\n线性思维\n在讲线性思维前，书里介绍了一个概念：点状思维，就是知识点并没有发生 “链接”，而是像一个一个孤岛一样，单独地存在大脑之中；反之，线性思维就是将两件事、两个概念建立链接，像一条线一样串联起成，由 A 推导出 B，由 B 联想到 C；\n关于建立链接书里给了三种方法：演绎法、归纳法、类比法\n演绎法\n演绎法，就是由 “因” 推导出 “果”，由一般推导出特殊的思维方式，其核心是三段论，一种 “大前提→小前提→结论” 式的推理过程，如著名的 “苏格拉底三段论”:\n大前提：所有的人都是要死的\n小前提：苏格拉底是人\n结论：所以苏格拉底是要死的\n演绎法在生活中能被应用到逻辑推理、逻辑验证中，比如说要验证自己 / 他人的结论的合理性，\n这里需要注意的是：在实际应用中，演绎法并不是那样标准的三段形态，或者是隐去了大前提，或者是隐去了小前提，或者是隐去了结论，需要显式地用三段论分析才行，如下是书里的一个例子\n\n我们再来看另外一句:“她穿得那么暴露，活该被色狼盯上......”\n这句话可能隐藏了什么大前提？\n我猜，他脑海中的大前提可能是:“受害者必有罪过。”\n(大前提) 受害者必有罪过。\n(小前提) 她是受害者 (被色狼盯上)。\n(结论) 她一定有罪过 (穿着暴露)。\n所以，他得出了这样的结论:“她穿得那么暴露，活该被色狼盯上......”\n\n归纳法\n归纳法，就是由 “结果” 出发，寻找 “原因”;\n这个方法非常常见，上一节讲的 “象、数、理” 的分析方法其实就是归纳法\n从 “结果找原因” 这句话过于笼统，实际操作中归纳法有 5 个具体的方法，即穆勒五法，这里的方法其实在现实中大多数人都会用到，只是不知道这些方法的具体名字而已\n1. 求同法\n通俗来说就是让相同的对象同时出现在不同场合下，然后研究这个对象是否是造成不同场合下相同结果的原因，求同法的一般模式如下所示\n\n2. 求异法:\n通俗来说就是让两个场合下只有一个变量，\n研究这个变量里的对象是否是造成不同结果的原因，常见的 AB\n实验就是这种模式； 求异法的一般模式如下所示\n\n3. 并用法:\n就是同时利用求同法和求异法，更加明确地确定某个研究对象是导致结果的原因，并用法的一般模式如下\n\n4. 共变法:\n通俗来说就是研究 2 个因素是否的相关性，\n通过改变其中的一个因素的特性 (比如说浓度，强度等不改变本质的特性)，研究结果是否会有相同的变化趋势，比如说冰箱温度和食物存储时长的关系，如下是共变法的一些模式\n\n5. 剩余法:\n通俗来说就是结果里的部分现象可由已知的原因解释，那剩下的还不可被解释的现象理论上也是由类似的原因造成的。书里举了如下例子\n\n1846 年，天文学家在观测天王星的时候，发现它有四次偏离了预定轨道。经过分析，发现其中有三次偏移是因为分别受到了已知行星的引力影响，还有一次原因不明。\n于是，科学家就推测，一定存在着另外一颗还没有被我们发现的行星，导致了这次天王星的偏离。\n根据这一猜想，天文学家们运用天体力学的理论，计算出了这颗未知行星的轨道，并且最终在 1846 年 9 月 18 日，用望远镜在与计算相差不到 1 度的地方，发现了这颗神秘的行星：海王星。\n\n类比法\n类比法比较好理解，就是拿一件事来理解另一件事，笔者觉得这个方法在跟沟通、演讲过程中非常有用，选择你的听众熟悉的事情来类比，往往会让你的观点更容易被人理解\n书里举了小米的例子，素有 “杂货铺” 之称的小米为什么除了手机，连毛巾、牙刷的都要卖？\n\n小米科技的副总裁刘德说:“这类生意对小米来说，是‘烤红薯生意’”\n“小米发展到今天，已经有 3 亿用户了，其中 2.5 亿是活跃用户。他们除了需要小米手机、充电宝、手环等科技产品，也需要毛巾、床垫等高品质的日用品。所以，与其让这些流量白白耗散掉，不如利用这些流量\n来转化一些营业额。就像一个火热的炉子，它的热气散就散了，不如借助余热顺便来烤一些红薯，这就是‘烤红薯生意’”\n\n知行合一\n只知道这三种链接的方式是没有用的；因为不会有人来问你，什么叫归纳法，什么叫演绎法。那在生活中应该怎么应用这几种方法\n1. 学习新知识后的链接练习\n比如：\n\n刚学到一个新规律，就可以试着找到一个现象，然后用 “演绎法” 做出一番预测\n\n刚学到一个新概念，可以试着找到生活中的哪些现象，也能 “归纳” 出这个结论\n\n刚学会的是一个比较复杂的理论，可以试着用类比法，寻找一个简单、形象的物体来给它做一次封装，让它变得更简单易懂\n\n...\n\n2. 练习写作和演讲\n核心在于通过输出的手段，强迫着你把学习到的知识点建立起结构严密的逻辑链接\n最后，虽然介绍了线性思维中的三种方法，但是线性思维的问题是会让你的思维变得单向而局限，会让你看不到事物之间更多方向、更复杂、更曲折的因果关系，有时候让你只关注到局部，而忽略整体。因此，书里在线性思维后又介绍了结构化思维和系统性思维\n结构化思维\n不少人可能会遇到这种情况：有人口若悬河地和你讲了半天，他说的每个字你都听得懂，然而组合在一起，你并不知道他想说什么，内容没有逻辑，语句没有重点\n这是因为大脑处理信息有两个规律：1. 太多的信息记不住\n2. 喜欢有规律的信息；而杂乱无章地表达自己的观点，哪怕自己很懂，其他人也会云里雾里；而语言没有逻辑是因为思维没有结构，书里举了如下例子\n\n当有人问你，你能说说你有哪些衣服吗？\n“嗯...... 我有很多衣服 (想法)......”\n能说得详细点吗？\n“我有一条蓝裤子，一条橘黄色裙子，一件白衬衫，还有件灰白条纹衬衫，一条牛仔裤，一条蓝色竖条纹的裤子，还有顶黑色的帽子，哦\n对了，还有一条蓝色裤子 (这个刚才好像说过了)......”\n\n而所谓结构化思维，就像是把衣橱里的这些衣服，分门别类地整理好，如下是书里给的例子\n\n比如按季节分类，按穿着场合分类，按服装风格分类，等等。\n这时候，如果别人再问你:“你有些什么衣服呢？”\n你可以这样回答：我一共有 208 件装备，分为:\n夏季、春秋季、冬季三大类；\n每个季节的衣服又分为工作装、休闲装、宴会装、运动装四大系列；\n其中，休闲装里有田园、淑女、简约三种风格；\n每种风格的衣服，拥有深色、浅色各 3 套搭配；\n另外配了 4 双运动鞋、5 双皮鞋、6 双休闲鞋、7 个包包、8 顶帽子来应对不同需要......\n\n笔者觉得这部分其实跟前面 “解开大脑封印” 中说的增强知识的结构性很像，要解决的核心问题还是怎么分门别类，只是前面的是知识，这里则是更广义的东西，比如说问题拆解，演讲、会议、日常沟通中要输出的内容等。而且这部分也给了比较详细的方法论\n金字塔结构\n1. 明确目的，找到分解角度\n将一个 “整体” 拆成一个个独立的 “要素”，再将一个个要素组合成结构，其实可以有很多种组合方式。因此，结构化思维并不是简单地做个分类汇总，而是要思考，分解后以什么方式组合，要达成什么目的。因为同样的要素，组合成不同的结构，就能实现不同的功能和目的。书里举了如下例子\n\n我们得在问题分解之前，先弄清楚分解的目的是什么，然后根据目的进行拆解与结构化。比如说，对于一个项目:\n如果目标是分析进度：那就按时间进度，过程阶段来分解；\n如果目标是分析成本：那就按工作项来分解；\n如果目标是分析客户：那就按性别、年龄、学历、职业、收入等 来分解。\n\n2. 按 MECE 原则，组成结构\n前面在讲知识结构话时，也讲了 MECE 原则，这是《金字塔原理》里面一个核心概念，意思是：相互独立、完全穷尽\n《金字塔原理》中提出了金字塔结构，其实就是思维导图，即从上而下逐层分解，形成金字塔结构，而相互独立、完全穷尽意味着金字塔的每一层，内容不能有重复的部分，也不能有遗漏的部分。以上面的衣服分类为例，如果把衣服分类为春秋季服饰和职业套装，那其实是有重复也有遗漏的；而分为春秋季服饰、夏季服饰和冬季服饰就符合\nMECE 原则。\n书里以三个月完成 100\n万元的销售业绩作为问题，通过以下两种方式来构建金字塔结构\n(1) 自上而下 “使用演绎法” 设计结构\n\n要完成 100 万元的业绩，关键是客户，因此我们可以根据客户的类\n别进行划分，对不同客户类别采取不同的营销策略来完成业绩。\n根据 MECE 原则我们发现，客户无非来源于三类:\n1. 陌生的新客户；\n2. 正在跟进中的准客户；\n3. 已经购买过的老客户\n因此，可以在金字塔的第一层，划分为新客户、跟进中客户、\n老客户这三类\n\n这样一划分，大致的思路就清楚了:\n1. 开拓更多获取新客户的渠道；\n2. 提高跟进中客户的付款率；\n3. 促进老客户的复购。\n根据这个策略，我们再继续往每个子分类中添加要素\n在这个过程中，第一层的分类最重要，它决定了你整个结构的整体功能\n\n(2) 自下而上 “使用归纳法” 提炼结构\n自上而下演绎法的好处是效率高，可以很快速地就把问题结构化。可是，这种方式有个前提，就是你得对问题的解决方法有深刻的理解，能够快速找到恰当的分解角度，或者大脑中已经有了现成的结构可以直接使用，比如：销售额 = 流量 × 转化率 × 客单价。\n当没有现成的结构或找不到分解的角度时，可以尝试自下而上的归纳法，大致的步骤是：头脑风暴，枚举所有可能的想法 -&gt; 梳理形成结构化内容；书里还是以上面的问题为例描述了这种方法\n\n我们还是回到前面的问题：如何在未来三个月完成 100 万元销售业绩？\n你需要开始头脑风暴，把能想到的所有相关信息都列出来，完全穷尽，也可以找来一些帮手，把大伙儿关在会议室里一起讨论，运用群体智慧，通过各种唇枪舌剑，各种奇思妙想，将想法、建议、点子铺满整个白板，\n如下图所示\n\n看着上面凌乱的信息很是让人头大，下一步需要用归纳法中的 “求同、求异、剩余法” 对内容进行分组 ,\n同时需要对内容进行一些增减修补\n经过一番调整，凌乱的内容很快被分成了五个大组：渠道、获客方法、产品活动、客户分类、沟通方式，这里的分组名称不要求太精确，\n因为后期可能还需要调整；如下图所示\n\n到这里稍微清晰了一些，但是这五类之间的逻辑，看上去还是比较混乱，内容之间也不\nMECE，而且也不是金字塔结构；\n因此，你需要进一步梳理这五组的逻辑关系。我们发现 “客户分类” 这个组和其他几组明显不同，渠道、获客方式、沟通方式应该是根据不同的客户采取的不同的拉新、促销的手段\n在调整的过程中，继续修改归类错误，结构不 MECE 的部分，比如:\n“客户分类” 中目前的内容是：新客户、准客户、老客户、机构客户。而机构客户和其他三类有重叠部分，不符合 MECE 原则，应剔除；\n之前的类别命名不精确，其实应该分为 “渠道、营销” 这两部分，不同的用户对应不同的渠道，不同的渠道对应不同的营销方式；\n头脑风暴时只考虑了新客户的渠道，没有考虑准客户和老客户的渠道，要补上，图下入所示\n\n“产品活动” 去哪了？不管是新客户、准客户还是老客户，我们采取的产品活动都是同一个，因此可以把产品活动放在它们的上方，作为金字塔的顶端，这也因此成了本次三个月业绩冲刺的核心活动，最终的金字塔结构如下所示\n\n\n平面切割\n平面切割就是在一张纸上通过线条切割出不同的部分，由于本来是一个整体，所以是无遗漏的；用线条把彼此分开，一定是相互独立不重叠的，即符合了\nMECE\n原则，比如说决定录用员工的标准，往往考核的是能力和态度，那么可以画出如下的平面切割图\n\n值得注意的是结构化思考是帮助我们分析问题的，至于分析完之后又应该如何给出更有效的解决方案，还得看你的背景知识量，上面这个案例省略了具体策略的制定过程\n而遇到问题时，不用每次都自己去分析和切割，因为已经有了一些成熟的模式，如下是一些可参考的切割模型\n\n上面基本都是两分法，其前提是任何事物都是一对矛盾的统一体，彼此对立，加在一\n起又是一个整体；而在二分法的中间添加一个 “过渡” 的状态，让分类变得更加细致，就是三分法；同时结合这两种画线技巧，可以设计出更复杂的结构模型（笔者觉得这些数字并不一定要是\n2 或 3 这么死板，要根据实际情况来）\n书里还是以未来三个月完成 100 万元销售业绩作为问题，阐述了平面切割这个方法的过程\n\n1. 用两分法，找到第一条切割线\n客户？产品？活动方案？营销？渠道？\n好像这些都不太适合，要么范围太小，无法包含所有相关的信息；要么无法切分成一个对立统一的整体。那应该怎么切？\n无论是从哪个角度思考，无非就分为公司的外部因素和公司的内部因素，这既包含了所有的相关要素，又对立统一。\n因此，我们第一条线可以先把结构分为 “内部因素、外部因素” 这两部分\n2. 第二条线怎么画\n要完成 100 万元的销售，无非就两个环节：(1) 把产品生产出来；(2) 把产品卖掉。因此，第二条线可以是 “生产与销售” 这对矛盾统一体。\n因此可画出如下的平面切割图\n\n然后再看一下每个部分是否还可以继续切割，如\n在 “营销 / 渠道” 这个分类中，不同的用户类别适用于不同的渠道和营销方式，可切成新客户、准客户、老客户三类\n在 “销售团队” 这个分类中，使用 X-Y 结构，设置业绩目标，施加外部压力；设置业务奖励，提供内在激励\n在 “市场需求” 这个分类中，用两分法切分为用户需求和市场热点，协助设计部门做出更受欢迎的产品或者活动\n在 “产品设计 / 供应” 这个分类中，用两分法切分为产品设计和供应链管理，针对用户的需求和未来这三个月可能出现的销售高峰，调整产品设计，对供应链进行优化等\n最终可画出如下切割图\n\n当方案开始执行，根据实际的销售情况、市场反馈，视图中右下角的 “用户需求” 部分的信息就会被更新；这将进一步帮助你优化产品设计，提供更受市场欢迎的产品，优化供应链的管理，以满足市场的增长；紧接着，调整团队业绩目标，优化激励方案；然后，继续改善渠道的营销方式...... 整个方案动起来了\n\n\n系统性思维\n上面讲平面切割的最后部分，提到整个方案的不同部分之间相互影响，其实这也是系统性思维的核心，就是要看到各个要素之间的关系\n定义\n书里从如下 3 点阐述了系统性思维的特点\n1. 系统性思维是一种 “基于要素之间关系” 的思维方式\n2. 系统性思维是一项 “看见整体” 的修炼\n3. 系统性思维是一种 “动态化” 的视角\n1. 系统性思维是一种 “基于要素之间关系” 的思维方式\n主要观点是：系统是由要素组成的，比要素更重要的是要素之间的关系，要学会看见要素之间的关系，而非盯着要素本身\n书里以管理为例子\n\n一群聪明人，如果不经管理，就无法组合成一个目标一致、互相配合的系统，他们就会你争我夺，各怀鬼胎，谁都不服谁，各自打着小算盘，最后公司成为一盘散沙，什么事也推进不了。\n管理的目的，就是把一群人组合成一个有效的系统，他们才能成为一支团队。看不见的关系，比看得见的要素要重要得多。系统性思维的第一步，就是要将你的视角从要素转移到关系\n\n2. 系统性思维是一项 “看见整体” 的修炼\n主要观点是：需要关注系统结构、并以系统整体功能和系统目标为导向，来优化内部结构和要素，同时需要关注当前系统所在的大系统\n怎么看到整体？书里列了三点\n\n要看到系统的内部结构\n\n任何一个要素的变动，影响的不是一条直线上的因果关系，而是会牵一发而动全身，按了葫芦起了瓢。只有看清了所有的关系和结构，才能找出系统中的杠杆解\n\n要看到系统的整体特性\n\n在分析问题的时候，除了要看到整体之下的要素、关系、结构，还要看到整个系统涌现出的特性和功能，以整体功能和系统目标为导向，优化系统内部的结构和要素\n\n要看到系统的外部结构\n\n一个系统，往往是一个更大的系统中的要素。因此不仅要考虑某个要素在系统内的互动关系，还要考虑系统作为一个要素，与外部系统之间的互动关系。\n3. 系统性思维是一种 “动态化” 的视角\n这里可以通过 “蝴蝶效应” 比较形象地解释，我们所处的是一个持续演化的混沌世界，现实的世界中往往没有绝对的因果关系，各个要素之间相互影响。\n反馈与回路\n系统中最常见的两个概念是反馈与回路。\n书里将反馈分为三种，其定义如下，比较好理解\n\n正反馈：代表两个要素之间是正比例关系，A 增强，B 增强\n\n负反馈：代表两个要素之间是反比例关系，A 增强，B 减少\n\n延迟反馈:\n代表两个要素之间的互动关系不是即刻发生的，A 发生，一段时间之后，B 才会有反应\n\n通过反馈可以构成 2 种回路：增强回路和调节回路\n\n增强回路：可以简单理解为马太效应\n\n调节回路：由 1 个负反馈加上若干个正反馈所组成的环路\n\n增强回路比较好理解，关于调节回路，书里举了下面的例子\n\n再比如，为了保持公司持续拥有竞争力，你需要保证公司内部有一定的人员流失率。\n比例太高，意味着招聘成本的增加以及业绩的损失；而太低意味着人员臃肿，考核过于宽松，你的团队会越来越没有战斗力。怎么办？\n经过统计，10% 的末尾淘汰率，是一个比较健康的流失率。因此，你需要设计一个调节回路，通过调节考核指标来控制人员流失比例\n\n两种模型\n如果把要素比作电子元器件，反馈方式比作电线，通过把电子元器件用电线一步步连接成一个系统的方式只适用于简单的系统，对于复杂的系统无能为力\n实际上一些常见的系统结构，也能被封装成像主板、显卡那样的一个个模块，可以直接拿来使用，就可以大大提高构建一个系统的效率。书里在这部分介绍了两种最常见的基本结构，生活中很多复杂的系统都可以由它们通过简单的变形、拆分、组合而成\n结构一：增长上限\n增强回路可能不会永远增强下去，因为系统在增强的过程中，会产生一些抑制增强的副作用，\n而副作用的不断累积，就会反过来制约增强回路，最终导致增强的停止，甚至会让它急转直下，即增长上限的结构模型，如下如所示（这个概念也叫做成长上限模式）\n\n书里以养鸡为例\n\n比如说鸡生蛋，蛋生鸡，看似很美好，但是随着鸡的数量不断增加，规模带来的养殖复杂度，也呈几何级上升，这就对你的管理能力提出了严峻的考验\n瘟疫、污染、水电等等因素，会因为你的管理能力不足而频频发生，大量的鸡会因为种种意外、管理不当开始死亡，一场瘟疫、两小时的停电等突发事件就有可能让整个养鸡场毁于一旦。\n而造成的影响，比如恶性瘟疫的发生，甚至会持续发酵，让你面临巨额的赔款，导致你倾家荡产\n如下图所示\n\n\n那当发现自己进入了一个 “增长上限” 的系统结构，应该怎么办？大多数的人会选择继续上图左圈的循环，因为这个方法曾经有效，并带来过指数级的增长，如今停滞了，那么我就应该更加努力；但这是不对的，书里给了如下的例子\n\n你们公司推出了一款新的产品，通过投放大量的广告，让产品的销售量暴增。钱多了，于是你们开始投放更多的广告，产品越卖越多，销售额呈指数级增长，这是一个增强回路......\n但是，产品卖得多了，产量就得跟着提升；产量提高了，你可能就得加人手，管理的复杂度就提升了；管理复杂度提升，就会带来次品率的上升，次品率会影响用户的口碑，用户的差评变多了，就会影响你产品的销量......\n而这个时候，如果你选择继续走左侧，增加广告投放量，你就会发现，广告对产品的销量拉动开始变得乏力，甚至由于市面上你的负面信息过多，这个时候的广告反而会带来反效果，引起大量的嘲讽和退货，造成销量的快速下跌，变成了一个逆向的增强回路......\n如下图所示\n\n\n实际上，发现自己身处一个 “增长上限” 的系统结构中，你应该找到右侧循环中的 “限制因素”，比如养鸡场案例里的 “管理能力”，新产品销售案例里的 “品控能力”，它们才是杠杆解，用心解决限制因素就能打开上限，让左侧的增强回路继续良性运转。\n当然，并不是所有的限制因素最终都会被消除，比如市场容量，它会让你的增长最终迎来极限。\n结构二：舍本逐末\n这个结构指的是只解决表面问题，没有找到本质问题去解决。也可参考舍本逐末模式，其特点是\n(1) 由两个 “调节回路” 组成，两个回路都想解决问题，上面一个回路代表能够快速解决问题的 “症状解”，但是效果只是暂时的；下面一个回路是能够从根本上解决问题的 “根本解”，但是存在时间延迟、见效慢、成本高、难度大等问题，但可以持久有效\n(2) 使用症状解的过程中，还会产生副作用，并隐含了一个增强回路，让问题症状在未来变得更难解决\n其结构如下图所示\n\n书里还是以上面养鸡为例子\n\n回到养鸡场的案例，现在养殖复杂度变得越来越高，鸡群的风险变得越来越大，你应该怎么办？\n增加人手？提高打扫卫生的频次？在饲料里添加抗生素？\n这些行为，确实可以在短时间内快速解决出现的问题，却会在长期的过程中，产生新的更严重的问题 (比如说长期食用抗生素，将带来的副作用是药物残留，产生抗药性，普通细菌会演化成超级病菌，这将进一步危害鸡群和人类的健康); 或者这些问题会一再地出现，你需要不断地去解决，成为救火队员。\n真正有效的解决办法，是引入一整套科学的现代养鸡体系，从硬件设施到软件管理，全部规范化、流程化......\n听到这里，你可能已经头大了，这得花多少钱，花多少时间啊，我现在的这些设施怎么办？人员怎么办？再搬个场地？开玩笑...... 现在已经出现问题了，你告诉我怎么解决？\n嗯，你说的没错，所以你只能是个救火队员......\n比起救火，更重要的工作是防火。如果觉得防火的工作麻烦而不去做，你就只能天天去做救火的事情。\n\n舍本逐末的结构，在生活中非常常见，比如说\n\n生病是 “症状”，去医院看病是 “症状解”，去医院能很快地缓解病情，而 “根本解” 是健康饮食和锻炼身体，耗时长，见效慢\n\n收入低是 “症状”，下班后去做兼职是 “症状解”，做兼职能够很快地增加收入，而 “根本解” 是提高自己的能力，耗时长，见效慢\n\n...\n\n如何逃离舍本逐末的死循环？书里没有给出很详细的方法论，只提了 “选择进入下面的回路，而避免走上面的回路了”；但笔者觉得这个现实指导意义有限，更合理做法是两个回路都要走，但逐步减弱上面的回路，增强下面的回路；\n比如说上面的低收入的例子，对于一些人不做这些低收入的工作可能生活都成问题；再比如说一些互联网公司里的架构问题，在当前架构修修补补是 “症状解”，但症状解都不做，可能公司就完了\n这部分内容跟前面思维能力的提升中 “找到本质问题” 有相似之处，可以\n案例\n书里还是以前面 “如何在未来三个月完成 100 万元的销售业绩” 为例，将系统性思维里面的概念和方法论串了起来。\n这里值得注意的是，使用系统性思维并不是说就要抛弃结构化思维，结构化思维能够让我们把事情想得很完整，这是基础。而系统性思维则是在这个基础之上，帮助我们找到要素之间的关系，发现整个事件的内部结构，同时拉高视角让我们看到事情的全貌，建立起一个整体的概念；并加上动态的时间轴，让我们能看到事情未来的演化方向。\n\n第一步，从调节回路开始，描述系统\n从系统的角度来看，系统的目标应该拉长时间，比如三年要实现一个什么目标；\n或者状态化，比如每天、每月要实现什么样的效果。\n因此，我们可以把系统的目标调整为：每月销售额达到 50 万元，这就是一个状态化的目标\n根据这个目标，可以画出调节回路中的 “现状、目标差距” 这两个要素 (见图 20-24)；那当今的销量是如何产生的？销量是经由你目前的 “销售系统” 产生的，销售系统的效率越高，当今的销量越高 (见图 20-25)\nPS：下面图中笔者不太理解的是为什么目标对业绩差距有正反馈作用，因为这只是个目标而不是动作，难道是指目标订得越高，与当前的业绩差距就越大？\n\n有了上面的拓扑图，下一个问题就是：业绩的差距如何能提高销售系统的效率？可以试着在业绩没达到之前，给团队设定 KPI，用压力工具，推动销售系统的运转\n\n第二步，优化结构，完成系统图，找到杠杆解\n当前的系统结构中，只有当\n团队业绩不达标的情况下，KPI 才会发挥作用，用压力推动销售系统，而业绩一旦达标，团队就没有了继续销售的动力，甚至会把当月的业绩藏到下个月用，怎么办？\n一个系统，增长的动力来自于 “增强回路”，因此要想办法在系统中构建一个增强回路，让系统不断地增强。比如可以试着用新增业绩的一部分作为激励，推动销售系统的继续运转，形成增强回路\n\n增强回路出现后，我们就要马上想到，会不会存在 “增长上限” 的结构 ? 也就是产品卖得越多，会不会产生其他的副作用？\n这个当然会，产品卖得越多，产品供应端的压力肯定就增大，产品质量、服务品质可能都会因此下降，如下图所示，而应对方法就是上面提到的 “保证左侧回路的正常运行，去除右侧回路的限制因素”\n\n第三步，抬头看天，放入大系统\n目前这个系统只是某一个业务层面的系统图，我们可以将这个系统放到整个公司的运营系统中:\n用公司中能带来大量现金流的业务，来给案例中的业务提供弹药，给人、给钱、给资源，帮助该业务更快地成长为明星业务\n\n同时可以将视角再往上拉一个维度；即把整个公司的业务，看作是一个要素，把它放在整个市场环境中去分析；这样就能从能从更宏观的层面，从上到下，系统性地逐层分析，找到更适合公司的发展策略、产品策略、营销策略\n\n这里也要注意，在分析任何问题时，都需要把全世界的每个角落都看一遍才能做决定，把与问题相关性最大的系统结构拿来分析即可\n解决所有问题\n做选择\n如果说 “选择&gt; 努力”，那就应该在 “选择” 这件事情上花更多的精力\n克服非理性\n人是非理性的，比如反射效应、确定效应、锚定效应、迷恋小概率事件等，\n在《思考，快与慢》这本书中，把人脑分成了 2 个思考模式来解释上面的非理性现象，分别是 “系统 1—— 快思考” 和 “系统 2—— 慢思考”，其含义如下\n系统 1：就是我们日常说的直觉思考，凭感觉行事，甚至是一种本能反应\n系统 2：理性思考，是逻辑推理，是量化分析；把问题拆开了、揉碎了、铺在面前，按照一定的步骤，开始分析各种利弊、因果关系、逻辑推理\n由于系统 1 是自动运行的，不需要调用大量的注意力，很节能。所以大脑本能的选择，都是凭感觉行事，速度很快但也很容易出错；看到这部分，笔者想起之前看到的一句话\n“毁掉一个人最快的方式，就是让他忙到没时间成长”，本质上就是在让人一直通过系统\n1 来思考和执行。\n因此，做决策首先需要克服非理性，其实就是要启动系统\n2，具体的方法是\n1. 启动元认知，开启系统 2\n即要意识到这是个重要的决策，需要做仔细的分析\n2. 让思考离开大脑，强制限速\n即要借助外部的媒介来记录思考的结果:\n比如拿出一张白纸，打开 Excel 表，或者使用 Xmind 等脑图软件，将自己思考分析的过程可视化，把自己变成第三人视角...\n除了上面提到的技巧，笔者还有一个经验就是尽量在一个安静的环境下进行上面的操作，过于吵杂比较难让大脑进入系统 2\n3. 设置外部提醒\n即通过外部因素来避免上面 1、2\n点没法很好被执行；比如遇到问题时多去问一下别人的意见，或者召开一次会议，让自己的想法接受别人的批评与挑战...... 通过这些外部的力量激活系统 2\n4. 识别认知偏误\n除了开头提到的几种非理性的效应，还有类似于损失规避、心理账户、禀赋效应、可得性偏差、比例偏见、结果偏见、鸡蛋理论等一系列人们非理性的行为特征，这些要去学习了解，以防踩坑；要认识到很多公司的营销策略，都在让用户的大脑只运行系统 1，让他不断买买买...\n启动系统 2\n后，分析各个选项也有套路，总结来说就是列标准 + 量化选项，列出什么是自己关注的，什么是自己不能接受的等等，然后基于这些标准给每个选项分配权重，打分，然后算出最后的分数\n笔者觉得这个过程中的量化虽然往往不是那么准确（正是由于不确定性才会让人产生选择困难），但关键是在你量化的这个过程中，你能更清楚自己更想要什么，能够舍弃什么，这比起最终不准确的量化结果更为重要。当然，这个过程中也可以借鉴其他人的意见，但最清楚你的情况的还是你自己。\n第三选择\n面对选择题，首先不要只看选项本身，而是要还原目标: 你要问自己，做这个题到底是为了什么？你最想要达到的结果是什么？\n而一旦问题有了目标，选择题变成了简答题，也就不局限于已有的选择，而是有 “第三选择”；有了这个思维习惯，当你再遇到 “选 A 还是选 B” 这样纠结的情况，你首先应该问自己的是:“目标是什么？还有更好的选择吗？”\n书里举了如下例子\n\n比如在谈判中，对方觉得你们的产品太贵了，希望可以降价。怎么办？\n你不要在降不降价的问题上纠结，你应该说:“通过与贵公司的合作，增加彼此的整体利润是我们共同的目标。降价是可以的，不过这样会对我们的利润造成很大的影响，我们对贵公司的 C 业务也很感兴趣，是否可以在 C 业务上给我们一个位置更好的广告位作为交换？”\n这就是跳出了原有的选择框架，找到了更好的第三选择，得到双赢的结果。\n\n小事不纠结\n上面提到了做选择需要准备的工作，既要慢思考避免认知偏误，又要量化分析做理性决策，还不能局限于现有的选项，要找到问题背后的目标，把它转化为简答题，再扩充选项，如果每一个选择都是这样，那会非常麻烦。\n因此，我们不需要在所有事情上都这么做，而是要紧盯核心目标，过滤大多数问题，不在小事上纠缠；那什么是核心目标？就是书的前半部分说的精神层次、身份层次里的那些事；\n就是 5 年、10 年、30 年之后，再回过头来看也依然觉得很重要，影响重大的那些事\n做计划\n计划 = 目标 + 实现路径，缺一不可；如果只有目标那只是愿望清单，而如果只有实现路径那是\nTODO List\n而对于不同难度的目标，做计划的方式也不同，书里把根据目标难度分为\nEasy、Normal 和 Hard\n三种模式，在不同模式下的具体策略也不同，如下图所示\n\nEasy 模式\n这种任务有两个特点\n1. 任务可以独立完成，你无须其他人帮助；\n2. 你什么都不缺，就缺行动力和时间，只要你想，就能把它轻松完 成。\n在这个场景下的任务虽然简单，但是多而杂乱，因此，你的应对策略，就是有效管理好这些待办任务，让它们变得有序且不遗漏，提高你的处理效率。\n书里提到了一个叫 GTD (Getting Things\nDone) 的方法，主要分为以下 4 步\n1. 收集：把任务从你的大脑中清空，\n记录到各种清单软件上\n2. 整理：删除一时兴起记下的、委托可给他人的、剩下的移入建立好的任务分类里，比如工作、家庭、健康、\n某某项目等\n3. 执行：保持专注，一次只做一件事\n4. 回顾：定期对所有任务进行一次回顾，看看收件箱里有没有未处理的任务\nNormal 模式\nNormal\n难度级别下的任务，光有执行力和能力已经不够了，还得每一步都正确，你不能走一步算一步，你需要提前规划好每一步。因为一旦中间某个环节你做错了，很可能整个任务都得推倒重来。\n书里给出了在 Normal 模式下完成目标的 4 步\n1. 设定一个目标\n即根据前面提到的 SMART\n原则设定一个大目标，目标一般可分为主动目标和被动目标两种\n(1) 被动目标：别人安排给你的任务，比如公司要求你达成的业绩\n目标。\n(2) 主动目标：是你自己想去完成的事情，是选择的结果\n同时值得注意的是，所有的结果都需要你用注意力和时间去交换，而你的时间和注意力是有限的，如果你什么都想要，最后就什么也得不到\n2. 把大目标拆解成小目标\n通过加法或乘法的方式将大目标拆成若干小目标\n（1）加法分解：按照时间或空间分解，比如说未来一年要赚多少钱是目标，可以按照时间分解到每个季度；也可按空间把收入分成多个组成部分，如工资收入、业绩奖金、期权折现等\n（2）乘法分解：需要找到与目标对应的数学公式，如销售额 = 流量 × 转化率 × 客单价\n3. 将目标拆解成任务\n这一步就是把 “结果” 翻译成实现结果的 “过程”，书里给了 2\n中具体的方法\n（1）正向规划：有成熟的步骤、参考资料、历史的经验可以借鉴\n（2）逆向规划：即以终为始，从结果倒推；同时可借助一种 “事前验尸” 的方法来列举出所有的失败可能，然后尽量避开这些失败可能\n4. 执行\n有了具体的规划和任务，下一步就可以把这些任务放入你的 GTD\n系统，开始逐项执行了\nHard 模式\n事情到了这个级别，想靠一己之力去完成目标已经不可能了，你得调动几十人、几百人甚至上千人协同作战才行\n这里借助了书的前半部分提到的\n“NLP 理解层次”，来设计一套能够调动一群人行动的计划方案，并称之为 “N\n计划”，其步骤如下\n1. 为目标赋予意义，统一共识\n为整个团队设定目标，不仅要符合 SMART 原则，还需要为目标赋予重大的意义；一个有意义的目标自带影响力，它能够吸引斗士、凝聚人心。那意义是什么？书里给的答案是\n\n意义就是你为什么要制定这个目标，而不是其他目标。完成这个目标我们团队能获得什么？对我们团队的长远发展能起到什么样的关键作用？客户能因此获得什么？整个社会能获得什么？你们每个人能获得什么？对成长的帮助，对收入的贡献......\n\n2. 确认领袖，组建核心班底\n就是选出项目的 POC,\n可以从过去、当下和未来三个方面去考虑\n(1) 过去：在这件事情上曾经有哪些成功的经验；\n(2) 当下：是否拥有领导的品质，比如领导力、格局、智慧、性格等等；\n(3) 未来:\n对这个目标如何达成的战术思路是否靠谱，信心是否足够，意愿是否强烈等等\n这里值得注意的是，千万不能抓壮丁，即有意愿去做但能力还不足的人；这样最终可能带来的是也许只有挫败感，打击的却是全团队的士气\n3. 制定团队的行事原则\n即上篇文章中提到的\nBVR 概念，通俗来说就是原则\n(1) B (Believe): 信念\n信念即认为事情应该是怎么样的？世界是怎么运行的？你们的行动应该遵循哪个法则？整个计划应该遵循什么因果逻辑？\n信念也可以是某套理论体系或概念，比如行为经济学、长尾理论、网络效应等等。当你相信某套理论，相信未来会按它说的那样发展，那么你就可以以它为基础，开始设计你的规则，安排你的计划，建立你的行为和之后结果之间的因果关系。\n(2) V (Value): 价值观\n价值观即认为什么是对的，什么是错的；什么是重要的，什么不重要；什么是我们要的，什么是我们应该该坚决说 NO 的。\n每个人都有自己的观点和喜好，没有绝对的正确与错误，但如果身处一个团队之中，所谓正确的事，就是符合团队价值观的事\n(3) R (Rule): 规条\n信念和价值观略显抽象，规条把它们具象为具体的行为要求，比如对国家来说就是宪法和各种条例，对企业来说是制度，对团队来说是规则，对学生来说是行为规范\n4. 配置人力、财力、物力\n5. 计划、分工、执行\n这两部分有重叠内容，书里放在一块讲了，总共有 6\n不，而且很多内容跟前面有重合，这里也不详细展开了\n(1) 分解目标\n(2) 将目标分解成任务\n(3) 分配资源:\n有多少资源就干多大的事，实在需要就想办法去借，或者调低目标，步步为营，把雪球滚起来了再干大事\n(4) 专业化分工：保证每个人只专注于一件事，让专注产生个体效率\n(5) 开始执行：通过 GTD 方法来执行\n(6) 执行力保障：保证各个任务进度\n6. 找势能高地借力\n即寻找部找资源，让团队站上有势能的高地，顺势而为，关于势能可以可参考上一篇文章\n因此，结合 NLP 理解层次可画出下图\n\n“计划” 还是 “演化”\n计划的有效性依赖 2 个前提\n1. 计划有正确的目标：“正确” 指的是计划能经得起时间的考验，即现在笃信的一个目标，在未来是否会是一个靠谱的目标\n2. 计划靠谱：“靠谱” 指的是在执行过程中能够能够克服 “不确定” 和 “复杂性” 两个问题，不确定指的是过程中可能有其他未曾设想的因素出现，复杂性则是指为了解决这些因素导致了总体方案膨胀，难以执行\n我们常听到的 “计划赶不上变化”，其实就是这 2\n个前提中的部分不成立了；因此，书里还提出了一种 “演化” 的思维方式，这种方式基本就是自然进化的思想：遗传变异和自然选择，进而提出构建自我演化能力的方法，可分为如下\n4 步\n1. 开启初始状态\n不管是产品也好、团队也好，都需要从 0 到 1，先开启一个初始状态，作为演化的起点。\n对于产品而言，这一步不要去憋大招，一上来就想去设计一个包罗万象，改变世界的产品；而是要开启一个演化循环，让你的产品或者团队快速进入并适应眼前的市场，好产品、好团队、好结果都是演化出来的。这一点比较好理解，软件开发中的过度设计说的也是这个问题\n对于团队而言，初始就是就是我们上节课讲到的创始团队 + 使命、愿景、价值观\n2. 自然选择\n这里可分为外部和内部两部分的选择\n外部指的是把自己的产品、服务投放到市场上去，接受用户的点赞或者吐槽，把用户的喜好当成一把自然剪刀，对自己进行一番修剪\n内部指的是在团队内部设计一个竞争环境，让员工、团队、产品在内部进行各种厮杀，互相 PK，优胜劣汰，完成企业整体的自我演化，比如腾讯著名的 “赛马” 机制，末尾淘汰机制等 (优劣这里就不讨论了，本文只是客观陈述这一现象)\n3. 变异\n变异在这里就是创新，变异可分为基因重组和基因变异，这里也对应着创新的两大类：重组式创新和突变式创新\n基因重组就是把原来就有的各种要素，用新的方式组合起来，形成一个新的个体；类比到重组式创新，一般优秀的要素越多，组合方式就越多，创新的空间也就越大；从产品层面具体来说，重组式创新要素包括：\n\n初代产品：特别是在上一步自然选择过程中体现出 “生存优势” 的部分\n\n外部多样性：可以是在内部赛马制中输掉的团队，虽然输了赛马，但是否也有亮点，被用户特别喜欢的部分；或者是同行的产品中，是否有那些已经被市场认可的功能和设计 (注意，是借鉴或者购买，不是抄袭)\n\n内部多样性：团队中的人彼此擅长的部分、所属的领域等差异越大，越容易产生新的创意\n\n....\n\n基因突变指的是某个特性，在某一代的身上突然出现了；类比到突变式创新，就是 “主动试错”，书里举了如下例子\n\n如谷歌有个神秘部门叫 “Google\nX”，专门研发一些天马行空的项目，比如太空电梯，冷聚变，在海面上盖房子，利用磁悬浮技术开发悬\n浮滑板，从海水中提取价格低廉的燃料，对抗衰老和死亡，当然也包括著名的 Google\nGlass......\n这些想法听着就很荒诞，很不靠谱......\n而事实上，它们也确实基本都失败了......\n但又有什么关系，就像是那些复制错误的基因，消亡就消亡吧，只要其中有一个项目能完成实质性的突破，能带来新的生存优势，也许就\n能再造一个 Google，甚至变成一个更厉害的物种。\n\n4. 遗传\n遗传这一步比较好懂，就是把经验和找到的生存优势遗传给下一代，让下一代站在巨人的肩膀上\n说完了演化的特点，你可能会问，应该选择计划还是演化？其实这两者不是非此即彼，而是可以配合使用，计划的缺点是抗风险能力差，但好处是效率高，可以在短时间内高效地完成任务；而演化的优点是抗风险能力强，不需要具体的目标就能越变越好，但缺点是效率低、耗时长！\n创新\n前面讲演化的时候简单提过了创新，这里则是给出了更详细的分析和方法论\n创新三要素\n创新要满足三个要素：新元素、价值增量和可实现\n\n新元素：必须得有新的元素，可以是新技术、新创意、新材料，也可以是新的定位、新的应用、新的解决方案\n\n价值增量：要能满足市场上的某些需求。没有价值的创新，只能称之为创意\n\n可实现：想法能落地；创新理论的鼻祖熊彼特说过:“所谓的创新，就是发明的第一次商业化应用。”\n\n重组式创新\n如同前面提到的，重组式创新就是把原来就有的各种元素，用新的方式组合起来，形成一个新的个体。书里给出重组式创新的三个操作步骤\n1. 创造适合的环境\n合适的环境需要包含 2 个条件：(1) 具有足够的多样性\n(2) 元素间能发生高效的连接\n\n具有足够的多样性\n\n简单来说就是要提高个人知识量，书里举了如下例子\n\n重组式创新就好像是玩乐高积木，积木的种类和数量越多，你能拼接出的作品就越多。因此，你想要创新，第一步就是要增加积木的数量和种类\n什么是积木的数量？\n就是你个人的背景知识量，你拥有的背景知识量越大，能用来组成创意的元素也就越多。因此，为了提高你的背景知识量，你需要阅读，大量地阅读，不停地阅读，这是创新的基础。\n什么是积木的种类？\n一个人看再多书也是有限的，而且大多会集中在你感兴趣的领域上，因此你在某个方面越专业，了解得越深入，可能就越缺乏其他领域的知识。所以，你需要找不同领域的人来和你一起碰撞想法，增加你们的背景知识总量。一个由五个不同领域的人组成的团队，比五个都是同行的团队，能碰撞出更多的创意火花\n\n\n元素之间能发生高效的连接\n\n这部分讲的更多是如何有效进行团队的沟通与协作，主要包含三部分：从 “No”\n到 “Yes...And...”、设置规则保证充分链接、创造彼此链接的办公环境\n首先，不管是你一个人在思考，还是很多人在一起讨论，记得都不要说 “No”，而是要说:“Yes...And...”，书里举了如下例子\n\nNo 是否定想法；\n好比在积木堆中出现了一块看上去很丑的积木，觉得没用准备把它扔了；但看上去很丑的积木，也许就是一个新品种出现了，目前看上去没有什么用，但是一旦积木越来越多了之后，你也许就会突然发现它大有用处。\n因此，你需要用 “Yes...And...” 来先肯定这个怪想法，然后基于这个怪想法接着思考:“如果加上了这个想法，接下来可能还会发生什么呢？” 这样就很容易跳出固有的思维框架，来到一片新大陆，产生意想不到的结果。\n\n其次，如果是多人讨论，你们就需要设置一些规则，来保证彼此能充分链接，让想法不断涌现。比如说书里提到的头脑风暴往往没有发挥很好的作用，如下例子所示\n\n会议刚开始不久，还没等几个新想法出现，有些在公司略有地位的人，就开始跳出来指点江山，接着，这些大佬们就开始试图彼此说服，急于证明自己的想法特别厉害，分分钟为别人的智商感到担忧。\n然后，一言不合就开启辩论模式，头脑风暴眼看就要变成一场辩论大会，场中央的领导实在看不下去了，一拍桌子举手表决！看着大佬们唇枪舌剑杀红了眼，明明有了新想法的小透明们也不敢多插一嘴，只得默默地把票投给了自己的上司......\n与其说这是头脑风暴，不如说是舞台选秀，谁的嗓门大，谁的地位高，谁的人缘好，谁就能赢取最终的胜利！请问，这样的方式之下会有创新吗？\n答案自然是否定的\n其实，大多数团队用错了这个工具。其实，头脑风暴就是群体版的 “Yes...And...”，是靠点子来激发点子的沟通方式，任何一个新想法的出现，都需要被鼓励，甚至是夸张的鼓掌支持，然后由这个不靠谱的想法展开，链接到越来越多不靠谱的想法，就像水中的涟漪，层层裹挟，不断散开，最终踏入一片未曾想象到的领域，进而迸发出真正的创新火花。\n\n最后，你还得创造一个能促进彼此链接的办公环境。即创造出成员之间更频繁、更平等的交流环境。书里举了如下例子\n\n比如，曾经创造了《玩具总动员》《怪物电力公司》《飞屋环游记》《寻梦环游记》等无数票房神话的皮克斯动画，为了在公司里营造更好的创新环境，他们撤掉了全公司的长会议桌和座次牌，因为老板觉得这些会造成成员之间的层级感，影响链接的效率，比如坐在长桌子中间的人，总是更有权威感，让其他位置上的人不敢畅所欲言，而全部换成圆桌之后，沟通效率大大提升。\n\n2. 收集创新元素\n有了激发创造力的环境后，下一步就是要去看创新是如何被产生的。书里通过下图来描述这个过程，图中横轴代表离创新本体的距离，纵轴代表元素属性的虚实，然后以 “创新本体” 为轴心 (以产品创新为例)，由近及远地把寻宝图分成 5 个层级 (同行、异业、原型、环境、时空)，共 10 个板块，\n\n\n同行层\n\n同行层是离创新本体最近的一层，是指你在同行中寻找那些可以借鉴的部分，然后把它们拿过来和自己的产品重组，从而让自己变成一个更好的创新产品，\n同行层中的元素包含两个部分，看得见实体的部分：产品，比如同行产品中某处优秀的界面设计，某个让人着迷的互动玩法，又或者是某项对用户帮助极大的功能和技术等；另一半是看不见的、虚的部分：模式，比如同行的战略定位、商业模式、组织运营等。\n这个过程需要的不是灵感，而是是大量的信息收集和配对测试；同时书里也强调了不是要去 “抄袭” 同行，而是在不侵害他人专利的情况下，善于借鉴、学习他人优秀的部分，对有专利保护的部分要舍得购买，然后引入自己的产品中\n\n异业层\n\n异业层和同行层一样，只不过对象换成了其他行业，拥有了更大的范围。比如你是做手机的，除了向同行学习借鉴之外，你还可以跑到汽车业、家具业甚至娱乐业等其他行业中去找寻创新元素。书里举了如下例子\n\n19 世纪末期，当时新生儿的死亡率奇高，一直得不到有效的解决。一个名叫斯蒂芬・塔尼的妇产科医生，在一次逛动物园的时候，偶然看到了园内的一个小鸡孵化器，刚出生的小鸡在这个小温室里\n活蹦乱跳，这给他带来了启发。之后，他便与奥迪尔・马丁一起借鉴了小鸡孵化器的模式，制造出了人类婴儿用的恒温箱，挽救了数以亿计的生命。\n\n\n原型层\n\n“原型层” 离大众视野就比较远，这一层的实体部分是可能还停留在实验室阶段的技术和原型，它们或找不到应用场景，或无法量产，或还没普及。书里给了如下的方法和例子\n\n在这一层寻找创新元素的时候，你得去全世界各地搜寻那些还未被广泛使用的黑科技，它不一定完全在实验室里，也可能在其他行业中已偶有尝试，只是效率可能还不高，或者使用的方法不对。你把它们找出来，并试着与你的产品重组，看看能否产生新的化学反应，提升现有价值，实现量产。一旦适配，这通常都是颠覆性的创新\n这其中特别要留意 “提高能量使用效率” 和 “提高信息传播效率” 方面的技术突破，这两方面一旦出现了可商用的技术突破，随之带来的可能就会是全行业的整体革命。你如果能够率先应用，将能取得划时代级别的领先优势，比如蒸汽机、内燃机、电力等提高能量使用效率的发明推动了第一次和第二次工业革命；纸张、印刷术、电话、无线通信、互联网等提高信息传播效率方面的技术突破，带来了人与人协作效率的大幅提升，催生出了很多全新的行业。\n\n\n环境层\n\n创新元素不仅可以是已经成型的技术、模式或者产品，还可以到 “自然界” 和 “人生经历” 中去寻找。书里给了如下的例子\n\n比如，第一次世界大战期间，德国第一次大规模地使用了毒气战，导致英法联军伤亡惨重。为了避免悲剧再次发生，英法联军开始投入研发防毒面具。经过一番调查，他们发现战场上大多数的\n动物都因中毒而亡，唯独野猪没有死。原来，当毒气来袭时，野猪们会把鼻子拱进泥土里，松软的土壤既保证了呼吸又过滤了毒气，这才让它们幸免于难。科学家就根据这个原理，发明了世界上第一批防毒面具。\n\n\n时空层\n\n时空层，就是脱离当下的束缚，跨越时空，回到过去，走进文化，甚至到并不存在的虚拟故事中去提取创新元素，书里举了如下例子\n\n创新元素可以是故事和传说，它们本身就是创新的结果，是虚拟创造出来的东西，你可以直接将它们变换形式，做二次、三次的创新。\n比如，你可以把虚拟的形象产品化：蜘蛛人的衣服、海贼王的手办、忍者神龟的背包、流氓兔的抱枕、蓝精灵的茶杯......\n你还可以把这些虚拟的形象现实化，比如迪士尼乐园、丹麦童话之城蒂沃利公园、日本环球影城、环球冒险岛乐园......\n你还可以把这些不同故事里的虚拟形象集合在一起，比如聚在一起拍部电影，叫作《复仇者联盟》; 聚在一起制作个游戏，叫作《英雄联盟》......\n\n3. 重组并验证可行性\n当我们填完这 10 个模块内的信息，基本上就能在其中找到适合自己的创新元素并开始重组。如果还找不到，那就是收集的信息还不够多\n突变式创新\n重组式创新是向外寻找的创新方法，与优秀为伍；而突变式创新是向内探求的创新方法，在一次次自我 “破坏” 中，找到创新的方向。书里给的操作方法是拆解、修改和验证可行性，且主要在阐述修改部分 (对应着基因变异)\n1. 拆解：即要清楚自己产品是由哪些元素组合起来的\n2. 修改：书里给出了如下四种修改，这里值得注意的是现在是，并不是每一种突变都要去尝试，因为失败的成本太高，你需要在沙盘上先不断地推演，只有凑齐了全部的三个创新要素才能开始实践\n(1) 减去一个元素\n对应生物的基因在进行遗传复制时，有可能会丢失部分遗传信息；但这里最好减去的是比较重要的部分，因为如果是无关紧要的部分，那么变化将不会很大。书里举了如下的例子\n\n比如风扇的例子中，你可以试试把风扇叶给减掉！\n什么！没有风扇叶的风扇？还真异想天开啊......\n没关系，假设要大胆，先别急着说 “No”，而是试着说 “Yes... And...”\n好，现在新元素肯定是有了，那是否能给它补齐创新三要素里的另外两条边 ——“价值增量” 和 “可实现” 呢？\n先思考：如果没有风扇叶，会有哪些好处？\n- 小孩子会更安全；\n- 噪音应该也能小很多；\n- 外表酷炫到没有朋友，很有未来感！\n看来价值增量还不少，应该是很有市场的，现在只差 “实现” 这一步\n了......\n英国人詹姆士・戴森根据干手器的原理，于 2009 年 10 月 12 日真的就发明了一款没有风扇叶的风扇，也就是我们后来熟悉的戴森的品牌了\n\n(2) 复制一个元素\n对应生物的基因在进行遗传复制时，可能会把某个碱基复制多次而发生变异；书里举了如下例子\n\n智能手机的组成部分有屏幕、主板、电池、按键、摄像头、SIM 卡槽、外壳等等。\n我们从中挑选一个元素进行复制，比如原来是一个摄像头，现在复制成两个，背面的用来拍别人，正面的用来拍自己；复制成三个，背面两个，正面一个，背面两个摄像头可以拍出更高质量的照片，或者拍出\n3D 效果。\n你还可以把一个屏幕复制成两个屏幕，正面一个液晶屏用来玩游戏、刷微博，背面一个墨水屏用来看书。\n你还可以把 SIM 卡槽进行复制，双卡双待，三卡三待，天啊，现在竟然都已经有四卡四待了...... 可四卡有什么价值？自己和自己打麻将吗？\n不是，在中国可能用处不大，但如果放在非洲就不一样了，由于非洲有多个通信运营商，不同运营商的电话互打资费很高，所以一个人办理多个卡，用同卡通话来减低通信费，变成了当地的刚需，就像我们国内用中国移动和中国移动的卡通话有优惠套餐一样。有一家来自中国的著名手机公司 “传音” 就生产了很多款四卡四待的手机，在非洲成了爆款，销量远超 iPhone、三星等，是非洲人心目中的手机第一品牌......\n\n(3) 重新组合元素\n对应生物的基因在进行遗传复制时，可能会出现碱基顺序的变动而发生变异；因此可以尝试将产品原本的结构打乱，将功能或者硬件 “重新组合” 成另一种形态来实现创新，书里举了如下例子\n\n比如把产品的某个部件抽离出来，放到其他位置上：在 20 世纪 60 年代之前，对电视机进行操作都是在电视机主体上进行的，后来在 1950 年由美国的一家叫 Zenith 的电器公司，将控制的部分从电视机主体上分离了出来，就变成了如今你熟悉的遥控器，大大方便了我们的日常使用\n上面是把原本一体的事物分离开，也可以试着把原本分离的事物整合在一起；比如，每次上完洗手间你都要去洗手，而马桶每次冲完也需要在蓄水箱里加水，这两件事一直是关联发生的，因此是否可以把这两件事放在一个框架内去思考？\n日本的 TOTO 卫浴就将这两件事结合在了一起，他们在马桶的蓄水池上装了个水龙头，每次冲完水，水龙头就会自动出水，可用于洗手，洗完手的水会进入蓄水池以备二次使用，这样既方便，还能节约空间，节约水资源\n\n(4) 改变元素的特性\n即通过放大、缩小、逆转等操作，改变某个元素的特性来实现创新；书里举了如下例子\n\n可以放大某个元素的功能来实现创新。你环顾四周，就能发现许多产品都是通过这种方式来实现创新的，比如更高的运行速度，更大的显示屏幕，更快的上网带宽，更强的输出动力...... 放大的好处自不必多说，这类创新更重要的是考虑可实现性\n同样可以通过缩小某个元素，让产品获得一种新的特性。比如，将硬盘的尺寸缩小，于是有了 U 盘；将博客的字数缩短，\n于是有了微博；将视频的长度缩短，于是有了抖音......\n还可以逆转某个元素的状态或者功能等来实现创新。比如，原来电吹风是将风吹向物体，现在反过来，将空气往机器里吸，于是有了吸尘器；原来是人走楼梯，现在反过来，人不动楼梯动，于是有了自动扶梯；原来电动机是电产生磁场，磁场移动物体，现在反过来，让移动产生磁场，磁场再产生电，于是有了发电机\n\n最后就是验证可行性了，按照上面的想法创意也许会来得很快，但是先别急着去实践，得先反复确认它是否满足创新三要素，只有创新三要素的三条边完全闭合了才能动手实践，并在初期仅做小范围的测试，获得真实的积极反馈后，再开始大量生产。\n小结\n书中后半部分主要内容如下\n1. 解开大脑的封印\n\n负面词语和负面情绪是大脑的两道封印\n\n解开封印需要提升解决问题能力，建立正确信念系统\n\n学习知识不是目的，解决问题才是，需要里利用知识提升解决问题时的思考能力\n\n知识获取前需要经过三个过滤器，善于利用工具对知识进行存储，并让知识链接起来\n\n提升效率，从内因和外因入手\n\n2. 思维能力的提升\n\n问题是现状和预期的落差，需要精准描述\n\n分析问题的 “透析三棱镜”: 校准目标 (SMART\n原则)、重构方法 (治本)、消除变量 (象、数、理)\n\n线性思维：重视因果性；三段论的演绎法，归纳法 (穆勒五法) 与类比法，要刻意练习\n\n结构化思维：描述问题 / 方案要全面；金字塔结构 (自上而下、自下而上)、平面切割法\n\n系统性思维：动态地看事情，厘清要素之间的相互作用；增长上限、舍本逐末是两种常见结构\n\n3. 解决所有问题\n\n选择大于努力，因此得更多时间在选择上，但是小事不要纠结\n\n人是非理性的，要启动慢思考克服这个问题\n\n盯着问题而不是选择，将选择题变为简答题，找到第三选择\n\n把目标分为 easy、normal 和 hard：easy 注重 “执行 (”GTD 方法)，normal\n注重 “执行 + 计划”，hard 注重 “执行 + 计划 + 协作”\n\n构建自我演化能力，应对不确定的世界\n\n满足创新三要素才去执行，否则只是创意\n\n重组式创新要创造合适的创新环境，并通过五个层来收集创新元素\n\n突变式传创新要拆解和修改，4 种修改方式\n\n因为概念较多，所以这里也做了思维导图，连同书的前半部分如下图所示，原始的\nxmind\n文件也可以从 github 下载\n\n","categories":["读书"],"tags":["读书","拾人牙慧"]},{"title":"一起这种艺术","url":"/2025/03/12/%E4%B8%80%E8%B5%B7%E8%BF%99%E7%A7%8D%E8%89%BA%E6%9C%AF/","content":"文章的源起，是在深入了解听过 n\n次但选择性忽略了歌词里所诉说的故事的《远在咫尺》后，对最后那句 “一起这种艺术，若果只是漫长忍让，应感激终身的伴侣”\n颇有感触，把储藏在脑子里的很多零散的观点拽出来了，只是这些观点现在也是凌乱不堪，所以尝试通过这篇文章来\nconnect the dots\n本文尝试对 “爱情” 这一命题展开一些探讨，涵盖 “心动” 的源起、人来人往的 “投射与认同” 游戏、已经是标品的相亲流程、以及 “一起” 的这种艺术，内容依旧是随心所欲的发散，祝开卷有益～\n\n心动还是冲动\n当我们谈论 “心动”，脑海中刻板浮现的出来的往往是月色下的凝望、指尖不经意的触碰，或是灵魂共振的刹那。然而，心理学却揭示了一个看似 “煞风景” 却无比真实的真相：许多被我们奉为圭臬的 “心动” 瞬间，其根源可能仅仅是一场美丽的误会：“吊桥效应”（Suspension\nBridge Effect）\n想象你正身处一座高悬峡谷、随风摇晃的吊桥之上。脚下是深渊，耳边是呼啸的风声，心脏在胸腔里狂跳，手心渗出冷汗，呼吸也变得急促。此刻，生理上强烈的唤醒感（Arousal）—— 心跳加速、肾上腺素飙升、神经高度紧张 —— 正席卷你的全身。如果此刻桥的另一端走来一位陌生人，你望向 TA 的眼神，很可能被自己解读为 “一见钟情” 的火花。吊桥效应的核心在于：你将因环境危险而产生的强烈生理反应（恐惧、紧张、兴奋），错误地归因于了对眼前人的 “爱慕” 之情\n这并非臆测，而是根植于心理学的 “情绪双因素理论”（Two-Factor Theory of\nEmotion）\n\n情绪体验 = 生理唤醒 + 认知标签\n当个体经历不明原因的生理唤醒，如心跳加速、手心出汗、呼吸急促等，会本能地扫描环境，寻找一个合理的解释来 “贴标签”。如果此时环境中恰好存在一个潜在的 “情绪对象”，比如一位有魅力的异性，大脑便可能 “顺手” 将这阵生理风暴标记为 “心动”、“爱慕” 或 “性吸引”\n\n这解释了为什么很多人会在冒险、旅行、极限运动中更容易产生感情，也是很多 “恋爱教程” 中，提示在约会场景选择一些适度刺激的活动，如过山车、密室逃脱、夜游等，可能更容易激发彼此的好感背后的原理\n攀登险峰后的精疲力竭与成就感、异国他乡的陌生与兴奋、长途跋涉的相互扶持... 这些环境往往都伴随着显著的生理唤醒。当两人共享这些高唤醒时刻时，彼此的存在便成了解释这份激动最 “合理” 的标签，情愫由此滋生；这也是为什么共同经历挑战（如完成艰难的徒步、参与惊险的漂流）的同伴更容易擦出火花。而看恐怖片、坐过山车、玩密室逃脱、夜探 “鬼屋” 或在氛围独特的酒吧小酌，正是在人为制造可控的生理唤醒环境，利用吊桥效应增加对方将这种 “刺激感” 与你本人关联起来的概率\n吊桥效应告诉我们：人类即时的情感判断极易受到当下生理状态和环境的裹挟。在特定情境（刺激的冒险、浪漫的旅途、迷离的夜色）下产生的 “心动”，一旦脱离那个特定的 “场”，回归到日常的、平淡的语境中，那份被误读的激情可能如潮水般退去，露出底下真实的沙滩：可能是一片空白，也可能只是普通的友情或好感\n进一步来看，吊桥效应所揭示的误读机制，不过是庞大 “爱情化学反应” 交响乐中的一个音符。从冷冰冰的科学视角看，爱情或许只是一场索然无趣却无比精妙的生物化学反应。现代神经科学告诉我们，爱情本质上是一场由多巴胺（愉悦与渴望）、苯乙胺（兴奋与眩晕）、去甲肾上腺素（心跳加速）、催产素（依恋与信任）、血清素（情绪调节）\n等多种神经递质和激素精密配合、此起彼伏演奏的交响曲。当外界刺激（如吊桥的惊险、伴侣的出现）以特定的方式触发了大脑中相应的奖赏回路和情感中枢，这套复杂的生化程序便被激活，我们称之为 “坠入爱河”\n这不由得让人想起钱钟书在《围城》中借方鸿渐之口发出的冷峻调侃：“世间哪有恋爱？压根儿是生殖冲动。”\n彼时的方渐鸿正在读叔本华，而这句话的思想源头其实也可以追溯至叔本华原来的论断：“所有两情相悦的情愫，不管表现得多么缠绵悱恻，都根源于性欲本能。”\n吊桥效应在一定程度上为这种 “祛魅” 观点提供了现代心理学的注脚 —— 浪漫心动的火花，可能只是生理唤醒被错误地贴上了 “爱情” 的标签，而性吸引往往是这种误读最直接的出口\n人来人往终无果，落花流水有温存\n人来人往的 “投射游戏”\n心动是容易的。哪怕没有吊桥效应，一个惊艳的回眸、一条 “今天第 xxx\n个心动女生” 的弹幕，甚至一丝朦胧的好感，都可能点燃火花。让两人靠近并不难，难的是此后的相知、相处与相守\n情感初探期，双方往往会因为不熟悉带来的相当程度上的保留与节制，而小心翼翼地开放自己的私人领域，同时也好奇地探索对方。他们修饰着自己也美化着对方，同时因为相互接触与了解的程度不深，也可以较少地受到对方自身特质的干扰，迅速地把自己的早期理想的客体形象，投射到对方身上；而对方为维系这份甜蜜，也迅速戴上相应面具，扮演起恋人理想的角色\n这是亲密关系最甜蜜的阶段，因为此时双方还没有很深的联结，还在憧憬自己构想的理想亲密关系中，还不被真正在一起之后各种鸡毛蒜皮的琐事所困扰。此刻的恋人就像是两个小孩子，围着一个神秘的果酱罐，一点一点地尝它，看看里面有多少甜。然而，爱情并不只是甜的，当那个果酱罐被一点点地打开之后，当罐盖彻底揭开，它的神秘感与新鲜感都将消失，那时呈现在眼前的或许只是一只平凡无奇的小罐头，留给你的或许只有一地的鸡毛以及脏兮兮的双手，并不同于最初的想象，甚至，会让你失望\n随着交往的加深，双方被压抑的真实自我在呼唤得到释放，他们无法再去扮演一个理想的恋人，而更想去做一个真实的恋人。因为持续扮演理想角色，如同穿着不合脚的水晶鞋跳舞，心力交瘁。那场支撑甜蜜的 “投射 — 认同” 双人舞，注定无法长久\n但这也是亲密关系的真谛。人们之所以想组建亲密关系，之所以想爱与被爱，就是想获得一种亲密感：亲密感意味着需要把自我最深处的部分向他人也向自己呈现，卸除掉层层的伪装与防护，建立起真我与真我之间的连接感。在这个过程中，双方的信任感、依恋感开始建立，沟通的深度和广度也有所发展，开始牵涉到较深的情感卷入。这是心与心之间相知的契机，但同时也是一场充满风险的赌博\n每个人在 TA\n的成长多多少少都会经历一些伤痛，正因为这些伤痛的存在，所以人们发展出了保护层和种种防御机制，以守护脆弱的内核。在这场充满风险的赌博中，当恋人鼓起勇气跟对方坦露自己的经历、一层层地剥落掉自己充满防御机制的外壳时，也是在将自己的伤痛展示给对方看，这是痛苦的，必定会像剥洋葱皮一样，一边剥落一边掉泪\n如果对方的反应是积极的、接纳的，那么恋人即便落泪痛苦，也会因为感受到情感上的陪伴与支持，而坚定地一层层剥落掉那些保护壳，直到露出完整的内核。倘若双方都能如此坦然地接纳对方，那么爱情就会发展到稳定交往阶段，琴瑟和鸣，风雨同舟\n但很多时候，受限于自己的狭隘、缺陷，以及种种未处理完的情结，人们渴望亲密，又害怕亲密，宁愿相信坦露真实的自我只会招致他人的批判、拒绝和抛弃，也很难相信有人肯接纳自己内在的真实。于是，在关系发展到一定程度时，就会退缩、害怕、扮演角色，在操纵与受控之间游走，生出嫉妒与厌恶、逃离与背叛之心，将关系场演变成游乐场，上演出种种的闹剧与幻想\n写到这里，不禁想起陈奕迅的《人来人往》，这首唱尽了这城市里来来往往的爱情真相的歌曲，多少所谓的爱情，不过是贪恋那初遇时眉梢眼底的几分新奇。当那层薄薄的新鲜感如霓虹灯影般悄然褪色，所谓情愫便骤然失了支撑，只留下片片碎裂的虚空\n歌中所唱 “感激车站里，尚有月台能让我们满足到落泪”，我们在人生经历中会上一列又一列的车厢，但每次停留的月台都象征着相遇的可能性和一个暂时停靠的港湾。此刻的朦胧为 “投射” 提供了完美画布 —— 恋人们将内心 “理想客体” 的幻影，浓墨重彩地投射于对方；对方为抓住心动，也迅速认同角色，戴上面具共舞于幻影之中。月台灯光柔和，果酱罐光泽诱人，满足感充盈如幻梦，尚未沾染现实尘埃\n但 “爱若难以放进手里，何不将这双手放进心里” 却也早已预言了这游戏的脆弱。紧握虚妄的手，终将触到现实的棱角。当果酱罐见底，投射光环消散，被修饰的棱角显露，“真实自我” 的呼唤无法再被压抑。持续扮演，心力交瘁。这 “投射 — 认同” 游戏的能量本质是耗竭的，无法为真实关系提供永续动力\n“闭起双眼你最挂念谁，眼睛睁开身边竟是谁”，看似追问旧爱新欢的歌词，却精准刺穿了亲密关系初期那场盛大而脆弱的 “投射与认同” 游戏的核心 —— 我们闭眼挂念的，往往并非眼前真实之人，而是自身投射其上、精心雕琢的幻影；睁眼所见身边之人，与闭眼挂念的、由自我投射并经由对方扮演共同构筑的幻影，判若云泥。这巨大的落差，正是 “人来人往” 最直接的原因。许多人，在这幻灭的废墟前，选择了歌词中的结局：“拥不拥有也会记住谁，快不快乐留在身体里”，然后转身，再次投入人潮，寻找下一个可供投射的 “月台” 和愿意扮演新角色的 “演员”，重复那短暂而注定幻灭的投射游戏。这循环，正是\n人来人往最本质的写照：投射、扮演、幻灭、离场、再投射……\n落花深处的 “温柔共振”\n人来人往的欲望都市里，发生着无数始终停留在 “投射与认同” 的浅层游戏、从未真正启程前往那个需要勇气与坦诚才能抵达的 “真我相遇” 之地的故事。月台永远拥挤，果酱罐永远吸引着新的 “演员” 和 “导演”。许多人或许终其一生，都在不同的月台张望，在不同的罐子前浅尝辄止，或许 “记住” 了许多人，留下了许多 “快不快乐” 的瞬间碎片，却唯独缺席了那唯一值得风雨同舟的旅程 —— 两个真实灵魂的相遇与共生\n然而，在这看似宿命般的 “人来人往” 循环与 “投射游戏” 的幻灭中，却也存在着歌曲末尾那 “最美丽长发未留在我手，我也开心饮过酒” 的洒脱。即使最终的结局看起来不是世俗意义上的美好，但至少也经历过，开心过，而不是像顾城的《避免》里那种习得性无助般的无力感\n\n你不愿意种花\n你说：\n“我不愿看见它\n一点点凋落”\n是的\n为了避免结束\n你避免了一切开始\n\n避免了一切的开始，也避免了一切的体验，如同《人间失格》文字透出来的遗憾一般：“若能避开猛烈的狂喜，自然不会有悲痛来袭。我试着去绕开所有的悲痛，但同时也是失去了所有的欢喜”\n陈奕迅的另一首歌《落花流水》，仿佛就是《人来人往》的续集一样，为我们提供了另一种温柔而深刻的视角：过程也许比较比结果更重要。纵然两人的经历就如同落花和流水一样：最后 “水点蒸发变做白云\n花瓣飘落下游生根”，纵然两人最终也只是 “淡淡交会过，各不留下印”，但是 “这趟旅行若算开心，亦是无负这一生”，但是双方也 “经历过最温柔共振”，也许这也就足够了\n这 “最温柔的共振”，恰恰可能发生在 “投射游戏” 的幕布偶尔被掀开一角的瞬间 —— 当双方在扮演的间隙，或因疲惫卸下心防，或因某个不经意的触动，短暂地、意外地瞥见了对方那未被修饰、未被投射覆盖的、真实的脆弱或光芒。即使这真实如惊鸿一瞥，即使它无法支撑两人走向长久的共生，但它确确实实发生过。这份 “共振”，不再是基于幻影的投射与认同，而是两个真实存在的灵魂，在生命长河的某一段交汇处，产生了片刻的、深刻的、无需扮演的共鸣与理解\n这种共振，或许是在一次深夜的倾谈中，当其中一人鼓起勇气袒露了某个深藏的伤痛，对方并非出于扮演 “理想恋人” 的责任感，而是发自本能地流露出心疼与理解，那一刻的静默与凝视，便是 “最温柔共振”；又或许是在共同面对一个突如其来的困境时，双方暂时抛开了精心维护的角色，以最本能的、甚至有些笨拙的方式相互扶持，那一刻的默契与依靠，便是 “最温柔共振”；又或许，仅仅是在某个平凡无奇的午后，两人都放下了 “应该怎样” 的期待，疲惫地、真实地 “存在” 在对方面前，没有修饰，没有投射，却意外地感到一种奇异的、平静的接纳感，那一刻的松弛与自在，亦是 “最温柔共振”\n即使一段关系最终未能逃脱 “人来人往” 的循环，即使那初期的投射游戏终告幻灭，只要其中曾有过哪怕一瞬这样的 “最温柔共振”—— 那份剥落了伪装、超越了角色扮演、触及了彼此真实内核的深刻触碰。这段旅程便不再是徒劳或虚无的，它已然成为生命长河中一枚独特的印记，一次珍贵的体验。在层层投射与防御之下，真实自我的触碰与共鸣是可能的，哪怕它短暂如烟火。这份体验本身，或许也足以对抗 “人来人往” 的虚无感，它提醒着我们，爱情的珍贵，不仅在于最终是否 “拥有”，更在于那 “交会时互放的光亮” 中，是否曾照亮过彼此灵魂深处真实的角落\n所以，在 “人来人往” 的月台上，在投射游戏不断上演又落幕的剧场里，或许我们不必执着于强行留住注定风化的 “前身”（投射的幻影），而是学着珍惜那如流水般清澈、如落花般偶然却美丽的 “交会”，珍视那曾发生过的、短暂却真实的 “最温柔共振”\n相亲、婚姻与彩礼\n虽然人来人往的投射与认同游戏略有遗憾，但是也经历过最温柔的共振。但有过一段或弥足珍贵、或意难平，甚至是狗血荒诞的体验，在没对人造成实质性不可逆伤害的时候，不也是一段难忘的体验么。而人生，不就是由这样的一段段体验构筑而成的么\n现实是并非所有人都有机会或勇气踏上这场 “投射与认同” 的勇敢者游戏，哪怕它短暂且充满幻灭的风险。当青春的潮水退去，而适婚年龄的礁石显露，更庞大的人群被推搡着涌向另一个截然不同的场域：相亲市场。《欲望都市 1：爱情并非必须，但人类还是很好玩的》里，猫草老师就针对相亲这个问题给了一个 “暴论”：相亲是一场有标准化流程的简历互筛，浪漫爱并非题中应有之义\n在相亲市场中，不再是朦胧月台与神秘果酱罐的浪漫想象，而是明码标价、寻求最大公约数的现实交易所：身高、学历、户口、收入，这些硬指标被置于聚光灯下，成为衡量 “匹配度” 的核心砝码。因为往往对于走到相亲这一步的恋人，最有诚意的做法，是把大家的都关注的最大公约数（身高，学历，户口，收入）拿出来相互对比和匹配，这是一种非常标准化的流程，而浪漫爱并非这种标准化流程中的应有之义，更像是锦上添花的、难以量化的附加项\n在这里，“闭起双眼你最挂念谁” 的追问是显得那么的不合时宜，“眼睛睁开身边竟是谁” 的答案，往往直接指向一份经过精密计算的、条件适配的清单。校园恋情的非标性可能（哪怕基于投射），在此被高效、务实的 “标准化匹配” 所取代，校园关系里的双方是有可能因为爱情在一块的，但相亲不是\n步入婚姻的门槛，则让这份现实感更为沉重，甚至处处透露出一种时代的 “拧巴”。从纯粹的经济理性看，对当代男女而言，婚姻的必要性正急剧下降，双方都有养活自己的能力，在经济上，其实不太需要婚姻来获得一张饭票的机会，甚至结婚在某种程度上会拉低双方生活水平：住房、育儿、责任捆绑，每一项都可能成为压垮骆驼的稻草。\n然而，许多人依然觉得自己 “需要” 婚姻。这 “需要” 的源头复杂而矛盾：或许是社会时钟滴答作响的规训，将 “成家” 刻入人生里程碑的焦虑；或许是深夜归家时一盏灯的温暖想象，驱散原子化社会冰冷的孤独；更深层地，是存在于人本身无法消解的那种孤独感\n是的，人是孤独的。人与外物、社会乃至世界本质的分离，制造出一种令人窒息的 “监禁感”，迫使个体拼命寻求与他人深度的、制度化的连接（如婚姻）。但讽刺的是，这种源于孤独的连接渴望，却常常将爱情的重心异化为 “体验被爱”（寻求安全港、避风港），而非更主动、更具风险也更滋养灵魂的 “体验爱”（付出、理解、共振）。于是，婚姻的期待与现实的落差，不断酿制出失望与矛盾\n这种拧巴的破解之道，或许在于我们还是要清醒地意识到自己在玩一个什么样的游戏，需要付出什么样的代价。选择 “标准化匹配”（相亲）路径的人，需要想清楚婚姻这份契约 “暗中标好的价格”，这个价格，也许是周末再也没有自己时间的 “不自由”，也许是责任驱动下为自己背上各种压力，也许是无数次无可奈何的妥协、以及可能的利益捆绑或牺牲，我们都需要问清楚自己，是否真心愿意做这场 “等价” 交换？而对于内心仍有悸动、渴望更深刻连接的人，则需要穿透社会给予我们的噪声和规训，想清楚自己真正渴求的体验是什么，而非社会时钟或他人眼光所定义的 “需要”\n而当恋人双方终于鼓足勇气步入这场名为 “婚姻” 的游戏时，横亘在前的往往是那张沉重的 “入门券”：彩礼。网络上的喧嚣与对立，常将其简化为 “态度” 的象征。然而，更残酷的现实是，真正需要 “天价” 彩礼的家庭，往往并非索要虚无的诚意，而是实实在在地需要这笔钱来改善困顿的生活（如为家中兄弟支付婚房首付、偿还债务等）。吊诡的是，能匹配这类需求的结婚对象，自身的经济能力往往难以负担这天价数字。世界的割裂在此赤裸呈现：一边是生存的刚性需求，一边是难以企及的门槛。网络空间激烈的性别对立与虚空对骂，正是这种深层次社会结构矛盾与经济断层在婚恋领域的扭曲映射与情绪宣泄\n最终，当我们审视这个时代，“爱” 与 “婚姻”，无论是基于投射幻影的勇敢游戏，还是基于现实考量的理性匹配，亦或是跨越经济鸿沟的制度结合，对绝大多数疲于奔命的年轻人而言，都显露着 “奢侈” 这两个字。在保守、收缩、低欲望的社会氛围下，在充满不确定性的经济环境中，生存与发展的压力已倾轧了太多空间。追求精神契合的 “爱”，需要充沛的精力、无畏的勇气和一定的经济底气；而承担 “婚姻” 的责任与成本，更是需要坚实的物质基础与强大的心理韧性。当点亮生活地图（工作、学习、看世界）已耗费了大部分心力与资源，当 “救公主”（建立深度亲密关系、步入婚姻）的成本高昂到令人却步，许多人只能无奈地选择延迟、观望，甚至放弃\n一起这种艺术\n相爱是容易的，甚至步入婚姻的殿堂也不难，难的是在一起后面对生活的风刀霜剑严相逼。因为相爱或许源于刹那的激情与投射的幻梦，步入婚姻殿堂也可能基于现实的匹配或冲动的勇气，但难的，是在日复一日的柴米油盐、在生活给予的责任和压力之下，如何维系那份连接，如何让 “在一起” 不至于沦为一种麻木的忍受或遥远的观望\n红玫瑰与白玫瑰\n《远在咫尺》以一个婚后男人的内心独白，跟我们重新诉说了一遍红玫瑰与白玫瑰的故事\n当婚姻的蜜月期过去，当双方的新鲜感消失，当最初的吃喝玩乐变成了鸡毛蒜皮的日常小事，当浪漫的想象撞上现实的礁石，“一起” 的考验才真正开始。而这也正应对着前面提到的果酱罐被挖空见底、“投射与认同游戏” 无法再继续持续下去的时刻\n\n很短暂狂热留下得一杯冷水\n认定是可歌可泣的一双一对\n长时期吃喝玩乐新鲜感减退\n\n无论是恋爱还是婚姻，红玫瑰与白玫瑰、蚊子血与朱砂痣、白月光与饭黏子，都是一个永恒的困境，因为人这种动物特别擅长美化自己所不曾选择的那条道路。用张爱玲在《红玫瑰与白玫瑰》那句脍炙人口的话是这么说的\n\n也许每一个男子都有过这样的两个女人，至少两个。娶了红玫瑰，久而久之，红的变了墙上的一抹蚊子血，白的还是 “床前明月光”；娶了白玫瑰，白的便是衣服上沾的一粒饭黏子，红的却是心口上一颗朱砂痣\n\n即使已与一人缔结婚约，心中仍可能为 “未得到” 或 “已失去” 的她预留空间，每个对婚姻失望的夜深人静的夜晚，也许都会在想 “贪恋她抛弃你，是否当初想法不对”，但也会转念想无论最后选择谁，可能都会有同样的问题。歌词道尽了人性的不知足与对 “未选择之路” 的永恒幻想，如果再来一次换成那条没走过的道路，如果跟那个曾经没能在一起的 “白月光” 或 “朱砂痣” 共同一起生活，是否还是 “同样会记挂她，身于咫尺心于千里”，这 “身于咫尺，心于千里” 的疏离感，是许多婚姻中无声的暗流：物理空间的 “在一起”，远不等于心灵的同在\n\n还是最尾我选择谁，同样背上这焦虑\n同谐白首会是谁，这决定一边狠心一边又后悔\n假使你是情侣 假使你共同一起生活里\n同样会记挂她身于咫尺心于千里不可抑压像潮水\n得不到多么好\n当得到不知怎算好\n奢侈的一声天荒地老\n如何能抵挡当中诱惑 谁更好\n\n“最爱” 的人在哪里？人终其一生，情感需求复杂多变，“最爱” 可能并非唯一。在不断的 “闹情绪” 与 “依偎” 的循环中，在 “数伴侣像流水” 的迷惘里，个体对自身身份和情感归属的游离感暴露无遗。活了一生都不知道 “最爱” 是谁，或许才是许多人的真实写照\n\n一起会闹情绪 分开了为何依偎生命里\n谁是我最爱的应该一个 应该三个 应该数到像流水\n数数到尽头终此一生之旅 尚有几多新伴侣\n\n《远在咫尺》某种程度上，展示了 “投射游戏” 彻底落幕、新鲜感完全消散后，婚姻生活可能陷入的僵局、疏离与精神游离。它无情地撕开了 “从此幸福快乐” 的童话面纱，暴露出 “一起” 的复杂、艰难与人性固有的矛盾\n那么，“一起” 是否注定走向这种无奈的疏离或永恒的遗憾？弗洛姆在《爱的艺术》中给出了否定的答案，并点明了症结与出路：我们错误地理解了 “爱”\n爱是一门艺术\n归根结底，相爱容易，而相守不易，也是因为我们错误地理解了 “爱”\n弗洛姆认为，爱不仅是一种情感，更是一种能力，一门需要学习和实践的艺术。\n它远非始于生理唤醒或 “投射 - 认同” 游戏后就能自然维持的状态。激情如同种子，而相依相伴的 “一起”，则需要在生活的土壤中，运用 “爱的艺术” 去精心培育\n这门艺术的核心要素是：\n\n关心\n(Care)：主动关注对方的需求与福祉，付出行动。这超越了最初的好奇与探索，是持续的关注与投入\n\n尊重\n(Respect)：正视并接纳对方的独特性与真实存在，包括对方的缺点、局限和与你不同的部分（尊重意味着 “不利用”，让对方按其本性成长，而非强求其符合你的投射）\n\n责任\n(Responsibility)：不仅仅是生活上义务的负担，也是对方精神成长、情感需求的一种积极回应和承担，这要求成熟与担当\n\n认识\n(Knowledge)：持续、深入地理解对方，认清楚另一半核心的渴望、恐惧、伤痛与潜力。这是尊重与关心的基础\n\n“一起” 的真谛，在于两个独立个体之间进行一场 “相互的驯化”，这不是控制或占有，而是在保持各自完整性的前提下，寻求 “不完整的融合”，是在亲密与距离之间寻找那个微妙的、动态的平衡点。它需要极大的耐心、时间，以及在漫长岁月中共同经历风雨、共同创造意义的能力。\n这绝非 “漫长忍让” 的消极消耗，而是主动运用爱的艺术去创造、去维系、去深化连接的过程\n做到这一切确实艰难。爱有其生命，从萌芽、生长到最终凋零，是自然规律。但弗洛姆的精髓在于：即便结局注定，“用心呵护爱情生命” 的过程本身，就是抵御虚无、创造永恒价值的方式，因为一切终将黯淡，你的名声、财富、地位、甚至生命，所有一切终将黯淡，但是唯有被爱的目光镀过金的日子依然在岁月的深谷里闪耀着永恒的光芒，这份光芒，是爱的艺术赠予用心者的礼物\n因此，当《远在咫尺》唱出\n“应感激终身的伴侣” 时，其深意并非仅仅感激对方的 “忍让”，而是感激在看清了人性的局限、经历了生活的磨损、抵御了 “红白玫瑰” 的诱惑之后，对方依然选择与你共同实践这门 “一起” 的艺术，共同守护那份在岁月深谷中依然闪耀的、被爱的目光镀过金的时光\n; 这份感激，指向的是对爱的艺术之艰难的认知，以及对共同跋涉的珍视\n\n一起这种艺术，若果只是漫长忍让，应感激终身的伴侣\n\n我们听了太说，也说过太多 “不相信爱情” 的话，但那往往不是对爱的否定，而是对自己是否有勇气、有耐心、有能力去学习和实践这门最艰难也最珍贵的 “一起” 的艺术（爱的艺术）的怀疑与退缩。\n而破解《远在咫尺》中困境的钥匙，或许就藏在弗洛姆的箴言里：爱，不是被动感受，而是主动创造的艺术\n在深谷等一场镀金\n从 “心动” 瞬间的生理迷障，到《人来人往》的投射幻影，再到《落花流水》中那惊鸿一瞥的真实共振；从相亲市场的精密匹配与婚姻现实的沉重门槛，到彩礼映射的社会断层与时代拧巴；直至《远在咫尺》里那 “身于咫尺，心于千里” 的疏离图景，我们遍历了 “爱” 的迷思与困境。而破解这一切的答案，或许指向了那个标题本身，也是《远在咫尺》最后的顿悟：一起这种艺术\n“一起” 这种艺术的核心，是 “主动创造” 而非 “被动感受”。\n它要求我们放下对 “完美伴侣” 的虚妄投射，放下对 “最爱” 的唯一执念，如同放下对一件完美艺术品的幻想，转而专注于眼前这块独一无二的 “璞石”，那个真实的、不完美的伴侣。共同雕琢、打磨，在亲密与距离的动态平衡中，在冲突与和解的交织里，创造出一种只属于彼此的、独特的 “不完整的融合”。这份融合可能布满修补的痕迹，却因其真实与共创，而拥有了抵御岁月侵蚀与人性弱点的力量\n“应感激终身的伴侣”，其深意并非感激对方的 “忍让”，而是感激那个愿意与你并肩站在生活的画布前，共同拿起 “爱的艺术” 工具，笨拙却坚韧地学习、实践、创造的同行者。\n感激 TA 与你一起，在看清了幻影易散、人性易移之后，依然选择在 “流水深处” 捕捉并珍存那些真实的回响，并努力将其谱写成属于两人的、虽不完美却独一无二的生命乐章，一段在岁月的深谷中被爱的目光镀过金的时光\n一起这种艺术，终究是两个人的作品。它的完成既需要个人的觉悟与能力，更需要命运赐予一个同频共振的伙伴。这样的人，或许比 “理想伴侣” 的幻影更稀有，比 “最大公约数” 的匹配更难寻。当流水渴望与落花共谱一曲，但水点注定蒸发、落花注定飘落下游，或当刻刀已备，却无人执手共雕 —— 这或许，才是爱情这门艺术背后，最深的遗憾与最辽阔的留白。如同在《一加一》中\nEason 最后的独白\n\n其实…… 有冇呢个人……\n根本有冇呢啲嘢嘅呢？\n\n\n本文一些参考材料\n\n《为什么相爱容易，相守不易？》\n\n《欲望都市 1：爱情并非必须，但人类还是很好玩的》\n\n《我值得被爱吗？聊聊低自尊与亲密关系》\n\n《日本系列 4：老龄化启示录 —— 收支、产业与生死美学观》\n\n陈奕迅的人来人往到底讲了个什么故事\n\n陈奕迅的落花流水讲了个什么故事\n\n《爱的艺术》\n\n《我（们）的孤老生活》\n\n深度解析《远在咫尺》\n\n大话西游之仙履奇缘\n\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"业务、组织与心态","url":"/2023/12/03/%E4%B8%9A%E5%8A%A1%E3%80%81%E7%BB%84%E7%BB%87%E4%B8%8E%E5%BF%83%E6%80%81/","content":"最近半年经历了一些业务与组织上的变化，对于这部分也有了一些新的理解和体会，值得写一篇文章来梳理与总结。本文主要讲了对业务和组织的一些看法，包括如何看待 “矛盾” 的业务定位和观点、组织的进化过程、团队组建里的识人与用人等；以及在这个过程中，该如何调整自己的心态。文章比较发散，纯属个人碎碎念，祝开卷有益\n\n业务\n从接手的业务说起\n\n业务就是你职业生涯的作品\n\n近几个月接手了一个业务，一个有点特殊的业务，特殊的地方在于这是一个被搁置了几个月没人迭代的业务；同时也是一个充满了负面与悲观情绪的业务\n这种悲观和负面情绪令我始料未及，无论是交接过程的粗糙、交接人员的态度或是之前合作团队之间的矛盾，都是我所未曾设想的：缺少有效信息的交接文档，一问三不知的回答、交互过程中展示出来的各种负面情绪，以及产研团队的历史矛盾，似乎都在暗示着这个业务过往推进时的不顺，以及未来推进的艰难；同时也能解释为何业务的迭代停滞了几个月\n说点题外话，人走茶凉的道理虽然也懂，也经历过不少次交接，但像这种对自己所做过的项目的态度消极到有点厌恶以至于不愿提起，的确是我第一次看到；笔者一直觉得，一个人做过的项目就是他的作品，跟他写的文字、录的播客都是一样；项目是一个人职业生涯里的一部分，每一部分都是一段弥足珍贵的经历，都能在一定程度上反映你当时的能力、状态与心境，套用之前看到的一段话是这么描述的\n\n作品即人。它不是人的一部分，而是人的一个阶段，是这个阶段的全部的那个人。你虚作品就虚，你燥作品就糙，你脆弱作品就生硬，你高傲作品就小气，你浅薄作品就邪恶。你懒，你就没作品。\n\n笔者往往被周围的同事认为对工作过于负责，究其原因，可能是面临着交付到笔者身上的每一个业务时，笔者都是当做是自己职业生涯里的一个作品去构思和实践的，这个业务的定位与价值，服务于什么目标，如何能够最大发挥这个业务的价值等等，都是笔者所乐于去思考和实践的。\n\n业务的价值在哪里\n\n回到这个业务，在后续了解相关业务的历史背景后，发现其实这个业务也并没有想象中的那么差劲，本身业务的价值还是存在的，且定位也是不可或缺的一部分（保密相关，这里就不详细展开业务的具体形态和详细信息）；只是因为做功的方向不是非常对，或者说是产品的定位不是非常明确和科学，导致了最终的业务产出和现状不是非常理想\n这里也想强调一点，业务能否成功以及业务价值需要分开来看；因为所谓的业务能成功往往需要依靠 “天时地利人和”，“天时地利” 听起来是比较虚的概念，简而言之，就是在当时的那个时机下，推动这个业务是否能刚好满足市场的诉求，适配公司的发展阶段，但 “天时地利” 往往是事后归因才能给出一个所谓正确的判断，更多的时候，我们都是一个随机漫步的傻瓜；相较于 “天时地利”，“人和” 是更具备主观能动性的部分，或者说是我们能影响的最大部分：你是否充分认识到业务的价值？是否对业务有足够的信心？是否能够清楚认识到自己的定位，以及如何与其他团队配合能否更好得完成业务目标等等\n那该如何判断业务是否有价值呢？这是一个非常大的问题，且往往不同行业，不同的业务形态，判断的标准也不一样；一般判断的业务的价值，都是根据业务已有的规模和市场的认可度来判断，但是这适用于业务已经进行了充分的探索的场景；但更多的情况下，业务往往没有被充分探索。这个时候，笔者倾向于从所谓的第一性原理出发，可以尝试问自己几个问题：\n\n业务的模式是什么？为谁提供了什么样的服务赚到了钱？需要完成什么样的目标才能赚到这笔钱？等等\n\n业务在当前组织里的定位是什么？包括对外的宣传口径是什么？对应的上下游是哪些？是否有类似的业务，两者的关系如何？公司是否能够把这个业务去掉而不影响其发展？等等\n\n当前的做法是否已经能够最大化这个业务应有的价值？包括业务目标是否合理？业务的动作是否已经到位？等等\n\n....\n\n“矛盾” 的观点与定位\n当我们去去思考以上提出的种种问题，去向身边的人做出提问时，会收到很多 “矛盾” 的观点，也会从不同的说法之中产觉到看似矛盾的产品定位，这种现象，刨除掉少数一些胡说八道的 case，更多的情况下是大家观测的视角不一样，或者说利益不一致\n举个例子，在商业化场景下，类似星图这种在内容社区中的 “商家 - 达人” 撮合平台，横跨在内容和商业化中间，一方面即承载着引导达人生产更好的素材、协助中小达人成长的角色，另一方面则是肩负着变现的角色（包括撮合的手续费以及撮合后的二次投广的费用）；这个时候，如果从内容社区的角色出发，会倾向于让这个产品的的定位更多是站在用户和达人角度，让达人生产的内容的用户价值最大化，而如果从商业化视角出发，会倾向于让产品定位更站在商家的角度，让生产的内容更好满足平台的商业价值\n这两种定位，就是两种看似 “矛盾” 的观点，如果用更加技术的视角，可以理解为一个业务往往有多个目标，如果按最优化来建模时，max\n的目标是其中一个，而其他目标只能是约束了；比如说上面的情况，第一种 max\n的用户价值，商业价值是约束，而第二种则刚好相反。实际中无法说明孰对孰错，更核心的点是哪一种更适合当前的公司总体发展阶段\n究其本质，还是我们面临的业务或世界过于复杂了，没法用一个简单的概括来描述所有的情况：人类很容易陷入一种\n01\n对立的二元思维，把世界分成相对的两端，这种二元的判断和分类似乎简化了世界的复杂性，给我们了一种表面的安全感和确定感，但不可避免的，它也一定程度上忽略了世界的复杂性和多面性，也导致了工作或生活中的不少矛盾\n除了聊到的业务，社交平台上的人生智慧或者建议，有很多都是自相矛盾的，从传播学上讲，简短是为了表意更加精确，同时传播上更方便，但是在一个人了解到的知识面有限的时候，很容易导致人走向一个极端\n那面对这些 “矛盾” 的观点，我们该怎么做？在今年上半年，自己对这一点有隐约的体会，也写下了下面的文字\n\n后面听了孟岩的无人知晓的 《中》的这一期，则是有了更深的理解和感悟\n孟岩的播客中提到的一个很关键点是：不要把这些建议当作圣经去朝圣，而是要意识到每条建议的适用性与局限性\nKeven Kelly\n也有类似的观点：很多类似的建议都很简短，一方面这会让表意更精确，另外一方面也更容易传播。但这些建议并不是词典，不是参考书，不是当你遇到事情时候来查阅的。相反，你可以换个方式去听一听，你可以去看一看，你可以去理解，你可以去感受这些建议，然后他们会自然而然地进入到你的潜意识里。当你在日常生活中遇到需要决策的场景时，你的潜意识就会给出答案\n因此，无论是看书、人物传记、充满金句的小册子，还是听播客，我们都不应该把这些东西当作方法论，当作工具书，更不应该把它们当作验证自己行为正当性的依据。相反，我们能做的是通过这些文字和声音更多的了解这个世界，了解我们自己。我们能做的是允许自己超越二元对立的维度，让我们能从更宽广、更高层次的维度来理解万事万物\n很多时候，当我们有强烈的倾向用一个金句或者建议来解释自己的行为时，也许我们就偏了；相反，我们要同时追求两件互相矛盾的事，因为当我们能够同时站在两个矛盾的观点上去看问题时，能够站到更高的维度去看待这个事情，理解这两句看似对立的话讲述的是世界的哪两个面相，理解他们有哪些各自的约束条件和应用场景，理解他们究竟在告诉我些什么。当我们真的理解了这些，我们的潜意识就可以帮助我在需要的时候去做出接近真实的决定\n查理芒格说过，如果我不能比持有相反观点的人更好地理解他们的观点，我就不配拥有自己对该问题的看法；菲斯杰拉德也说过，具有一流智慧的人能够在心里同时容纳两个截然相反的观点，而且仍然能够正常思考和行动\n回到业务里也一样，当我们听到跟自己相反的观点的时候，第一时间不是回击（虽然很多时候屁股决定脑袋），而是要想一下其他人说的这个观点的立场，只有我们能充分理解提出这个观点的人的立场和出发点时，才能做出更科学的业务判断，往往也能有更加愉快的合作体验\nego\n还是尽量要小点，多了解一些其他的观点和现象，因为绝大多数情况下，我们只经历了这个世界的\n0.00000001%，却希望用它去解释这个世界的 80%\n那些 AB 无法度量的事情\n过去的我比较信任 AB 实验，而 AB\n实验也在很多互联网公司里的快速发展中也证明了其价值：消除了很多仅靠主观臆想导致的观点的局限性，在观点不一致情况下提供了决策的重要依据，也为技术迭代指明了道路和方向；最为典型的就是字节跳动这家公司\n虽然 AB 实验也存在一些局限，如分组的 variance\n问题、实验设置导致结果不可看（如组内抢量问题）、小流量与大流量效果不一致、AB\n无法反映 AA\n等问题，但相应的也有相关的技术手段来规避或缓解这些问题，如分组后实验前组内的指标是平的、通过预算分桶等手段保证实验结果的可观测性、大流量反转来观测效果、长期观测\nAA 变化等\n但是最近一段时间越发觉得，有很多事情是无法靠 AB\n直接度量的，以产品满意度为例，虽然能拆解很多 AB 中间指标（如点击率、APP\n内的停留时间、负反馈率等），然后通过技术的迭代提升这些中间指标，期待最终长时间\nAA\n观测到总体产品满意度的提升。如果类比为机器学习中的优化，也可以理解为是做了很多的局部优化，期待最终能在全局中往更优的地方前进了一步，但是我们知道，并不是所有的这种局部优化都能带来全局更优的；这么拆解需要保证中间指标与最终指标的是有较强的正相关性的\nAB\n更难以度量的地方，在于一些更抽象和上层的决策，比如说产品的定位与发展方向，面对产品可能的多种定位，只能选择其中的一个选项，这个时候当然可以用很多数据来辅助这个判断和决策，但最终决策的正确性更多考的是对业务的理解、历史经验积累下来的直觉；并且在选择后也没法证实这个选择的对错，就有点类似\nuplift 中的 “what if” 问题\n说到底，AB\n实验只是一个工具，一个在很多情况下都是在方向明确后，迭代优化空间中的不断做出局部调优的一个工具，这个方法的确增强了产品和业务的迭代效率，但同时也要意识到这个方法的局限性，尽信书不如无书\n组织\n组织服务于业务\n组织与业务是什么关系？笔者认为组织是为业务目标负责的，需要随着业务目标变化而变化\n这也是我们能看到很多公司的组织架构日常频繁调整的原因，原因就是业务目标变了，需要相应的支撑的组织有相应变化\n大公司效率低最根本的原因是什么？应该如何应对？，展示了业务从小变大过程中，组织相应的一个变化和改变，从中我们也能理解为什么何谓大公司病，从何而来\n\n阶段 1：以老板为中心一个簇\n\n当公司比较小时，业务刚起步时，往往所有事情都是老板一个人做。随着业务的扩张，老板会招一些 (工具) 人进来，此时的公司是一个以老板为中心的扁平化组织，每个人都可以直接和老板沟通，老板对大大小小事物亲自拍板，指哪打哪，彼此之间只存在分工，不存在合作，也就不存在沟通问题。\n\n阶段 2：独立的部门\n\n随着公司业务进一步发展，老板必然无法处理所有业务的琐事；此时老板的想法是招几个中层管理人员，小事让中层过滤一下，以后只要大事才找我；于是销售主管、人事行政主管、产品开发主管、财务主管统统招起来，就形成了各种部门\n小事是不能惊动老板的，部门要自己解决，有些事情虽小但也要牵涉其他部门的，这样就形成了部门之间的合作，也昌产生了沟通的问题\n\n阶段 3：中台的成立与拆分\n\n公司业务进一步发展，每个部门都有了自己的相关基建和制度，这个时候就会面临一个问题：相似功能集中管理还是拆分到各个业务部门，或者说，是否要成立中台\n如果平均分到每个独立的部门，也就像是成立单独的小王国一样，麻雀虽小五脏俱全，好处是一个部门独享资源，沟通成本低，干什么事情都方便，可以按照自己的需要随时调整资源的配置；但是带来的问题一是造成资源的重复建设产生浪费，二是导致各个部门标准越来越不统一\n如果把这些共享职能拆分出来，统一管理也成立的单独的中台部门，好处虽然是节省了资源，集中管理也便于整个公司统一规范，但坏处是极大的增加了沟通成本。因为这是一个不隶属于所有部门的共享工具人，第一，它没办法分清楚所有部门业务的轻重缓急，因为往往中台部门对业务的理解有限，基本只能按照队列的方式处理到来的\nrequest（虽然所有业务方都会说自己的需求优先级很高）。第二，工具人也有自己的立场和私心，也想要摸鱼，对于麻烦的事情能推就推，你不知道它到底是真做不到还是假做不到，往往它随便找一个冠冕堂皇的理由就可以拒绝你。\n对于中台部门上面这种弊端，也不是真的没有办法，遇到真正遇到紧急或者重要的事情，可以调动上层资源给它施压，但一旦优先了你的项目，势必要把别人的项目延后，工具人也拿不定主意，这时候就要把相关部门都召集起来开会，各部门又有自己的立场和私心，看谁吵得过谁了。\n业务进一步发展，业务部门持续增加，中台需要对接的业务部门也会逐步增加，容易出现业务部门被响应的需求不均匀情况，比如说某些业务压根插不上队的，这个时中台部门会扩招，然后分为若干个小组，每个小组专门对口特定的业务部门，这样每个业务部门的事务都是平行处理，就不用排队了。\n但还是无法从根本上解决问题，因为部门的 boss 只有一个，所有的事情最后都要经过他批准，这些批准汇总到一起还是要排队，服务的部门越多，他一个人也签不过来，又只能拖长周期了。很多部门抱怨的是，HR 招个人怎么要这么久？而且完全不了解我的需求。那这个时候该怎么办？\n\n阶段 4：双重领导架构\n\n我国从中央到地方的各级政府就是双重架构。如市公安局同时归当地市政府和省公安厅管，市公安局的人事、财政由市政府管理，但业务上是服从省公安厅的管理。市公安局一把手的任命需要征得省公安厅的同意。这种架构的好处是，即保证地方机构运转的灵活性，又利于中央对地方统一指挥和协调，遇到全国性的紧急事务可以快速实现全国总动员。而西方联邦制国家，每个城市和每个州政府都是完全独立于中央，各自为政，只是把外交和军事委托给联邦政府，联邦政府无权对地方事务进行干涉，从疫情中也能看到这种架构的局限性，连戴口罩和打疫苗都无法顺利推行。\n现在的很多公司也效仿了这种双重架构；如传统的 HR 是按照招聘、培训、绩效管理、薪酬福利、员工关系、组织发展、企业文化等七大模块分组，每个小组负责专门的职能。而新兴的模式则是在总部 HR 部门之外，每个业务部门也有自己的 HR，这种 HR 叫\nHRBP，人事关系隶属于业务部门，一切听从业务部门 boss 的安排，同时虚线向总部 HR 部门汇报。业务部门自己的 HRBP 是全能型的，从招聘到员工关系都能做，所以招人也不需要通过总部 HR，效率就大大提升。那总部 HR 还有什么存在的意义呢？总部 HR 保留了战略规划、政策制定和共享服务提供\n（如招聘软件、薪酬管理系统、社保录入等等）的职能。\n因此，只垂直管理就会影响地方的效率，只分级管理影响中央到地方的沟通效率，横竖都是影响效率。双重领导是一个折中的做法。\n\n阶段 5：项目部 / 组架构\n\n这种架构一般服务于公司的高优项目，公司需要指派一名项目经理负责这个项目；然后项目经理按需从各个职能部门点将，抽调人才组成一个多功能的项目组\n而其中的关键点在于项目经理是否要承担对团队的管理职责；在项目部架构下，所有人的人事关系暂时隶属于项目部，项目经理是真正的老大，\n掌握着团队成员的绩效评分。但在项目组架构下，每个人都还是原部门的人，归原部门老板管，这中模式下可能会带来问题；这里面都有一个关键点，即需要给项目经理放权，如果项目经理没有权利，最终可能只是充当背锅侠的角色，因为没法很好驱动下面的人去干活。\n项目部因为是直接打绩效，所以有直接管辖权，这个问题还好；但对于项目组，需要有一些额外的处理，在许多成熟大公司里，特别是汽车行业和 IT 行业，项目经理是每周和大老板开会的，这一点很重要，因为经常碰面就意味着项目经理就有告状的权利了，大老板知道情况后就会拿职能部门的头是问，这样谁都怕得罪项目经理了。项目经理几乎就拥有一人之下万人之上的权力，就会很有效率了。\n从政府的运作里也可以找到一些类似的机制，中央部委里，除了常设的机构外，还许多 “领导小组”，这种领导小组不是常设机构，只是针对某一项议题成立的项目团队，有跨部门协调的权力，比如中央农村工作领导小组、中央教育工作领导小组等；以之前的新冠疫情的工作为例，应对如此重大的危机，揪着卫健委一个部门去处理肯定是不行的，因为卫健委最多只能管得到医院、疾控中心等单位，像封锁、隔离、停市停课、核酸检测、打疫苗、统计确诊病例、分析传播链条等工作，涉及到公安、城管、工商、教育、基层社区、财政等部门，卫健委对他们没有管辖权，是指挥不动的，只有位高权重的人亲自挂帅才能统筹安排。如国务院有设置 “中央应对新型冠状病毒感染肺炎疫情工作领导小组”\n上面提到的各个阶段，不是简单的替换关系，更多是相互叠加的关系；这种叠加，是为了解决前一个阶段里暴露出来的一个问题，但同时也会创造一个新的问题，所有的方法只能是暂时的解决每一阶段面临的主要矛盾。下一阶段又会遇到新的问题，大的组织和公司始终是船大难掉头，百年企业往往也意味着历史包袱太大，改革面临重重阻力，因此大公司也不可能无限制大下去，发展到一定程度又会被新的创业公司颠覆，新的创业公司变大之后也会重复以前大公司遇到的问题，又会被下一批新的创业公司干掉，这其实也是一件好事，对整个社会来说是激励兼容的，保证了社会的活力\n团队的组建\n团队组建的基本理念，跟之前写的 《聊聊管理》\n里提到的观点差不多：像打造一个产品去打造一个团队\n而在今年基本算从零开始搭建团队时，有了更加切身的体会，也在实际中遇到了很多问题：从明确业务定位与方向到拍定\nhc，招聘的考核方式，如何在短暂的面试时间充分获取信息，以及新人来之后的\nlanding 过程等等\n在这个过程中，判断当前业务在哪个阶段、业务方向怎么划分、需要什么样的组织架构去支撑这部分业务、每个业务需要放多少人、放什么样的人，等问题都是需要考虑的，前半部分跟具体的业务强相关，而后半部分涉及到 “识人与用人” 这个话题，也是今年有特别深体验的部分\n识人与用人意味着你要通过短暂的面试或后续的合作阶段来判断候选人的性格、热情、技术栈等，把人放到合适的位置上，达到人力和业务的效率最大化；当然，这是正确的废话，实际中往往需要一些具体的方法；关于这个问题，知乎上有个不错的回答\n\n对聪明人：收心放权，用利益吸引，用人品留住，用智慧防守。和聪明人，不要画饼，利弊关系一开始说清楚，各取所需互相支持。职场，无非是资深人士提供资源观点思路策略，新人提供时间精力情绪价值和事情落地。聪明，意味着利己，利己没错，不伤害他人利益能帮团队就好。要聪明人踏实下来，比让笨的人聪明起来还难\n对笨点的：要事事耐心教导，他们很多观点和做事方法论是混沌的，要替他们解构梳理重建，需要时日。你不能急，你一急，他们一害怕就更慌乱，这样原本的水平都保不住。笨点的，大多忠心，但就像切菜用刀背，指望它锋利是不可能了，但用刀的重量也能拍蒜。很多事，聪明人看一眼五分钟就会了，但是教别人时，对方吭哧瘪肚地做了一天还做错了，如果不修炼心性，很多聪明人就会因此急躁，但是要学会为别人创造学会的环境，才是聪明人能获得领导力的转变。\n对自作聪明的笨蛋：是杀敌八百自伤一千六的，如果不想和他们工作一辈子，那就一天都不要留。因为自作聪明意味着他们不甘平庸，但是智商上限低又让他们注定大多数小动作都很迷惑，既花哨多余又损人不利己。其实领导都是过来人，属下心里的小九九不会看不出来。很多时候看破不说破罢了。凡事得有个度，职场虽然不是江湖，但是懂点江湖道义，也不是坏事。\n对于大智若愚的下属：我想说，找到聪明又仗义还有分寸感的下属，那种幸福感真是太棒了，分分钟想唱热血燃烧，然后变成复联一起扛刀去打天下。一定要珍惜，要以诚相待，要兼顾利益和情怀。\n\n心态\n今年印象比较深的一个作品是《男孩、鼹鼠、狐狸和马》，尤其是里面提到了几个观点，无论是 “我们只能看到外在，但一切都发生在内在”，还是 “一个人最大的自由，是选择怎样回应事情”,\n又或者是 “有时候，只是头脑骗你，告诉你，你不好，一切都没有希望” 都是在告诉我们，很多时候，外部发生的事情是无法避免的，但是怎么看待以及回应这些事情，都是由我们来决定的，或者说在很多事情上，心态决定了你的动作\n理性乐观派\n从工业革命开始，人们害怕机器会替代劳动力，人们会失去大批工作；到人口大爆炸，人们担忧粮食不足，导致全球性饥荒；再到石油资源的预测，人们害怕不可再生能源的有限，导致环境破坏和经济崩塌；再到如今 AI 技术的出现，人们害怕自己从事的工种被替代... 我们的焦虑永远不会停止，仿佛这个世界下一秒就要马上毁灭掉\n但事实是工业革命之后，更多操作机器的工种出现，弥补了劳工工作的空缺；人口大爆炸推动了农业自动化和贸易全球化，农业产量增高，大批进出口的农作物不仅填饱了新生人口的肚子，还促进了不同国家间的交流与经济；每当我们认为石油将要用完时，都会有新的油田被发现，各项技术的发展使得石油的利用率大大增加，研发新的清洁能源，混合动力的发展，共同支撑起了新能源消耗型产业\n格雷格・伊斯特布鲁克称这种集体焦虑叫 “集体性地拒不相信生活会越变越好”。然而，每次我们担心的问题，都在不知不觉中被解决了。\n原因是对于媒体而言，往往坏消息才是好新闻；对专家而言，唱衰仿佛更彰显理性；对大众而言，坏消息更容易让我们感同身受。人类更倾向于虚幻的想象，却往往忽略了实际的数据，\n《理性乐观派》一书就是在通过具体的数据来打破这些虚幻的想象\n\n1980~2000 年，的贫困人口消费量增加比世界整体快两倍。中国人比 50 年前富裕 10 倍，预期寿命长了 28 年。... 据联合国估计，过去 50 年里贫困人口减少得比此前 500 年里还要多。\n\n但即便如此，处在当下的我们，还是会觉得当下的时代是如此的糟糕；财富的增长，生活的便利，乃至节省的时间，这都是发展在不知不觉中带给我们的好处，然而我们往往忽略了这些好处，用压力、快节奏、竞争等词替换掉便利的现代生活特征。\n人类的大脑中有一个名为 “杏仁核” 的器官，主要帮助我们应对生活中的危机事件，所以对 “坏消息” 极其敏感，因此坏消息更容易让我们感同身受。在必要时，杏仁核可以救命。但是在日常生活中，我们也很容易被 “杏仁核” 挟持，让我们倾向于看负面消息，觉得生活中充满危机。\n回到生活或工作中，理性乐观意味着要看数据、看事实，从数据和第一性原理出发，去判断一件事情的好坏，而不是原始的恐惧情绪\n那当外部有很多负面的声音的时候，我们又应该如何应对呢？《钝感力》这本书有一句笔者比较喜欢的话\n\n在人缺乏自信或犹豫不决的时候，无论怎样的左思右想都于事无补。因此在这种时候，就要摒弃杂念，更为大胆、充满自信地向前迈进才行。\n犹豫不决，不仅根本无法前行一步，还有可能往后倒退。\n对于你的犹豫不决，很多人当然会有他们各自的看法。而这时，我们要做的就是从中选出自己听起来最为顺耳，最能够使自己振作并快乐地努力下去的话语，从而坚定不移地向前迈进\n\n当我们去接手一个业务或开拓一个新业务，往往会面临着非常多的外部负面信息；这些信息，有很多纯粹是情绪驱动的信息，也有很多道听途说忽略数据事实的信息，有些也许说得没错，但是也只是一个切面，没有抓住业务最核心的点；这个时候，最为重要也许就是坚定自己的决心，当然前提是要自己判断和认可业务是有价值的\n此刻的咒语\n《此时此刻》这一期播客里提到了一个有趣的现象：“咒语”，笔者认为是帮助我们达到理性乐观的很重要的途径和工具\n咒语是什么？咒语是帮助我们看见情绪的” 中断器 “，因为人类的脑子里存在着无休止的、甚至是不靠谱的想法和情绪，而咒语就是中断这些无休止的想法的中断器\n这种中断在感受和行为之间留出了足够的空间，这其实就对应着下面播客里的这一句咒语\n\n刺激和回应之间存在一段距离，成长和幸福的关键就在那里\n\n这句话的英文原版是下面这个\n\nBetween stimulus and response there is a space. In that space is our\npower to choose our response. In our response lies our growth and our\nfreedom.\n\n在人类漫长的进化过程中，我们会对外界的刺激做很多应激反应（Reactive）：战或逃（Fight\nor\nFlight），这些应激反应可以帮助人类躲避外在危险，更好的保护自己。从进化的角度看，这些应激反应是为了完成生存和繁衍的任务，但并不能让我们更幸福、更自由。\n这句咒语的意思是：如果我们能够感知到外界的刺激（Stimulus），能够在本能的应激反应（Reactive）之前，给自己一段距离，选择合适的回应方式（Responsive），我们就能寻找到更多的幸福和自由。孟岩为这句话写的文章里，举了如下例子，笔者认为这里涵盖了很多的场景:\n无论是延迟满足感，还是降低自己的\nego、实事求是，或是冥想，其实都是在实践这句话\n\n当你在看书的时候，突然想拿起手机刷一下朋友圈，这是来自大脑的刺激。这时，你可以应激反应（Reactive），马上拿起手机刷一会儿。你也可以在觉知后给自己一段距离（Space），你知道短暂的愉悦后会很空虚，知道继续看书会有更大的愉悦感，于是你选择用继续看书来回应（Responseive）；\n当别人在讨论中指出你的问题或是批评你做的不对，你感受到自己被冒犯、高大形象岌岌可危。这时，你可以应激反应（Reactive），明知自己错了但依然防卫性的讨论。你也可以在觉知后给自己一段距离（Space），你知道 \"找到什么是正确的 “比捍卫\" 自己是正确的 \"，要重要的多，于是你选择接受对方的批评来回应（Responseive）；\n当你脑海中不停萦绕最近发生的一件令人懊恼的事情，让你无法专心工作或生活。这时，你可以应激反应（Reactive），继续沉浸其中不可自拔。你也可以在觉知后给自己一段距离（Space），你知道过去的事情已经无法改变，但你能做到的是过好现在这个时刻，继而去影响旁边的人或事，从而引发更多积极的变化，于是你选择积极主动、专注当下来回应（Responseive）；\n当你看到朋友圈里一片哀嚎，当你被告知中国要崩溃了，股市要跌到 2000 点。这时，你可以应激反应（Reactive），点一下清仓卖出的按钮。你也可以在觉知后给自己一段距离（Space），你知道金融市场总会有春夏秋冬，你知道自己投资的初心是中国经济的增长，而这并没有改变，于是你选择继续持有、甚至加大买入来回应（Responseive）；\n\n\n所有东西在眼前的时候都感觉更大\n\n这是另一句笔者深有体会的咒语，类比机器学习，流式更新的 online model\n会更加容易受到最近的样本的影响，而人同样是一个 online\nmodel，会更容易被眼前的事情影响，因为人是环境的产物\n当我们大拇指在眼前 1cm\n的位置，会显得非常大，但如果我们把大拇指从眼前放回一臂的距离，总体感觉就会不一样。我们遇到的很多困难或问题，就是这个大拇指，无论是随着时间的流逝还是问题的逐步解决，我们的大拇指会慢慢从眼前放回到到一臂的距离\n因此，未来会释然是真实的，但当下的那种痛苦也是真实的，通过这个咒语，会让我们在当下能够更加从容面对这些问题\n回到生活或工作上，面对每天的 “大拇指”，不妨想象把这些大拇指拿到一臂之外，也许就不会让我们当下如此焦虑或恐惧了\n\n没有什么能把我们的人生分成两段\n\n《我们这一代人的困惑》里是这么说的\n\n我们的一生好像都是在实现目标中挣扎着度过的。上初中的时候，老师告诉你，中考的淘汰率是最高的，只要闯过去，上了高中一切就好了。但上了高中的时候发现不是那么回事嘛，高中老师又说了啊，考上大学就进了天堂。于是你考上了大学，依然空虚迷茫各种草样年华，父母老师又告诉你，找到工作就好了。工作之后发现烦恼和忧虑依然都在，女朋友给你看马云的故事，告诉你等你事业有成就好了……\n你发现了吗，其实人这一辈子的每一个阶段都有新的痛苦和顾虑，周而复始，生生不息。绝对不会因为你考上大学，事业有成，迎娶了女神就从此\nhappily ever\nafter。但每一个阶段也有每一个阶段的快乐，无法替代。生活不是安徒生童话也不是好莱坞电影，从出生的那一刻起直到生命的尽头，都不存在什么节点，过去了之后一切幸福美满无忧无虑。\n每一段岁月都有它存在的价值，没有高低贵贱之分，都不应该被辜负。而我能想到的人这一生能做的最愚蠢的事情，就是把全部人生的希望都孤注一掷到未来的某个节点上，而忽略了生活本身应有的乐趣。哪怕你以后真正实现了那个执念中的目标，才会发现它远远没你想的那么美好。\n年轻的时候和哥们在操场上打篮球喝可乐的快乐，是以后高尔夫球会所里品红酒替代不了的。尤其男生，千万不要总想着等将来有钱了如何如何，且不说你以后很可能不会太有钱，而且相信我，就是有钱了也真的不能怎么样。生命就在每天的生活里，一切执念都是虚妄。和身边的人愉快相处，认真安排好每一天的活动，用心去感受每一天的心境，就是生活的意义本身。这其实是我今天最想分享给你们的事情。\n\n因此，不存在 “有了... 就会...”，我们也不要 “等到... 就...”，而是要学会 “一边... 一边...”；幸福不是发生在过去的某个瞬间，也不是未来的某个阶段，而是此时此刻的感受和体验\n工作上也是如此，不存在做了某个业务，从此就顺风顺水了；不存在解决了某个问题，业务就能好起来了。难做从来就是常态，问题会无休止地涌向；我们要做的不是想着等我解决这个问题，业务就好起来了，就能一劳永逸了，而是想着我要一边处理问题过程中，一边完成自我的成长与业务的增长（当然，前提是业务真的有价值，你真的能从中成长）\n\n平静是意识到世界并非我们想象的那样\n\n世界其实跟我们想象的很不一样，无论是前面的《理性乐观派》还是《事实》，这两本书都在用真实的数字告诉我们，个人的想象与事实之间的\ngap 有多大\n我们都是在一个很小的局部，在盲人观象；通过自己的经验和感受，对一个非常复杂的世界作出预测；还是前面那句话：我们只经历了这个世界的\n0.00000001%，却希望用它去解释这个世界的 80%；降低自己的\nego，不忘初心\n\n欢迎来到荷兰\n\n面对当前的困难，人很容易有这样的错觉：如果我不是现在这样...，就能... 了；这种简答的因果线性思维，跟前面谈到的 “没有什么能把我们的人生分成两段” 一样，是极其浮于表面与不合理的。\n生活充满了随机性，我们很多时候的困难与痛苦来自这些随机性，但生活的美妙也是来自这些随机性\n面对随机性，无论是 做一个清醒的傻瓜 ,\n还是从焦虑谈起，聊聊生活的可能性与随机性，都在尝试告诉我们随机性是无法避免的，与其焦虑或恐惧，还不如直面这些不确定性，过好当下的日子，对自己说一句：欢迎来到荷兰\n小结\n不知不觉就过去了几个月，这几个月里也经历了不少变动，接手了一个有争议的业务，在充满争议和 “矛盾” 的定位反复讨论与拉扯，虽然有点耗心力，但对业务也有了进一步的认知，也能逐步理解那些看似矛盾的定位，实际上更多是对业务发展阶段的判断不同；在负责了团队组建的工作后，也更能理解了组织为业务发展这一理念，一个组织从小到大的发展过程中遇到的种种问题，以及为了缓解当前问题提出的解决方案，如何催生出了下一个问题\n问题是无休止了，我们能调控的更多是自己的心态与做事情的态度，因为 “我们都能看到外部，但是一切发生在内部”，而 “一个人最大的自由，是选择怎样回应事情”；心态会极大地影响我们的动作，难从来都是常态，做个理性乐观派，尝试把那些在眼前看着非常大的 “大拇指” 从眼前移开一臂的距离，我们会发现这些难点只是在眼前被放大了，世界并非我们想象的那样，即使我们原定在意大利的航班落在了荷兰，也一样能够感受荷兰此刻的迷人之处～\n","categories":["闲话几句"],"tags":["闲话几句","管理"]},{"title":"为什么相爱容易，相守不易？","url":"/2015/12/12/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%9B%B8%E7%88%B1%E5%AE%B9%E6%98%93-%E7%9B%B8%E5%AE%88%E4%B8%8D%E6%98%93%EF%BC%9F/","content":"\n文章为转载，作者：淡水天\n\n该文从比较科学的角度阐述了一个不怎么科学的现象，值得一看。下面为原文\n一桩桩分手或出轨的事件，像恼人的烟雾一样缠绕住心怀，更是让许多人直呼 “再也不相信爱情了”。哎，为何世间好物如此不坚牢，开头美好，结局却潦倒？相爱容易，相守却很难？\n\n哪里有什么爱情？不过是生殖冲动\n爱是什么？大约谁也道不清。它是一道隐秘的风，没有轮廓，没有重量，吹过人们柔软的身体，也吹皱他们的心。初初相爱的男女为轻飘飘的快乐所托起，在云层间来回轻踱。\n爱没有名字。若是非要给它找一个丈量的器具，那么怦然跃动的心，手掌里沁出的汗，面颊上映出的云霞.... 或可作为一种参照。在爱之开端，激情浇灌着两颗年轻的心。然而，也因了爱情模糊的面目，个体的紧张与唤醒状态也会被贴上 “爱情” 的标签。心理学家哈特菲尔德甚至认为，爱情就是生理唤醒和心理标签相互作用的结果。\n阿瑟・阿伦（ArthurAron，1974）曾经做过一个经典的实验，他找来一位漂亮的女助手，由她去找男性被试完成一个简单的问卷，再让他们根据一张图片编一个小故事。参加实验的大学生被分为三组，分别在安静的公园、坚固而低矮的石桥，以及危险的吊桥上接受调查。最后，这位漂亮的女助手把自己的联系方式告诉了每一位参加实验的大学生，如果他们想进一步了解实验可以给她打电话。\n猜一猜，最后的结果如何？没错，与其他两组相比，在危险的吊桥上参加实验的大学生给女调查者打电话的人数最多，而他们所编撰的故事中，也更多含有情爱的色彩。\n这就是著名的 “吊桥效应”，其理论来源是沙赫特的 “情绪三因素理论”，情绪 = 刺激 × 生理唤醒 × 认知标签。个体如何解释情境及生理反应，往往与外部的线索和 “诱因” 有关。大部分男性把横渡吊桥时因为紧张所致的口渴感，以及心跳加速等生理上的兴奋误认为性冲动，于是在心底埋下了一颗爱情的种子。\n事实上，在加拿大温哥华北部的千山万壑之中确实存在着一座 “爱情桥”—— 卡皮兰诺吊桥。它全长 137 米，悬挂在高达 70 米的河谷上。无数往来的男女在这座惊心动魄的吊桥上演绎了一段段爱情佳话。\n\n爱情正是这样不靠谱的存在，无怪乎钱钟书先生在《围城》一书中借方鸿渐之口说出，“哪里有什么爱情？不过是生殖冲动。” 爱情的产生就是一场索然无趣的化学反应，一段美好的爱情关系，通常是人体内几种不同的化学物质（如多巴胺、催产素等）相互作用的结果。大脑神经会经历很多不同的化学反应过程，当外界的刺激按照正确的顺序，刺激到正确的复合体时，人们就会相爱。\n短暂的 “投射 — 认同” 游戏\n相爱是不难的，难的是相依相伴，相守终生。犹记得《笑傲江湖》里，苍茫大雪纷飞，林平之执剑在少室山的雪人身后刻下 “海枯石烂，此情不渝” 八个大字，那时的情浓意长，你焉能说他没有一点真心？却也还是敌不住日后的风霜刀剑严相逼啊。\n爱情也有生命，也会经历生老病死，种种的一切。爱意的萌生或许只是刹那间的电光火石，但维护爱情的生命却需付诸百倍的时间与耐性。作为最强烈的人际吸引形式，爱情也有其循序渐进的过程，在时日缓慢的雕琢中，从定向阶段，到情感探索阶段，再到情感交流阶段，最后才会步入稳定交往阶段。\n所谓的 “经营爱情” 大多指的就是在 “情感探索” 和 “情感交流” 这两个阶段的相处与沟通之道。通常在情感探索阶段的初期，双方因为相当程度上的保留与节制，仍然通过 “投射” 与 “认同” 来维护关系，恋人小心翼翼地开放着自己的私人领域，也好奇地探索着关于对方的一切，他们修饰着自己也美化着对方，同时因为接触与了解的程度不深，也可以较少地受到对方自身特质的干扰，迅速地把自己的早期客体形象，投射到对方身上。而对方为了讨得恋人的欢心，也会迅速地认同这个投射，做出恋人理想中的样子来。\n这是最甜蜜的阶段，他们就像是两个小孩子，围着一个神秘的果酱罐，一点一点地尝它，看看里面有多少甜。\n然而，爱情并不只是甜的。当那个果酱罐被一点点地打开之后，它的神秘感与新鲜感都将消失，那时呈现在眼前的或许只是一只平凡无奇的小罐头而已，不同于最初的想象。\n甚至，令你失望。\n我是流泪的洋葱，你还爱我吗？\n随着交往的加深，早期的 “投射 — 认同” 游戏已经无法再继续下去，恋人被压抑的真实自我在呼唤得到释放，他们无法再去扮演一个理想的恋人，而更想去做一个真实的恋人。\n这也是亲密关系的真谛。人们之所以想组建亲密关系，之所以想爱与被爱，就是想获得一种亲密感。亲密需要把自我最深处的部分向他人也向自己呈现，卸除掉层层的伪装与防护，建立起真我与真我之间的连接感。\n在这个阶段，双方的信任感、依恋感开始建立，沟通的深度和广度也有所发展，开始牵涉到较深的情感卷入。这是心与心之间相知的契机，但同时也是一场充满风险的赌博。\n人们的内心常常由三层组成：最外面一层是保护层，接下来是伤痛，而最深处是真我。每个人的成长多多少少都会经历一些伤痛，或者来自于父母，或者来自重要他人，或者是一些创伤性的体验。因为这些伤痛的存在，所以人们发展出了保护层和种种防御机制，以守护脆弱的内核。当恋人鼓起勇气一层层地剥落掉充满防御机制的外壳时，也是在将自己的伤痛展示给对方看，这是痛苦的，必定会像剥洋葱皮一样，一边剥落一边掉泪。\n如果对方的反应是积极的、接纳的，那么恋人即便落泪痛苦，也会因为感受到情感上的陪伴与支持，而坚定地一层层剥落掉那些保护壳，直到露出完整的内核。倘若双方都能如此坦然地接纳对方，那么爱情就会发展到稳定交往阶段，琴瑟和鸣，风雨同舟。\n但很多时候，受限于自己的狭隘、缺陷，以及种种未处理完的情结，人们渴望亲密，又害怕亲密，宁愿相信坦露真实的自我只会招致他人的批判、拒绝和抛弃，也很难相信有人肯接纳自己内在的真实。于是，在关系发展到一定程度时，就会退缩、害怕、扮演角色，在操纵与受控之间游走，生出嫉妒与厌恶、逃离与背叛之心，将关系场演变成游乐场，上演出种种的闹剧与幻想。\n爱是一门艺术\n归根结底，相爱容易，而相守不易，也是因为我们错误地理解了 “爱”。\n人是孤独的，他与外物、与社会、与世界分离，这种孤独创造了一种无法忍受的监禁感，使得人们迫切地寻求与他人的连接。但也正因如此，人们才将爱情的重心放在 “体验被爱”，而不是 “体验爱” 上。于是酿制出种种矛盾，衍生出种种失望。\n弗洛姆认为，爱不仅是情感，爱也是一种能力，更是一门艺术。如果说爱情始于生理唤醒，又在 “你投射，我认同” 的模式下发酵发展，那么能够相依相伴必定是因为习得了爱的能力，掌握了爱的艺术。\n爱的四要素包括关心、尊重、责任和认识。只有当我们从整体上理解爱人，为对方投注以主动的关怀，尊重他的真实存在，为他的精神成长负起责任时，我们才能维持长久的关系。\n这是一场相互的驯化，是两个个体之间 “不完整的融合”，是在距离感与亲密感之间取得一个合理的平衡点。它需要很多的耐心与时间，又在耐心与时间中酿造出新的意义。\n乱入图片一张\n\n做到这一切并不容易。爱有起点，也必有终点，以死亡为句读是爱最好的结局，也是最难的结局。但当你用心地去呵护爱情的生命时，你会发现，一切终将黯淡，唯有被爱的目光镀过金的日子依然在岁月的深谷里闪耀着永恒的光芒。\n那是赠与你的礼物。即便爱情终将死亡，你也可以做那个延长它生命的人。即便你护卫不了对方的心，你也始终可以护卫自己的心，守住自己对爱的这份忠贞。\n别再说什么不相信爱情了，我知道 —— 你只是不愿去相信自己。\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"二叉树的遍历方法","url":"/2016/06/05/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86%E6%96%B9%E6%B3%95/","content":"二叉树的遍历方法有三种，分别是前序遍历，中序遍历和后序遍历。其中的前、中、后分别表示根节点在遍历中被访问的次序。因此，各个遍历方式的访问顺序如下所示：\n\n前序遍历：根节点 --&gt; 左子树 --&gt; 右子树\n中序遍历：左子树 --&gt; 根节点 --&gt; 右子树\n后序遍历：左子树 --&gt; 右子树 --&gt; 根节点\n下面通过 python\n分别实现这三种遍历的递归方法和非递归方法，首先定义每个节点如下所示\n# Definition for a binary tree node.class TreeNode(object):    def __init__(self, x):        self.val = x        self.left = None        self.right = None\n前序遍历\n递归实现\n递归实现的方法非常简单，就是先访问根节点，然后访问左子树，最后访问右子树即可。实现代码如下所示：\ndef preorderTraversal(root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    result = []    if root == None:        return result    result.append(root.val)    result += preorderTraversal(root.left)    result += preorderTraversal(root.right)    return result\n非递归实现\n非递归的方法通过栈来实现，流程如下 （1）将根节点设为当前节点\n（2）记录当前节点 C 的值，然后将 C 入栈，将当前节点设为 C 的左子节点\n（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素 D，并将当前节点设为 D 的右子节点，重复步骤（2）\n（4) 重复步骤（2）~（3）直到栈为空\n实现代码如下所示： def preorderTraversal(root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    stack, result = [], []    curr_node = root    while len(stack) != 0 or curr_node!= None:        if curr_node != None:            result.append(curr_node.val)            stack.append(curr_node)            curr_node = curr_node.left        else:            tmp = stack.pop()            curr_node = tmp.right    return result  \n中序遍历\n递归实现\n递归实现也非常简单，按照顺序访问即可，实现代码如下： def inorderTraversal(root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    result = []    if root == None:        return result    result += inorderTraversal(root.left)    result.append(root.val)    result += inorderTraversal(root.right)    return result\n### 非递归实现\n非递归实现的过程类似于前序遍历的非递归实现，只是在记录节点的时间不同，中序周游记录是在元素从栈里弹出的时候才记录元素的值。实现的步骤如下：\n（1）将根节点设为当前节点\n（2）将当前节点 C 入栈，然后将当前节点设为 C 的左子节点\n（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素 D，记录元素 D 的值，并将当前节点设为 D 的右子节点，重复步骤（2）\n（4) 重复步骤（2）~（3）直到栈为空\ndef inorderTraversal( root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    stack, result= [], []    curr_node = root    while len(stack) != 0 or curr_node != None:        if curr_node != None:            stack.append(curr_node)            curr_node = curr_node.left        else:            tmp = stack.pop()            result.append(tmp.val)            curr_node = tmp.right    return result\n后序遍历\n递归实现\n后序遍历的实现也非常简单，只需要按照顺序访问即可，实现代码如下所示：\ndef postorderTraversal(root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    result = []    if root == None:        return result    result += postorderTraversal(root.left)    result += postorderTraversal(root.right)    result.append(root.val)    return result\n非递归实现\n后续遍历的非递归实现利用了一点小技巧，就是先进行根节点--&gt;右子树--&gt;左子树的遍历，然后将得到的结果进行反转（reverse）即可。这是因为当根节点最后访问时无法确定何时该记录这个值。\n实现的过程也类似于前序遍历，具体过程如下： （1）将根节点设为当前节点\n（2）记录当前节点 C 的值，然后将 C 入栈，将当前节点设为 C 的右子节点\n（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素 D，并将当前节点设为 D 的左子节点，重复步骤（2）\n（4) 重复步骤（2）~（3）直到栈为空\n实现代码如下所示： def postorderTraversal(self, root):    \"\"\"    :type root: TreeNode    :rtype: List[int]    \"\"\"    stack, result = [], []    curr_node = root    while len(stack)!=0 or curr_node != None:        if curr_node != None:            result.append(curr_node.val)            stack.append(curr_node)            curr_node = curr_node.right        else:            tmp = stack.pop()            curr_node = tmp.left    result.reverse()     return result\n","categories":["python"],"tags":["python","树"]},{"title":"中文维基百科语料库词向量的训练","url":"/2016/10/12/%E4%B8%AD%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%AD%E7%BB%83/","content":"要通过计算机进行自然语言处理，首先就需要将这些文本数字化。目前用得最广泛的方法是词向量，根据训练使用算法的不同，目前主要有\nWord2Vec 和 GloVe\n两大方法，本文主要讲述通过这两个方法分别训练中文维基百科语料库的词向量。\n\n获取并处理中文维基百科语料库\n下载\n中文维基百科语料库的下载链接为：https://dumps.wikimedia.org/zhwiki/,\n本试验下载的是最新的 zhwiki-latest-pages-articles.xml.bz2。这个压缩包里面存的是标题、正文部分，该目录下还包括了其他类型的语料库，如仅包含标题，摘要等。\n抽取内容\nWikipedia Extractor\n是一个开源的用于抽取维基百科语料库的工具，由 python 写成，通过这个工具可以很容易地从语料库中抽取出相关内容。使用方法如下：\n$ git clone https://github.com/attardi/wikiextractor.git wikiextractor$ wikiextractor/WikiExtractor.py  -b 2000M -o zhwiki_extracted zhwiki-latest-pages-articles.xml.bz2\n由于这个工具就是一个 python 脚本，因此无需安装，-b\n参数指对提取出来的内容进行切片后每个文件的大小，如果要将所有内容保存在同一个文件，那么就需要把这个参数设得大一下，-o\n的参数指提取出来的文件放置的目录，抽取出来的文件的路径为 zhwiki_extract/AA/wiki_00。更多参数可参考其 github 主页的说明。\n抽取后的内容格式为每篇文章被一对 &lt;doc&gt; &lt;/doc&gt; 包起来，而 &lt;doc&gt; 中的包含了属性有文章的 id、url 和 title 属性，如 &lt;doc id=\"13\" url=\"https://zh.wikipedia.org/wiki?curid=13\" title=\"数学\"&gt;。\n繁简转换\n由上一步提取出来的中文维基百科中的语料中既有繁体字也有简体字，这里需要将其统一变为简体字，采用的工具也是开源的\nOpenCC\n转换器。使用方法如下：\n$ git clone https://github.com/BYVoid/OpenCC.git$ cd OpenCC &amp;&amp; make &amp;&amp; make install$ opencc -i zhwiki_extract/AA/wiki_00 -o zhwiki_extract/zhs_wiki -c /home/nlp/OpenCC/data/config/t2s.json\n我使用的是\ncentos，yum 源中找不到这个软件，因此通过编译安装最新的版本，需要注意的是编译 OpenCC\n要求 gcc 的版本最低为 4.6 。其中 -i 表示输入文件路径，\n-o 表示输出的文件\n，-c 表示转换的配置文件，这里使用的繁体转简体的配置文件，OpenCC 自带了一系列的转换配置文件，可参考其 github 主页的说明。\n去除标点符号\n去除标点符号有两个问题需要解决，一是像下面这种为了解决各地术语名称不同的问题\n他的主要成就包括Emacs及後來的GNU Emacs，GNU C 編譯器及-{zh-hant:GNU 除錯器;zh-hans:GDB 调试器}-。\n另外一个就是将所有标点符号替换成空字符，通过正则表达式均可解决这两个问题，下面是具体实现的 python 代码。\n#!/usr/bin/python# -*- coding: utf-8 -*- import sysimport reimport ioreload(sys)sys.setdefaultencoding('utf-8')def pre_process(input_file, output_file):    multi_version = re.compile(ur'-\\{.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\\}-')    punctuation = re.compile(u\"[-~!@#$%^&amp;*()_+`=\\[\\]\\\\\\{\\}\\\"|;':,./&lt;&gt;?·！@#￥%……&amp;*（）——+【】、；‘：“”，。、《》？「『」』]\")    with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile:        with io.open(input_file, mode = 'r', encoding ='utf-8') as infile:            for line in infile:                line = multi_version.sub(ur'\\2', line)                line = punctuation.sub('', line.decode('utf8'))                outfile.write(line)if __name__ == '__main__':    if len(sys.argv) != 3:        print \"Usage: python script.py input_file output_file\"        sys.exit()    input_file, output_file = sys.argv[1], sys.argv[2]    pre_process(input_file, output_file)\n分词\n经过上面的步骤基本得到了都是简体中文的纯净文本，下面需要对其进行分词并且整理成每行一篇文本的格式，从而方便后续的处理。\n分词采用 python 的分词工具 jieba，通过\npip install jieba\n安装即可。且将一篇文章分词后的结果存储在一行，由前面可知，每篇文章存储在一对 &lt;doc&gt;&lt;/doc&gt; 标签中，由于前面去掉了标点，所以现在变成了 doc doc, 所以只要判断当前行为 doc 时即可认为文章结束，从而开始在新的一行记录下一篇文章的分词结果。实现的 python 代码如下:\n#!/usr/bin/python# -*- coding: utf-8 -*-import sysimport ioimport jiebareload(sys)sys.setdefaultencoding('utf-8')def cut_words(input_file, output_file):    count = 0    with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile:        with io.open(input_file, mode = 'r', encoding = 'utf-8') as infile:            for line in infile:                line = line.strip()                if len(line) &lt; 1:  # empty line                    continue                if line.startswith('doc'): # start or end of a passage                    if line == 'doc': # end of a passage                        outfile.write(u'\\n')                        count = count + 1                        if(count % 1000 == 0):                            print('%s articles were finished.......' %count)                    continue                for word in jieba.cut(line):                    outfile.write(word + ' ')    print('%s articles were finished.......' %count)if __name__ == '__main__':    if len(sys.argv) &lt; 3:        print \"Usage: python script.py input_file output_file\"        sys.exit()    input_file, output_file = sys.argv[1], sys.argv[2]    cut_words(input_file, output_file)\n通过 Word2Vec 训练词向量\nWord2vec 中包含了两种训练词向量的方法：Continuous Bag of\nWords (CBOW) 和 Skip-gram。CBOW 的目标是根据上下文来预测当前词语的概率。Skip-gram 刚好相反，根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机 N 维向量。训练时，该算法利用 CBOW 或者 Skip-gram 的方法获得了每个单词的最优向量。\n最初 Google 开源的 Word2Vec 是用 C 来写的，后面陆续有了 Python ，Java\n等语言的版本，这里采用的是 Python 版本的 gensim。通过\ngensim 提供的 API\n可以比较容易地进行词向量的训练。gensim 的建议通过 conda 安装，步骤如下:\n$ wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh$ bash Anaconda2-4.1.1-Linux-x86_64.sh$ conda update conda$ conda install gensim\nLinux 系统一般原来会带有 python，直接执行 python\n命令可能会调用系统内置的 python 解释器，因此如果要使用 conda 安装的\npython， 执行 python 命令的时候需要输入指定其通过 conda\n安装的完整目录，或者将这个路径添加在环境变量 $PATH 之前。\n下面是对上面处理后的语料库进行训练的一个简单例子。\n#!/usr/bin/python# -*- coding: utf-8 -*-import os, sysimport multiprocessingimport gensim  reload(sys)sys.setdefaultencoding('utf-8')def word2vec_train(input_file, output_file):    sentences = gensim.models.word2vec.LineSentence(input_file)    model = gensim.models.Word2Vec(sentences, size=300, min_count=10, sg=0, workers=multiprocessing.cpu_count())    model.save(output_file)    model.save_word2vec_format(output_file + '.vector', binary=True)if __name__ == '__main__':    if len(sys.argv) &lt; 3:        print \"Usage: python script.py infile outfile\"        sys.exit()    input_file, output_file = sys.argv[1], sys.argv[2]    word2vec_train(input_file, output_file)\n上面的训练过程首先将输入的文件转为 gensim 内部的 LineSentence\n对象，要求输入的文件的格式为每行一篇文章，每篇文章的词语以空格隔开。\n然后通过 gensim.models.Word2Vec 初始化一个 Word2Vec\n模型，size 参数表示训练的向量的维数；min_count 表示忽略那些出现次数小于这个数值的词语，认为他们是没有意义的词语，一般的取值范围为（0，100）；sg 表示采用何种算法进行训练，取 0 时表示采用 CBOW 模型，取 1 表示采用 skip-gram 模型；workers 表示开多少个进程进行训练，采用多进程训练可以加快训练过程，这里开的进程数与 CPU 的核数相等。\n最后将训练后的得到的词向量存储在文件中，存储的格式可以是 gensim\n提供的默认格式 (save 方法)，也可以与原始 c 版本 word2vec 的\nvector 相同的格式 (save_word2vec_format 方法)，加载时分别采用\nload 方法和 load_word2vec_format\n方法即可。更详细的 API 可参考\nhttps://rare-technologies.com/word2vec-tutorial/ 和\nhttp://radimrehurek.com/gensim/models/word2vec.html。\n假设我们训练好了一个语料库的词向量，当一些新的文章加入这个语料库时，如何训练这些新增的文章从而更新我们的语料库？将全部文章再进行一次训练显然是费时费力的，gensim 提供了一种类似于 “增量训练” 的方法。即可在原来的 model 基础上仅对新增的文章进行训练。如下所示为一个简单的例子：\nmodel = gensim.models.Word2Vec.load(exist_model)model.train(new_sentences)\n上面的代码先加载了一个已经训练好的词向量模型，然后再添加新的文章进行训练，同样新增的文章的格式也要满足每行一篇文章，每篇文章的词语通过空格分开的格式。这里需要注意的是加载的模型只能\n是通过 model.save() 存储的模型，从 model.save_word2vec_format() 恢复过来的模型只能用于查询.\n通过 Glove 训练词向量\n除了上面的 Word2Vec ，通过 Glove\n也可以训练出词向量，只是这种方法并没有 Word2Vec\n用得那么广泛。这里简单提及，也算是为训练词向量提供多一个选择。\n首先需要下载并编译 Glove，步骤如下： $ wget http://www-nlp.stanford.edu/software/GloVe-1.2.zip$ unzip Glove-1.2.zip $ cd Glove-1.2 &amp;&amp; make\n编译后会在 Glove-1.2 目录下生成一个 build\n目录，里面包含了训练需要用到的工具。目录结构如下所示： build/|-- cooccur|-- glove|-- shuffle`-- vocab_count\n训练过程总共分为四步，对应上面的四个工具，顺序依次为 vocab_count --&gt; cooccur --&gt; shuffle --&gt; glove，下面是具体的训练过程\n$ build/vocab_count -min-count 5 -verbose 2 &lt; zhs_wiki_cutwords &gt; zhs_wiki_vocab$ build/cooccur -memory 4.0 -vocab-file zhs_wiki_vocab  -verbose 2 -window-size 5 &lt; zhs_wiki_cutwords &gt; zhs_wiki_cooccurence.bin$ build/shuffle  -memory 4.0 -verbose 2 &lt; zhs_wiki_cooccurence.bin &gt;zhs_wiki_shuff.bin$ build/glove -save-file zhs_wiki_glove.vectors -threads 8 -input-file zhs_wiki_shuff.bin -vocab-file zhs_wiki_vocab -x-max 10 -iter 5 -vector-size 300 -binary 2 -verbose 2 \n上面四条命令分别对应于训练的四个步骤，每个步骤含义如下\n\nvocab_count 从语料库 (zhs_wiki_cutwords 是上面第一步处理好的语料库) 中统计词频，输出文件\nzhs_wiki_vocab，每行为词语 词频；-min-count 5 指示词频低于 5 的词舍弃，-verbose 2 控制屏幕打印信息的，设为 0 表示不输出\ncooccur 从语料库中统计词共现，输出文件\nzhs_wiki_cooccurence.bin，格式为非文本的二进制；-memory 4.0 指示 bigram_table 缓冲器，-vocab-file 指上一步得到的文件，-verbose 2 同上，-window-size 5 指示词窗口大小。\nshuffle 对 zhs_wiki_cooccurence.bin\n重新整理，输出文件 zhs_wiki_shuff.bin\nglove\n训练模型，输出词向量文件。-save-file\n、-threads 、-input-file\n和 -vocab-file\n直接按照字面应该就可以理解了，-iter\n表示迭代次数，-vector-size\n表示向量维度大小，-binary\n控制输出格式 0: save as text files; 1: save as binary; 2: both\n\n训练后得到的二进制词向量模型格式与原始 c 版本 word2vec 的 vector\n格式也相同，可以通过下面的方法统一加载使用。\n使用词向量模型\n训练好的词向量可以供后续的多项自然语言处理工作使用，下面是通过 gensim\n加载训练好的词向量模型并进行查询的例子\n# 加载模型&gt;&gt;&gt; from gensim.models import Word2Vec&gt;&gt;&gt; model = Word2Vec.load_word2vec_format('/home/nlp/zhs_wiki_trained.vector',binary = True)# 词向量维度&gt;&gt;&gt; len(model[u'男人'])300# 具体词向量的值&gt;&gt;&gt; model[u'男人']array([ 0.56559366, -1.96017861, -1.57303607,  1.2871722 , -1.38108838.....# 词语相似性&gt;&gt;&gt; model.similarity(u'男人',u'女人')0.86309866214314379# 找某个词的近义词，反义词&gt;&gt;&gt; words = model.most_similar(u\"男人\")&gt;&gt;&gt; for word in words:...     print word[0], word[1]... 女人 0.863098621368女孩 0.67369633913女孩子 0.658665597439陌生人 0.654322624207小女孩 0.637025117874小孩 0.630155563354男孩 0.625135600567男孩子 0.617452859879小孩子 0.613232254982老婆 0.584552764893&gt;&gt;&gt; words = model.most_similar(positive=[u\"女人\", u\"皇后\"], negative=[u\"男人\"], topn=5)&gt;&gt;&gt; for word in words:...     print word[0], word[1]... 皇太后 0.630089104176太后 0.613425552845王妃 0.581929504871贵妃 0.581658065319王后 0.577878117561# 若干个词中剔除与其他最不相关的&gt;&gt;&gt; print  model.doesnt_match(u\"早餐 晚餐 午餐 食堂\".split())食堂&gt;&gt;&gt; print  model.doesnt_match(u\"早餐 晚餐 午餐 食堂 教室\".split())教室# 多个词语的相似性&gt;&gt;&gt; model.n_similarity([u\"女人\", u\"皇帝\"], [u\"男人\", u\"皇后\"])0.76359309631510597\n这里并没有对训练出来的词向量质量进行评估，虽然 Google\n提供了一种测试集，约 20000 句法和语义的测试实例（questions-words.txt），检查如 A对于B类似C对于D 这种线性平移关系。由于测试集是英文的，因此可以考虑翻译过来然后对中文的采用同样的评估方法，但是实际的效果还是要看具体应用中的效果。\n\n参考： https://flystarhe.github.io/2016/09/04/word2vec-test/\nhttps://flystarhe.github.io/2016/08/31/wiki-corpus-zh/\nhttp://radimrehurek.com/gensim/models/word2vec.html\nhttps://rare-technologies.com/word2vec-tutorial/\n","categories":["NLP"],"tags":["python","NLP"]},{"title":"什么才算是真正的编程能力？","url":"/2016/01/04/%E4%BB%80%E4%B9%88%E6%89%8D%E7%AE%97%E6%98%AF%E7%9C%9F%E6%AD%A3%E7%9A%84%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%EF%BC%9F/","content":"本文综合整理自知乎同名问答帖\n链接：https://www.zhihu.com/question/31034164/\n\n题主问题：\n&gt; 还在读书，也在实验室帮忙做了些东西，自己也搭过几个网站。在周围人看来似乎好像我很厉害，做了那么多东西，但是我发现这些东西虽然是我做的，但是实际上我手把手自己写的代码却并没有多少，很多都是用开源的东西，我写的代码无非是把别人的东西整合下，类似于胶水一样的工作。\n\n&gt; 我之前所认为的编程是全手动一行一行敲代码，但是现在我发现哪怕是工程上，也有很多人是复制黏贴来解决问题的，并且提倡不要重复造轮子。\n\n但是靠谷歌和复制别人的轮子，虽然我做出了很多东西，可是我并不觉得自己能力上有提升，倒是利用搜索引擎的能力的确提升了不少。而学校里另外一部分在搞 ACM 的人，他们每天都在刷题练算法，但单凭我个人的感受感觉他们似乎对工程上有些东西并不了解，或许算法的能力才算是实打实的编程能力？那” 胶水” 的能力和整合轮子的能力算不算编程能力呢？\n\n\n所以我现在就很困惑，所谓的编程能力到底是什么，我该如何提升自己的编程能力？\n\n下面是两个人的回答：\n下面是刘贺的回复，专栏：http://zhuanlan.zhihu.com/shanhu\n链接：https://www.zhihu.com/question/31034164/answer/50423838\n\n非常好的一个问题。这可能是我在知乎见到过的问编程有关的问题中问得最好的一个了。我非常喜欢这个问题。\n\n\n计算机科学有两类根本问题。一类是理论：算法，数据结构，复杂度，机器学习，模式识别，等等等。一类是系统：操作系统，网络系统，分布式系统，存储系统，游戏引擎，等等等等。\n\n\n理论走的是深度，是在追问在给定的计算能力约束下如何把一个问题解决得更快更好。而系统走的是广度，是在追问对于一个现实的需求如何在众多的技术中设计出最多快好省的技术组合。\n\n\n搞 ACM 的人，只练第一类。像你这样的更偏向于第二类。其实挺难得的，但很可惜的是第二类能力没有简单高效的测量考察方法，不像算法和数据结构有 ACM 竞赛，所以很多系统的苗子都因为缺少激励和正确引导慢慢就消隐了。\n\n\n所以比尔盖茨才会说，看到现在学编程的人经常都把编程看作解各种脑筋急转弯的问题，他觉得很遗憾。\n\n\n做系统，确实不提倡 “重复发明轮子”。但注意，是不提倡 “重复发明”，不是不提倡 “重新制造”。恰恰相反的，我以为，系统的编程能力正体现在 “重新制造” 的能力。\n\n\n能把已有的部件接起来，这很好。但当你恰好缺一种关键的胶水的时候，你能写出来吗？当一个已有的部件不完全符合你的需求的时候，你能改进它吗？如果你用的部件中有 bug，你能把它修好吗？在网上繁多的类似功能的部件中，谁好谁坏？为什么？差别本质吗？一个开源代码库，你能把它从一个语言翻译到另一个语言吗？从一个平台移植到另一个平台吗？能准确估计自己翻译和移植的过程需要多少时间吗？能准确估计翻译和移植之后性能是会有提升还是会有所下降吗？\n\n\n系统编程能力体现在把已有的代码拿来并变成更好的代码，体现在把没用的代码拿来并变成有用的代码，体现在把一个做好的轮子拿来能画出来轮子的设计蓝图，并用道理解释出设计蓝图中哪些地方是关键的，哪些地方是次要的，哪些地方是不容触碰的，哪些地方是还可以改进的。\n\n\n如果你一点不懂理论，还是应该学点的。对于系统性能的设计上，算法和数据结构就像在自己手头的钱一样，它们不是万能的，但不懂是万万不行的。\n\n\n怎么提高系统编程能力呢？土办法：多造轮子。就像学画画要画鸡蛋一样，不是这世界上没有人会画鸡蛋，但画鸡蛋能驯服手指，感受阴影线条和笔触。所以，自己多写点东西吧。写个编译器？渲染器？操作系统？web 服务器？web 浏览器？部件都一个个换成自己手写的，然后和已有的现成部件比一比，看看谁的性能好，谁的易用性好？好在哪儿？差在哪儿？为什么？\n\n\n更聪明一点的办法：多拆轮子。多研究别人的代码是怎么写的。然而这个实践起来经常很难。原因：大部分工业上用的轮子可能设计上的思想和技术是好的，都设计和制造过程都很烂，里面乱成一团，让人乍一看毫无头绪，导致其对新手来说非常难拆。这种状况其实非常糟糕。所以，此办法一般只对比较简单的轮子好使，对于复杂的轮子，请量力而行。\n\n\n轮子不好拆，其实是一个非常严重的问题。重复发明轮子固然是时间的浪费，但当轮子复杂而又不好拆的时候，尤其是原来造轮子的人已经不在场的时候，重新发明和建造轮子往往会成为无奈之下最好的选择。这是为什么工业界在明知道重复发明 / 制造轮子非常不好的情况下还在不断重复发明 / 制造轮子的根本原因。\n\n\n程序本质是逻辑演绎的形式化表达，记载的是人类对这个世界的数字化理解。不能拆的轮子就像那一篇篇丢了曲谱的宋词一样，能读，却不能唱。\n\n\n鄙人不才，正在自己研究怎么设计建造一种既好用又好拆的轮子。您没那么幸运，恐怕是等不到鄙人的技术做出来并发扬光大了。在那之前，多造轮子，多拆好拆的小轮子，应该是提高编程能力最好的办法了。\n\n\n以上。嗯。\n\n\n（文章属个人观点，与本人工作雇主无关。）\n\n下面是疯坦克的回答\n链接：https://www.zhihu.com/question/31034164/answer/74716106\n\n真正的编程能力其实并不是对语法细节的理解，也不在于手写或者复制粘贴，更不在于对什么操作系统的使用，或者常用库的 api 的记忆。而是找出解决方法的能力，把现实问题转换为代码逻辑的能力。这个是最重要的。语法很好学，只要看一看，再不行网上搜一搜都有，但是解决问题的能力，在网上搜不到，找不来，谁也帮不了。只能在长期的分析问题解决问题的过程中得到。\n\n\n在工作中，见过太多面试的时候打高分，把什么 const char*,\nchar const*,\nchar*const i+++++i 这种奇技淫巧玩的烂熟，解决问题的时候一筹莫展的。只能你清晰明了的告诉他流程他才能实现。这样的人，要是不思进取，沉浸在这种很多公司禁用的语法技巧里沾沾自喜，可能永远只能是个代码流水线工人。也有很多人面试的时候各种语法都模棱两可，提起做过的项目和程序，却能够条理清楚，头头是道。给他一个问题，他几分钟就给出还不错的解决方案。这样的人，随便什么语言，什么语法，什么库，对他来说都是工具。他知道与否，都能最终解决问题。其实不管是复制黏贴也好，自己手写也好，关键的是解决问题。编程最终还是个生产工具，目的是解决问题，不能解决问题的，一切都是空中楼阁，毫无价值。\n\n","categories":["编程"],"tags":["编程"]},{"title":"使用 Screen 管理 Linux 远程会话","url":"/2015/12/10/%E4%BD%BF%E7%94%A8Screen%E7%AE%A1%E7%90%86Linux%E8%BF%9C%E7%A8%8B%E4%BC%9A%E8%AF%9D/","content":"通过 SSH 或者 telent 远程登录到\nLinux 服务器执行一些长时间运行的任务，比如系统备份、ftp\n传输等等。因为他们执行的时间太长了。必须等待它执行完毕，在此期间可不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。\n本文分析了这个问题的原因以及解决方法。\n\n为什么关掉窗口 / 断开连接会使得正在运行的程序死掉？\n元凶：SIGHUP 信号\n在 Linux/Unix 中，有这样几个概念：\n\n进程组（process\ngroup）：一个或多个进程的集合，每一个进程组有唯一一个进程组 ID，即组长进程的 ID。\n会话（session）：一个或多个进程组的集合，开始于用户登录，终止与用户退出，此期间所有进程都属于这个会话。一个会话一般包含一个会话首进程、一个前台进程组和一个后台进程组。\n守护进程（daemon）：Linux 大多数服务都是通过守护进程实现的，完成许多系统任务如 0 号进程为调度进程，是内核一部分；1 号进程为 init 进程，负责内核启动后启动 Linux 系统。守护进程不因为用户、终端或者其他的变化而受到影响。\n\n当终端接口检测到网络连接断开，将挂断信号（SIGHUP）发送给控制进程（会话期首进程）。而挂断信号默认的动作是终止程序。如果会话期首进程终止，则该信号发送到该会话期前台进程组。\n也就是说：ssh 打开以后，bash 等都是他的子程序，一旦 ssh 关闭，系统将所有前台进程杀掉。（后台进程和守护进程不会被关闭！！！）\n测试案例\n测试例一\n打开两个 SSH 终端窗口，在其中一个运行了一个循环打印的 python 脚本。执行命令如下：\n[root@localhost ~]# python test.py\ntest.py 内容如下：\nwhile True:      print \"hehe\"  ```  另外一个终端用`pstree -p`查看当前的进程树。显示如下  ```  [root@localhost ~]# pstree -p  （省略）  ├─sshd(958)─┬─sshd(1282)───bash(1286)───pstree(1436)  │           └─sshd(1410)───bash(1414)───python(1433)  ```  可以看到2个bash进程代表了2个终端，pstree是当前进程正在运行的程序，而python进程则是另外一个终端正在运行的程序。关掉启动python的终端，在刚刚执行pstree的终端上查找pid为1433的进程（也就是原来的python进程），发现没有这个pid的进程，说明python随着终端的关闭而终止了，此时输入`pstree -p`变为了下面这样：  ```  [root@localhost ~]# pstree -p  （省略）  ├─sshd(958)──sshd(1282)───bash(1286)───pstree(1436)\n测试例二\n步骤同例一，只是在执行 python 脚本时将其放到后台执行，执行命令如下：\n[root@localhost ~]# python test.py &amp;\n这样在关闭执行 python 的中断后，python 进程并没有被中断，通过 pstree -p 查看到进程数类似于下面的情况：\n(省略)  ├─python(1493)  ├─sshd(958)───sshd(1282)───bash(1286)───pstree(1497)```  因为python执行的是个后台进程，而SIGHup信号只会发送给前台进程组，当父进程结束后，其原来子进程中的后台进程会成为孤儿进程被init进程收养。详见[孤儿进程和僵尸进程][1]注：网上一些资料显示执行某些复杂程序的时候，只加`&amp;`也会终止，但是博主还没遇到过这种情况，因为我不会这样去执行一个执行时间较长的程序。同样,nohup命令可以达到这个目的，值得注意的是nohup命令只是使得程序忽略SIGHUP信号，还需要使用标记&amp;把它放在后台运行。这种情况能够保证程序不会被终止。  `nohup &lt;command&gt; [argument…] &amp;`##  使用screen管理远程会话  虽然nohup和后台进程很容易使用，但还是比较“简陋”的，对于简单的命令能够应付过来，对于复杂的需要人机交互的任务就麻烦了。其实我们可以使用一个更为强大的实用程序screen。简单来说，Screen是一个可以在多个进程之间多路复用一个**物理终端**的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口。### 创建新的会话在screen中创建一个新的会话有2种方式**1．直接在命令行键入screen命令**  `[root@localhost ~]# screen`  Screen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样。在该窗口中键入exit退出该窗口，如果这是该screen会话的唯一窗口，该screen会话退出，否则screen自动切换到前一个窗口。  也可通过`screen -S name` 来为启动的session取名字。  **2．Screen命令后跟你要执行的程序**  [root@localhost ~]# python test.py  Screen创建一个执行python test.py的单窗口会话，终止进程将退出该窗口/会话。### 进入已创建会话即使关闭了启动所有终端，在screen会话中启动的进程也不会终止，再次连接时可通过`screen -ls`查看已经启动的screen会话(detached状态)，用`screen -r name`恢复指定会话，也可在会话中通过exit退出screen会话。  ```  [root@localhost ~]# screen -ls  There is a screen on:  \t1518.lc\t(Detached)  1 Socket in /var/run/screen/S-root.  重新连接会话  [root@localhost ~]# screen -r lc或screen -r 1518  退出当前screen会话  [root@localhost ~]#exit  \nscreen 的一些常用参数如下所示\n\n分享操作\n\nscreen -x name 进入一个还在连接着（attached）的 screen，然后所有操作能够被另外所有正在连着的 screen 看到\n\n分屏\n\n创建一个新的窗口：ctrl+a+S\n（注意是大写的 s）, 此时新的窗口还没启动 bash\n\n启动新窗口的 bash：ctrl+a+c\n\n切换窗口：ctrl+a+tab\n\n关掉当前窗口:ctrl+a+X（注意是大写的 x）\n\n","categories":["工具使用"],"tags":["Linux","工具使用"]},{"title":"从焦虑谈起，聊聊生活的可能性与随机性","url":"/2022/07/31/%E4%BB%8E%E7%84%A6%E8%99%91%E8%B0%88%E8%B5%B7%EF%BC%8C%E8%81%8A%E8%81%8A%E7%94%9F%E6%B4%BB%E7%9A%84%E5%8F%AF%E8%83%BD%E6%80%A7%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%80%A7/","content":"35\n岁这个槛，似乎是悬在互联网人头上的达摩克利斯之剑，一直在各大媒体上被鼓吹放大，虽不明真假，但也让人不寒而栗；而持续了三年的疫情加上\n2022 随之而来互联网裁员潮，王兴那句开玩笑似的 “2019\n可能会是过去十年里最差的一年，但却是未来十年里最好的一年”\n似乎一语成谶。\n走在奔三的路上，凝望这前面那个迷雾般的 35\n岁路口，焦虑就一直萦绕在心头；伴随着的是一系列的疑问：大厂为什么要裁员？所谓的\n35 岁危机从何而来？只有互联网才有这个危机么？普通人只能眼巴巴地等待着 35\n岁到来然后被裁掉？我们能做些什么？\n当我最开始问出自己这一串问题时，焦虑中带着点绝望，因为前方的路虽然处于迷雾，却又似乎一眼能望到头；但是准备写这篇文章，心里又平静了很多，也许是经过这一个多月的调研和自我教育，看到了生活的可能性和随机性，接受了与焦虑为伴这个事实。本文算是写给自己的心理按摩，文章可能会有点发散，祝开卷有益。\n\n\n裁员、危机与焦虑\n裁员原因\n先说大厂裁员，大厂裁员本质上是在停止增长后的断臂求生；你会发现，首先裁员的部门基本是那些还没有实现盈利、具备较大不确定性的部门，而经济下行时，企业更多地会追求稳定性而放弃未来不确定的可能性；在这种前提下，往往会存在整条业务线被优化掉的情况，有时候哪怕你是一个高职级的员工、或者是应届生，也难逃被裁的结果\n上面很关键的一个词是是 “停止增长”，从微观层面的角度来看，停止增长指的是公司的\nDAU\n可能已经见顶，广告等收入也见顶，而能够带来新的现金流的业务还没出现；从宏观层面来看，则是整个互联网行业面临着没有新要素的困境 (新要素指的是之前的移动互联网、移动支付)，目前很难去找到一个所谓风口，或者说蓝海。无人驾驶，元宇宙，web3\n等等，都还无法没看到变现的迹象，大厂基本的现金流还是传统的广告、游戏和电商\nPS：其实这三年反复的疫情，各行各业都不好过，也并非只有互联网大厂在裁员，只是刚好这个人群在互联网上的声音比较大，裁员其实是从传统行业一直蔓延到互联网大厂的，因为疫情最先影响的就是实体的生意，然后才是互联网，实体生意都没了，还需要啥营销和广告呢\n裁员现象其实跟资本二级市场里的一些现象有点类似，比如说裁员存在恐慌情绪的蔓延，大厂开始裁完了，中厂也会跟着裁；举个可能不是很恰当的例子，裁员就是公司看空未来的经济发展，跟二级市场大家看空某个股票疯狂抛售并无二样，且抛售的股票无人接盘\n裁员另一个很重要的原因是人员冗余，因为在经济好、资本充足时，一切看起来欣欣向荣；在这个时候大厂招人并不理性，会疯狂招人，而且大的组织做很多事情都有惯性，招人会一直招到非常冗余，这个惯性就有点像钟摆效应，如同在大牛市顶端进去接盘的人们，大家都是怀揣着上证指数要上\n1 万点的心态进场的\n疯狂招人背后，边际收益的递减是必然存在的，乱翻书在 54\n期播客聊到这个话题时 (相关链接会放在最后)，里面有句话特别传神：“飞驰的火车上，每个人都觉得自己在开火车，其实大家都在车厢里做布朗运动”\n所以，大家会发现裁员后，对公司业务的影响可能是很小的，工作业绩影响微乎其微，但是财报会立马更好看，其实这个也可以用二八法则来解释 (最近越来越发现，几乎一切皆可二八法则)，这就好比在一个广告系统里，把那些无跑量素材或计划砍掉，你会看到对整个大盘的几乎是无损的，但是所谓的生态会好很多，但从另一个角度来看，这牺牲了长尾计划 / 素材的探索空间，回到公司上，则是牺牲了未来的可能性。\n裁员迹象\n裁员一般会有什么迹象？在乱翻书的播客里提到了，裁员一般有 2 种情况\n（1）中小公司，突然遭受资金上的问题，然后急于去控制成本，会迅速的在所有层面，包括连行政费用都会裁掉，比如说员工的福利等\n（2）大厂，经济性的裁员，提前预示到了未来会有什么风险，会有什么趋势的变化，前置的进行成本优化。整个成本结构被拉出来，人力成本的优化只是其中的一部分\n定下了裁员目标后，会盘点业务，然后决定部门需要裁员的比例，这个时候可能会出现频繁的汇报，写\nPPT 描述你在做什么，产出了什么价值等\n另外，裁员的决策往往没有一个很长周期的准备和测试，有时候就只是上面突然把这个指令传达到\nhr\n的一句话，然后立马开始了 “约谈 -&gt; 赔偿 -&gt; 走人” 等一系列流程；所以可能会出现前脚刚给五星，后脚就立马裁人；前面还在疯狂招人，后面就开始冻结\nhc 并开始优化\n被裁了怎么办\n首先要对大环境有一个判断，就是自己所处的专业的人才供需情况，裁员初始阶段会存在大量的人不愿意激降身价的情况，趁着这个时候市场上的供给端还比较充足，需要从你有经验的这个细分领域里边找到一个比较优质的岗位迅速地嵌进去\n说到底了还是供需关系，这也是为什么在知乎上相关的回答有很多建议，如果公司大趋势就是要裁员，且你所在的业务线有较大的风险，\n就争取做第一批被裁的\n那怎么市面上的工作的好坏？基本上，只要业务能够有良好的现金流，业务看上去比较靠谱，这些岗位普遍就好。像那种还依靠着融资的钱生存，还没有正向现金流的业务，并且在你可预期的未来，都不太可能有稳定的现金流，\n这种预判非常考验个人的判断力，同时也有运气成分在里面。\n比如说自动驾驶、新能源车这些它未来是趋势吗？一定是趋势，但是你不知道它什么时候能到那个节点。万一它是十年，而假如你在一个行业等十年，那这十年你是不是真的能积累下来一些经验？\n35 岁危机 / 中年危机\n为什么媒体一直把 35 岁锚定为中年危机的年龄？其实定 35\n这个坎是为了方便大众的认知和媒体的传播，真相是这个年龄段上下的都算是这个范围\n而这个年龄段会有的一些共性：上有老下有小，需要照看的东西更多了，放到工作上的精力会更少，同时学习能力和学习意愿普遍的会偏弱化一些 (尤其面对一些创新业务的时候)；\n当然，这是普遍意义，但不是绝对意义上的。因此，在面对一些经验价值没那么高的业务类型上，找一个年轻的更有冲劲的精力更充沛的，性价比会更高；而互联网又有多少经验价值高的业务呢？\n说白了就是只要你不是不可替代的，找到了更便宜好用的螺丝钉，就会把老的生锈螺丝钉摘掉，这么说可能会比较残酷，但目前看来事实就是如此\n如果说的更残酷一点，所谓的中年危机，就是钱不够，不够满足当前的需求和欲望，无法如预期水平地去照看你关注的那些人和事；面对收入的断崖式下跌，心理和生活上都难免会受到冲击\n被规训的大厂人\n规训这个词，从播客里听到的时候就觉得非常的妙，一般来说每个厂都有自己的一些文化，入乡就要随俗，而很多人从毕业开始就在一个大厂环境里面，里里外外地被规训，他认识世界的方式和做事的方式全部是依赖这个大厂形成的规则，但他意识不到是被规训的\n这种规训最大的问题在于，他内心的秩序和意义是来自于这个大厂的，你如果问他喜欢什么，做什么事情能沉浸有热忱，他突然会发现卡壳\n这样的人一旦离开，要面对的问题还是挺多的，尤其是心理上的；有个很好的比喻就是 “这就像在一个鱼缸里边圈养了\n50\n年的一个鲸鱼放归大海”，我们认为是自由，但是对他来说不是他认识世界的方式。这跟\nze ran 在 “程序员的悲哀是什么” 这个问题写下的回答：“大学毕业了很多年，还有种初入社会的疏离感”，感觉很相似\n戴某 demo\n在播客里更是一针见血的指出了这种现象的问题\n\n你在一个厂里边没有升到高管，又能做十几年，这本身代表着一些事情。其实那个脑子比较开放性的意愿比较多的，比较强的，普遍的会有很多想法，要么在大厂里边自己管一块业务，要么就蹦出来，可能成、可能不成，那是命运的问题；但是不会在一个大厂里面像做一辈子一样去做十几年，但是公司不开我，我可能做\n20\n年对吧，最终这个漏斗本身筛下来能做那么久的又不是高管的，是同一类人，相对追求确定性，安稳，开放性没那么足。专业上垂直，因为待得久，有一定的专业能力，但这个专业能力要依附于某个大平台\n\n在所谓的大厂裁员和中年危机双重 debuff\n的加持下，焦虑自然是难免的了，我们不禁会问，我们能做什么？如果离开大厂，是否就能消解掉这两个\ndebuff？\n不止一种叙事方式\n这部分不少案例来自刘飞的 “非大厂叙事” 系列播客，主要讲了生活更多的可能性，即除了在各个大厂反复横跳，或者是融资烧钱、做具备规模效应的创业以外，还存在的一些其他可能性，笼统地概括就是自己找到一个现金流比较好、不追求多大规模的生意，把眼前的事情做好，用内容支撑品牌，用播客里的话来说就是 “浙商思维”。\n而刘飞也提到了很多人不做这件事的原因，那就是信息差：“很多老同事老朋友他们非常缺乏安全感和有\n35\n岁焦虑，就是因为不知道。我觉得他们的能力他们的喜好想法其实挺适合做一些别的事情的，但是就是单纯不知道。因为他们看到的例子、我们在大厂里接触到例子，都是这种互联网大厂的玩法，和创业的拿融资烧钱创业的这种玩法，我觉得知道之后，可能真的是有很多可选的机会”\n那为什么大家眼里只能看到那种高举高打的创业？因为接收到的信息有偏。我们看很多商业分析，或者是自媒体的文章，很容易产生一个错觉：就是你创业你做互联网只能做大，你不做行业第一，DAU\n上亿，你不做市值十亿一百亿，你就根本下不去这个手；久而久之，我们看到的创业成功的模式似乎都是那种具备规模效应的模式，我们以为创业只有这一种模式\n与之相反的是，也有不少人从大厂出来，选择了一个自己比较清晰 (就是他知道这个事情的商业逻辑是什么样)，然后同时又有一个比较好的现金流的事情，这就是一个比较好的生意；这个生意如果从互联网视角看，好像没那么性感，但是这个生意它会让你觉得好像更踏实一些，更有积累一些\n而本质上，创业也是做生意，只不过创业打上了一个理想主义的标签，最后你追求的还是一个能不能转起来的生意，而不是说只是把\nDAU 烧起来，剩下的什么都不管了\n下面主要摘录自刘飞在非大厂叙事里提到了几个小生意，里面有基于轻量级开发团队做的效率工具，小工具\napp，也有涉及实体的二手店和淘宝店，总之就是像我们展示了更多的可能性，这里引用刘飞的一段话\n\n之前我们做互联网很多时候做的，我理解它更多的是效率平台，效率平台做的很多事情都是撮合，都是我把供需匹配好。然后我在这个过程中有的是用人力，有的是用运营，有的是用算法把这个效率最大化。但是现在消费市场变化了之后，会有很多垂直的领域，小众的领域大家追求的是内容产品或者内容品牌，就这些是面向垂直的用户，小众群体。\n就比如说某些是消费品，包括刚才我提到的像 flomo，你很难说 flomo\n它是一个可以对标备忘录、对标印象笔记、或者对标飞书的一个协作工具。它就是一个很小众的工具，它是做知识管理。它不是一个大众的产品，它有自己的用户群体，它自己的用户群体有非常好的复购率，这个用户群体有非常强的粘性，而且非常认同这个产品。就类似这样的产品，还有像那个多抓鱼，还有像那个三顿半，这些产品其实它们都是类似的这种属性的。就我有自己的目标用户垂直领域，然后它们有比较好的复购，然后我持续地深耕做这些用户\n\n下面的几个故事，会言简意赅地摘录一些笔者关注的重点，推荐去听一下原播客，也许每个人能听到的重点都不一样\nflomo\nflomo\n是一个快速记录想法的笔记软件，创始人是少楠和 Lightory，与其他工具简单的说明书不同，flomo\n提供了一个 flomo\n101，详细的讲了这个工具背后所代表的理念，其实就是在通过内容讲一个品牌故事。\n少楠在刘飞的三五环做客了非常多次，下面的一些内容也是两人的播客里聊到的，\n相关的一些播客链接也会放到最后\n为什么做 flomo\n少楠在提到离职创业时，辞去提供一大笔年薪的工作，的确会比较恐惧，尤其是上有老下有小的情况下；\n但是转念一想，如果这个事情等到 40\n多岁再来做，成本又会变得特别高；而相比于这个，在相当长的时间里都在用五年前或七年前的知识来解决问题让他更恐惧，因为这相当于他的知识体系不更新了，而他的知识是最值钱的东西\n《黑天鹅》这本书提到：“毒品，碳水化合物，月薪，是三种最有害的瘾”，月薪之所以有害，是因为无论做的好坏，月薪还是那么多，最多绩效会差一点；\n长期下来，会让人有一种麻痹感和错觉，就是我不用怎么努力，更新自己的知识体系，也能持续拿到这样的薪水\n虽然做 flomo\n后拿到的钱变少了，但播客里有个观点是这样的，就是 “每一笔钱是不平等的”：大厂里拿到的钱，跟创业拿到的钱，即使是等额的，但是背后代表的东西是不一样的；大厂那笔钱的背后是你的交换出去的时间和价值；而创业那笔钱背后是你创造的资产，能够持续创造现金流；而如果按照创业公司\nPE 来算的话，从创业拿到的工资折算回去，资产的价值还是比较高的\n但是，我觉得大厂积累下来的经验和履历也是一种资产，虽然大厂履历带来的光环越来越暗淡，\n需要警惕的是在边缘部门做一些边缘事情，积累不下任何经验\n焦虑\n收入焦虑，某种程度上也是知识焦虑，要多问下自己的知识资产是在消耗还是积累，有没有变现手段？比如说自己的知识体系是否已经非常老旧了，或者垂直性非常强，只有在某个领域或某个公司才适用\n做 flomo 的一些经验\n效率产品，背后都需要有一些被验证过有效的理念，且这些理念要传递给用户 (通过内容创作构造出品牌)\n用户增长，往往同时需要考虑 2\n个方面，一是讲好品牌故事 (烧钱投广告增长不可持续)，二是做好投放渠道 (内容渠道和广告渠道，酒香也怕巷子深)\n虽然 ab\n实验是一种比较科学的决策方法，但是很多决策没法拿短期的数据来衡量，比如说把品牌种在更多的用户心里；打造品牌往往需要做一个长期贪婪的事情，少楠做这件事的方式就是写很多文章和内容，教用户怎么使用，甚至是改变思维方式\n谜底科技\n开发 flomo 的是一个轻量级的团队 (初期只有 2\n个人，现在应该也是个位数)，而谜底科技则是另一个这样的团队，创始人之一\n61\n在很早就看到了很多国外的的独立开发者：一个设计师，一个开发去搭档，很多\nMac/IOS 上优秀的 App 其实都是这样一个人或两个人的作品，\n那时候心里的目标就是成为一个这样的团队\n一直想做一个中国版的 product hunt，但是效果一直不达预期；反倒是一些\nsite project，在数据上的表现很不错，改变了 61 只想做 big\n的想法，更专注在一些 small but effective\n的东西上；这个过程的关键是看数据，数据是不会骗人的，所以这个过程一定要有一套评估的系统，能直观看到数据\n两个小工具类产品，offscreen 和谜底时钟，都是打磨了一年后，随着 IOS\n上线带来的小组件特性而火了起来，具有一定的运气成分，但同时也是为这次运气做了一年的默默耕耘后，才能在机会来的时候快速抓住\n小团队做增长的一些路径：找到各种媒体上的 KOL，如果东西是好的，KOL\n也是会愿意去推荐的（follower\n会喜欢），同时海外市场也是一个有潜力的市场；提到了增长，在这一期的播客里也\ncue\n到了少楠之前的一些感悟：第一次创业关注的是体验，第二次创业关注的是增长\nflomo\n和谜底科技的几款产品都有一个共同特点，就是为了控制成本，不会把事情搞得很复杂，无论是团队规模或者是产品功能；谜底科技的\nApp\n基本都没有服务端，选型目的是想让团队比较轻量级，重运营和重服务端的都不做；即使不做这些，依然可以挑出很多有商业价值、有前景的项目，写这篇文章也发现谜底科技的另一款小工具谜底黑胶又在\napp store 登顶了\n所以，不要特别瞧不起小的东西，不一定就要做一个特别大的公司，比较好地解决垂直领域的一个问题，也只很值得去做的，比如说小宇宙；古典产品也许在慢慢回归\n山茶子\n上面两个案例都是偏互联网工具类的产品，后面的两个例子则是涉及到了实体类的小生意。山茶子是牙刷味做的一个茶叶小品牌，在这一期能够看到牙刷味一个人怎么把整个茶叶品牌搭起来，以及除了打工和创业以之外的第三条路\n在牙刷味持续工作过程中，也在持续地想自己想要做什么，工作只是单纯地为了生活；决定做做茶叶时，开始做的时候也没有想得很清楚；因为有时候如果充分调研，可能就不会去做了，会觉得这个事情太大了。\n与前面的提到的两个互联网产品不同，互联网产品的边际成本基本为\n0，但是涉及到实体的生意还是存在实打实的原材料成本的，因此也需要考虑供应链，以这个茶叶生意为例，基本的供应链就是，茶叶原料\n-&gt; 代加工 -&gt; 包装 -&gt; 推广 -&gt; 发货\n具体搭建茶叶品牌的流程就是先去做了竞品调研，买了市场上这个领域所有的产品，只要相同价格下品质做的更好就可以，但是利润空间会更小；原材料在杭州龙井茶村找，代加工厂在\n1688\n上找，但是前期规模小，不一定能找到很好的工厂；这一系列事情其实牵涉了在播客里反复被提及的浙商思维：先去做眼前的事情，尝试把眼前的事情做好\n虽然如此，实际中也存在很多不充分调研，考虑不周全而倒下的创业者，只是大家没有看到，这里面感觉还是有一定的运气成分\n这里有一个理念跟前面的两个案例很相似，那就是也不会一开始就考虑很大的团队，除非招一个人能赚出他自己的工资\n做生意不像在公司，这家不行就换下一家，但是在公司不行，对接的研发 / 产品不行，很难换一个\n谈到了更多可能性，警惕自己给自己贴标签，如果因为你比较成功地做过一些事情，其他人给你贴上了你擅长这件事的标签，不要误认为自己只能做这件事，需要接触和尝试，不一定每种可能性都要尝试，但是要从接触开始\n环境对人的影响是很大的，人的生活状态，价值观和叙事方式都是由环境塑造的，在环境里很容易被周围的人影响，\n举了一个比较极端的例子：比如说这公司都快倒了。但是只要这公司里比如说公司租的楼够好，公司里面的员工大家每天还是按时上班，甚至还在加班。还是比较有昂扬斗志的。开会大家还是在\nargue 加在这争来争去的。那你就会觉得这个公司好像没啥问题\n最后，还提到了一种很有趣生活态度：把生活当做游戏，自己是主角，其他人都是\nNPC；不同 NPC\n可能会给你带来不同的线索，体验不同的支线任务；但是能奖励你的那个事情才是主线，找到能给你正反馈的主线和\nNPC\n多抓鱼\n多抓鱼是一家二手书店，创始人之一猫助在三五环也做客过，里面聊到创办多抓鱼的经历以及多抓鱼的一些基本模式\n猫助最早想开二手店，可以追溯到大学的时候；当时没有去大厂，而是去了刚成立的知乎，因为觉得去了创业公司，至少能看到一个企业是怎么做起来的，将来就可以去做自己的企业了\n最早多抓鱼的 MVP\n版本是：“我们找了一些可能有意愿卖二手书的朋友，然后说如果你想卖书你就 @群主，群主会给你约快递，最后群主会单独把钱打给你。群主收到书之后会把书名写在 Excel 表上面，然后再发到群里说，我们今天上新了，快来挑挑都有什么你喜欢的书。这就是最早的多抓鱼”\n选择实体店 or\n线上？当前多抓鱼两个模式都有，实体店偏本地，租金贵但没有邮费，退货率也非常低，就基本没有退货了；线上是全国的匹配，调用了物流；非标的东西 (没有明确规格和型号的商品) 只在线下售卖，但是偏标准品的东西还是说偏线上全国消化 (当前主要是书，衣服和小数码)\n世界上很多很多的生存方式和路径，不要只看到一种路径：从 p4\n到 p6，p6 到 p8\nc2b2c (多抓鱼) 相比于 c2c (闲鱼),\n能够提高卖家的效率，保证买家的质量，而这两个原因也是猫助选择了多抓鱼现有的模式\n在多抓鱼，没有什么纯粹的管理工作，上到 CEO\n下到一个实习生，全都是要搬砖的\n跳出社会预期：老耿 (70 后) 以前是豆瓣的 VP,\n对自己的预期特别低，而他就活的状态像个少年，什么事情都能做；其实也是我们常说的\nego 小的一种体现\n围城外的巨人\n现实中，很多事情都是一座围城，城外的人想进去，城里的人想出去\n如果说大厂是一座围城，那上面都是围城里的人出去并且活得还不错的故事，但是城外都是毫无风险的吗？围城外是否会存在让人回想起被支配的恐惧的 “巨人”？\n当然存在，上面看到的都是成功的或者说目前公司还存在的案例，必然存在一个幸存者偏差，死掉的项目是没人关注的。而看到很多成功项目时，我们很容易幻想：这些事情自己好像也有能力做，做成也不难，却忽略了媒体报导的偏差性、人和人的差异性，以及生活的随机性，很容易会因为当前手上或烦人或无聊工作，而脑子一热冲出围城，然后就会面临着现实生活这个 “巨人” 的一顿毒打\n在三五环的一期播客里也提到了这一现象：很多大厂离职出去的员工会面临着 “返贫” 的现象，原因是不懂得打理资产；资产最终会回流到会管理他们的人身上；而打理资产的能力，无论是上学或者上班，都没有人教我们；上学是在学教科书里的知识，上班更多学习的是专业能力和管理能力\n什么是打理资产的能力？无论是投资金融类资产，投资其他人的项目，还是创业做个小生意，大致含义就是能够看到完整的商业逻辑，看到钱从哪里来，\n怎么保证现金流\n对于普通人来说，要意识到自己在公司学习到的能力的局限性，绝大部分人都是一个可弃用的螺丝钉，对于全貌未必看得比较准确；哪怕看到了全貌，历史经验也是具备局限性的，因为很多事情会受到经济周期的影响，在经济上行时总结的经验，未必就适用于所有的时期\n我们常常说，大学是一座象牙塔；其实大厂也算是一座象牙塔 (尤其是离一线销售岗位较远的岗位)，大部分人工作中每天接触到的清一色的都是相同的人和事，不需要关注公司的现金流情况 (大多数情况也没有这样的机会)，只知道广告能有收入，却不知道广告里的每个细分行业的生意是如何运转的，广告主为什么愿意花钱投广告；只知道商家的\nroi&gt;1\n看起来好像就是正的，却不知道刨除商品、人工、场地、营销等成本，可能就是个赔钱生意；这些东西，围城里没有人会教我们。\n即刻上刘玮冬发的不少动态，能更真实地反映出一个普通生意人在实际创业时面临的问题和烦恼，能让我们更清楚看到城外巨人的狰狞面目。其中提到的一个现象，也是很多从大厂出去创业的人面临的典型问题：想通过互联网思维去改变一切，却发现成本控制不了，现金流转不起来 ;\n其实互联网改造传统行业并没有那么容易\n\n做电商这几年，也见过很多 “精英人士” 入局做电商，可能是我段位不行，接触的都是精英人士做电商失败或者被毒打的案例。这些失败案例的步骤也比较一致\n第一步：精英人士泛指：名牌大学海龟精英 / 互联网大厂精英 / 外企 500 强白领精英 / 有钱的用过好东西的富二代 / 有点审美天赋的艺术精英\n第二步：锚定一个品类，可能是宠物 / 护肤 / 美妆 / 服装 / 电子产品 / 运动 / 食品 / 母婴产品\n等等\n第三步：做这些品类的产品调研，用了该品类 TOP10 的产品之后，发出各种吐槽：这是人吃的么？人用的么？质量那么差？这么丑的衣服是人穿的么？狗都不会用的东西........\n第四步：由此认为该品类机会大有可为，认为自己进去一定能颠覆品类，成为该品类第一，并且认为自己将是国货崛起的代言人，自己有着巨大的使命，一定要让中国人都用上好东西\n第五步：自诩为乔布斯化身，开始非常极致的产品开发，选最好的材料 / 面料 / 食材，各个环节都用最好的工艺，各种版本，包材，样本打了几十个版本，一次又一次推翻，跑遍了全国各种代工厂\n第六步：在产品设计上罗永浩附体，字体大小，颜色，VI，排版，行间距，字间距，深浅调了几十版，学习张小龙，每一个像素都抠的非常细\n第七步：半年或一年后，产品正式上线，由于产品各种选取最优质的材料，那个成本也高出天际，然后那个产品售价超出主流市场价格带一大截。\n第八步：产品好是好，但因为太贵，老百姓买不起，所以销量寥寥无几，非常讽刺，目标是希望国人用上好东西，但结果国人用不起。\n第九步：东西太贵，只能抓精准富裕中产精英群体，但又没有特别渠道，手段，抓不到这群用户，花钱做了很多广告，转化率极差。\n第十步：找各个主播，渠道去推，主播渠道都表示东西很好，但太贵了，推不动，能不能售价降 50%，咬咬牙同意了，爆了几波销量后发现卖一个亏一个，别人都说你家直播卖了好多钱，只有你知道，算上税，退货，邮费，主播分成，产品成本，你甚至还赔钱。\n第十一步：销量上不去，也没法融资，公司财务开始恶化，资金不断消耗，推广费用捉襟见肘，没钱开发二代，三代产品，开始各种暴躁，骂员工，员工合伙人纷纷离职\n第十二步：库存堆成山，卖不出去，各种供应商找上门来，找家里要钱 / 拿出工资储蓄 / 卖了房子之后终于把欠债还完了。\n第十三步：出来逢人就总结，自己的产品非常优秀，非常好，可惜中国老百姓不识货啊！\n\n接受焦虑，享受随机性\n读到这里，你会发现，长期呆在大厂里会里里外外被规训，也面临着所谓的中年危机；出去外面，面对着的是更加复杂的人和事，看到的都是成功的案例，具备较强的随机性和不可复制性，真正去做的时候可能一不小心就返贫，把过去十多年的积累一次清空。那我们能怎么办，似乎没有一个一劳永逸的方法？\n是的，生活本来就没有一个一劳永逸的答案。在大厂可能会被裁员，难道开公司当老板，就会永远盈利不倒闭么？我们看到上面的还算成功的非大厂的各种叙事，能活多长时间，可能连创始人也没法告诉你。生活本来就是混沌的，充满随机性的，无法预测的，过分追求安稳与确定性，有时候会很痛苦。\n《我们这一代人的困惑》是这么说的\n\n仔细想想，我们的一生好像都是在实现目标中挣扎着度过的。上初中的时候，老师告诉你，中考的淘汰率是最高的，只要闯过去，上了高中一切就好了。但上了高中的时候发现不是那么回事嘛，高中老师又说了啊，考上大学就进了天堂。于是你考上了大学，依然空虚迷茫各种草样年华，父母老师又告诉你，找到工作就好了。工作之后发现烦恼和忧虑依然都在，女朋友给你看马云的故事，告诉你等你事业有成就好了……\n你发现了吗，其实人这一辈子的每一个阶段都有新的痛苦和顾虑，周而复始，生生不息。绝对不会因为你考上大学，事业有成，迎娶了女神就从此\nhappily ever\nafter。但每一个阶段也有每一个阶段的快乐，无法替代。生活不是安徒生童话也不是好莱坞电影，从出生的那一刻起直到生命的尽头，都不存在什么节点，过去了之后一切幸福美满无忧无虑。\n\n接受人这一生都要与焦虑为伴的事实，不要与焦虑对抗、尝试解决或者抹除焦虑，也许会是更自然与平和的状态，因为\nwhat you resist\npersists，往往越抵抗，反噬可能会越严重；所以需要学会接受焦虑，学会与焦虑相处，因为很多事情，你接受了，就不会痛苦，或者说有了预期，当事情发生时就不会那么痛苦（人的快乐和痛苦往往都是来源于发生了预期以外的事情）\n但接受焦虑，并不意味着要躺平，因为焦虑往往就是有一些需要做的事情但是没有做，那件事在你的心底大声地发出嘲讽，让你愈发恐慌；所以往往什么都不做，会让你更加焦虑，因此焦虑时合理的手段也几乎只有一种：进一步做好你当前的事（工作、学习），并选择一个 “至少不是错的” 的方向去努力\n对于还在大厂的人，就是好好把手上的事情做好，提升自己的能力；至于升职这类事情，能力和运气都是必要条件，我们无法左右运气，但至少在能力上，我们还是具备主观能动性的\n而在本职工作以外，又有什么是值得我们去追求的呢？\n找到内生的信念\n无论是继续留在大厂或者是决定是否从大厂离开，都要想清楚，自己是为了什么去做这个事情的，或者说要找到内生的信念\n内生的信念，指的是你真的有你想要去做的事情；这种情况下，无论你是辞职还是创业，无论结果是好还是坏，普遍感知上会更好一些；因为这种情况下收到的正向的反馈是大于你获得的外在工资的。这个正反馈可以是你在外面做内容做得好，受到读者的好评；也可以是你在外面做一个品牌，你感受的一些成就感等等\n如果说你还没有找到这些东西，只在对比工作的状态和收到的工资的话，那确实在大厂整体肯定还是对比起来更舒适的，因为从大厂那种有一定的基建保障的环境中出来之后，会面对一个更混沌的外部系统，它资源整合调度做事的难度其实并没有比之前降低，只是对体感上你的好像自由决策，没人能管得了\n反之，如果没找到自己喜欢做的事情，被外部诱惑从某个大厂离开的人，经常会有落差，最后的情况都不会那么好\n因为如果你是被诱惑出来的，可能是觉得现在工作特别不开心，特别累，出来可能更开心一点；或者说外面有一个机会，这个机会给你承诺的很多，但是你没有看到这个风险。这种情况出来的话，很容易就会感觉到心力支撑不住，因为没有原来这种确定性和安全感了，在外面自己又没有特别想做的事情，那就确实会比较痛苦；所以有很多人就回去了，但是也有很多人回不去了\n差异化竞争\n很多情况下，我们应该摆脱竞争，竞争就必然会内卷，只要你跟别人做一样的事情，就一定会产生这种消耗性的竞争。你要想办法为自己找到一个差异的点，然后去做跟别人不一样的事情。我们常说的 “找到自己的那条赛道，在那条赛道上我们就是第一”，其实也是这么一回事\n如何找到自己的赛道？对不起，这也没有一个明确的答案，但是比较明确的是，我们需要看到更多的可能性，收集足够全的信息，然后自己做判断，而不是直接收集的是别人的结论，因为个人的差异性和生活的随机性决定了适合每个人的赛道是不一样的\n工作之余，可以尽量多地做低成本的尝试，万一哪个尝试成了，就可以比较从容地应对所谓的裁员和中年危机；以戴某 demo 为例，之前在业余做投资的事情，做顾问的事情已经做了好多年了，在离开公司的时候其实并不是一个很突兀的要转型，而是一个很自然的过程\n对于更多的人来说，可行的方案之一，也许是真诚的有品质的表达，因为这是一个高效的社交活动。当你把你的思考放出来去寻求回应的时候，很多看到并与你共频的人会给你那个回响 (念念不忘，必有回响),\n这比你挨个自己去找人这容易的太多了，而这些表达说不定会给你吸引来更多的可能性和机会；所以我一直认为有品质的写作、播客、视频等，都是一个高效的社交\n把自己当做资产\n这里其实是套用了孟岩写的文章的标题：把自己当作资产，\n里面提到了比较核心的一个概念：人力资本，简单来说，就是把人当做资产，这个资产未来会不断释放现金流；这一点在作客组织进化论的播客中也有提及\n\n我们把人力资本释放的现金流，不断通过金融市场，转换为金融资本。随着时间的流逝，我们的金融投资获得不错的回报，金融资本也慢慢增长成为我们总财富的主要部分。\n年轻时，尽管我们完全没有意识到，我们可能远比自己想象的有钱。只是在这个时候，财富更多以人力资本的形式存在。\n因为拥有相对安全的人力资本资产，这就意味着年轻人可以比年长者把更多的金融资产投资于更高风险的资产，比如股票，甚至可以包括少量的杠杆和债务。原因很简单，这时候的损失，无论是\n5 % 还是\n30%，尽管当下看起来很多，但在我们总资产（包含人力资本）的盘子里，实在微不足道。\n\n在谈工作的时候，大家很容易会把着眼点放在收入上，这无可厚非。但它带来的副作用是你过于集中在这个点上，你就会忽略你整个职业生涯里，真正应该积累的可能是视野、知识能力、品格、人脉、口碑等。你如果真正真是并积累它，钱会找到你\n另外，无论你所在的企业如何，都要记住，企业愿意聘用你，他们花钱购买的是你的专业能力，这是一切的前提；而你把着眼点反过来放在那个纯收益上，有可能忽视了真正要积累的东西，反而不一定会有一个很好的结果，因为这样下来竞争力可能会越来越低，同时有可能失去了你自己的自我认同\n很多大厂里的员工，其实并没有在大厂里边真正把自己当成一个产品或者一个资产去创造什么东西，更多的是把焦点盯在工资和收益上，但把焦点盯在你的经济收益和盯在创造上，你的过程和结果可能都会不一样，尤其是你的体感。而飞书建议大家写的个人使用说明书，本质上也是在建议大家把自己当做一个产品或者说资产，这里我觉得可以用《The Almanack of\nNaval》里的一句话经常反问自己\n\nAm I productizing it? Am I scaling it? Am I scaling with labor or\nwith capital or with code or with media?\n\n享受随机性\n放眼互联网公司的发展或者个人的成长，我们会发现，一个绕不多去的话题就是运气，或者说生活的随机性；什么 “我的成功你也能复制”，都是在扯犊子，忽略了时代的不同和生活的随机性\n我们生活中遇到的事情基本可以分为三类，第一类基本由能力决定，比如说考试；第二类纯粹由随机性决定，比如布朗运动；第三类，也是我们最常遇到的，由能力和随机性共同决定，比如创业、投资、恋爱或是梦想。\n我们最大的问题是，面临生活中大部分的事情时，往往会自动地归为第一类，而忽略了随机性对结果的影响。大到互联网的发展，小到一个个体去选择自己的职业生涯，其实很多都是随机性决定的；有的时候我们事后因为人性的原因，各喜欢各种归因，总是在这找原因，其实没啥可归纳的，也没什么指导意义，因为时代在变，大环境在变\n当你意识到世界很多事情都有随机性，甚至是被随机性主导的；当你深知这件事情的随机性也许永远不会青睐你，你是选择继续坚持下去，还是选择躺平呢？\n而一个人最宝贵的财富，也许就是在做一件事情时，能够清楚地意识到随机性和运气在其中的占比，并心平气和地接受这个事实\n需要意识到的另一个事实是：随机性会带来坏的事情，但也可能会带来好的事情，而在我们努力没达到一定程度前，我们连面对好的随机性的资格都没有，也可能会随时被坏运气冲垮；这也意味着需要做好\n2 个准备\n（1）抓住好运气的能力\n（2）对冲坏运气带来的风险\n抓住好运气的能力，意味着我们要不断地提高自己的各项能力，即使现阶段不能给你带来即刻的回报；比如说现在要提拔你，你觉得你的管理能力是否能够\nhold\n住现有的团队？你的专业能力是否能带领整个团队攻克当前遇到的问题？比如说\nweb3\n现在如火如荼，这里面是否真的有好的项目，你是否有能力去分辨其中各个项目的价值，而不是去当绿油油的韭菜？\n对冲坏运气的能力，意味着我们要能够尽量识别出未来可能发生的风险，比如说工作上的风险，身体健康的风险，经济上的风险等发生时，你有做好兜底的方案吗？\n面对随机性，有时候会让人绝望，因为即使你做了准备，对最终的结果可能也是毫无意义的；但面对随机性，我们唯一能做的也是好好地活在当下 (虽然被说烂了，但的确是真理)，放下一些不必要的执念和无意义的比较，与身边的人愉快相处，认真安排好每一天的活动，用心去感受每一天的心境\n因为一直瞻前顾后，容易陷入对过去的后悔和对未来的担忧中，这种焦虑带来的是所有非工作状态的生活都被填满；尤其当你做的是一个寻求机会的事情时，准确地规划是很难的，当你试图用理性来假设和分析时，很多时候回过头再看，这些假设跟真实的差异会很大，但这样的分析往往会令人陷入悲观主义，因为往往假设会越做越谨慎，似乎一定要万无一失才能开始行动\n既然随机性不可避免，而过分追求确定性可能会让我们瞻前顾后、步步惊心，还不如学着接受并享受生活的随机性；既然预定要飞向意大利的航班，最终有可能让你降落在荷兰，还不如好好享受荷兰的风光，对自己说一句：欢迎来到荷兰\n最后，借用《夜航西飞》里的一段话来回答开头的问题，虽然前方的\n35 岁路口被迷雾遮挡，但是相信走进去，就会云开雾散～\n\n我学会了如果你必须离开一个地方，一个你曾经住过、爱过、深埋着你所有过往的地方，无论以何种方式离开，都不要慢慢离开，要尽你所能决绝地离开，永远不要回头，也永远不要相信过去的时光才是更好的，因为它们已经消亡。过去的岁月看来安全无害，被轻易跨越，而未来藏在迷雾之中，隔着距离，叫人看来胆怯。但当你踏足其中，就会云开雾散。我学会了这一点，但就像所有人一样，待到学会，为时太晚\n\n\n文中相关的一些播客\n\n大厂裁员的原因、方案和连锁反应\n\n离开大厂的中年人，何去何从\n\n跟少楠聊聊大厂和创业，以及如今的独立开发者生活\n\n跟少楠聊聊做\nflomo 的新体会，以及什么是 indie\n\n跟\n61 聊聊他曲折又坦然的创业故事\n\n跟牙刷味聊聊打工和创业之外的第三条路\n\n跟多抓鱼的猫助聊聊二手生意和创业态度\n\n跟浩翔聊聊离开大厂的「返贫」现象和背后的思考\n\n识别运气，并享受生活的随机性\n\n愿我们都做一条脱钩的鱼\n\n孟岩：最好的投资，是投资自己\n\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"使用 sklearn 优雅地进行数据挖掘","url":"/2017/01/16/%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","content":"本文转载自 http://www.cnblogs.com/jasonfreak/p/5448462.html，\n主要讲述如何通过 sklearn 完成数据挖掘中的一个完整的流程。\n\n数据挖掘的步骤\n数据挖掘通常包括数据采集，数据分析，特征工程，训练模型，模型评估等步骤。使用 sklearn 工具可以方便地进行特征工程和模型训练工作，在《使用 sklearn 做单机特征工程》中，我们最后留下了一些疑问：特征处理类都有三个方法\nfit、transform 和 fit_transform，而\nfit 方法居然和模型训练方法 fit\n同名（不光同名，参数列表都一样），这难道都是巧合？\n显然，这不是巧合，这正是 sklearn 的设计风格。我们能够更加优雅地使用 sklearn 进行特征工程和模型训练工作。此时，不妨从一个基本的数据挖掘场景入手：\n\n\n基本数据挖掘场景\n\n我们使用 sklearn\n进行虚线框内的工作（sklearn 也可以进行文本特征提取）。通过分析 sklearn\n源码，我们可以看到除训练，预测和评估以外，处理其他工作的类都实现了 3 个方法：fit、transform 和 fit_transform。从命名中可以看到，fit_transform 方法是先调用 fit 然后调用 transform，我们只需要关注\nfit 方法和 transform 方法即可。\ntransform\n方法主要用来对特征进行转换。从可利用信息的角度来说，转换分为无信息转换和有信息转换。无信息转换是指不利用任何其他信息进行转换，比如指数、对数函数转换等。有信息转换从是否利用目标值向量又可分为无监督转换和有监督转换。无监督转换指只利用特征的统计信息的转换，统计信息包括均值、标准差、边界等等，比如标准化、PCA 法降维等。有监督转换指既利用了特征信息又利用了目标值信息的转换，比如通过模型选择特征、LDA 法降维等。通过总结常用的转换类，我们得到下表：\n\n\n\n\n\n\n\n\n\n\n\n\n包\n类\n参数列表\n类别\n fit 方法有用\n说明\n\n\n\n\n\n sklearn.preprocessing\nStandardScaler\n 特征\n无监督\n Y\n 标准化\n\n\n\n sklearn.preprocessing\nMinMaxScaler\n 特征\n无监督\n Y\n 区间缩放\n\n\n\n sklearn.preprocessing\nBinarizer\n 特征\n无信息\n N\n 定量特征二值化\n\n\n\n sklearn.preprocessing\nOneHotEncoder\n 特征\n无监督\n Y\n 定性特征编码\n\n\n\n sklearn.preprocessing\nImputer\n 特征\n无监督\n Y\n 缺失值计算\n\n\n\n sklearn.preprocessing\nPolynomialFeatures\n 特征\n无信息\n N\n 多项式变换（fit 方法仅仅生成了多项式的表达式）\n\n\n\nsklearn.preprocessing\nFunctionTransformer\n 特征\n无信息\n N\n 自定义函数变换（自定义函数在 transform 方法中调用）\n\n\n\nsklearn.feature_selection\nVarianceThreshold\n 特征\n无监督\n Y\n 方差选择法\n\n\n\n sklearn.feature_selection\nSelectKBest\n 特征 / 特征 + 目标值\n无监督 / 有监督\n Y\n 自定义特征评分选择法\n\n\n\n sklearn.feature_selection\nSelectKBest+chi2\n 特征 + 目标值\n有监督\n Y\n 卡方检验选择法\n\n\n\n sklearn.feature_selection\nRFE\n 特征 + 目标值\n有监督\n Y\n 递归特征消除法\n\n\n\n sklearn.feature_selection\nSelectFromModel\n 特征 + 目标值\n有监督\n Y\n 自定义模型训练选择法\n\n\n\n sklearn.decomposition\nPCA\n 特征\n无监督\n Y\nPCA 降维\n\n\n\n sklearn.lda\nLDA\n 特征 + 目标值\n有监督\n Y\nLDA 降维\n\n\n\n\n不难看到，只有有信息的转换类的 fit 方法才实际有用，显然 fit 方法的主要工作是获取特征信息和目标值信息，在这点上，fit 方法和模型训练时的 fit 方法就能够联系在一起了：都是通过分析特征和目标值，提取有价值的信息，对于转换类来说是某些统计量，对于模型来说可能是特征的权值系数等。另外，只有有监督的转换类的 fit 和 transform 方法才需要特征和目标值两个参数。fit 方法无用不代表其没实现，而是除合法性校验以外，其并没有对特征和目标值进行任何处理，Normalizer 的 fit 方法实现如下：\ndef fit(self, X, y=None):        \"\"\"Do nothing and return the estimator unchanged        This method is just there to implement the usual API and hence        work in pipelines.        \"\"\"        X = check_array(X, accept_sparse='csr')        return self\n基于这些特征处理工作都有共同的方法，那么试想可不可以将他们组合在一起？在本文假设的场景中，我们可以看到这些工作的组合形式有两种：流水线式和并行式。基于流水线组合的工作需要依次进行，前一个工作的输出是后一个工作的输入；基于并行式的工作可以同时进行，其使用同样的输入，所有工作完成后将各自的输出合并之后输出。sklearn 提供了包 pipeline 来完成流水线式和并行式的工作。\n数据初貌\n在此，我们仍然使用 IRIS 数据集来进行说明。为了适应提出的场景，对原数据集需要稍微加工：\nfrom numpy import hstack, vstack, array, median, nanfrom numpy.random import choicefrom sklearn.datasets import load_iris#特征矩阵加工#使用vstack增加一行含缺失值的样本(nan, nan, nan, nan)#使用hstack增加一列表示花的颜色（0-白、1-黄、2-红），花的颜色是随机的，意味着颜色并不影响花的分类iris.data = hstack((choice([0, 1, 2], size=iris.data.shape[0]+1).reshape(-1,1), vstack((iris.data, array([nan, nan, nan, nan]).reshape(1,-1)))))#目标值向量加工#增加一个目标值，对应含缺失值的样本，值为众数iris.target = hstack((iris.target, array([median(iris.target)])))\n关键技术\n并行处理，流水线处理，自动化调参，持久化是使用 sklearn 优雅地进行数据挖掘的核心。并行处理和流水线处理将多个特征处理工作，甚至包括模型训练工作组合成一个工作（从代码的角度来说，即将多个对象组合成了一个对象）。在组合的前提下，自动化调参技术帮我们省去了人工调参的反锁。训练好的模型是贮存在内存中的数据，持久化能够将这些数据保存在文件系统中，之后使用时无需再进行训练，直接从文件系统中加载即可。\n并行技术\n并行处理使得多个特征处理工作能够并行地进行。根据对特征矩阵的读取方式不同，可分为整体并行处理和部分并行处理。整体并行处理，即并行处理的每个工作的输入都是特征矩阵的整体；部分并行处理，即可定义每个工作需要输入的特征矩阵的列。\n#### 整体并行处理\npipeline 包提供了 FeatureUnion 类来进行整体并行处理：\nfrom numpy import log1pfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.pipeline import FeatureUnion#新建将整体特征矩阵进行对数函数转换的对象step2_1 = ('ToLog', FunctionTransformer(log1p))#新建将整体特征矩阵进行二值化类的对象step2_2 = ('ToBinary', Binarizer())#新建整体并行处理对象#该对象也有fit和transform方法，fit和transform方法均是并行地调用需要并行处理的对象的fit和transform方法#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象step2 = ('FeatureUnion', FeatureUnion(transformer_list=[step2_1, step2_2, step2_3]))\n部分并行处理\n整体并行处理有其缺陷，在一些场景下，我们只需要对特征矩阵的某些列进行转换，而不是所有列。pipeline 并没有提供相应的类（仅 OneHotEncoder 类实现了该功能），需要我们在 FeatureUnion 的基础上进行优化：\nfrom sklearn.pipeline import FeatureUnion, _fit_one_transformer, _fit_transform_one, _transform_one from sklearn.externals.joblib import Parallel, delayedfrom scipy import sparseimport numpy as np#部分并行处理，继承FeatureUnionclass FeatureUnionExt(FeatureUnion):    #相比FeatureUnion，多了idx_list参数，其表示每个并行工作需要读取的特征矩阵的列    def __init__(self, transformer_list, idx_list, n_jobs=1, transformer_weights=None):        self.idx_list = idx_list        FeatureUnion.__init__(self, transformer_list=map(lambda trans:(trans[0], trans[1]), transformer_list), n_jobs=n_jobs, transformer_weights=transformer_weights)    #由于只部分读取特征矩阵，方法fit需要重构    def fit(self, X, y=None):        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)        transformers = Parallel(n_jobs=self.n_jobs)(            #从特征矩阵中提取部分输入fit方法            delayed(_fit_one_transformer)(trans, X[:,idx], y)            for name, trans, idx in transformer_idx_list)        self._update_transformer_list(transformers)        return self    #由于只部分读取特征矩阵，方法fit_transform需要重构    def fit_transform(self, X, y=None, **fit_params):        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)        result = Parallel(n_jobs=self.n_jobs)(            #从特征矩阵中提取部分输入fit_transform方法            delayed(_fit_transform_one)(trans, name, X[:,idx], y,                                        self.transformer_weights, **fit_params)            for name, trans, idx in transformer_idx_list)        Xs, transformers = zip(*result)        self._update_transformer_list(transformers)        if any(sparse.issparse(f) for f in Xs):            Xs = sparse.hstack(Xs).tocsr()        else:            Xs = np.hstack(Xs)        return Xs    #由于只部分读取特征矩阵，方法transform需要重构    def transform(self, X):        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)        Xs = Parallel(n_jobs=self.n_jobs)(            #从特征矩阵中提取部分输入transform方法            delayed(_transform_one)(trans, name, X[:,idx], self.transformer_weights)            for name, trans, idx in transformer_idx_list)        if any(sparse.issparse(f) for f in Xs):            Xs = sparse.hstack(Xs).tocsr()        else:            Xs = np.hstack(Xs)        return Xs\n在本文提出的场景中，我们对特征矩阵的第 1 列（花的颜色）进行定性特征编码，对第 2、3、4 列进行对数函数转换，对第 5 列进行定量特征二值化处理。使用 FeatureUnionExt 类进行部分并行处理的代码如下：\nfrom numpy import log1pfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizer#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象#参数idx_list为相应的需要读取的特征矩阵的列step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))\n流水线处理\npipeline 包提供了 Pipeline 类来进行流水线处理。流水线上除最后一个工作以外，其他都要执行 fit_transform 方法，且上一个工作输出作为下一个工作的输入。最后一个工作必须实现 fit 方法，输入为上一个工作的输出；但是不限定一定有 transform 方法，因为流水线的最后一个工作可能是训练！\n根据本文提出的场景，结合并行处理，构建完整的流水线的代码如下：\nfrom numpy import log1pfrom sklearn.preprocessing import Imputerfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.decomposition import PCAfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipeline#新建计算缺失值的对象step1 = ('Imputer', Imputer())#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象，返回值为每个并行工作的输出的合并step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))#新建无量纲化对象step3 = ('MinMaxScaler', MinMaxScaler())#新建卡方校验选择特征的对象step4 = ('SelectKBest', SelectKBest(chi2, k=3))#新建PCA降维的对象step5 = ('PCA', PCA(n_components=2))#新建逻辑回归的对象，其为待训练的模型作为流水线的最后一步step6 = ('LogisticRegression', LogisticRegression(penalty='l2'))#新建流水线处理对象#参数steps为需要流水线处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象pipeline = Pipeline(steps=[step1, step2, step3, step4, step5, step6])\n自动化调参\n网格搜索为自动化调参的常见技术之一，grid_search 包提供了自动化调参的工具，包括 GridSearchCV 类。对组合好的对象进行训练以及调参的代码如下：\nfrom sklearn.grid_search import GridSearchCV#新建网格搜索对象#第一参数为待训练的模型 #param_grid为待调参数组成的网格，字典格式，键为参数名称（格式“对象名称__子对象名称__参数名称”），值为可取的参数值列表 grid_search = GridSearchCV(pipeline, param_grid={'FeatureUnionExt__ToBinary__threshold':[1.0, 2.0, 3.0, 4.0], 'LogisticRegression__C':[0.1, 0.2, 0.4, 0.8]})#训练以及调参grid_search.fit(iris.data, iris.target)\n持久化\nexternals.joblib 包提供了 dump 和 load 方法来持久化和加载内存数据：\n#持久化数据#第一个参数为内存中的对象#第二个参数为保存在文件系统中的名称#第三个参数为压缩级别，0为不压缩，3为合适的压缩级别dump(grid_search, 'grid_search.dmp', compress=3)#从文件系统中加载数据到内存中grid_search = load('grid_search.dmp')\n小结\n\n\n\n包\n类或方法\n说明\n\n\n\n\n sklearn.pipeline\nPipeline\n 流水线处理\n\n\n sklearn.pipeline\nFeatureUnion\n 并行处理\n\n\n sklearn.grid_search\nGridSearchCV\n 网格搜索调参\n\n\n externals.joblib\ndump\n 数据持久化\n\n\n externals.joblib\nload\n 从文件系统中加载数据至内存\n\n\n\n组合和持久化都会涉及 pickle 技术，在 sklearn 的技术文档中有说明，将 lambda 定义的函数作为 FunctionTransformer 的自定义转换函数将不能 pickle 化。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"以太网中的 MTU 与 MSS","url":"/2015/12/08/%E4%BB%A5%E5%A4%AA%E7%BD%91%E4%B8%AD%E7%9A%84MTU%E4%B8%8EMSS/","content":"以太网（Ethernet）最大的数据帧是 1518 字节。以太网帧的帧头的 14 字节和帧尾 CRC 校验 4 字节共占了 18 字节，剩下的承载上层协议的地方也就是 Data 域最大就只剩 1500 字节。这个值我们就把它称之为 MTU。MTU 的全称是 maximum\ntransmission\nunit（最大传输单元）。MTU 可以认为是网络层能够传输的最大 IP 包。\n\n而 MSS（Maximum segment\nsize）可以认为是传输层的概念，也就是 TCP 数据包每次能够传输的最大量。为了达到最佳的传输效能，TCP 协议在建立连接的时候通常要协商双方的 MSS 值，这个值 TCP 协议在实现的时候往往用 MTU 值代替（需要减去 IP 数据包包头的大小 20Bytes 和 TCP 数据段的包头 20Bytes）所以往往 MSS 为 1460。通讯双方会根据双方提供的 MSS 值得最小值确定为这次连接的最大 MSS 值。\nMSS 为 1460 是由 1500-20（IP 头）-20（TCP 头）计算出的。但是在实际场景下，TCP 包头中会带有 12 字节的选项 -- 时间戳（用户在发送每一个 TCP 报文的时候都放置一个时间戳，接受方在确认中返回这个时间戳值。发送方就可以根据这个时间戳来计算 RTT（往返传输时间 -- 发送端从发送 TCP 包开始到接收到它的立即响应所耗费的传输时间.）。从而使得 RTT 更加精确，减少不必要的重传。减低网络的负载。)\n这样，单个 TCP 包实际传输的最大量就缩减为 1448 字节。1448=1500-20（IP 头）-32（20 字节 TCP 头和 12 字节 TCP 选项时间戳）。\n问题来了：“每个 TCP 包在理论上应该能打包更多数据才对，但是实际场景下 TCP 传输为什么会以这个 1448 作为打包单位呢？”\n理论上，单个 TCP 包能打包的数据量远远多于 1448 字节，现在为了适应 MTU，只要在以太网上跑 TCP，系统就默认最大以 1448 字节打包 TCP。\n假如我们用更大的数据量来打包会有什么结果呢？\n答案是降低了传输效率。\n超过 MTU 的大包反而降低效率的原因如下：\nIP 层非常关心 MTU，因为 IP 层会根据 MTU 来决定是否把上层传下来的数据进行分片。就像一条运输线路的承载能力是有限的，碰到大东西要运输，只能把大东西拆开成为散件，分开运输，到达目的地之后还必须能再次组装起来。\n当两台远程 PC 互联的时候，它们的数据需要穿过很多的路由器和各种各样的网络媒介才能到达对端，网络中不同媒介的 MTU 各不相同，就好比一长段的水管，由不同粗细的水管组成（MTU 不同\n:)）通过这段水管最大水量就要由中间最细的水管决定。\n对于网络层的上层协议而言（我们以 TCP/IP 协议族为例）它们对水管粗细不在意，它们认为这个是网络层的事情。网络层 IP 协议会检查每个从上层协议下来的数据包的大小，并根据本机 MTU 的大小决定是否作 “分片” 处理。分片最大的坏处就是降低了传输性能，本来一次可以搞定的事情，分成多次搞定，所以在网络层更高一层（就是传输层）的实现中往往会对此加以注意！\n这个就是在以太网上，TCP 不发大包，反而发送 1448 小包的原因。只要这个值 TCP 才能对链路进行效能最高的利用。\n","categories":["杂"],"tags":["计算机网络"]},{"title":"使用 sklearn 做单机特征工程","url":"/2017/01/03/%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","content":"本文转载自 http://www.cnblogs.com/jasonfreak/p/5448385.html\n, 在某些部分会拓展补充一些内容，全文主要讲述有关特征工程中通常使用的方法以及在 sklearn 中的相关实现。\n\n特征工程是什么\n有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面：\n\n\n特征工程思维导图\n\n特征处理是特征工程的核心部分，sklearn 提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到 sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！\n本文中使用 sklearn 中的 IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS 数据集由 Fisher 在 1936 年整理，包含 4 个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris\nSetosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris\nVirginica（维吉尼亚鸢尾））。导入 IRIS 数据集的代码如下：\nfrom sklearn.datasets import load_iris#导入IRIS数据集iris = load_iris()#特征矩阵iris.data#目标向量iris.target\n数据预处理\n通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：\n\n不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。\n信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心 “及格” 或不 “及格”，那么需要将定量的考分，转换成 “1” 和 “0” 表示及格和未及格。二值化可以解决这一问题。\n定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有 N 种定性值，则将这一个特征扩展为 N 种特征，当原始特征值为第 i 种定性值时，第 i 个扩展特征赋值为 1，其他扩展特征赋值为 0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。\n存在缺失值：缺失值需要补充。\n信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。\n\n我们使用 sklearn 中的 preproccessing\n库来进行数据预处理，可以覆盖以上问题的解决方案。\n无量纲化\n无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和归一化。\n标准化一般指的是 0 均值标准化 (zero-mean\nnormalization) 也叫 z-score 标准化。z-score 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布，即均值为 0，标准差为 1。\n归一化一般指的是区间缩放法，利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如 [0,\n1] 等。\n标准化\n0 均值标准化需要计算特征的均值和标准差，公式表达为：\\[x' = \\frac{x - \\mu}{\\sigma}\\] 公式中的\n\\(\\mu\\) 为原始数据的期望， \\(\\sigma\\) 为原始数据的标准差。\n使用 preproccessing 库的 StandardScaler\n类对数据进行标准化的代码如下：\nfrom sklearn.preprocessing import StandardScaler#标准化，返回值为标准化后的数据StandardScaler().fit_transform(iris.data)\n标准化的原理比较复杂，它表示的是原始值与均值之间差多少个标准差，是一个相对值，所以也有去除量纲的功效。同时，它还带来两个附加的好处：均值为 0，标准差为 1。关于这两个属性的具体好处参考\nhttp://www.zhaokv.com/2016/01/normalization-and-standardization.html\n如下\n\n均值为 0 有什么好处呢？它可以使数据以 0 为中心左右分布（这不是废话嘛），而数据以 0 为中心左右分布会带来很多便利。比如在去中心化的数据上做 SVD 分解等价于在原始数据上做 PCA；机器学习中很多函数如 Sigmoid、Tanh、Softmax 等都以 0 为中心左右分布（不一定对称）。\n标准差为 1 有什么好处呢？这个更复杂一些。对于 \\(x_i, x_{i'}\\) 两点间距离，往往表示为\n\\[\\begin{align}D(x_i,x_{i'})=\\sum\\limits_{j=1}^pw_j\\cdot\nd_j(x_{ij},x_{i'j})(\\sum\\limits_{j=1}^pw_j=1)\\end{align}\\]\n其中 \\(d_j(x_{ij},x_{i'j})\\) 是属性\n\\(j\\) 两个点之间的距离，\\(w_j\\) 是该属性间距离在总距离中的权重 注意设\n\\(w_j=1,\\forall\nj\\) 并不能实现每个属性对最后的结果贡献度相同。对于给定的数据集，所有点对间距离的平均值是个定值，即\n\\[\\begin{align}\\bar{D}=\\frac{1}{N^2}\\sum\\limits_{i=1}^N\\sum\\limits_{i'=1}^ND(x_i,x_{i'})=\\sum\\limits_{j=1}^pw_j\\cdot\n\\bar{d}_j\\end{align}\\] 是个常数，其中 \\(\\bar{d}_j=\\frac{1}{N^2}\\sum\\limits_{i=1}^N\\sum\\limits_{i'=1}^Nd_j(x_{ij},\nx_{x'j})\\), 可见第 \\(j\\)\n个变量对最终整体平均距离的影响是 \\(w_j\\cdot\n\\bar{d}_j\\), 所以设 \\(w_j\\sim\n1/\\bar{d}_j\\)\n可以使所有属性对全体数据集平均距离的贡献相同。现在设 \\(d_j\\)\n为欧氏距离的平方，它是最常用的距离衡量方法之一，则有 \\[\\bar{d_j}=\\frac{1}{N^2}\\sum\\limits_{i=1}^N\\sum\\limits_{i'=1}^N(x_{ij}-x_{i'j})^2=2\\cdot\nvar_j\\] 其中 \\(var_j\\) 是 \\(Var(X_j)\\)\n样本估计，也就是说每个变量的重要程度正比于这个变量在这个数据集上的方差。如果我们让每一维变量的标准差都为 1（即方差都为 1），每维变量在计算距离的时候重要程度相同。\n\n因此在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用 PCA 技术进行降维的时候，往往采用标准化方法。\n归一化\n区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：\\[x' = \\frac{x - Min}{Max - Min}\\] 使用\npreproccessing 库的 MinMaxScaler\n类对数据进行区间缩放的代码如下：\nfrom sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data)\n归一化的依据非常简单，不同变量往往量纲不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。比如两个人体重差 10KG，身高差 0.02M，在衡量两个人的差别时体重的差距会把身高的差距完全掩盖，归一化之后就不会有这样的问题。\n除此之外，归一化后能够加快梯度下降求最优解的速度，如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征\n\\(X1\\) 和 \\(X2\\) 的区间相差非常大，\\(X1\\) 区间是 [0,2000]，\\(X2\\) 区间是 [1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走 “之字型” 路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。\n\n\n归一化对收敛的影响\n\n因此。在涉及到计算点与点之间的距离时，使用归一化或标准化都会对最后的结果有所提升，甚至会有质的区别。那在归一化与标准化之间应该如何选择呢？根据上面论述我们看到，如果把所有维度的变量一视同仁，在最后计算距离中发挥相同的作用应该选择标准化，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化。另外，标准化更适合现代嘈杂大数据场景。\n对定量特征二值化\n定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为 1，小于等于阈值的赋值为 0。使用\npreproccessing 库的 Binarizer\n类对数据进行二值化的代码如下：\nfrom sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data)\n对定性特征哑编码\n由于 IRIS 数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用\npreproccessing 库的 OneHotEncoder\n类对数据进行哑编码的代码如下：\nfrom sklearn.preprocessing import OneHotEncoder#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))\n缺失值计算\n由于 IRIS 数据集没有缺失值，故对数据集新增一个样本，4 个特征均赋值为 NaN，表示数据缺失。使用\npreproccessing 库的 Imputer\n类对数据进行缺失值计算的代码如下：\nfrom numpy import vstack, array, nanfrom sklearn.preprocessing import Imputer#缺失值计算，返回值为计算缺失值后的数据#参数missing_value为缺失值的表示形式，默认为NaN#参数strategy为缺失值填充方式，默认为mean（均值）Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))\n数据变换\n数据变换就是通过组合或变换方式将原来的特征转换为新的特征，常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。\n例如，输入的一个二维特征为 \\([a,\nb]\\), 则度为 2 的多项式特征为 $ [1, a, b, a^2, ab, b^2]$\n使用 preproccessing 库的 PolynomialFeatures\n类对数据进行多项式转换的代码如下：\nfrom sklearn.preprocessing import PolynomialFeatures#多项式转换#参数degree为度，默认值为2PolynomialFeatures().fit_transform(iris.data)\n基于单变元函数的数据变换可以使用一个统一的方式完成，使用\npreproccessing 库的 FunctionTransformer\n对数据进行对数函数转换的代码如下：\nfrom numpy import log1pfrom sklearn.preprocessing import FunctionTransformer#自定义转换函数为对数函数的数据变换#第一个参数是单变元函数FunctionTransformer(log1p).fit_transform(iris.data)\n小结\n\n\n\n\n\n\n\n\n类\n功能\n说明\n\n\n\n\n StandardScaler\n 无量纲化\n标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布\n\n\n MinMaxScaler\n 无量纲化\n区间缩放，基于最大最小值，将特征值转换到 [0, 1] 区间上\n\n\n Normalizer\n 归一化\n基于特征矩阵的行，将样本向量转换为 “单位向量”\n\n\nBinarizer\n 二值化\n基于给定阈值，将定量特征按阈值划分\n\n\n OneHotEncoder\n 哑编码\n将定性数据编码为定量数据\n\n\n Imputer\n 缺失值计算\n计算缺失值，缺失值可填充为均值等\n\n\n PolynomialFeatures\n 多项式数据转换\n多项式数据转换\n\n\n FunctionTransformer\n 自定义单元数据转换\n使用单变元的函数来转换数据\n\n\n\n特征选择\n当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：\n\n特征是否发散：如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\n特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。\n\n根据特征选择的形式又可以将特征选择方法分为 3 种：\n\nFilter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\nWrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\nEmbedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于 Filter 方法，但是是通过训练来确定特征的优劣。\n\n我们使用 sklearn 中的 feature_selection 库来进行特征选择。\nFilter\n方差选择法\n使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用\nfeature_selection 库的 VarianceThreshold\n类来选择特征的代码如下： from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data)\n相关系数法\n使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用\nfeature_selection 库的 SelectKBest\n类结合相关系数来选择特征的代码如下：\nfrom sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)\n卡方检验\n经典的卡方检验是检验定性自变量对定性因变量的相关性。具体的意义可参考这篇文章卡方检验原理及应用。用\nfeature_selection 库的 SelectKBest\n类结合卡方检验来选择特征的代码如下\nfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n互信息法\n经典的互信息也是变量间相互依赖性的量度，互信息计算公式如下： \\[\\begin{align} I(X,Y) = \\sum_{x \\in X}\\sum_{y \\in\nY}p(x,y)log\\frac{p(x,y)}{p(x)p(y)} \\end{align}\\]\n为了处理定量数据，最大信息系数法被提出，使用\nfeature_selection 库的 SelectKBest\n类结合最大信息系数法来选择特征的代码如下：\nfrom sklearn.feature_selection import SelectKBestfrom minepy import MINE#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5def mic(x, y):    m = MINE()    m.compute_score(x, y)    return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)\nWrapper\n递归特征消除法\n递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：\nfrom sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target) ----\nEmbedded\n基于惩罚项的特征选择法\n使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用 feature_selection 库的 SelectFromModel 类结合带 L1 惩罚项的逻辑回归模型，来选择特征的代码如下\nfrom sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)\nL1 惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合 L2 惩罚项来优化。具体操作为：若一个特征在 L1 中的权值为 1，选择在 L2 中权值差别不大且在 L1 中权值为 0 的特征构成同类集合，将这一集合中的特征平分 L1 中的权值，故需要构建一个新的逻辑回归模型：\nfrom sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression):    def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0,                 fit_intercept=True, intercept_scaling=1, class_weight=None,                 random_state=None, solver='liblinear', max_iter=100,                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):        #权值相近的阈值        self.threshold = threshold        LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C,                 fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight,                 random_state=random_state, solver=solver, max_iter=max_iter,                 multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)        #使用同样的参数创建L2逻辑回归        self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)    def fit(self, X, y, sample_weight=None):        #训练L1逻辑回归        super(LR, self).fit(X, y, sample_weight=sample_weight)        self.coef_old_ = self.coef_.copy()        #训练L2逻辑回归        self.l2.fit(X, y, sample_weight=sample_weight)        cntOfRow, cntOfCol = self.coef_.shape        #权值系数矩阵的行数对应目标值的种类数目        for i in range(cntOfRow):            for j in range(cntOfCol):                coef = self.coef_[i][j]                #L1逻辑回归的权值系数不为0                if coef != 0:                    idx = [j]                    #对应在L2逻辑回归中的权值系数                    coef1 = self.l2.coef_[i][j]                    for k in range(cntOfCol):                        coef2 = self.l2.coef_[i][k]                        #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0                        if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0:                            idx.append(k)                    #计算这一类特征的权值系数均值                    mean = coef / len(idx)                    self.coef_[i][idx] = mean        return self\n使用 feature_selection 库的 SelectFromModel 类结合带 L1 以及 L2 惩罚项的逻辑回归模型，来选择特征的代码如下：\nfrom sklearn.feature_selection import SelectFromModel#带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)\n基于树模型的特征选择法\n树模型中 GBDT 也可用来作为基模型进行特征选择，使用\nfeature_selection 库的 SelectFromModel 类结合\nGBDT 模型，来选择特征的代码如下：\nfrom sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier#GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)\n小结\n\n\n\n\n\n\n\n\n类\n所属方式\n说明\n\n\n\n\n VarianceThreshold\nFilter\n 方差选择法\n\n\n SelectKBest\nFilter\n 可选关联系数、卡方校验、最大信息系数作为得分计算的方法\n\n\n RFE\nWrapper\n 递归地训练基模型，将权值系数较小的特征从特征集合中消除\n\n\n SelectFromModel\nEmbedded\n 训练基模型，选择权值系数较高的特征\n\n\n\n降维\n当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于 L1 惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA 和 LDA 有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是 PCA 和 LDA 的映射目标不一样：PCA 是为了让映射后的样本具有最大的发散性；而 LDA 是为了让映射后的样本有最好的分类性能。所以说 PCA 是一种无监督的降维方法，而 LDA 是一种有监督的降维方法。\n主成分分析法（PCA）\n使用 decomposition 库的 PCA 类选择特征的代码如下：\nfrom sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data)\n线性判别分析法（LDA）\n使用 lda 库的 LDA 类选择特征的代码如下： from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target)\n小结\n\n\n\n库\n类\n说明\n\n\n\n\n decomposition\nPCA\n 主成分分析法\n\n\n lda\nLDA\n 线性判别分析法\n\n\n\n总结\n再让我们回归一下本文开始的特征工程的思维导图，我们可以使用 sklearn 完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法 fit_transform 完成的，fit_transform 要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法 fit_transform 中有 fit 这一单词，它和训练模型的 fit 方法有关联吗？接下来，在《使用 sklearn 优雅地进行数据挖掘》中将会阐述其中的奥妙！\n\n参考： 归一化与标准化\n研究｜数据预处理｜归一化\n（标准化）\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"修改 Linux 下 bash 的命令提示符","url":"/2015/10/11/%E4%BF%AE%E6%94%B9Linux%E4%B8%8Bbash%E7%9A%84%E5%91%BD%E4%BB%A4%E6%8F%90%E7%A4%BA%E7%AC%A6/","content":"Linux 终端的命令提示符能够显示诸如当前用户，主机名，时间等信息，根据自己实际需要配置自己的命令提示符能够使得工作更为便利。下面介绍一下修改命令提示符的方法。\n\n根据用户的不同，可以为系统所有用户修改命令提示符，也可以为单一用户修改命令提示符。两者的区别仅仅是修改的配置文件不同，前者需要修改 /etc/profile 文件，后者则只需要修改用户主目录下的\n.bashrc 文件。\n修改的内容就是 PS1 这个环境变量，一种修改方法如下所示：\nexport PS1='[\\u@\\H: \\d \\t \\w \\$]'  \n将上面的内容添加到当前用户的 ~/.bashrc 文件中，各个参数的具体含义后面会讲。\n但是此时配置还没生效，需要运行下面的命令让该文件配置立即生效。\nsource ~/.bashrc  \n生效后可以看到命令提示符变成了下面的形式：\n[root@memcached1: Sat Jan 23 03:03:13 /etc/sysconfig #]  \n此时便可以明白了上面修改的 PS1 的各个参数的含义了，\\u 表示当前用户，\\H 表示主机名，\\d 表示当前日期，而且格式是 “weekday\nmonth\ndate”\\t 表示当前时间，24 小时制，格式是 \"HH:MM:SS\",\\w 表示用完整路径表示当前的目录，\\$ 表示提示字符，root 用户用#, 一般用户用 $。\n除了上面提到的几个比较常用的参数外，还有下面一些参数及其含义：\n\n\n\n参数\n含义\n\n\n\n\n\n显示当前路径的相对路径，小写的 w 是完整路径\n\n\n\n表示当前时间，12 小时制，格式是 \"HH:MM:SS\"\n\n\n\\#\n执行的第几个命令\n\n\n BASH 的版本信息\n\n\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"修改 Linux 主机名","url":"/2015/10/23/%E4%BF%AE%E6%94%B9Linux%E4%B8%BB%E6%9C%BA%E5%90%8D/","content":"修改 Linux 主机可分为临时修改和永久修改\n\n\n临时修改：通过 hostname NewHostName 命令临时修改，修改后通过 exit 注销后重新登录就可以在命令提示符看到新的主机名称，但是重启后会失效\n永久修改：修改配置文件 /etc/sysconfig/network 中的 HOSTNAME 项，修改后需要重启才能生效。\n\n另外一个与主机名相关的配置文件是 /etc/hosts, 但是这个文件与本机的主机名无关，一般是用来记录网络上其他主机的 ip 和主机名的对应关系，其作用有点像本地的 DNS 服务器。\n比如说本机没有配置 DNS 服务器，与本机在同一个局域网的一台名为 memcached 的主机的 ip 是 192.168.0.23，那么此时本机输入命令 ping  192.168.0.23 能够 ping 通这个 ip，但是如果 ping memcached 则 ping 不通了，因为本机不知道 memcached 这台主机的 ip 是多少，同时也无法向 DNS 查询。这时可以在本机的 /etc/hosts 文件中添加这样一行\n192.168.0.23  memcached  \n表示这个 ip 和这个主机名的对应关系。然后再 ping 主机 memcached 就可以 ping 通了。\n","categories":["Linux"],"tags":["Linux"]},{"title":"做一个清醒的傻瓜","url":"/2023/02/05/%E5%81%9A%E4%B8%80%E4%B8%AA%E6%B8%85%E9%86%92%E7%9A%84%E5%82%BB%E7%93%9C/","content":"随机性，如同带刺的玫瑰，危险而又迷人；随机性可能会给你带来生活中的惊喜，但也有可能会降临足以毁灭生活的灾难\n在去年的年度小结提到，22\n年最大的感悟，是生活存在非常多的随机性，正是这些随机性导致了无法预测和无人知晓的未来；1\n月份在查资料过程中看到了塔勒布的 “不确定性” 四部曲：《随机漫步的傻瓜》、《黑天鹅》、《反脆弱》和《非对称风险》，相比于笔者简单的感悟，这几本书花了很大功夫来描述随机性这件事情\n如果单纯看随机性，很容易让我们陷入虚无主义中，因为没什么是确定的，或者说没什么是能够坚信的，因为随机性几乎主导了一切。幸运的是，书里给出了还给出了一些也许可供参考的方法论，虽然作者行文有时候较为晦涩，但总体还是值得一读的。本文主要是\n《随机漫步的傻瓜》和《黑天鹅》两本书的一些笔记以及拓展，祝开卷有益。\n\n我们都是随机漫步的傻瓜\n生活往往被随机性左右，你的成功或失败不见得是因为比其他人高明或愚蠢，很可能只是单纯的运气的结果\n上面这句话，也许是《随机漫步的傻瓜》最想给我们揭示的道理\n《异类》一书中也提到以下现象\n\n英超联赛大部分球员都在 9 月至 11 月出生；\n比尔・盖茨和史蒂夫・乔布斯都出生在 1955 年；\n纽约很多著名律师事务所的开创者竟然都是犹太人后裔，并且他们的祖辈大多是在纽约的服装行业谋生。\n为什么对那些成功人士进行的统计结果会这样一致 “意外”？这是因为：\n英超球员注册时间是 9 月。在同龄的球员中，9 月份出生的人实际上比 8 月份出生的人几乎大了一岁，一岁的差距对他们的职业生涯有着不可低估的影响；\n1955 年前后正是计算机革命的时期，如果你出生太早，就无法拥有个人电脑，如果出生太晚，计算机革命的好机会又被别人占去了；\n犹太人律师事务所的成长，是因为他们正赶上企业重组的法律诉讼出现革新的时候，而他们移民到美国的祖辈们的经历又让他们出色地掌握了抓住机遇的能力。\n\n很多时候，大的成功往往靠的是天时地利人和，而且天时地利可能是主导因素。看人物自传往往会让我们热血澎湃，似乎遵循着成功者的路线，他的成功我也可以复制，但事实往往是他们在那个时代提供了那个时代最需要的东西，然后在合适的时机，时代给予了他们足够大的正向的反馈，不同时代的人压根无法复制。我们看到的往往都是幸存者偏差，因为无论概率多小，只要尝试的基数足够大，每个时代最终都能选出那一批为人所熟知的幸存者\n这里并不是说人物自传一无是处，也不是说那些名人的成功是纯粹由机遇和随机性指导，他们的思考方式、他们付出的努力，都是无法粗暴地被 “机遇” 二字淹没的\n但我们需要清楚地意识到，影响一件事情的成功或失败的因素往往是错综复杂，这其中有多少是运气成分，有多少又是努力的成分，往往无法给出一个明确的量化标准；而当我们面临一个选择时，如果能够清楚地意识到随机性在其中的影响，并心平气和的接收这个事实及其可能带来的后果，也许能让我们活得更加轻松和豁达\n在股市上，我们往往也都是一个随机漫步的傻瓜，市场总体上升时，几乎每个人都觉得是自己英明的抉择选择才让自己赚得盆满钵满的，但其中是\nalpha 收益主导，还是 beta\n收益主导，可能大多数人都说不清楚；因为在短的时间内，市场很多所谓表现都是波动；各类新闻报纸每天都要点评股市，试图为今天的涨跌说出个理由，因为媒体总会想说点什么来明他们有存在的价值；但实际上没有什么理由，每天的股价变动由随机性决定更多\n关于这本书，知乎上有个比较精彩的回答，《随机漫步的傻瓜》是怎样的一本书？；笔者把核心观点提炼如下，\n这里其实也提到了《黑天鹅》中的叙述谬误的现象，因为《随机漫步的傻瓜》出版得比《黑天鹅》更早，两者的观点会有些许重合\n\n我们万事都需要一个解释，以获得内心的宁静。没有解释的事情会让人茶饭不思、寝卧难安\n因此我们急于去总结成功的经验和失败的教训，其结果就是我们常常顾不得去分析这个成功或失败的原因是否存在\n但常常我么也会找不到确定的因果，于是我们只好迷信；这完全是我们从先古部落首领那里传承下来的习惯，比如他们采用了某种祭拜姿势就可以求得龙王降雨\n这本书多少有一点虚无主义色彩。你没有什么可以相信的，如果有人因为努力而获得了巨大成就，要相信，事情没有那么复杂 —— 大部分时候，都是运气使然。虚无主义的意思是，没有什么是确定的，或者说，没有什么会找到确定的原因，一切都是不确定的。\n可以把它当成《心灵鸡汤》一类的读物。它让你接受现实 —— 当然，前提是你足够理性，同时足够努力和聪明。准确地说，这不过是一本写给迷失在财富追求的过程之中的人们，换言之，是写给有点钱、有点追求、但是钱又不太多的人们看的。很重要的一个提示是：不要老看着多少人比你强，要看到多少人比你差\n\n黑天鹅从何而来\n不要忽视那些概率低、但是有极大的冲击性的事情，这些事情无法预测，但是在行动时要考虑到\n上面这句话，也许是《黑天鹅》最想给我们揭示的道理\n书里还给了很多由于惯性思维、人性等原因带来的问题，意识到这些点，也许能让我们能够更好地认知到很多事情或结论的局限性\n叙述谬误\n很多时候，在黑天鹅发生后，人的本性促使我们在事后为它的发生编造理由，并且或多或少认为它是可解释和可预测的；而这其实也是黑天鹅发生的原因之一\n书中将这一现象称作 “叙述谬误”，叙述谬误指的是我们无法在不编造理由或者强加一种逻辑关系的情况下观察一系列事实。换句话说，我们在观察一组事实的时候，一定编造相应的理由或者逻辑关系，让我们觉得这组事实是可以解释的，符合逻辑的；如果不这样，我们就无法观察和表达。造成叙述谬误有以下几个原因\n1）解释习惯；我们习惯对观察到的事实梳理因果关系，让事情变得可解释，这个过程虽然好，但是也会可能导致一些看起来无关的事实就会被我们忽视掉。而那些被忽略的事实往往会成为导致黑天鹅现象的原因，并且在黑天鹅事件发生后，被人们重新翻查出来，并加以解释\n2）模式辨识习惯：《习惯的力量》指出，习惯之所以出现，是因为大脑一直在寻找可以省力的方式。模式就是大脑的省力方式，为了高效处理信息，作出反应，我们不得不训练自己形成各种习惯，但是这些习惯也会阻碍我们接受新的观念\n3）简化习惯：信息理论核心问题：1. 信息获得是有代价的；2. 信息的存储也是有代价的；3. 信息的处理和提取也是有代价的正是这些代价，让我们简化了很多东西，倾向于讲故事而不是注重事实。而我们的简化行为使得我们以为世界的随机性要比实际的小\n而如果进一步深究上面几个原因，我们会发现这都是人性导致的\n\n解释习惯是因为我们天生喜欢有条理的东西，为了让因果关系看起来更加可解释；而常常忽略了一些所谓的\noutlier，如同在机器学习中去除 outlier\n一样，有时候其实样本并不异常，只是我们的模型不足以学习到这些所谓的\noutlier，而人为地把这些样本过滤掉\n\n模式辨识习惯和简化习惯，都是我们为了更省力地去做事情而发展出来的，毕竟如果每件事情都从头去想和做，是在是太费时费力了\n\n在书里的 “自我欺骗的人类” 一章中，则提到了人类的思想有三重迷雾：\n1）假想的理解，也就是在一个超出人们想象之外的复杂或随机的世界，人们都以为自己知道其中正在发生什么；很多发生过的事情本来应该被认为是完全不可思议的，但在事情发生之后，看上去却没那么不可思议了。这种事后合理性在表面上降低了事件的稀有性，并使事件看上去可以理解\n2）反省的误差，也就是我们只能在事后评价事物，就像只能以后视镜里看东西，历史在历史书中比在经验现实中显得更加清晰和有条理；在一个历史事件发生之前存在无数个事实，其中只有相当少的一部分会在后来你对历史事件的理解中有帮助，而这些有条理的时间会被记入史书中\n3）对事实性信息价值的高估\n在生活中，当我们作出一些判断和决定前，如果能够清楚意识到叙述谬误这一现象的存在，以及上面提到的由人性的局限性，也许就是这本书能够给我们带来的价值之一\n极端斯坦、平均斯坦与择业\n这是《黑天鹅》中提到的两个概念，基本含义如下\n平均斯坦：当样本数量最够大，任何个例都不会对整体产生重大影响（正态分布）\n极端斯坦：不平均性现象，个体能够对整体产生不成比例的影响（长尾分布）\n如果从分布上来说，极端斯坦有点像方差较大的一个正态分布，极端的个体较少；而极端斯坦则是一个长尾分布，头部的一些个体会影响全局，跟二八法则有点像\n这个概念往往会被应用在择业上\n有些技术性的职业，比如牙医、咨询师、按摩师的收入是不可能具有突破性的，他们受到既定的时间内服务的病人或客户的最大数量的限制。在这些职业中，不论报酬多高，收入总是受到限制，他们不可能一夜致富，财富总是来自于持续的积累。并且这种工作在很大程度上是可预测的：他的收入可以增长，但不可能达到一天的收入超过余生的收入总和的程度，也就是说这种职业不会受到黑天鹅的驱使，\n假如你是一名会计师或外科医生，你不可能一夜之间成为超级英雄，也不太可能成为一名失败者。\n而另外还有一些职业如果干得好，收入可以十倍、百倍地增长，同时几乎不需要增加额外的努力。比如股票交易员，买 100 股股票与买 100 万股股票的工作量是一样的，要打同样的电话，做同样的计算，对交易的正确性做同样的确认，收入却截然不同。同样，演员、录音师、作家对于同一作品，他们的工作量也是一样的，但这类职业可以不必增加劳动量就可能十倍、百倍地增加收入，一部发行量大的影片、一本畅销的书籍都能带来突破性的收入\n一般智力，科学和艺术行为等属于极端斯坦，在这里成功是高度集中的，少量赢者得到蛋糕的大部分。其实极端斯坦基本上适用于所有 “有意思” 的职业，但从事他们并不是好主意，因为收入具有突破性的职业，只有在你成功的时候才对你有利，这样的职业竞争激烈，导致了更大的不平均和不确定性，在努力和回报之间存在巨大差异，可能会出现赢家通吃的现象，普通人或许什么也得不到；挨饿的演员比挨饿的会计师要多\n无法预测的未来\n很多事情不能只靠过去的经验来判断，某件事情 1000 天的历史不会告诉你弟 1001 天的任何信息；我们从过去获得的实际上顶多是无关痛痒和虚假的信息，甚至是危险的信号\n其实这个观点《随机漫步的傻瓜》中就有了，只是这里做了进一步的说明和延伸，同时提出了以下有趣的观点\n\n信息会妨碍知识\n\n因为人们对于经验现实的细节知识了解的越多，看到的噪点就越多，就可能把它们错当成真实信息。同时，一旦你的思维被某种世界观占据，你会习惯于只关注证明你正确的事例。吊诡之处在于，你拥有越多的信息，你就越认为自己正确\n因此，我们在接收信息时一定要谨慎过滤，不要轻易全盘接受，这里有 2\n个也许可供参考的方法\n（1）注重事实而不是故事；我们天生喜欢故事，喜欢总结，喜欢简化，也就是减少事情的影响因素。没有人愿意去听某个无聊的抽象统计学讲座，我们喜欢听故事，这没有错，我们只是要去更加彻底的审视，故事是否对事实做了严重扭曲\n（2）控制接收信息的频率；从机器学习角度来讲，人其实是一个持续流式更行的\nonline\nmodel，对近期收到的信息和样本会更加敏感，每小时收听广播比阅读周刊之所以要糟糕，是我们会被短时间的很多\nnoise\n干扰了想法和思绪，而较长的时间间隔能够过滤掉一些无关重要信息；这里不得不提以下当前软件流行的信息流模式，很容易让人建立多巴胺回路，让人一刷接着一刷欲罢不能，说到底，还是要保护好我们自己的注意力\n\n认知自大\n\n我们在自以为拥有的知识方面非常自大，我们有一种内在的倾向，以为我们比实际上知道得多一些，这一点常常招致严重的麻烦。认知自大有双重影响：我们高估我们的知识，低估不确定性，也就是低估未知事物的范围\n“淹死的都是水性好的”，说得也是这个道理，过度的自信容易让我们忽略未知的风险，从而做出错误的判断和预测\n如果黑天鹅无法避免\n《随机漫步的傻瓜》告诉了我们生活中存在着种种随机性，需要清楚地意识到成功或失败中的运气成分，《黑天鹅》则是告诉了我们那些概率极低但破坏极大的事情的诱因，那回到最终要的方法论上，也就是面对这些随机性，我们能做些什么？\n黑天鹅的序里写了这段话：如果把塔勒布和巴菲特的共同经验总结，可以得到应对黑天鹅事件的 5 个基本原则：1. 不要预测；2. 谨慎预防；3. 危中取机；4. 最重要的一点，保持充足冗余；5. 不要负债\n下面针对这 5 点进行一些拓展和讨论\n不预测，只应对\n黑天鹅事件往往无法预测、无法解释，很多事后的归因往往也是无效的；所以才有了我们前面提到的\n2 条原则：不要预测和谨慎预防\n这里的小标题套用了孟岩的文章《不预测，只应对》\n我们往往面临的三个主要问题：（1）总结出来的规律并不靠谱（2）用于预测的信息也不客观（3）未来充满不可预期的「随机事件」\n（1）总结出来的规律并不靠谱；指的是我们总结出来的规律往往是从过去发生的事件中归纳出来的，\n而已发生的事情，往往是因果规律和随机性共同作用的结果，但我们的大脑回路天生并不习惯于捕捉「随机事件」，往往都希望根据因果规律解释已发生的事情，而随机性可能会导致相同的事情总结出不同的规律（想象有一个平行世界发生了相同的事情，但是结果不同，那么总结出来的规律也会不一样），这也是《黑天鹅》中提到的 “叙述谬误” 的现象\n（2）用于预测的信息也不客观；作出预测需要信息，信息本身是客观的，但是每个人接收信息的方式并不客观，亦即 “一千个人眼里就有一千个哈姆雷特”；我们听到的信息都是经过我们大脑加工的，往往会带上我们的个人偏好和特性\n（3）未来充满不可预期的随机事件；这个比较好理解，不确定性四部曲以及整篇文章就是在阐述这种现象\n针对上面的三个问题，文章也分别给了一些方法\n（1）总结出来的规律并不靠谱 -&gt;\n运用尽量少的规律，并且这些规律最好是普世的、第一性的、经过长时间考验的；\n（2）用于预测的信息也不客观 -&gt; 去除\nEgo，尽可能客观地去收集和看待信息，排除人为干扰；这个跟纳瓦尔宝典里提到的\n“Shed your Identity to see reality” 说的是同一件事\n（3）未来充满不可预期的随机事件 -&gt; 有足够的容错性，对各种随机事件的发生做好准备\n这里着重讲一下第 1 点，也是笔者认为比较重要的一样，这里提到的方法跟\nFirst\nprinciple\n很相似，其思想都是从事情的本质上去思考，仅从一些最简单的规律 (“cannot be\ndeduced from any other proposition or assumption”) 去推导，这里有一个关于\nFrist Principle 的例子，首要原则（First Principles\nThinkings）：埃隆・马斯克谈为自己思考的力量\n而回到投资里的不确定性，文章给出的方法论跟上面提到的基本原理是一样的\n&gt;\n从最基本的一些普世规律和常识出发，打造一套自己的「投资系统」，并且对未来各种「随机事件」做好充分的准备，不会死。用系统去根据市场的信息做出「应对」，不断迭代升级，循环往复\n祸福相依\n祸兮福之所倚，福兮祸之所伏；很多事情都是祸福相依的；往往黑天鹅事件是以危机形式展现，但危中有机，危后出机，最大的危机会出现最大的暴跌，也就会形成最好的投资良机，黑天鹅的序中提到了：正确的策略应该是尽可能多地尝试和尽可能多地把握黑天鹅机会\n当然，这个事情说起来是这么个道理，但实践起来会很难，就拿投资来说，你永远无法预估到绝对的底在哪里，且往往这种黑天鹅事件出现时，意味着我们相信的很多法则都会消失，这个时候根据历史经验做判断可能是行不通的；更重要的是，在黑天鹅事件发生后，人和市场都是很容易出现恐慌的，这种情况下无法预估到底加上恐慌，很容易让我们成为割肉逃跑而不是逢低加仓的人\n比如说 08\n年的次贷危机，虽然惨烈，但的确有人从中获利了，这也正是电影《大空头》讲到的故事；但需要意识到，《大空头》中也存在着幸存者偏差，我们看到的是成功做空获利的几个例子，但其中做了同样的事情却最终失败了的，不会有人感兴趣，因为我们都喜欢听成功的故事，都崇拜以一己之力获胜的英雄主义；关于这部分可参考：如何评价电影《大空头》（The\nBig Short）？\n除了在黑天鹅中看到机会，日常还可以尽量做一些高杠杆的事情，即一些成本很低，但是成功了能够给你带来较大回报的事情；比如说无风险套利，当然这种事情都是可遇不可求的\n往往风险和收益是正相关的，为什么能捕获风险低但收益高的事情？《在黑天鹅频发的年代，重读塔勒布的四本书》给了一个可供参考的解释，即也风险和收益的正相关只是在期望上成立，现实世界中每次事件的风险和收益都是在期望上线波动，如果能够找到风险低于期望，但收益高于期望的事情；但这种事情往往发生的概率会很低\n上面其实是一个期望与概率的例子，关于这点可以延伸说说，上面提到的观点是要捕获概率低但收益高的事情；而在后面的《反脆弱》的策略更多的提到了要关注期望而不是概率：买卖不应取决于股票上涨概率，而应取决于上涨期望。比如有 90% 的可能上涨 1 元，有 10% 的可能下跌 100 元，那么期望是下跌 1 元，应该卖空，而非买入。这里提到的例子跟反脆弱中的观点也是类似\n\n如果人生会重复一万次，牙医在每次 “人生” 中都很有可能过上富裕的生活，而靠中彩票大奖过上富裕生活的流浪汉，却很难再中一次大奖。因此，不能纯以是否富有的结果导向来对人进行评价。可以将人生的种种可能看作是一种分布，现在当下发生的只是这个分布的一个抽样。牙医人生的整体分布更优于流浪汉，即使流浪汉偶尔中了一次超级大奖，但对人生的整体分布影响不大\n\n两个观点看似矛盾，但实则不是，笔者认为其中差异在于这些事件发生的次数；期望是经过多次\nIID\n的试验得到的一个值，或者说买卖次数要足够多，这条定理才成立；而黑天鹅时间往往是稀有的，期望在其中也许并不成立，但交易是会发生多次的；所以结论也就很明显了，\n保持冗余\n这部分也很好理解，也是我们常说的鲁棒性或健壮性（robustness），比如说在工程上留出足够的\nbuffer，预防低概率事件发生，以及永远不要 all\nin，无论是在投资还是生活，都要尽量给自己留一条后路\n这部分涉及到反脆弱中的一些观点的，其中最令人深思的是：“系统的反脆弱性是通过牺牲个体为代价取得的”，这句话可以解释很多现象\n回到与所有人都息息相关的日常工作中，这篇文章提出了一些比较有趣的观点，反脆弱：为什么稳定的工作，只是一种人生的幻觉\n\n一个国家是如何渡过经济衰退期的呢？是靠死掉一大批没有竞争力的企业，让坚持下来的公司减少竞争对手\n“人力资源市场” 的 “反脆弱性” 也是如此：经济衰退期，淘汰一批没有竞争力的人，让留下来的人更容易找到工作；经济过热期，淘汰掉一批给不起高薪的企业，让给得起高薪的企业更容易找人\n“反脆弱性” 会牺牲一部分最脆弱的人的利益，来提高整个系统的生存资源的利用效率，而这正是 “35 岁现象” 的本质，因为某些凭借年龄获得优势的行业与职业，一过 35 岁，脆弱性会大大增加\n\n那大公司的稳定性从何而来？\n系统的反脆弱性是通过牺牲个体为代价取得的，大公司之所以稳定，是因为 “体系至上、公司文化至上、业务至上”，任何一个员工的利益都无足轻重，随时可以被牺牲掉，在巨大的危机面前，就算是 CEO 也可以牺牲\n对个人来说，致命的是 “大公司稳定幻觉”，让你陷入舒适，从而忽视了为自己建立 “反脆弱结构”；这种工作造成的稳定性的幻觉，就像火鸡，它们活的时间越长，就越 “确信” 农场主的仁慈，直到感恩节来临的那一天\n保持冗余，本质也是反脆弱，不要 all in\n一件事情，多为自己留一些后路，至于具体的方法和途径真的是因人而异了，毕竟这些也都是比较耗心力的事情，但本质还是要\nproductize yourself\n不要负债\n套用塔勒布的原话是这么说的，“有一条是对于个人和机构非常重要的戒律：我们可以降低经济生活中 90% 的黑天鹅风险…… 我们所做的只是取消投机性的债务。”\n这里的投机性的债务没有一个明确的定义，笔者猜测这里指的是类似\n08 年次贷危机的那种炒房式的负债\n房子在各个国家好像都会被当做金融工具，也的确其中有人会赚得盆满钵满，但需要清楚，你进去的时候是否会是最终的接盘侠\n小结\n说实话，看完这两本总感觉比较反人性，因为人性就是希望事情都是可总结可解释的，包括笔者在写这篇文章，做的事情就是总结塔勒布这几本书的内容，期待从中获取一些可指导生活的规律和原理\n但无论是《随机漫步的傻瓜》还是《黑天鹅》，都在告诉我们很多事情都是无法预测的，我们从历史获取的是无关痛痒，甚至是危险的信号；我们看到的很多成功也都是幸存者偏差，是运气和因果性共同主导的结果，且往往运气占了大头\n如同开头所说，仅仅看到未来无法预测这一面，有时候会让人更绝望，很容易陷入虚无主义中，因为没什么是确定的，或者说没什么是能够坚信的，因为随机性几乎主导了一切；而我们，就是在其中随机漫步的傻瓜\n既然生活的随机性的影响可能要远远大于努力，为什么还要努力？\n因为在努力还没达到一定程度前，我们连面对随机性的机会都没有，或者说幸存者偏差也是有门槛的，当你的能力不足时，进入决赛圈的资格都没有\n而面对随机性，书里这 5\n条原则也许值得我们参考与实践：1. 不要预测；2. 谨慎预防；3. 危中取机；4. 保持充足冗余；5. 不要负债\n写到这里，好像隐约读懂了罗曼罗兰的那句 “世上只有一种真正英雄主义，那就是在认清生活的真相后仍然热爱它”；虽然 “生活的真相” 有很多种，生活充满了随机性或许也是其中一种，在了解这个真相后仍然能够 “尽人事，听天命”，认认真真把手上的每件事情做好，也许就是一个清醒的傻瓜\n","categories":["读书"],"tags":["读书","塔勒布"]},{"title":"先验概率，后验概率，共轭分布与共轭先验","url":"/2017/01/08/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C/","content":"本文主要讲述先验概率，后验概率，共轭分布和共轭先验这几个概念。\n\n众所周知，概率论中有两大学派：频率学派和贝叶斯学派。先验概率，后验概率，共轭分布和共轭先验是贝叶斯学派中的几个概念。原因是贝叶斯学派认为分布存在先验分布和后验分布的不同，而频率学派则认为一个事件的概率只有一个。\n下面先以一个直观的例子来说明先验概率和后验概率的概念\n比如说，你来到一个山洞，这个山洞里可能有熊也可能没有熊，\n记你觉得山洞有熊的为事件 \\(Y\\).\n然后，你也许听到山洞里传来熊的吼声，记听到熊吼声为事件 \\(X\\). 你一开始认为山洞有熊的概率是 \\(P(Y)\\); 听到熊的吼声之后，你认为有熊的概率是\n\\(P(Y|X)\\)。在这里，\\(P(Y)\\) 就是先验概率，\\(P(Y|X)\\) 是后验概率.\n回到概率论中一个经典的例子：抛硬币。抛硬币时抛出正面的概率为多大？假如事前关于这枚硬币没有任何额外信息，那么一般都会认为是\n1/2，这时候的 1/2 就是正面朝上的先验概率\n。但是在经过一系列实验确认后再得到的正面朝上的概率很可能就不是 1/2 了 (受到到硬币的质地，重量分布等因素的影响)，这个概率便是后验概率。\n简单理解就是在事件发生之前，根据以往的经验推测的与该事件相关的概率就是先验概率，而在事件 (试验) 真正发生后，通过事件 (试验) 的结果可以修正先验概率，从而得到后验概率。\n那么对于抛硬币这个事件来说，抛出正面硬币的概率就应该是一个概率的概率，也就是说它的结果不是一个单一的值\n1/2，而是一个概率分布，可能有很高的概率是 1/2，但是也有一定的概率是 100%（比如抛 100 次结果还真都 100 次都是正面）。那么在这里这个概率的分布用函数来表示就是一个似然函数，所以似然函数也被称为 “分布的分布”。用公式来表示就是：\n** 后验概率（posterior probability）∝ 似然函数（likelyhood\nfunction）* 先验概率（prior probability）**\n即：\n\\(P(X|D) ∝ P(μ|D)*P(X)\\)\n这里 \\(D\\)\n表示一组观测实验 (比如我扔了五次硬币得到 5 次正反面的结果)，\\(X\\)\n表示随机变量（在这里是硬币的正反面），表示随机函数里面的参数（在这里就是硬币掷为正面的概率）。\n注意这里是正比于而不是等于，这个是理解似然函数的一个关键，右侧直接的乘积其实是不满足概率分布归一化的条件的（就是右侧的积分最后不会等于 1）那么这个正比符号怎样才能变成等号呢？其实只要再除以一个系数进行归一化就可以了：\n\\(P(X|D)  =P(μ|D)*P(X)/P(D)\\)\n这个归一化的系数是怎么来的呢？让我们回忆一下贝叶斯公式：\n\\(P(X|D)*P(D)=P(XD)\\)\n而 \\(P(μ|D)=P(D|X)\\)(似然函数在计算时的做法就是将 D 的观察结果代入 P (X) 的分布式子中去得到的)\n于是\n\\((P(μ|D)/P(D))\\*P(X)=P(D|X)\\*P(X)/P(D)=P(XD)/P(D)=P(X|D)\\)\n似然函数的形式是依赖于观测值的，它在贝叶斯学派与频率学派都有很大的作用，不过在两家的用法并不相同。\n频率学派认为每个事件的概率是一个客观存在的常数值，只是我们不知道而已。比如抛硬币，在实验估计之前我们不知道它是多少，频率学派也不会管之前大家说抛硬币出现正面的概率是 1/2 还是多少，所谓 “眼见为实，耳听为虚”，他们的最终结论只和在实验中观测到的数据有关系。但是它肯定是一个确定的常数，然后我们通过观察实验，获得一组样本值\n\\(D\\)，再将这组样本值代入似然函数 \\(P(D|X)\\)\n，求解使得似然函数最大的值就是估计出来的（当然由于实验的结果不同，这个估计出来的也很可能不是 1/2，实验不同得到的结果也不同，但是根据大数定律，理论上实验次数足够多以后，求出来的是会越来越接近真实的概率的）。也就是说频率学派认为答案只有一个，我们不断地通过各种估计法来猜测这个值。\n而贝叶斯学派并不会完全拒绝大家之前所说的 “硬币扔出正面的概率是 1/2” 的说法，只是贝叶斯学派认为最终硬币扔出正面反面的概率并不是一个常数值，不是一个有唯一答案的真理，这个值本身应该也是一个随机变量，是在不断变化的一个数值，如何得到这个值，贝叶斯学派认为也需要通过实验在 “硬币扔出正面的概率是 1/2” 的说法（先验概率）的基础上通过实验数据（似然函数）不断去预估这个扔出正面概率的实际分布（后验分布）。\n举个例子：假如我扔了 5 次硬币，先出现了 3 次正面，后出现了两次反面，那么这时的似然函数就应该是\n\\(P(μ|D)=P(D|X)=L(μ)=μ\\*μ\\*μ\\*（1-μ）\\*（1-μ）\\)\n(\\(\\mu\\)\n是硬币抛正面的概率，在似然函数里就相当于概率分布函数里的随机变量一样变成一个随机变化的值了）\n如果用我们以前统计课本上的频率学派的最大似然估计法，对 \\(L(μ)\\) 求导求最大值，得到 \\(μ=3/5\\)，\n那么得出结论就是最后抛硬币为正面的概率就是\n3/5，当然还要附上一个参数估计的置信度，表示这个结论自然不是 100% 准确的\n但是如果采用贝叶斯学派的后验概率 \\(P(X|D) =\nP(D|X)\\*P(X)/P(D)=L(μ)\\*P(X)/P(D)\\)，\n其中 \\(P(D)\\)\n可以简单地由古典概型算出来：\\(P(D)=1/=1/32=0.03125\\)。如果 \\(μ\\) 取了\n3/5，代入上式那么抛硬币为正面的概率就是\n0.55296，而不是 1/2，当然贝叶斯学派最终得到的后验概率是一个随\n\\(μ\\)\n变化的分布，只不过在这种情况这个分布取到 0.55296\n这个值的概率最大而已\n清楚似然函数、先验概率、后验概率的几个贝叶斯学派的基本概念，要明白共轭分布和共轭先验就很简单了，所谓共轭分布就是先验概率和后验概率具有一样函数形式的分布形式，举个例子就是假如先验分布函数是形如\n\\(C_1\\mu^a (1-\\mu)^b\\)\n的形式（比如二项分布就是这种形式）而后验分布是 \\(C_2\\mu^m (1-\\mu)^n\\)\n这样的形式，两者只是具体参数值不同，或者先验分布和后验分布都是高斯分布等等的情形就可为认为先验分布和后验分布具有同样的形式。\n这种形式不变的好处是，我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续到后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易有物理解释。同时能够后验分布和先验分布共轭的情况下是可以大大简化计算量。\n那么共轭先验又是什么概念呢？因为在现实建模问题中，往往我们先得到和固定的反而是似然函数（其实也很好理解，客观的实验观察数据才是第一手最 solid 的材料），这时先验函数（可以理解为先验知识或者是对后验分布的一种假设和猜测）是可以选择的。这时如果我选的先验分布最后乘上这个似然函数，使得后验分布与先验分布共轭，那么我们就称这个先验函数为似然函数的共轭先验。基于上面说到的共轭分布的好处，往往选择先验函数时都会让先验概率分布和后验概率分布共轭。\n共轭分布与共轭先验 条件概率和后验概率有什么不同？\n","categories":["数学"],"tags":["数学"]},{"title":"关键词抽取算法的研究","url":"/2016/05/28/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6/","content":"关键词抽取的一般步骤为：\n分词 --&gt; 过滤停止词，得到候选关键词 --&gt; 从候选关键词中选出文章的关键词\n从候选关键词中选出文章的关键词需要通过关键词抽取算法实现，而关键词抽取算法可以根据是否需要人工标注的语料进行训练而分为有监督的提取和无监督的提取。\n\n有监督的提取需要人工标注的语料进行训练，人工预处理的代价较高。而无监督的抽取算法直接利用需要提取关键词的文本即可进行关键词的提取，因此适用性较强。\n关键词抽取中无监督的抽取算法可分为三大类：\n1）基于统计特征的，如 TF-IDF 2）基于词图模型的，如 TextRank\n3）基于主题模型的，如 LDA\n本文主要讲述 TF-IDF 算法、TextRank 算法、以及通过组合两者得到的三种新方法，然后通过 Java 实现这几种方法并比较这几种方法在特定语料库上进行关键词抽取的效果。\nTF-IDF 算法\nTF-IDF（Term Frequency-Inverse Document\nFrequency）算法是一种基于统计特征的非常经典的算法，通过计算一个词的 TF 值和 IDF 值的乘积作为该值的得分，然后根据得分从大到小对词语排序，选择分数高的词语作为关键词。\nTF 值指词语在文本中出现的频率，如某篇文章分词并过滤停止词后的词语的数量为 n，而其中的某个词语 w 出现的个数为 m, 则词 w 的 TF 值为\n\n\nIDF 值则指词语在整个语料库中的出现的频率大小。这里首先要指出的是 TF-IDF 算法是针对一个语料库（也就是多篇文档进行）进行关键词提取的算法。假如语料库中共有 N 篇文档，而出现了词语 w 的文档数为 M。则词 w 的 IDF 值为\n\n\n\\(TF(w)*IDF(w)\\) 则为词 w 的 TF-IDF 值，根据这个值对候选词从大到小排序，选择前 n 个作为候选关键词即可。\n通过 Java 的实现并不难，主要是利用 Java 的集合框架 Map、List 等存储词语的中间得分、以及候选关键词等。\n实现的完整代码见:\nhttps://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TFIDF.java\nTextRank 算法\nTextRank 算法是借鉴 PageRank 算法在语言处理中的一个算法，关于 PageRank 算法可参考这篇文章。无论是 PageRank 还是 TextRank，其关键思想都是重要性传递。\n以 PageRank 为例，假如一个大型网站有一个超链接指向了某个小网站，那么小网站的重要性会上升，而上升的量则依据指向它的大网站的重要性。下图所示的就是一个例子：\n\n假设网页 A,B 原来的重要性为 100 和 9，那么根据他们指向的网页，传递给 C、D 的重要性分别为 53 和 50。\n在 TextRank 中将上图的网页替换成词语，将网页间的超链接换成词语间的语义关系；假如两个词的距离小于预设的距离，那么就认为这两个词间存在语义关系，否则不存在。这个预设的距离在 TextRank 算法中被称为同现窗口（co-occurance\nwindow）。这样便可构建出一个词的图模型。\n但是在实际中应用时我们是无法预先知道网页 A、B 的重要性的，又或者说假如我们已经知道了网页的重要性，那么也不需要通过算法计算出网页的重要性了。这就成了一个先有鸡还是先有蛋的问题。\nPageRank 的原始论文提出了解决这个问题的方法，这篇文章中通过具体的例子提到了相关的理论依据，就是幂法求特征向量与初始值无关。具体做法就是，先给每个网页随机附一个初值，然后通过迭代计算直至收敛，理论证明了收敛的值与初始值无关。\n同样的，TextRank 也采取了相同的方法，就是先随机赋初值，然后通过迭代计算得到每个\n词的重要性的得分。词语 \\(V_i\\) 的得分计算公式如下所示：\n\n上式中各符号表示如下\n\n\n\n实现的一个关键点在于构建词的图模型，在 Java 中通过队列实现，队列大小即为同现窗口的大小，移动队列的过程中将队列内部的词语互相连接。连接的形式通过 java 的 Map&lt;String,Set&lt;String&gt;&gt; 类型实现，表示指向词语（第一个 String）的所有其他词语（Set&lt;String&gt;）的实现的关键代码如下：\nMap&lt;String, Set&lt;String&gt;&gt; words = new HashMap&lt;String, Set&lt;String&gt;&gt;();Queue&lt;String&gt; que = new LinkedList&lt;String&gt;();for (String w : wordList) //wordList为候选关键词{ if (!words.containsKey(w)) {     words.put(w, new HashSet&lt;String&gt;()); } que.offer(w);    // 入队 if (que.size() &gt; coOccuranceWindow) {     que.poll();  // 出队 } for (String w1 : que) {     for (String w2 : que)     {         if (w1.equals(w2))         {             continue;         }         words.get(w1).add(w2);         words.get(w2).add(w1);     } }}\n另外一个实现关键点就是判断算法是否收敛，可以认为前后两次计算出来的值小于指定的阈值（一般取值较小，如 0.000001）时算法收敛，或者超过设定的最大迭代次数时停止。实现的关键代码如下所示：\nmin_diff = 0.000001Map&lt;String, Float&gt; score = new HashMap&lt;String, Float&gt;(); for (int i = 0; i &lt; max_iter; ++i) {     Map&lt;String, Float&gt; m = new HashMap&lt;String, Float&gt;();     float max_diff = 0;     for (Map.Entry&lt;String, Set&lt;String&gt;&gt; entry : words.entrySet())     {         String key = entry.getKey();         Set&lt;String&gt; value = entry.getValue();         m.put(key, 1 - d);         for (String other : value)         {             int size = words.get(other).size();             if (key.equals(other) || size == 0) continue;             m.put(key, m.get(key) + d / size * (score.get(other) == null ? 0 : score.get(other)));          }         max_diff = Math.max(max_diff, Math.abs(m.get(key) - (score.get(key) == null ? 1 : score.get(key))));     }     score = m;     //exit once recurse     if (max_diff &lt;= min_diff)       break; }\n完整的实现代码见：\nhttps://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRank.java\n综合 TextRank 多同现窗口\n由于 TextRank 的同现窗口的大小会影响提取的效果，如下图是同现窗口为 2~10 的时候评估值为 F1 值的变化情况。（测试语料）\n\n而原始的 TextRank 算法仅仅是建议该值设为 2~10，无法知道对于一篇文章的最优同现窗口，因此本方法会综合 TextRank 多同现窗口的结果，将一个词语在不同大小的窗口下的得分相加，作为该词的总得分，然后根据总得分对词语排序，选择得分较高的前 n 个词作为候选关键词\n。\n该算法的效果与原始的 TextRank 算法的效果对比如下（测试语料）\n\n图中的 textrank 表示原始的 TextRank 算法的效果，而 multi_window_textrank 表示综合了大小为 2~10 的同现窗口的结果的效果。从图中可知，在提取关键词个数大于 4 个的时候，该方法的效果要优于原始的 TextRank 算法，但是 F1 值的提升幅度不大，并且实际运行的时候，综合多同现窗口的方法花费的时间是原始 TextRank 算法的 14 倍左右。\n代码的具体实现见：\nhttps://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithMultiWin.java\nTextRank 与 TF-IDF 综合\n考虑词语的 IDF 值\n由于 TextRank 算法仅考虑文档内部的结构信息，导致一些在各个文档的出现频率均较高且不属于停止词的词语最总的得分较高。原因是没有考虑词语在整个语料库中的权重。因此在 TextRank 算法得到的每个词的得分基础上，乘上这个词在整个语料库的 IDF 值，IDF 值是 TF-IDF 算法中的一个概念，该值越大，表示这个词在语料库中出现的次数越少，越能代表该文档。\n将词语的 TextRank 得分乘上这个词的 IDF 值后作为该词的新得分，然后根据得分从大到小排序，选择得分高的前 n 个词作为关键词即可。\n下面是考虑了词语的 IDF 值的方法与原始的 TextRank 算法的效果对比图（测试语料）\n\n从图中可知，考虑了词语的 IDF 值后的方法的效果要优于原始的 TextRank 算法，运行时间约为 TextRank 算法的两倍。\n完整的代码实现见：\nhttps://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithTFIDF.java\nTextRank 与 TF-IDF 投票\n这种方法也是针对 TextRank 算法仅考虑文档内部的信息而忽略了文档外部的信息，综合 TextRank 算法和 TF-IDF 算法提取出来的结果。\n具体的流程为：确定要抽取的关键词个数 n, 通过 TextRank 算法和 TF-IDF 算法对语料库分别提取 2n 个关键词，选择同时在两个算法得到的结果中出现的词语作为关键词，假如同时出现的词语不足 n 个，那么剩下的词语从 TextRank 的结果或 TF-IDF 的结果中补。\n下面是 TextRank 和 TF-IDF 投票方法的结果与原始的 TextRank 算法的结果的对比图（测试语料）\n\n从结果可知，两个算法综合投票的方法的效果要优于原始的 TextRank 算法。运行的时间约为原始的 TextRank 的两倍。\n完整的代码实现见：\nhttps://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithTFIDF.java\n总结\n本文主要讲述了 TextRank 算法以及对其进行简单改进的三种方法：综合多同现窗口的结果、考虑词语的 IDF 值、TF-IDF 与 TextRank 共同投票。通过 Java 实现并比较其效果（评判指标为 F1 值）。下图是这几个算法的总效果对比图。（测试语料）\n\n\n综合多同现窗口的改进方案后的效果虽然要略优于原始的 TextRank\n算法，但是消耗的时间是原始 TextRank 算法的 14 倍左右；综合 TextRank\n算法和 TF-IDF 算法后的结果是改进算法后最优的，其次是考虑 TextRank\n提取出的关键词的 IDF 值的改进方案，两者的效果均要优于原始的 TextRank\n算法， 消耗的时间也比原始的 TextRank 算法要多。\n因此，若需要对单篇文档提取的关键词时，可采用原始的 TextRank 算法或综合多同现窗口的方法，\n假如对提取效果的要求较高且对时间要求不高时，可以采用综合多同现窗口的方法，\n反之直接采用原始的 TextRank 算法。如果需要对多文档进行关键词抽取时，四种方法都可以采用，但是考虑提取的效果以及消耗的时间，\n建议使用 TextRank 算法和 TF-IDF 算法综合投票的方法或 TextRank\n结合 IDF 值的方法，并且根据着重点是时间还是提取的精度，选择 TF-IDF\n算法综合投票的方法或 TextRank 结合 IDF 值的方法。\n上文提到的所有代码的地址为：https://github.com/WuLC/KeywordExtraction\n除了算法的实现，还包括了语料库的导入、F1 值的计算方法的实现等。\n","categories":["NLP"],"tags":["Java","NLP"]},{"title":"共享键鼠神器 Synergy","url":"/2018/03/31/%E5%85%B1%E4%BA%AB%E9%94%AE%E9%BC%A0%E7%A5%9E%E5%99%A8%20Synergy/","content":"最近需要频繁切换使用台式机和笔记本，但是我的小桌子上实在没法同时放得下一个键盘和笔记本\n（≧0≦）。哪怕凑合挤下，还得不停在两台电脑之间切换键鼠，因此就想着有没有共享键鼠的方案，结果在网上找到了\nSynergy，试了几天后发现这真的是一个共享键鼠的神器。\n\n安装与配置\n这个软件在 2.0 版本前支持自己编译的免费模式的，核心源码见 github，官方还贴心地提供了各种操作系统下编译的文档：)，这里编译出来的是\nstable 版本，如果还需要更高级功能，可以去官网上购买 pro\n版本支持一下。网上已经有雷锋帮我们编译了若干个 stable\nversion，详见下面链接\n\nhttps://www.brahma.world/synergy-stable-builds/\n\n这个软件的工作模式采用 Client-Server 模式，只允许有一台\nServer，但是可有多台 client，且只需要在 Server\n端配键鼠就行了。我这里以笔记本为 server，台式机为 client，注意 Server 和\nClient 应该在同一局域网内\nServer\nServer 端的配置过程如下\n首先在下图点击设置服务器\n\n\nserver_stat.png-87.8kB\n\n出现下图后从右上角将电脑图标拖入网格内，有几台 clinet 就拖几台，然后进行命名（下图我拖了一台 client\n并命名为 435LC）这里的命名保持和客户端一致即可。\n\n\nserver.png-53.7kB\n\nClient\n首先要勾选 client 的选项，并输入上图的 Server\n的 IP，然后需要在编辑中设置屏幕名与 Server\n中的屏幕名一致，接着点击启动即可，下面的 console 会输出相应的 log\n信息\n\n\nclient\n\n通过这种方式就能够愉快地通过笔记本的键盘和触摸板来控制其他电脑了。\n修改快捷键\n由于笔记本是 mbp，所以复制粘贴用的是 command+c/v, 跟 Windows\n的 ctrl+c/v 不一样，切换的时候经常搞错，因此这里将 mbp 复制粘贴也改成\nctrl+c/v\n如下是打开系统偏好设置 -&gt; 键盘后的界面，通过 +\n号添加相应的快捷键即可\n\n\nkey binding\n\n需要注意的是上面的快捷键的名称需要与你系统保持一致，具体可通过任意窗口的菜单中的编辑项查询，如下就是我所用的\nmbp 的一个快捷键的名称，比如撤消不能写成撤销，否则快捷键不生效。\n\n\nkey name\n\n文件传输\n有时候还需要在两台主机上传输一些文件，Synergy\n的普通版本不支持这种粘贴复制的传输，但是 pro\n版本支持。我这边找不到 pro 版。。。。。。但是因为我经常要用到 FTP\n传输文件， windows 装了 FileZilla 这个 FTP 客户端，所以我直接在 windows\n上通过 FTP 连到 mbp 上进行传输，在大文件的传输上速度也有保证。\n最后祭上一张我的工作台的图片，不得不说 mbp\n的触摸板就是好用，有效预防鼠标手 ～(￣▽￣～)(～￣▽￣)～\n\n\nwork desk\n\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"关键跃升：心态","url":"/2024/08/18/%E5%85%B3%E9%94%AE%E8%B7%83%E5%8D%87_%E5%BF%83%E6%80%81/","content":"最近在看《关键跃升》，里面的不少观点还是颇有启发的，用作者的话来说，这是 “一本适合中国管理者的管理读本”；但无论是管理者还是被管理者，笔者觉得都有必要去了解一下管理的相关内容，才能更好地扮演自己在职场中的角色\n书名的跃升，粗糙来说就是通过 “自己完成任务” 跃升到 “通过别人完成任务”；书里把这部分拆成了两部分：心法和剑法，翻译一下就是心态和行动上需要做出的改变；心法包括了 “责任、沟通、关系，自我” 四大块，剑法则是强调管理者需要扮演好四种角色 “鼓手、教练、政委、指挥”；整本书的内容脉络较为清晰，比较通俗易懂，值得一读；本文主要是心法部分～\n\n书里一开始就提出了一个灵魂拷问：为什么 “不干活” 比 “干活” 的拿钱多？在数量上，这些 “不干活” 的管理者和干活的人相比，大约是 1:5 的关系。也就是说，大概每 5 个 “干活的人”​，就有 1 个 “不干活的人” 管着\n这个自然是调侃的说法，“不干活” 的管理者专注的事情是 “自然效率”\n的作战上，所谓 “自然效率”，就是在在 “自发分工，随机协作” 机制下的工作效率，往往是不高效的\n而管理者的工作，即使是用管理效率打败自然效率，管理者的价值，就体现为在有你和没你两种情况下团队所创造的价值的差额，即如下的公式\n你的价值 = 团队价值 ×（管理效率 - 自然效率）\n所以，书里的观点是，管理者最重要的工作不是直接撸起袖子去干活，而是要把管理效率提升上去；不过笔者觉得这里要考虑到管理者的团队规模，对于规模不大的团队，通过流程机制去提升效率有限，而管理者本身能够亲自下场把手弄脏也是有必要的\n那怎么提高管理效率？靠的就是上面提到的 “心法 + 剑法” 部分，这里主要讲心法部分，也就是心态上的需要做出的一些转变；因为在成为管理者后，首先需要改变的就是心态\n心态跃升包括了四部分：责任、沟通、关系和自我，而这四部分，是从 “自己做事” 到 “通过别人做事” 需要的四个心理上的跃升；这里还是用书里的一张图来讲述\n\n责任\n责任跃升，简单来说就是要从对任务负责过渡到对目标负责\n书里列举了四种类型的责任感：时间、任务、目标、使命；如下图所示\n\n对时间负责的人，遇到挫败时会说 “我一直都在忙，又没闲着”​；对任务负责的人，遇到挫败时会说 “该做的我都做了，问心无愧”​；对目标负责的人，遇到挫败时会说 “一定有办法能走出去，继续寻找”​；对目标负责，管理者需要为不如意的结果与员工犯的错无限兜底，俗称 “能扛事”\n对目标负责，涉及到的很重要的一环是要把目标拆解成任务，这个需要对业务足够了解，并借用一定的方法和技巧，书里就给了这么一个例子，虽然不是非常严谨，但是也给了我们一个新思路：“先拆解，再拆分；拆解是做乘除法，拆分是做加减法”\n\n今年要完成 5000 万元的服装销售额。管理者立马跟我聊：​“我的团队里有 5 个小伙伴，每人完成 1000 万元行不行？​” 我说：​“这是不行的，这是做加减法，是拆分，不是拆解。​”\n拆解是做乘除法。基于对业务的深度理解，可这样拆解销售目标。销售目标 = 店铺粉丝数 × 转化率 × 客单价，然后可以针对目标拆解出来的每一项再做专项提升\n拆分是做加减法。以粉丝数的拆分为例，将达成 17.5 万粉丝的目标拆分为季度目标：第一、第二季度的任务要更重些，这样上半年的业绩数据才不至于难看。然后怎么做？第一，通过优质短视频加粉；第二，通过裂变加粉，以打折等优惠活动推动用户分享店铺给朋友，朋友加粉后再分享给各自的朋友；第三，通过平台做推广活动，用便宜商品引导加粉。这三个任务分别由不同的员工负责。\n\n管理者把目标拆解为任务之后，每个员工就可以专心去忙自己的任务。如果之后还是没达成目标呢？这时候管理者要记住一句话，​“降妖除魔你去，背黑锅我来”​。管理者要扛住这个责任，因为团队管理者要承担目标层面的后果，而员工只承担任务层面的后果\n沟通\n沟通跃升，简单来说就是从用自己的手到用别人的脑\n在这个过程中由于涉及到了多层漏斗，往往会出现低效的问题；因为过去你自己能完成的任务，现在要通过员工来完成，沟通机制发生了变化，你正在从 “无损沟通” 走向 “有损沟通”​，原因如下图所示\n\n沟通是一个很大的话题，在后面会有专门一章来讲这部分，这里主要提出了让沟通尽量无损的方式是，对员工不做直接的否定，书里针对这点给了\n4 种话术，如下图所示\n\n（1）不要说 “这不行”​，而要说 “如果…… 就……”\n从不直接否定别人，而是以建设性的方式跟人沟通，员工缺块砖，你就应该给员工加块砖，员工缺根木头，你就应该给员工加根木头。用好​“如果…… 就……”​，​“如果” 的后面就是你加上去的建议\n（2）不要说 “but”​（但是）​，而要说 “yes…and…”​（是的…… 同时……）\n这个跟 “如果…… 就……” 有点像，不直接否定，因为一说 “但是”​，你就和对方站在了对立面。而只有你和对方站在同一边的时候，才更加有助于达成共识\n（3）“你是不是这么觉得的？​”\n当对方一通说，可能你没有很好抓住对方重点时，可以尝试说 “我帮你总结一下，你是不是这样觉得的？”，然后说出第一点、第二点、第三点……；这个过程会考验你的理解和总结能力；同时这样做还有一个好处，就是当你帮对方总结，对方也会把你整理过的内容当做他的观点\n（4）“我知道你是出于善意”\n这种话主要是应用在这种场景下：你在与下属沟通时，有时他是抵触的，有时他就是想让你难堪，有时他只是要证明自己能力强，跟你观点不一样。这个时候不要跟下属直接吵起来或当面对质，因为没有人会认为自己是那个 “坏人”​，就算他做了坏事，他也一定为自己找好了理由。哪怕你戳穿了他，他也一定会本能地从认知协调出发，维护自己的动机，于是他就会记仇\n这个时候我们可以这么沟通：我注意到你最近做了件什么事（描述行为）​，我知道你是出于善意（肯定动机）​，我看出来了，你还瞒着我，我非常感激。虽然这份善意没有真的起到作用，甚至对我有一些不好的影响，但我还是很感激（表达善意）​。如果你能这样做，就更好了（给出建议）​\n这几种谈话方式，都对管理者的胸怀、格局以及情绪控制等能力都有一定要求，因为善意待人并不是每个管理者都能做到的，尤其是在面对业务压力、来自上一级的压力的情况下；但是如果能做到了，必然是个有魅力的老板\n权力分为三种，即法定权力、专业权力和魅力权力。想让自己的脑指挥别人的手，要充分理解并运用权力的作用机制。如果你只有法定权力，现在年轻的员工根本就不听你的，因为 “90 后”​“95 后” 年轻人的成长环境更多元，家境相对优越，个体意识很强，对权威不盲从。所以团队管理者应将其他两种权力发挥到极致，也就是说，要想指挥优秀年轻人的手，你有两条路：一是让他们佩服你，二是让他们喜欢你；而良好的沟通，在其中是一个很重要的途径\n关系\n关系跃升，简单来说就是从左右的伙伴到上下的战友\n在心理上，管理者需要意识到，员工的关系既不是家人关系，也不是朋友关系，而是战斗友谊\n因为公司或团队是一个战斗单元。是战斗，就有战斗目标。大家是为了达成目标而集结的，一旦目标达成，可能就会解散。这个战斗单元会长期存在，但是战斗单元中的人会时常更换，有人会加入，有人会离开。在战斗中，大家会结下深厚的情谊，但这种情谊依然是战斗情谊。我们在这里际会，但我们不是家人关系，也不是对人不对事的朋友关系\n在行动上，管理者要和员工保持亲而不密的关系\n什么叫 “亲而不密”​？管理者对员工一定要认真地关心，比如关心他们的身心健康，关心他们的持续成长，但是不要保持那么亲密的关系，因为越亲密的关系，越容易导致对人不对事。作为管理者，你要让员工明白，他能如鱼得水，一定是因为承担了更大的责任，而不是跟你有更近的关系\n那该如何做到 “亲而不密”？书里给了两点建议 1）不要拿员工的一针一线\n2）日常交际不要关系过于亲密\n第一点是因为书里提到的一个现象：有的员工送多了东西，觉得自己跟管理者更亲近，甚至会恃宠而骄，仗势欺人，制造团队分裂。但是实际中，会有员工真的因为感激老板这一段时间的帮助和教导，而愿意请老板吃饭或者送一些东西的，即使两者不再\n另外书里的学员案例，还给出了一些比较实用的案例，值得参考\n\n从你被提拔到管理岗位到大家认可你的管理，是存在时间差的。在这个时间窗口，不要一上来就啃硬骨头，跟 “反对派” 死磕，死磕往往是磕不动的，因为你缺乏根基。你应该先好好工作、展现实力，争取得到上级领导的肯定和下属中中间派的拥护\n我有一次开会时与一个资深组员争执得面红耳赤，谁都说服不了对方。那时我忽然意识到，其实我们的方案都有合理的地方，但是碍于面子谁都不愿意退让。于是，后来我在需要做重要决策的会议前，都会先私下跟组员沟通，了解每个人的想法和诉求，再做一些协调工作，确保跟每个组员都能基本达成一致后，再开会把事情敲定下来。这样一来，激烈的争执就不会发生了（体面）\n上司不讲情面，按照要求该批评就批评，该严厉就严厉，坚定地对团队目标负责，看似心硬、心狠，其实这才是真正为所有人好\n\n自我\n自我跃升，简单来说就是从小我的满足到大我的成就\n职场上一直有这一种说法：不要轻易在老板面前提出反对的观点，老板不会表现出自己是同意的，因为他没有办法在下属面前承认自己是错的。书里把这个现象叫 “瞬间顽固症”​：即证明他是对的，比证明这件事本身是对还是错更重要；而原因则是个人的心中的自我 (ego) 边界太小，能量太强\n那什么是自我？猎豹移动（原金山网络）CEO 傅盛对 “自我” 的看法是这样的：​“自我” 是非常感情化的东西，它会在人的内心建立起一种心理防御机制。因为你不喜欢犯错误的感觉，你的本能就总想强行辩驳，别人一批评，你就怒了；因为你害怕面对复杂的东西，你就本能地希望把问题简单化。你的出发点不是为了面对现实，而是充满了 “我我我”—— 这就是自我的障碍\n这个防御的围墙，就是自我边界，或者更通俗来讲就是常说的 “格局”；自我边界会把 “我” 以及 “我关心的” 和他人隔离开，你的同事做得比你好，你会有点嫉妒，因为同事在你的自我边界之外；但是你的孩子做得比你好，你会嫉妒吗？不会的。你甚至会因为孩子做得比自己好而更加高兴，因为孩子进入了你的自我边界内\n一个人的成长，就是从没有自我边界到形成自我边界，再到延展自我边界的过程\n\n当你是员工的时候，有较小的自我是可以的。但是从员工晋升到管理者后，这就有问题了，你会不断面临关于自我的挑战，可能会产生三种重要的害怕：（1）怕自己被证明是错的（2）怕下属的能力超过自己（3）怕下属的影响力超过自己；这会几大影响自己和员工的关系\n那管理者怎么扩大自我的边界？书里给的建议是：关注全局效率，把关注点放到更大的格局上，找两个榜样，父母和君王。管理者要向他们学习，从追求 “小我” 的满足，变成追求 “大我”—— 团队和企业的成就，这样才能拥有更广阔的一片天\n\n一个人的格局、胸怀、气度，指的就是这个人的自我包含了多少东西。管理者要靠团队的成功来获得成功，这就至少要走出第一步，把自我的边界往外扩一层，把下属全都划进来。放大自我并不意味着牺牲自己，下属的成功就是你的成功，他们的快乐就是你的快乐\n","categories":["管理"],"tags":["读书","管理"]},{"title":"凸优化总结","url":"/2017/05/20/%E5%87%B8%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/","content":"之前曾写过一篇最优化课程总结，\n涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。\n本文主要讲述下面内容，凸优化的概念以及凸优化中的三类常见解法：梯度类方法，对偶方法和 ADMM 方法。\n\n凸集，凸函数与凸优化\n凸集\n凸集的定义非常清楚\n\n对于集合 \\(K\\) ，\\(\\forall x_1,x_2 \\in K\\), 若 \\(\\alpha x_1 + (1-\\alpha)x_2 \\in\nK\\), 其中 \\(\\alpha \\in [0,1])\\), 则\n\\(K\\) 为凸集\n\n即集合中任意两点的连线均在凸集中，如在下图中左边的是凸集而右边的不是\n\n\n凸集概念\n\n有时候需要对某个凸集进行放缩转换等操作，对凸集进行以下操作后，得到的集合依然是凸集\n\n凸集的重叠（intersection）部分任然为凸集\n若 \\(C\\) 为凸集，则 \\[aC+b = \\lbrace ax+b , x \\in C, \\forall a,\nb\\rbrace\\] 也为凸集\n对于函数 \\(f(x) = Ax+b\\), 若 \\(C\\)\n为凸集，则下面得到的转换也为凸集，注意这里的 \\(A\\) 是矩阵 \\[f(C)\n= \\lbrace f(x):x\\in C\\rbrace\\] 而当 \\(D\\)\n是一个凸集的时候，下面得到的转换也是凸集 \\[f^{-1}(D) = \\lbrace x: f(x)\\in\nD\\rbrace\\] 这两个转换互为逆反关系\n\n常见的凸集有下面这些 (下式中 \\(a, x,\nb\\) 均为向量，\\(A\\) 为矩阵)\n\n点（point）、线（line）、面（plane）\nnorm ball: \\(\\lbrace x: ||x|| \\le\nr\\rbrace\\)\nhyperplane: \\(\\lbrace x:\na^Tx=b\\rbrace\\)\nhalfspace: \\(\\lbrace x: a^Tx \\le\nb\\rbrace\\)\naffine space: \\(\\lbrace x: Ax =\nb\\rbrace\\)\npolyhedron: \\(\\lbrace x: Ax &lt;\nb\\rbrace\\)\n\n\npolyheron 的图像为 \n\n凸函数\n凸函数的定义如下\n\n设 \\(f(x)\\) 为定义在 n 维欧氏空间中某个凸集 S 上的函数，若对于任何实数 \\(\\alpha(0&lt;\\alpha&lt;1)\\) 以及 S 中的任意不同两点\n\\(x\\) 和 \\(y\\)，均有 \\[f(\\alpha x+ (1-\\alpha)y) \\le \\alpha f(x) +\n(1-\\alpha)f(y)\\] 则称 \\(f(x)\\) 为定义在凸集 S 上的凸函数\n\n凸函数的定义也很好理解，任意两点的连线必然在函数的上方，如下是一个典型的凸函数\n\n\n凸函数\n\n严格凸 (strictly convex) 与強凸 (strongly convex)\n\n严格凸指的是假如将上面不等式中的 \\(\\le\\) 改为 \\(\\lt\\)， 则称该函数为严格凸函数。\n严格凸指的是 \\(\\forall m &gt; 0, f -\n\\frac{m}{2}||x||_2^2\\)\n也是凸的，其含义就是该凸函数的 “凸性” 比二次函数还要强，即使减去一个二次函数还是凸函数\n\n凸函数有几个非常重要的性质，对于一个凸函数 \\(f\\), 其重要性质 1.\n一阶特性（First-order characterization）： \\[f(y) \\ge f(x) + \\nabla f(x)(y - x)\\] 2.\n二阶特性（Second-order characterization）： \\[\\nabla^2f(x) \\succeq 0\\] 这里的 \\(\\succeq 0\\) 表示 Hessian 矩阵是半正定的。\n3. Jensen 不等式（Jensen’s\ninequality）：\\[f(E(x)) \\le\nE(f(x))\\] 这里的 \\(E\\)\n表示的是期望，这是从凸函数拓展到概率论的一个推论，这里不详细展开。 4.\nsublevel sets，即集合 \\(\\lbrace x:f(x) \\le t\\rbrace\\)\n是一个凸集\n其中，一阶特性或二阶特性是一个函数为凸函数的充要条件，通常用来证明一个函数是凸函数。\n常见的凸函数有下面这些\n\n仿射函数 (Affine function): \\(a^Tx +\nb\\)\n 二次函数 (quadratic function), 注意这里的 \\(Q\\) 必须为半正定矩阵: \\(\\frac{1}{2}x^TQx + b^Tx+c(Q \\succeq\n0)\\)\n 最小平方误差 (Least squares loss): \\(||y-Ax||_2^2\\) (总是凸的，因为 \\(A^TA\\) 总是半正定的)\n 示性函数（Indicator function）：\\[I_C(X) = \\begin{cases} 0&amp;x \\in C\\\\\\\n\\infty &amp; x \\notin C\\end{cases}\\]\nmax function: \\(f(x) = max \\lbrace\nx_1,...x_n \\rbrace\\)\n 范数（Norm）：范数分为向量范数和矩阵范数，任意范数均为凸的，各种范数的定义如下\n\n向量范数\n\n0 范数：$||x||_0 $= 向量中非零元素的个数 1 范数： \\(||x||_1 = \\sum_{i=1}^n |x_i|\\) \\(p\\) 范数：\\(||x||_p = (\\sum_{i=1}^nx_i^p)^{1/p}~~(p &gt;\n1)\\) 无穷范数: \\(||x||_{\\infty} =\nmax_{i=1,...n} |x_i|\\)\n\n矩阵范数 &gt; 核 (nuclear) 范数: \\(||X||_{tr} = \\sum_{i=1}^{r}\\sigma_i(X)\\) ,\n(\\(\\sigma_i(X)\\) 是矩阵分解后的奇异值，核范数即为矩阵所有奇异值之和)\n&gt; 谱（spectral）范数：\\(||X||_{op} =\nmax_{i=1,...r}\\sigma_i(X)\\), 即为最大的奇异值\n凸优化\n对于下面的优化问题\n\\[\\begin{align}\n&amp;\\min_x\\quad f(x)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;g_i(x) \\le 0,~i=1,\\ldots,m\\\\\\\n&amp;h_j(x)=0,~j=1,\\ldots,r\n\\end{array}\n\\end{align}\\]\n当 \\(f(x), g_i(x)\\) 均为凸函数，\n而 \\(h_j(x)\\) 为仿射函数（affine\nfunction）时，该优化称为凸优化，注意上面的 \\(\\min\\) 以及约束条件的符号均要符合规定。\n凸优化也可以解释为目标函数 \\(f(x)\\)\n为凸函数而起约束围成的可行域为一个凸集。\n常见的一些凸优化问题有：线性规划（linear\nprograms），二次规划（quadratic programs），半正定规划（semidefinite\nprograms），且 \\(LP \\in QP \\in\nSDP\\), 即后者是包含前者的关系。\n线性规划问题一般原型如下 (\\(c\\) 为向量，\\(D,A\\) 为矩阵)\n\\[\\begin{align}\n&amp;\\min_x\\quad c^Tx\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;Dx \\le d\\\\\\\n&amp;Ax=b\n\\end{array}\n\\end{align}\\]\n二次规划问题一般原型如下（要求矩阵 \\(Q\\) 半正定）\n\\[\\begin{align}\n&amp;\\min_x\\quad \\frac{1}{2}x^TQx+c^Tx\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;Dx \\le d\\\\\\\n&amp;Ax=b\n\\end{array}\n\\end{align}\\]\n而半正定规划问题一般原型如下 (\\(X\\)\n在这里表示矩阵) \\[\\begin{align}\n&amp;\\min_X\\quad CX\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;A_iX \\le b_i, i=1,...m\\\\\\\n&amp;X \\succeq 0\n\\end{array}\n\\end{align}\\]\n梯度类方法\n梯度类方法是无约束优化中非常常用的方法，其依据的最根本的事实就是梯度的负方向是函数值下降最快的方向。但是常用的\ngradient descent\n必须要求函数的连续可导，而对于某些连续不可导的问题（如 lasso\nregression），gradient descent\n无能为力，这是需要用到 subgradient descent 和 proximal gradient descent.\ngradient descent\n梯度下降法的迭代公式为 \\[x^{(k)} =\nx^{(k-1)} - t_k\\nabla f(x^{(k-1)} )\\]\n上式中上标 \\((k)\\) 表示第 \\(k\\) 次迭代，而 \\(t_k\\) 表示步长，\\(\\nabla f(x^{(k-1)} )\\) 表示在点 \\(x^{(k-1)}\\) 的梯度。\n这里对于梯度下降主要讨论其步长选择的问题，\n最简单直接的方式是固定每次的步长为一个恒定值，但是如果步长过大或过小时，可能会导致结果难以收敛或者收敛速度很慢。因此提出了可变长步长的方法，可变长步长的方法指的是根据每次迭代依照一定的规则改变步长，下面介绍两种：backtracking line search\n和 exact line serach。\nbacktracking line search\nbacktracking line search 需要先选择两个固定的参数 \\(\\alpha, \\beta\\), 要求 \\(0 &lt; \\beta &lt; 1,\n0&lt;\\alpha&lt;1/2\\)\n每次迭代的时候，假如下式成立\n\\[\\begin{align} f(x - t\\nabla f(x)) &gt;\nf(x) - \\alpha t||\\nabla f(x)||_2^2 \\end{align}\\]\n则改变步长为 \\(t = \\beta t\\),\n否则步长不变。\n这种方法的思想是当步长过大的时候 (即跨过了最优点)，减小步长，否则保持步长不变，如下式是一个简单的例子\n\n\nbacktracking line search\n\nexact line serach\nexact line serach 则是得到先计算出梯度 \\(\\nabla f(x^{(k-1)}\n)\\), 然后代入下面的函数中，此时只有步长 \\(t_k\\) 是未知，因此可对 \\(t_k\\) 进行求导并令其为 0，求得的 \\(t_k\\)\n即为当前的最优的步长，因为这个步长令当前迭代下降的距离最大。\n\\[\\begin{align} f(x^{(k-1)} - t_k\\nabla\nf(x^{(k-1)} )) \\end{align}\\]\n这种方法也被称为最速下降法。\nsubgradient descent\nsubgradient 可以说是 gradient\n的升级版，用于解决求导时某些连续不可导的函数梯度不存在的问题，我们知道，对于可微的凸函数有一阶特性，即\n\\[\\begin{align} f(y) \\ge f(x) + \\nabla^T\nf(x)(y - x) \\end{align}\\]\n加入将上面的 \\(\\nabla^T f(x)\\) 换成\n\\(g^T\\) 且不等式恒成立，则 \\(g\\) 被称为 subgradient，当函数可微时，\\(\\nabla f(x) = g\\)\n，但是若函数不可微，subgradient\n不一定存在，下面是几个特殊函数的 subgradient 例子。\n对于函数 \\(f(x) =\n|x|\\), 其图像如下\n\n\nsub1\n\n其 subgradient 为 \\[g = \\begin{cases}sign(x)\n&amp;x \\neq 0\\\\\\\n[-1,1] &amp; x=0\\end{cases}\\]\n对于函数 \\(f(x) =\n||x||_2\\), 其图像如下\n\n\nsub2\n\n其 subgradient 为 \\[g = \\begin{cases}\nx/||x||_2&amp;x \\neq 0\\\\\\\n\\lbrace z: ||z||_2 \\le 1\\rbrace &amp; x=0\\end{cases}\\]\n对于函数 \\(f(x) =\n||x||_1\\), 其图像如下\n\n\nsub3\n\n其 subgradient 为 \\[g = \\begin{cases}\nsign(x_i) &amp;x_i \\neq 0\\\\\\\n[-1,1] &amp; x_i=0\\end{cases}\\]\n对于两个相交的函数 \\(f(x) = \\max \\lbrace\nf(x_1), f(x_2)\\rbrace\\), 设其函数图像如下\n\n\nsub4\n\n则其 subgradient 为 \\[g = \\begin{cases}\n\\nabla f(x_1) &amp;f(x_1) &gt; f(x_2) \\\\\\\n\\nabla f(x_2) &amp;f(x_1) &lt; f(x_2) \\\\\\\n[\\min \\lbrace \\nabla f(x_1),\\nabla f(x_2)\\rbrace, \\max \\lbrace \\nabla\nf(x_1), \\nabla f(x_2)\\rbrace] &amp;f(x_1) = f(x_2)\\end{cases}\n\\]\n而 subgradient descent 与 gradient descent\n的不同地方就是当函数不可微的时候，将 gradient descent 中更新公式中的\ngradient 换成 subgradient。下面看一个经典的\nlasso regression 问题。\n\\[\\begin{align} \\min_{\\beta \\in \\mathbb\n{R}^n} \\frac{1}{2} ||y-\\beta||_2^2 + \\lambda ||\\beta||_1~~(\\lambda \\ge\n0) \\end{align}\\]\n对目标函数求导并令其为 0，其中 \\(||\\beta||_1\\)\n项不可导，用前面提到的 subgradient 代替，则有以下等式\n\\[\\begin{cases} y_i - \\beta_i = \\lambda\nsign(\\beta_i) &amp;\\beta_i \\neq 0\\\\\\\n|y_i - \\beta_i| \\le \\lambda &amp;\\beta_i = 0\n\\end{cases}\\]\n则解可表示成\n\\[\\beta_i = \\begin{cases} y_i - \\lambda\n&amp; y_i &gt; \\lambda\\\\\\\n0 &amp;-\\lambda \\le y_i \\le \\lambda \\\\\\\ny_i + \\lambda &amp; y_i &lt; -\\lambda\\end{cases}\\]\n上面实际上是简化了的 lasso regression，\n因为更一般的 lasso 问题表示如下\n\\[\\begin{align} \\min_{\\beta \\in \\mathbb\n{R}^n} \\frac{1}{2} ||y-X\\beta||_2^2 + \\lambda ||\\beta||_1~~(\\lambda \\ge\n0) \\end{align}\\]\n这时如果采用上面的解法，那么会得到\n\\[\\begin{cases} X_i^T(y - X\\beta) =\n\\lambda sign(\\beta_i) &amp;\\beta_i \\neq 0\\\\\\\n|X_i^T(y - X\\beta)| \\le \\lambda &amp;\\beta_i = 0\n\\end{cases}\\]\n上式并没有为这个 lasso\n问题提供一个明确的解，这个问题可以通过下面要提到的 proximal gradient\n进行求解，但是上面的式子一定程度上解释了 L1 regularization 的导致的参数的稀疏性的特点，从上面的表达式可知，只有当\n\\(X_i^T(y - X\\beta) = \\lambda\nsign(\\beta_i)\\) 时，对应的 \\(\\beta_i\\) 才不为 0，而其他大多数的情况下\n\\(\\beta_i\\) 为 0.\nproximal gradient descent\nproximal gradient descent 也可以说是 subgradient\n的升级版，proximal 通过对原问题的拆分并利用 proximal\nmapping，能够解决 subgradient descent 无法解决的问题（如上面的一般化\nlasso 问题）。\n一般来说，这类方法将目标函数描述成以下形式\n\\[\\begin{align} f(X) = g(x) + h(x)\n\\end{align}\\]\n上面的 \\(g(x)\\) 是凸且可微的， 而\n\\(h(x)\\) 也是凸的，但是不一定可微。则\nproximal gradient descent 的迭代公式为\n\\[\\begin{align}x^{(k)} = x^{(k-1)} -\nt_kG_{t_k}(x^{(k-1)})\\\\\\\nG_{t}(x) = \\frac{x - prox_{th}(x-t\\nabla g(x))}{t}\\\\\\\nprox_{th}(x) = argmin_{z\\in \\mathbb {R}^n}\n\\frac{1}{2t}||x-z||_2^2+h(z)\\end{align}\\]\n上面 \\(prox_{th}(x)\\) 表示对函数\n\\(h\\) proximal\nmapping。这里仅给出结论，证明过程略，接下来以上面没有解决的一般化的\nlasso 问题为例讲述这种方法的应用。\n\\[\\begin{align} \\min_{\\beta \\in \\mathbb\n{R}^n} \\frac{1}{2} ||y-X\\beta||_2^2 + \\lambda ||\\beta||_1~~(\\lambda \\ge\n0) \\end{align}\\]\n记 \\(g(\\beta) = \\frac{1}{2}\n||y-X\\beta||_2^2, h(\\beta) = \\lambda ||\\beta||_1\\)\n则有 \\[\\begin{align} prox_{th}(\\beta) =\nargmin_{z \\in \\mathbb {R}^n} \\frac{1}{2t}||\\beta -\nz||_2^2+h(z)  \\end{align}\\]\n这个问题通过前面的 subgradient 方法已经解出来，结果为\n\\[z_i = \\begin{cases} y_i - \\lambda t\n&amp; y_i &gt; \\lambda t\\\\\\\n0 &amp;-\\lambda t \\le y_i \\le \\lambda t\\\\\\\ny_i + \\lambda t &amp; y_i &lt; -\\lambda t\\end{cases}\\]\n将上面的解记为 \\(S_{\\lambda t}(y)\\),\n同时 \\(\\nabla g(\\beta) = -\nX^T(y-X\\beta)\\)\n代取上面列出的 proximal gradient descent 列出的迭代公式，则 \\(\\beta\\) 的迭代公式如下\n\\[\\begin{align} \\beta^{(k)} = S_{\\lambda\nt}(\\beta^{(k-1)} + t_kX^T(y-X\\beta^{(k-1)})) \\end{align}\\]\n其中 \\(t_k\\) 为步长。\n这种方法之所以比 subgradient 方法更加一般化，是因为 \\(prox_{th}(x)\\) 对于绝大部分的 \\(h\\) 是易求的。\n对偶方法与 KKT 条件\n对偶理论在最优化中非常重要，其中具有代表性的两条定理是弱对偶定理和强对偶定理，弱对偶定理告诉了我们最优化的目标的上界 (\nmax 问题) 或下界 (min 问题)，而强对偶定理告诉了当 KKT\n条件满足的时候，可以通过对偶问题的解推出原问题的解。\n弱对偶条件总是成立，而强对偶需要在 Slater's condition\n成立时才成立，该条件描述如下\n对于优化问题\n\\[\\begin{align}\n&amp;\\min_x\\quad f(x)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;g_i(x) \\le 0,~i=1,\\ldots,m\\\\\\\n&amp;h_j(x)=0,~j=1,\\ldots,r\n\\end{array}\n\\end{align}\\]\n假如存在可行解 \\(x'\\) 使得 \\(g_i(x') &lt; 0(i=1,...m)\\),\n即不等式约束严格成立（注意同时等式约束也要成立，否则就不是可行解了），那么称 Slater's condition\n成立，同时强对偶也成立。\n线性规划对偶\n对于形如下面的线性规划问题\n\\[\\begin{align}\n&amp;\\min_x\\quad c^Tx\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;Ax = b\\\\\\\n&amp;Gx \\le h\n\\end{array}\n\\end{align}\\]\n其对偶问题为\n\\[\\begin{align}\n&amp;\\min_{u,v}\\quad -b^Tu - h^Tv\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;-A^Tu - G^Tv = c\\\\\\\n&amp; v \\ge 0\n\\end{array}\n\\end{align}\\]\n对于 LP 问题，其对偶的特殊性在于只要存在原问题存在可行解，那么\nSlater's condition 一定成立，因此强对偶也成立.\nLP 对偶问题的一个经典的例子是最大流 (max flow) 问题和最小割 (min\ncut) 问题.\n拉格朗日对偶\n对于更一般的非 LP\n问题的对偶问题，需要用到拉格朗日对偶理论得到，并称该问题为拉格朗日对偶问题。\n这个方法可以说是对求解等式约束的拉格朗日乘子法的一个推广。\n对于下面的优化问题\n\\[\\begin{align}\n&amp;\\min_x\\quad f(x)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;g_i(x) \\le 0,~i=1,\\ldots,m\\\\\\\n&amp;h_j(x)=0,~j=1,\\ldots,r\n\\end{array}\n\\end{align}\\]\n其増广拉格朗日函数为\n\\[\\begin{align} L(x,u,v) = f(x) +\n\\sum_{i=1}^m u_ig_i(x) + \\sum_{j=1}^r v_jh_j(x)~~(u_i \\ge 0)\n\\end{align}\\]\n则原问题的拉格朗日对偶函数为\n\\[\\begin{align}  g(u,v) = \\min_x L(x,u,v)\n\\end{align}\\]\n且原问题的对偶问题为\n\\[\\begin{align}\n&amp;\\max_{u,v}\\quad g(u,v)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;u_i \\ge 0,~i=1,\\ldots,m\n\\end{array}\n\\end{align}\\]\n上面得到对偶问题的简单推导流程如下：\n首先原问题的目标函数加上约束可以表示为\n\\[\\begin{align} f(x) = \\max_{u,v} L(x,u,v)\n\\end{align}\\]\n原因是在增广拉格朗日函数中，假如 \\(x\\) 违反了约束条件 (即 $ g (x) &gt; 0 $\n)，那么 \\(f(x)\\)\n会趋向无穷大；而不违反约束条件时，\\(u\\)\n必须取 0 才能使得目标最小。\n进一步，原问题可以表示为 \\[\\min_x\n\\max_{u,v} L(x,u,v) \\]\n而由于下式恒成立（弱对偶定理）\n\\[\\begin{align} \\min_x \\max_{u,v} L(x,u,v)\n\\ge \\max_{u,v} \\min_xL(x,u,v)  \\end{align}\\]\n因此将 \\(\\min_xL(x,u,v)\\)\n作为对偶函数， \\(\\max_{u,v}\n\\min_xL(x,u,v)\\) 作为对偶问题。\n假设现在找到了对偶问题，并且对偶问题比原问题要容易求解得多，也求出了对偶问题的解，那么该转化去求原问题的解，这里就要用到下面的 KKT 条件。\nKKT 条件\nKKT\n条件是非线性规划领域中最重要的理论成果之一，是确定某点是最优点的一阶必要条件，只要是最优点就一定满足这个条件，但是一般来说不是充分条件，因此满足这个点的不一定是最优点。但对于凸优化而言，KKT 条件是最优点的充要条件。\n同样地\n对于下面的优化问题\n\\[\\begin{align}\n&amp;\\min_x\\quad f(x)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp;g_i(x) \\le 0,~i=1,\\ldots,m\\\\\\\n&amp;h_j(x)=0,~j=1,\\ldots,r\n\\end{array}\n\\end{align}\\]\n其増广拉格朗日函数为\n\\[\\begin{align} L(x,u,v) = f(x) +\n\\sum_{i=1}^m u_ig_i(x) + \\sum_{j=1}^r v_jh_j(x)~~(u_i \\ge 0)\n\\end{align}\\]\n则 KKT 条件为\n\\[\\begin{cases} \\nabla_x L(x,u,v) = 0\\\\\\\nu_ig(x) = 0\\\\\\\nu_i \\ge 0\\\\\\\ng_i(x) \\le 0, h_j(x)=0\n\\end{cases}\\]\n因此其实只要能够求解出上面的联立方程组，得到的解就是最优解（对于凸优化而言，非凸的问题一般用 KKT 来验证最优解）。\n但是上面的方程组往往很难求解，一些特殊情况下解是有限的，可以分类讨论；但是更一般的情况下可能的解是无限的，因此无法求解。这里要结合上面的拉格朗日对偶问题得到的解进行求解。\n求解之前，首先要知道下面的定理 &gt; 假如一个问题满足强对偶，那么 \\(x',u',v'\\)\n是原问题和对偶问题的最优解 \\(\\longleftrightarrow\\) \\(x',u',v'\\) 满足 KKT 条件。\n因此通过对偶问题求得 \\(u',v'\\) 后，带入上面的 KKT\n条件即可求出 \\(x'\\) 。\nsvm\n是利用拉格朗日对偶和 KKT 条件进行求解的经典问题，这里不详细展开，有兴趣的可以参考\nAndrew\nNg 公开课中关于 svm 的那章或这篇文章。\nADMM\nADMM (Alternating Direction Method of Multipliers)\n是解决带约束的凸优化问题的一种迭代解法，当初提出这个算法最主要的目的是为了在分布式环境 (Hadoop,\nMPI 等) 中迭代求解这个问题，关于这方面的资料可参考这里\nADMM 将要解决的问题描述成以下形式\n\\[\\begin{align}\n&amp;\\min_x\\quad f_1(x_1) + f_2(x_2)\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t.&amp; A_1x_1+A_2x_2 = b\n\\end{array}\n\\end{align}\\]\n这里省略证明过程，直接给出 ADMM 的迭代公式\n\\[\\begin{align}\n&amp;x_1^{(k)} = argmin_{x_1} f_1(x_1) +\n\\frac{\\rho}{2}||A_1x_1+A_2x_2^{(k-1)} - b + w^{(k-1)}||_2^2\\\\\\\n&amp;x_2^{(k)} = argmin_{x_2} f_2(x_2) + \\frac{\\rho}{2}||A_1x_1^{(k)} +\nA_2x_2 - b + w^{(k-1)}||_2^2\\\\\\\n&amp;w^{(k)} = w^{(k-1)} + A_1x_1^{(k)} + A_2x_2^{(k)} - b\n\\end{align}\\]\n上式中的 \\(\\rho\\)\n是事先选择的参数，可以选择定值，选择定值的时候会遇到跟梯度下降固定步长的类似问题，因此也可以根据每次迭代情况改变\n\\(\\rho\\) 的值，具体可参考这篇文献。\n实际中，难点在于把一个问题变为 ADMM\n求解的形式，即上面列出的优化问题的形式。下面给出一个例子说明这个问题，这个例子是一个更一般的\nlasso regression 问题，称为 fused lasso regression.\n\\[\\begin{align} \\min_{\\beta \\in \\mathbb\n{R}^n} \\frac{1}{2} ||y-X\\beta||_2^2 + \\lambda ||D\\beta||_1~~(\\lambda \\ge\n0) \\end{align}\\]\n一般的 lasso regression 问题中 \\(D =\nI\\). 下面用 ADMM 的形式表示上面的问题\n\\[\\begin{align}\\min_{\\beta \\in \\mathbb\n{R}^n，\\alpha \\in \\mathbb {R}^n} \\frac{1}{2} ||y-X\\beta||_2^2 + \\lambda\n||\\alpha||_1~~(\\lambda \\ge 0)\\\\\\\ns.t. D\\beta - \\alpha = 0\\end{align}\\]\n将这个问题映射到上面的优化问题的模式有\n\\[\\begin{align}\\frac{1}{2}\n||y-X\\beta||_2^2\\rightarrow f_1(x1) \\\\\\\n\\lambda ||\\alpha||_1 \\rightarrow f_2(x_2)\\\\\\\nD\\beta - \\alpha = 0 \\rightarrow A_1x_1+A_2x_2 =\nb\\end{align}\\]\n则根据上面的迭代公式计算（对问题变量求导且令结果为 0）可得到下面的迭代公式\n\\[\\begin{align}\n&amp;\\beta^{(k)} = (X^TX+\\rho D^TD)^{-1}(X^Ty+\\rho\nD^T(\\alpha^{(k-1)}-w^{(k-1)})\\\\\\\n&amp;\\alpha^{(k)} = S_{\\lambda/\\rho}(D\\beta^{(k)}+w^{(k-1)})\\\\\\\n&amp;w^{(k)} = w^{(k-1)} + D\\beta^{(k)} - \\alpha^{(k)}\n\\end{align}\\]\n而对于无约束的优化也可以通过 ADMM 求解，例如对于下面的问题\n\\[\\begin{align} \\min_x \\sum_{i=1}^B f_i(x)\n\\end{align}\\]\n将其表示为 ADMM 的形式如下\n\\[\\begin{align}\\min_{x_1,..x_B,x}\n\\sum_{i=1}^B f_i(x_i)\\\\\\\ns.t. x_i = x,~~i=1,..B\\end{align}\\]\n则 ADMM 的迭代规则如下\n\\[\\begin{align}\n&amp;x_i^{(k)} = argmin_{x_i}\nf_i(x_i)+\\frac{\\rho}{2}||x_i-x^{(k-1)}+w_i^{(k-1)}||~(i=1,..B)\\\\\\\n&amp;x^{(k)} = \\frac{1}{B} \\sum_{i=1}^B (x_i^{(k)} + w_i^{(k-1)})\\\\\\\n&amp;w_i^{(k)} = w_i^{(k-1)} + x_i^{(k)} - x^{(k)}~(i=1,..B)\n\\end{align}\\]\n","categories":["数学"],"tags":["数学"]},{"title":"关键跃升：角色","url":"/2024/09/17/%E5%85%B3%E9%94%AE%E8%B7%83%E5%8D%87_%E8%A7%92%E8%89%B2/","content":"在此前的文章 《关键跃升：心态》中，介绍了书的上半部分，即心态上的需要做出的一些改变，本文主要讲的是剑法法部分，即管理者需要扮演好四种角色 “鼓手、教练、政委、指挥”，这部分也涉及到比较多的方法论，值得一读～\n\n怎么提升管理效率，书里给出了这个公式 “管理效率 = 动力 × 能力 × 沟通 × 协作”；管理者和团队的关系，就像赛车手和赛车的关系，管理者需要理解这辆车的构成，才能让这辆车跑起来；书里也把上面提到的四部分类比成了汽车的组成部分，如下图所示（ps，书里给出了很多直观易懂的图，而这也是高效沟通的方式之一，以图代写，后面也会提及）\n\n\n动力：员工愿不愿意干，除了员工自身的责任感，管理者要激励手段，扮演好 “鼓手” 的角色\n能力：员工能不能干，激励不能获得能力，除了通过招聘筛选，管理者还需要投入精力培养员工，扮演好 “教练” 的角色\n沟通：让整个团队更有凝聚力，需要建立管理者与员工的沟通机制，管理者需要扮演好 “政委” 的角色\n协作：如何调配整个团队，让每个人能够各司其职，管理者需要扮演好 “指挥” 的角色\n\n动力和能力是针对个体的，沟通和协作是针对整体的；四个要素彼此相乘，才能得到预期的效率。上面的公式之所以用乘法，是因为其中任何一个要素为\n0，都会导致团队满盘皆输；而合格的管理者需要拆解好这四个部分来提高团队效率\n鼓手：动力\n作为管理者，你要通过别人，而不是通过自己完成工作了；以前你可能不需要理解 “人” 是怎么回事，但现在你必须理解了。因此学管理第一件要做的事情就是趴在地上学人性，而不是浮到空中指挥交通\n理解 “人” 的第一步，就是把人的 “动力系统” 拆开，仔细看清楚里边的结构。然后针对不同的员工，用不同的方法去激发他们的动力，这样团队才能由一个火车头带动变成动车组联动\n人心动力系统，包括四台发动机，即防御动力、获得动力、结伴动力和学习动力，方向、强度和持久性，就是人心发动机的性能特征。情绪是动力的燃料，6\n种燃料类型分别是愤怒与恐惧、寻赏与意义，责任、爱好，如下图所示\n\n防御动力\n如何启动防御动力发动机？依赖的是愤怒和恐惧两种情绪燃料\n愤怒和恐惧的动力强劲但不持久，管理者可以不常用，但一定要会用。恐惧会让人努力，如绩效考核、末位淘汰是最常见的借助恐惧情绪；设立各种排行榜是借助了愤怒情绪（好胜心）\n书里给的办法是：一只鸡、一条鱼、一个炉子和一个假想敌，见下图\n\n\n一只鸡：杀鸡儆猴\n\n遇到业绩不好的，一定要处理，要么进行降薪降职，要么调岗、淘汰。让 “南郭先生” 不能混日子，是对业绩好的人的尊重。对 “南郭先生” 的宽容，就是对努力干活人的残忍\n\n一条鱼：鲶鱼效应\n\n要有优秀后备员工，这能给懈怠的老员工带来危机感\n\n一个炉子：热炉法则\n\n善用恐惧让员工远离不好的事情。如果员工违背企业文化，管理者应该批评他；如果员工违反规章制度，管理者必须惩罚他，这是用人的底线\n热炉法则的四个原则：警告性（提前亮红灯，告知红线）、一致性（每次触碰都一定会收到到惩罚）、即时性（在触碰后立即启动惩罚，不拖延）、公平性（一视同仁）\n\n一个假想敌\n\n内部矛盾向外部转移，eg“我们的产品虽然在市场上相对领先，但又有一家创业公司发展起来了，又有一个新产品的势头起来了，这些对手对我们构成了巨大的威胁，我们必须打败他们”\n获得动力\n寻赏与意义是获得发动机的燃料。寻赏就是找到大家最需要什么，比如常见的是升职加薪；意义则是给事情赋予更大的意义感，比如我们做的事情是在改变一部分人的生活\n其中，寻赏是极为重要的燃料，因为对于绝大部分人来讲，工作就是为了养家糊口，升职加薪是很多人动作的最大动力。任正非说：​“钱分好了，管理的一大半问题就解决了。​”\n赏罚需要分明，书里给出了三条原则\n（1）要按照大家认可的规则，而不是按照自己的价值观来行使奖赏权力\n（2）奖赏规则一定要在大家努力之前就制定好，不要等结果出来后再定\n（3）奖赏一定要跟个体的努力所做出的贡献相关，而不是跟群体的结果相关\n对应着的反例，就是即兴分配、有结果再讨论分配以及无差别分配，如下图所示\n\n关于第（3）点，主要是想强调不能吃大锅饭，这意味着要为每个人明确其具体职责，不要让间接影响结果的人，去承担直接的目标。比如说不能让技术人员去扛销售目标。为什么？因为技术人员的努力不能直接改变销售的结果。虽然技术人员的努力可以提高产品质量，并且产品质量提高了，肯定会影响销售结果，但是，这种影响是间接的。你可以为技术人员设定与产品质量相关的考核指标，让他直接对产品质量负责。但千万不要绕弯子，让他对销售结果负责\n每个人都只能对自己直接能改变的结果负全责。所以，钱只能分给那些通过自己努力能够改变结果的人。这也叫 “责任承包制”​，管理者根据每个人的努力结果来赏和罚，要做到发货人只对发货的结果有责任，客服只对客服的结果有责任，技术只对产品的质量而不是销量有责任\n获得动力的另一种燃料，是意义，这是一个听起来很虚的概念；因为对于大部分人来说，生存焦虑就够足够烦了，工作只是谋生的一种手段，没有太多意义；但是对于已经摆脱了生存焦虑的人，或者薪酬差不多情况下，选择更有意义的工作，也是一个合理的选择\n书里对意义的定义也比较宏大：意义，就是比自身更重要的事情。意义是利他的，超越了基因所控制的生存、繁衍等人生目的，我们可以从中获得真正的自由。意义，是脱离了基本需求的、真正高级的人想要创造的人生价值\n当然，如果意义连你自己都不信，你却要员工信，这就不叫意义了，而叫画饼，甚至是欺骗\n结伴动力\n结伴动力的燃料是责任感\n书里把这里的责任感分为三大类：自我、团队和客户，如下图所示\n\n如何建立员工的责任感？除了员工自身本来的责任感，也有一些方法来实现这一点，有一个这样的例子\n\n一个下属把方案做砸了，你非常恼火。这时候你该怎么办？\n是对他说 “这次做得不够好，说明你还有很大的提升空间”​，还是说 “你这个猪脑子，这种错你也能犯”​？建议都不要说。前者是压抑着怒火的鼓励，后者是情绪失控的批评，二者都不能解决问题，都不太好\n也许你可以对他说：最近你遇到什么事了吗？你一直是一个特别积极、特别为集体着想的人，但是在这件事上没有表现出来，到底是哪里出了问题呢？\n\n建议的说法先给了对方一个认知 “你是一个负责的人”，然后说对方的行为跟这个不符，然后做出改变；那为什么这样说有效\n有个概念叫 “认知协调”：它是人的一种心理机制，让你的行为和你的认知始终保持一致。如果二者不一致，你就会难受。这种难受会促使你调整行为或者调整认知，最终实现协调；而如果直接骂对方” 做事不负责 “，有可能会让对方心里也认同” 自己不负责 “的这个概念，然后在行为上也表现出跟这个说法不一致的情况\n上面这个案例其实就是增加员工自我责任感的一种重要方法。尊重对方的高自尊，而不是\nPUA， 才是批评的基础\n具体的实施过程跟上面的案例差不多，你首先肯定他是对自己有高要求的人，这样他也会告诉自己：对啊，这不是我这样的人该做的事啊，到底出了什么问题？这样你们就可以平心静气地讨论问题，他才会改进，因为你很好地激发了他对自己名声的责任感。而在交代完任务之后，管理者可以表达自己对员工的期待：​“我认为你完全具备这个能力，好好做，别让我失望。​” 某种程度上这是一种他人预言的实现\n团队责任感，作者常跟员工说的一句话是：你的一言一行都代表着整个团队，而不仅仅是你自己；书里提到了一个 “月度感谢卡” 活动，用来增强团队责任感，简单来说就是每个月组织一个员工之间的感谢活动，核心的点有四个\n1）感谢的事件要具体\n2）感谢的事件不能是对方的本职工作；你感谢的内容，应该是对方在本职工作以外给你的帮助\n3）不能感谢上级，上级为你做的，都是他应该做的\n4）当面表达，如月度下午茶会\n对于客户责任感，需要员工不仅仅关注数据和指标，而是要关注到使用这些产品的背后有血肉、有情感、有生活的人，才能激发员工对客户的的责任感；可以用客户的真实案例和反馈来建立员工的责任感。比如说让一些技术、产品等员工做客户回访\n学习动力\n学习动力的燃料是爱好\n几乎没有一个人的爱好是上班本身这件事，但是上班这个过程能给我们带来的一些附加物，这些附加物能够让我们在上班这件事上获得一些持续的动力\n寻赏是群体的基本诉求，但爱好是个性化的诉求，爱好有三个源头：兴趣、成就感、成长；如下图所示\n\n1）兴趣：基本性格有关，比如说更喜欢做业务还是横向技术迭代\n2）成就感：利用了人的反馈机制\n3）成长：利用了人的上进心\n具体的做法，上面图里也列出来了，在兴趣这一点上，多观察员工的兴趣点在什么地方，并经常跟员工聊一聊他喜欢做的事；在上成就感，尝试 “游戏化管理”，积分、勋章（比如说公司常见的各种奖励和头衔等，如优秀导师、xx\n奖项等）；成长则是尽量让一个人处于 “学习区”\n什么是 “学习区”？当一个人做的是自己非常擅长的事情时，他处于 “舒适区”​，这时他是没有成就感的。当他做的是自己完全不擅长的事情时，他处于 “恐惧区”​，心理上的严重不适可能会让他崩溃。当他做的事在他擅长和不擅长之间时，他处在舒适区和恐惧区之间，也就是处在学习区，他会有一种攻克难关之后的成长的快乐，如下图所示\n\n但坦白说，真的有人喜欢工作么？答案基本是否定了，但是在其位谋其职，需要把本质工作做好，同时如果能在这个过程中找到那么一点热情，会让自己、团队和公司都受益\n教练：能力\n靠谱的员工是团队或者说公司最宝贵的财富，因此，管理者需要掌握的很重要能力，就是 “选、育、用、留、汰”，书里的这部分，不怎么包含 “选” 即面试选人这部分，但是其他几个部分都有涉及\n从 IC 到\nmanager，最容易犯的一个错误是认为员工不如自己；但是仔细想想，如果员工比你强，升职轮得到你么？他会服你么？答案显然是不（当然这里也有时机问题，比如很早进入了一家快速发展\n的公司，但是要相信随着时间，一切都会均值回归）\n假如你有两个选择，一是你没升职，比你差的同事升职了，你有一个不行的上司，你痛不欲生；二是你作为最强的员工被提升为管理者，但有一届不行的员工，你苦不堪言。请问你选哪一个？\n面对眼前的各种 “P0” 的任务，管理者把自己降级使用去干活当然可以，但是长期来看这样效率是很低，所以要开始担任起教练的角色，开始培养员工（不过同时也要保证自己有时刻能下场干活的能力）\n管理者应该把 40% 的精力花在提升今天的业绩上，40% 的精力花在提升团队能力上，另外 20% 的精力花在提升自己的能力上；提升团队能力就等于提升明天的业绩\n\n而作为教练需要做好三件事：训练、调岗和替换；训练是为了提高效率，调岗是为人找到更合适的岗位，替换是为岗位找到更合适的人\n\n除了鼓手，管理者很重要的身份是教练，如果一个运动员跟着教练学不到东西，那这教练还能叫教练吗？仔细想想很多人离职的原因，不就是因为 “一将无能” 么\n训练：育\n做中学\n一个人能获得的成长，70% 是在工作中完成的，20% 是在与他人的互相学习中完成的，只有 10% 从课程等正式学习中得来。因为在工作完成后，不可避免会思考、探索和改进工作的方法，这样员工通过工作本身就能够积累知识，即知识积累有时候不是 “学习” 的结果，而是 “工作” 的副产品，这种方式也叫 “做中学（learning\nby doing）”，是一种类似 “费曼学习法” 的学习方法\n教练真正的重点并不是教，而是让员工去练；犯错、解决、改进、成长，这就是管理者对员工的培养过程\n管理者不能让员工一直做他熟悉和擅长的事情，而要让员工走出舒适区，去做一些他不熟悉和不擅长的事情。这就需要管理者具备一种修养，他要允许员工犯错，管理者要把员工犯错当成是培养的成本，然后跟员工一起来解决问题\n管理者需要创造员工对工作不断思考的场景，可借助的工具：周记、分享和复盘\n\n\n周记：不仅仅是用来检查进度，还需要做一些更深入的思考；一种模板：我做对了什么，做错了什么（关注点在事，记录结果）​，收获了什么经验，有哪些今后要避免的教训（关注点在人，提升能力)\n\n分享：如果管理者发现员工的周记中有特别好的内容，可以建议他给大家分享，讲讲做好这件事的原因是什么。因为这相比周记更加正式了，他就会想自己怎么讲比较好，然后进一步地思考，把自己的经历提炼成经验，再分享给大家\n\n复盘：1）对事不对人 2）分析怎么能提高做事的效率，跟周记一样；不要让\n10 年工作经验，只是 1 年复制 10 次，下面给了一个复盘模板\n\n\n还有一个 “年度复盘” 清单，也能为写年度总结和规划文档提供一个思路\n\n传授\n书里列举了传授的 5 种等级\n\n\n白水级是最差劲，但也是最常见的，即只会情绪化地训人，俗称对人不对事\n\n啤酒级：传授经历，告诉员工自己或他人的经历，只是一个故事，如果员工不能悟出更多东西，实用性会有限\n\n黄酒级：传授经验，分析成功原因（也许不全，但是属于充分发挥主观能动性里可以做的）；如这件事能成功是因为他做对了哪几件事，为了做对这几件事，需要拥有哪几种能力或者资源\n红酒级：传授方法论，如一些工具、表格和流程\n白酒级：总结方法论成立的条件和边界，抽象出理论\n\n对于白酒级中的抽象出理论，书里也提供了一套方法论：经历经验化、经验方法化、方法理论化\n\n\n经历经验化：跟前面的黄酒级一样，要说清楚某件事能成是做对了哪几件事，需要哪些能力和资源\n经验方法化：建立流程，比如医生在查房的时候会拿着一个本子，本子上有第一、第二、第三等事项。这叫检查清单 (check\nlist)，这说明医生已经把查房这件事变成流程步骤，形成一个方法论了\n方法理论化：找到方法适用的条件和边界；\n世界上所有的方法论都是有毒的。为什么？同一个方法论在这种情况下特别有效，就一定会在某些情况下完全失效，因为条件和边界变了\n\n培训\n新任管理者往往都希望充分利用员工的时间，尽快创造价值，但是我们要思考一个问题：我们是想用员工的时间赚钱，还是想让员工的时间更值钱？这中间需要一个平衡。让员工的时间更值钱的一个好办法就是培训\n这里培训的定义，指的是从团队外部获得不可能内生，或者内生速度太慢的知识、技能或态度，从而突破团队的能力天花板\n培训有 3 件重要的事：1）认识培训的价值 2）善用资源 3）自律 + 他律\n\n1）认识培训的价值\n培训内容可以分为三类：知识、技能和态度\n知识培训是指讲解 “概率论”​“4P 理论” 等理论，它们对完整地认识和分析事物很有帮助。技能培训是指培训演讲技能、谈判技能等技巧和能力，技能培训需要大量地训练。态度培训是指塑造正确的价值观，说明如何做正确的事、如何协作、如何利他等，比如十几年前我接受的培训 —— 学习《高效能人士的七个习惯》​\n技能比态度更可塑，因此如果你觉得这个员工待不长，可以培训他的技能；如果你觉得这个员工待的时间会比较长，可以培训他的知识；如果他待的时间会很长很长，你可以培训他的态度\n这里还提到了培训与咨询的关系\n\n2）善用资源\n利用网络免费资源，或者需要付费但是费用并不高的资源，比如说员工可以凭借学完课程的电子版证书，公司报销一半，这个过程也能区分出谁更爱学习\n3）自律 + 他律\n建立共同学习的氛围（但因人而异，并不是每个人都这么喜欢学习的）\n调岗：留与用\n当我们对一个员工进行了训练后，发现收效甚微，我们是继续培养吗？可是做中学​、传授和培训都试过了，他都没有明显的提升\n这里有一个重要的理念：不需要培养每一个员工，因为管理者真正的任务是提高团队的总体能力水平，而不是提高每一个员工的能力水平\n通用电气有一个 “271” 逻辑：20% 的 A 类人才、70% 的 B 类人才和 10% 的 C 类人才。这在任何公司都是客观存在的一个规律。20% 的人是非常优秀的，他们不需要激励，或者被激励得很好了；他们的能力也很强，你告诉他们要做某件事，他们自己就能完成了。70% 的人是中间水平。还有 10% 的人，你给他们同样的任务和激励，他们的业绩始终是很差的\n所以我们的方法论：重用 A 类员工，培养 B 类员工，去除 C 类员工​。做中学​、传授和培训，这三件事其实都是在培养 B 类员工。B 类员工是可以培养的，不要把精力花在把 C 类员工变成 B 类员工上，但可以把精力花在把 B 类员工变成 A 类员工上\n\n如何区分或划分 A、B、C，需要我们能够识人 7，对人要有的 3\n个基础的认知：1）认识到人的多样性 2）认识到错配\n3）“用对人” 大于 “培养人”\n\n1）认识人的多样性：一些工具如 MBTI、DISC\n等，但这也是很粗糙的多样性\n2）认识人和岗位很可能是错配的，因为招人的时候很着急，同时面试者会迎合面试官的喜好来表演，所以很多时候匹配度是高估的（一些原因，首因效应、光环效应、近因效应）\n\n因为面试者会尽量展现出自己与这个岗位是很匹配的，而你又着急招人，就把他招进来了。要记住，在你很着急招人，而面试者又有很强烈的渴望得到这个职位时，你们彼此间认为的匹配度很有可能是被高估的，甚至很有可能出现错配。\n\n3）用对人比培养人更重要，管理者重要的问题不是改变人，而是用对人\n\n管理学大师德鲁克有一个理念，管理者的任务不是去改变人，而是知人善用，清醒地认识到每个人都是不同的，把合适的人放在合适的位置上\n\n书里提出了一个 “能力胜任度模型”：即先把每个岗位抽象为若干种能力，然后把能力细化为四个级别，并具体描述每个级别要做到的程度；如果管理者想清楚招聘要求怎么写，这一点也就做到了\n\n替换：汰\n如果你发现一个员工的成长速度与时间不成正比，或者总是没有成长，那么你必须学会亲手解雇一个员工，换上更合适的人。换人，其实就是用钱来买提升团队能力的时间\n这里也有一个理念：团队不是越稳定越好，合理的员工流失率不是坏事，比如 10%，​“流水不腐，户枢不蠹”​；其次，流失率有好坏之分，20% 的优秀员工离职率，是坏流失率；10% 的末位员工离职率，是好流失率\n管理者在此刻需要面对的问题，就是如何解雇不合格的员工？书里提到了 3\n个重点：1）识别 2）亲自解雇 3）组织的鲁棒性\n1）识别\n根据业绩和价值观，把员工分为了四个象限：明星、野狗、土狗和小白兔；书里提到土狗是要坚决被清除的，而在阿里，价值观无法改变的野狗和业绩持续很差的小白兔，都是要清除的\n\n2）亲自解雇\n亲手解雇一名不合格的员工，是管理者的 “成人礼”​：面对对方的恐惧，面对对方的愤怒，面对对方的失望甚至绝望时，能够温柔而坚定地说出解雇对方的决定，给出扎实的理由，并从感情上给对方以慰藉，这说明你已经完成了一场真正的 “成人礼”​。\n换人听上去很残忍，但这是员工晋升为管理者后的一个 “成人礼”​，你必须亲自做这件事。面对对方的恐惧，面对对方的愤怒，面对对方的失望甚至绝望时，能够温柔而坚定地说出解雇对方的决定，给出扎实的理由，并从感情上给对方以慰藉。这时候你才完成了作为管理者的 “成人礼”​（慈不掌兵）\n3）组织的鲁班性\n即任何员工的劝退或离职，都不会影响团队的战斗力，书里也给了三个建议\n一是不管每个职位是否满岗，管理者都要跟人力资源部沟通，大量地去看简历，保证能随时招人进来\n二是招聘兼职或实习生，或找外部的人做临时项目，这能让管理者接触到外部的优秀人才\n三是对于有资源的公司、团队，甚至可以做 “缓冲式招聘”​，设置 1～2 个冗余岗位，相当于打篮球有板凳队员，在场上队员出状况时能立刻上场救急\n这一点在实操上，可以让团队中的人两两互为备份，他们的工作可以无缝衔接。同时利用代码审查和修改漏洞的时间，尽量让所有团队成员了解所有的代码\n政委：沟通\n动力和能力，能让员工成为 “超级个体”；但是在当前复杂的商业世界里，员工不太可能单兵作战；让 “超级个体” 变成 “超级集体”，有两种重要的粘合剂：**** 沟通和协作 ****\n而除了鼓手和教练，管理者还要成为善于沟通的 “政委 “，因为管理者的工作不是 “干活”​，而是让一群人因为你而干出更多的活、更好的活、更有价值的活。而要做到这一点，核心就是靠 “沟通”​；作为管理者，需要具备” 想清楚，说明白 “，让员工” 能接受 “的沟通能力\n通过向员工宣导公司的文化内涵和价值观念，让公司的使命和愿景深入人心；通过与员工沟通交流，解决员工的思想问题和心理问题；更重要的是，让员工明白一个任务或项目的为什么 (Why)、是什么 (What) 和怎么办 (How)，减少信息不对称，提高员工的战斗力，这都是管理者需要通过沟通来达成的目标\n减少信息不对称、惊喜与意外\n沟通是为了减少信息不对称，避免惊喜与意外\n江湖上流传已久的一句话，员工离职无非两点：“钱没给到位，或者心委屈了”​。什么叫 “心委屈了”​？调查显示，绝大部分员工加入一家公司，是因为对这家公司抱有期待；而离开一家公司，主要是因为对直接上司很失望。你当上管理者后，开始成为员工离职的重要原因了。未来你的优秀员工离职，你要知道，很可能是因为你，因为你让他受委屈\n\n向下沟通\n\n避免出现这种委屈，管理者就需要学会向下沟通：从 IC 到\nmanager，需要从原来只 “向上沟通” 多拓展出向下沟通\n日常向下沟通时候，需要注意一些细节，比如说沟通要做到对事不对人，比如说下面这个例子，你觉得员工没有团队合作精神。如果你直接说 “我觉得你没有团队合作精神”​，这就是在评价人。而如果你说 “在这件事情上，我没有看到团队合作精神的体现”​，这就是在评价事。​“我觉得你工作不积极”​，这是在评价人。​“我没有在这个十万火急的项目上看到你积极的表现”​，这是在评价事\n\n减少信息不对称\n\n因为不可能每个决策都由管理者来做，很多需要员工来做；而要让员工做出有效的决策，就需要让员工有更充分和全面的信息\n如何减少信息不对称？避免\n1）把信息当权利，即利用信息差来做管理 2）说好不说坏\n3）当老好人；2 和 3\n的主要原因是逃避沟通，即通过减少沟通来减少眼前的冲突\n\n想明白，说清楚，能接受\n\n想明白指的是给员工安排任务，要想清楚 4\n个问题，为什么做？怎么做？预期结果是什么？deadline 是什么时候\n说清楚指的是表达要到位，考虑受众的认知水平选择语言\n能接受指的是除了用权利（比如说 top-down\n的决策，没法改变），要尝试更多方面去让员工接受\n\n员工不能理解，不能接受，此时你用的是他的手\n员工能够理解，不能接受，此时你用的是他的脑\n员工能够理解，能够接受，此时你用的是他的心\n\n后面也主要围绕着 “想明白，说清楚，能接受” 展开这部分的方法论\n想清楚：搞清楚 why、what、how\n想清楚，意味着要在三个维度上理清楚：即 “为什么”​“是什么”​“怎么做”（Why、What、How）\n表达清楚的前提是能 “想清楚”，要清楚对方想听的是 “是什么”、​“为什么” 还是 “怎么做”；只有当你所表达的跟对方想听的相匹配时，沟通才是有效的，书里给了这么一个案例\n\n有一次，我和一组企业家开私董会。某个企业家的问题是：如何给高管降薪？如何招到 80 后的总管理者？现在请问：这是什么类型的问题？这是关于 “怎么做” 的问题。他想找到 “无痛解雇” 高管，以及招 “80 后” 总管理者的方法和步骤。他之所以这么问，是因为他心中已经有了两个确定的 “是什么”​，那就是：解雇高管和招 “80 后” 的总管理者。他觉得这两个 “是什么” 不需要讨论，这是确定的，只需要讨论 “怎么做” 就行了\n\n“为什么”​“是什么”​“怎么做”；三者的关系见下图\n\n很多时候，当对方问 “怎么做​“的时候，先明确下对方的 “是什么”（结论）的原因，即 “为什么” 要这么做，因为有时候对方都不知道真正的原因，都是浮于表面的原因，实际上能够有更好的解法；书里给的三步是这样的\n第一步，从 “是什么” 倒推出 “为什么”​\n第二步，再从 “为什么” 推出新的 “是什么”​（即找到真正的原因）\n第三步，再从新的 “是什么” 推出新的 “怎么做”​\n沟通模式应该始于 “为什么”，终于 “怎么做 “；没有这两者就是鸡汤；为什么有时你的员工不能接受？因为你没有讲 “为什么”​；为什么有时你的员工不会执行？因为你没有讲 “怎么做”​\n管理者需要重视 “为什么”、​“是什么”、​“怎么做” 这个看起来简单的框架，如果一个管理者不能想清楚” 怎么做 “，某种程度上也是这个管理者不能解决这个问题的自检；如果认为自己所想的 “怎么做” 可能只是保底方案，那怎样引导员工提出一个更优的 “怎么做”​，是管理者需要思考的问题\n说明白：听不如说，说不如写，写不如画\n一个常见的问题是：下属没听懂，执行不到位，是谁的错\n书里提到的观点是：“谁的损失最大，就是谁的错”，本质上是一种课题分离的做法\n如果下属没听懂，显然是你的损失最大，你的兵出了问题你要负责，老板会骂你，所以这是你的错。要记住，谁的损失最大，就是谁的错\n所以不要问员工：你听懂了吗？因为这句话的主语是 “你”​，那听懂这件事就是他的责任，他一般不敢说 “我没听懂”​。在这种情况下，管理者要说：我讲清楚了吗？主语变成了 “我” 之后，责任就转移了，因为管理者能控制的是自己的讲，而不是别人的听\n因此，把信息传到位，确保员工能充分理解，主要责任在管理者层面；沟通有四维度：听、说、写、画，它们的信息沟通效率从左到右依次提升，降维沟通让接受信息的人面对的难度大大降低，但是对传递信息的人的要求大大提高。如下图所示\n\n用 8 个字来概括这张图，就是 “升维思考，降维沟通 “；前面介绍的\n“想清楚”，其实就是一个 “升维思考” 的过程；而 “降维沟通”，就是我们下面要说到的说明白：说替代听，写替代说，画代替写\n首先搞清楚 why；这里涉及到三个 “不如”：听不如说、说不如写、写不如画\n\n听不如说：听到的信息如果不复述出来，其实是会有较多损耗的，或者其实也没能很好理解\n说不如写：说话时思维是发散的，只有写清楚了，才真的是逻辑清楚。因为 “说” 这件事，可以有信息的往来，可以有信息的重复，可以有口头禅，不需要找到观点与观点之间的先后次序和因果顺序。但 “写” 是不能这样发散的，写的时候你要思考内容的逻辑，你得把一条逻辑线索梳理出来。而卓越的写作能力，不仅有助于提升行政管理水平，还有助于显著提高企业管理者的效率，因为写工作计划、工作总结、演讲稿、会议稿、工作汇报等，都需要很强的写作能力\n写不如画：简单来说就是一图胜千言，研究者有过分析，图像是具象的，调动的多是感性思维；文字是抽象的，调动的多是理性思维。人说到底是感性动物，所以更容易接受图像\n\n那该如何做？上面也提到了：说替代听，写替代说，画代替写\n说替代听\n在员工听完你的描述且表示清楚后，让员工复述一遍你说的话，在这个过程既能保证员工跟你的理解是一致的，能确保工作的正常进行，同时能检验你的沟通能力\n写替代说\n在常见的 11\n沟通后，让员工把今天沟通的内容写下来发给你，共同对齐聊过的内容；同样的，会议纪要也是想解决这个问题\n画代替写\n可以把你要讲的比较复杂的概念，通过图来呈现出来；这样对于你的讲解或员工的理解效率是更高的。咨询公司的核心能力之一，就是建模的能力。从某种意义上来讲，建模的能力就是深度思考的能力，就是 “画图” 的能力\n书里提到了一些比较实用的学员案例\n\n我一直都有不善口头表达的心结，因此对沟通这件事心存敬畏。对于一些重要沟通，我会借助于思维导图和 visio 这样的工具，即使没有电脑也会借助清单工具，这种心态就像一个认为自己腿脚不利索的人给自己找了一对拐棍一样。但就是这对拐棍，让我在公司顺利完成了一个口齿伶俐的人都没能完成的需要多单位、多部门协作完成的工作\n写不出东西，就意味着观点没逻辑、工作没思路、计划没统筹。所以，手不释卷、笔耕不辍，是任何一个管理者都必须坚守的修行方式。虽然过程可能很痛苦，但选择一条艰难而正确的路，才会越走越简单\n\n书里还提到了沟通的 7\n种武器：一对一沟通、即时沟通、电子邮件、走动管理、例会、看板、周报\n\n\n一对一沟通\n\n一对一沟通的目的，就是解决那些只有私下才能讨论的问题；具体执行上有三个点需要注意\n第一，锁定时间段\n第二，主要谈员工自己主动提出来的议题。比如谈遇到的职业困扰、家庭的情况、未来的发展规划，而不是谈工作进展。一对一沟通的目的是解决以上这些工作会议解决不了的问题，从而有效降低员工的离职率\n第三，进行对事不对人的工作失误沟通。一对一沟通是非常重要的指出员工工作失误的时机，管理者要把这个失误明确地讲出来，但是要记住，要针对这件事本身，而不是针对这个人。不要对员工说 “你不行，你怎么这么不努力”​，这是针对人的，而要说 “这件事没有达到我们预期的效果，这件事出了几个问题，必须加以改进”​\n但有个问题是，\n一对一沟通往往是管理者发起的，对大部分原来来说，员工是不会做准备的，所以员工会用最简单的方式结束沟通，也就是说，他们沟通的目的是结束沟通；所以在一一沟通前，管理者要告诉员工想讨论什么可以先准备好，而如果管理者真的有问题想问，也可以给员工一张问题清单；如下\n\n最近有哪些让你特别振奋和惊喜的事情？\n有哪些让你沮丧和纠结的事情？\n你未来 3～5 年的职业目标是什么？和公司的目标怎么结合？\n你最近在哪些地方可以提升？有什么计划？我如何帮你？\n管理者做哪些事情，你的业绩可以更好？\n你还有什么问题问管理者？\n\n\n电子邮件\n\n发电子邮件有一个重要目的，就是留下记录\n电子邮件也适用于公开表扬。批评要私下进行，表扬要公开进行\n\n走动管理\n\n走动管理的意思是你不能天天坐在自己的办公室里，要经常走到员工中去，在员工的主场沟通，目的是 “闻味道”​，了解情况，展现亲和力。\n比如说把自己的工位设置在每次从茶水间或卫生间走回去，都能看到所有人的工位；然后要想想，最近你和谁沟通得比较少。然后主动走到他旁边聊两句，问问他：项目最近进展得怎么样？你上次跟我说的那个困难，后来解决了吗？上次我们沟通过的问题，你后来想明白了吗？（存疑，不是所有人都喜欢这种方式）\n走动管理能让员工感受到自己被重视，知道你在随时关注他的任务、关心他的细节\n\n例会\n\n开例会的关键，是设定项目检查点，并定期沟通。这其实是一种强制沟通，就是让大家把想说和不想说的问题都说出来，从而预先去解决问题\n在例会上停止说 “这行不通”​，而是要引导员工思考\n\n看板\n\n这里特指项目管理工具。常见的如准备一块白板，把项目的进展展示到白板上，目的是让大家同步展示，知道彼此的进度\n\n周报\n\n周报的关键点，是管理者要以身作则。只有你带头写，员工才会有写的动力\n梳理在这部分也给出了一个案例，用于说明这一点：文字是低维沟通，不要只依靠文字就乱猜测，要去升维沟通\n\n与领导沟通，尤其要注意这一点，因为领导与你进行文字沟通时，通常很少用表情。有时候领导用微信问了一下你的项目进度，他可能是在表达对你项目进度的不满，也可能只是单纯地在问你进度，抑或是在关心你，向你示好。如果你的项目有一点延期，千万别自己瞎揣测领导的意思。面对这种情况，要努力做到把沟通 “升维”​，打个电话汇报一下，或者去领导办公室面谈\n我曾经掉进过这个坑里，合伙人在群里跟我说了一句中性的话，当时正赶上我心烦，就 “曲解” 成他在找我的茬，差点一个电话打过去跟他鱼死网破。后来我冷静下来，问了几个关系不错的同事，他们纷纷表示合伙人说的这句话没问题，是我太敏感了\n虽然情绪往往比逻辑跑得快，但也要用理性把情绪拉回来。面对不完整的信息，别自己瞎想，冷静下来，再次确认，把信息补全才是第一要务\n\n能接受：避免阳奉阴违\n一些管理者喜欢对员工采用服从性测试，书里举了以下的一些例子\n\n有些管理者喜欢用言语营造压迫感，制造不平等。比如说，​“我不明白你在说什么”​，这句话其实是在对员工表达一种极度的不耐烦：你在说什么，连你自己都没有想清楚，就来找我说\n有些管理者会对员工说 “直接讲重点”​。下属正在会议室里满心期待地讲着自己的计划和方案，精美的 PPT 还是昨晚通宵加班做的。但是管理者才听了几句就不耐烦了，一直催促着说：​“下一页，下一页…… 不用说了，我自己先看一看…… 再下一页，嗯，好，我看得差不多了，你直接说重点。​” 这句话背后的心智模式，是想表达你说的全是废话，你说的我全都不关心，别浪费我的时间了。这是想告诉别人：我的脑子转得特别快，我已经知道你想说什么了，你根本不具备用最有效的方式表达观点的能力\n当一个管理者说出 “我这人说话比较直” 时，看上去他好像是想说我是个直接的人，其实他的潜台词是：下面我说的话可能会伤害到你，但是对不起，我对此毫不在意。而说出 “我就是这样的人” 的人，则是把自己的缺点标榜为特点：也许这是我的缺点，但是对不起，我不打算改了。会说这两句话的管理者，放弃了用员工更能接受的方式去沟通交流，也放弃了改变自我的意愿\n\n这些言行都会给行程一种压制，属于服从性测试，目的是强化地位的不平等，树立自身的权威\n有些管理者是无意识间这么做了，他们是被不良的职场文化潜移默化地影响了。有些管理者则是刻意进行服从性测试，他们往往还会有后续的手段 —— 如果不服从，我就开除你，让那些愿意服从的人来做这件事\n实行服从性训练，效果是有限的：第一，只能对一部分下属有用；第二，只在一段时间内有用；第三，只是表面有用\n所以服从性测试往往只能获得 “阳奉”，它的副作用是 “阴违”；关键还是要提高员工的接受度，与员工建立信任\n这也是前面 “心态跃升” 中沟通部分提到的：权力分为三种，即法定权力、专业权力和魅力权力。想让自己的脑指挥别人的手，要充分理解并运用权力的作用机制，不要只用法定权利\n具体的执行上，除了 “心态跃升” 中提到的 4 种话术，这里还给了 4\n条建议：没有私心、不要偏袒、赏罚分明和展示专业性\n\n\n没有私心：信任来自员工对你的品德的信赖，比如说你会对大家共同的业绩负责，但你对用不良手段获得业绩这件事完全没有兴趣，你要用实际行动来证明这一点\n不要偏袒：怎么才能做到不偏袒？管理者不要跟任何一个员工走得太近，一旦走得太近，就容易造成偏袒\n赏罚分明：很多管理者喜欢表扬和奖励员工，因为员工受到表扬和奖励后会很感谢他，他很享受这个过程。但是，员工犯了错误之后，他却不敢惩罚，因为惩罚可能会引起冲突，很多人是害怕冲突的。一旦不惩罚那个犯错的人，做对的人就会觉得特别不公平。他们就会因为你的懦弱而不敢相信你\n展示专业性：在日常的一些发言中展示自己的专业性，在业务上做对一些决策，拿到成果\n\n流程、制度、价值观\n有些约定成俗的东西，如果对不同的人一遍遍重复说，效率会很低下，这个时候就要依赖这里提到的三件套：流程、制度和价值观\n\n流程、制度和价值观是穿越时间的沟通机制，分别对应着方法论、合规性和决策力\n\n流程：方法论\n\n制定流程的目的是提升做事的效率，把正确的事重复做\n整个公司就是个大流程，公司的小团队有自身的特殊性，有自己专门做的事，要给自己制定专门的小流程。制定小流程有个办法，叫 “最佳实践手册”​\n\n制度：合规性\n\n制度与流程有时候会比较相似，那这两者的区别是什么？这里可以借用 “章法” 这个词解释\n“章”​（制度）​，是规定，是契约，关注的是什么能做，什么不能做；​“法”​（流程）​，关注的是做事的方法、做事的顺序，是如何做\n比如说公司红线要求、上下班时间要求等规定，就属于制度，制度是用来保证合规性的\n\n价值观：决策力\n\n价值观很多时候听起来很虚，但是在很多时候，尤其是做比较高层的决策的时候，如果遇到多种方案都可行（往往很多时候都是这个样子），从技术、市场、成本等方面都无法做出决断的时候，那么最后的判断依据就是价值观\n价值观，是让决策力穿越时间的沟通工具。树立了这个价值观后，大家做事就会保持高度的一致性\n指挥：协作\n在目标是 “突破自然效率” 的这辆赛车里，动力是燃料，能力是车辆架构，沟通是仪表盘，而接下来要说的协作，则是驾驶技术\n如果说沟通的目的是要达成团队的意识一致，那么协作的目的就是要达成团队的行为一致，所以这一章会介绍一些团队协作的方法论\n执行、提高与发展\n这部分对应着书里的三个章节：执行靠闭环、提高靠循环、发展靠规划；分别对应着短期、中期和长期的策略，或者说短期内完成任务靠闭环，中期的团队成长靠循环，长期的做大做强靠规划\n执行\n当管理者把一些事情分发下去的时候，有时候会发现并没有收到员工的回应，这个仅仅是员工的错吗？\n虽说这跟员工的责任感有一定的关系，但是跟管理者的管理方式也有关系，为了避免这种情况，书里提到要做好三件事：凡事有交代、件件有着落、事事有回音，亦即标题提到的 “闭环”；如下图所示\n\n\n凡事有交代：3W\n\n一句话说这部分，就是 “谁在什么时间点前做完什么事情 (Who do what by\nwhen)”。也可以称之为 “3W 方法”​：Who 就是指定人，What 就是说清事，When 就是卡时间。\n如果任务需要几个人来协作完成，那么就需要分解任务，分别指派每个人做什么。而不是说，你们一起把这件事做了。因为，责任除以 2 等于 0\n另外，deadline 也是很重要的，毕竟 deadline 往往是第一生产力\n\n件件有着落：工具 + 流程\n\n“凡事有交代” 的主体是管理者，​“件件有着落” 的主体是系统。系统就是 “工具 + 流程”\n记忆是不靠谱的，且日常事情比较多的时候容易遗忘。所以要把这些事情记录下来，有各式各样的工具可用，如企业微信、飞书、钉钉、白板、进度跟踪邮件、贴在电脑上的小纸条、便利贴、日程表等\n\n事事有回音：\n\n即不要允许一件事情不了了之，可以改指定人，改事情，改最后期限，甚至任务也可以 “明确” 放弃掉（说清楚放弃的原因），但是不能被默默地删除\n任务如果经常不了了之，会造成以下两种情况：\n（1）让接受任务的人觉得任务不完成也没事，领导不会说什么，以后的工作也可以不完成\n（2）是质疑领导的权威性，这个任务也许就是个伪需求？领导怎么分配这种任务？\n提高\n提高靠循环，书里认为每一个闭环的业务水平都比上一个闭环的业务水平有所提高，就称之为循环\n书里提到的一个方法教 PDCA 循环（又叫戴明环）​：周密计划 (Plan)、严格执行 (Do)、同步检查 (Check) 和及时调整 (Act)\n\n\n 周密计划\n\n制订一个好的计划，不是要花大量时间在精美的表现形式上，比如高大上的图表设计或漂亮的 PPT，而是要花大量的时间在调查研究上\n通往利益最近的道路，可能不是最正确的道路（短期与长期的问题）。你必须一开始就把目光放长远，在看到更大的地图之后，规划出那条最正确的路径，这就是周密计划。千万不要用执行的勤奋，掩盖计划的懒惰\n\n严格执行\n\n首先是要拆解清楚（心态跃升中提到了一个任务拆解的例子），有了基于计划分解的子任务后，需要明确分配到每个人任务栏里的、有时间限制的具体任务，执行就变得责任明确、优先级清晰\n\n同步检查\n\n检查现状相对于计划中设定的标准的偏移度\n\n及时调整\n\n基于检查出来的结果做及时改进，并把成功的经验加以推广，固化成流程或标准\n每一次闭环都往上走一步，工作水平就能实现螺旋式上升。如果没有往上走，就是简单的重复；只有往上走了，就是迭代\n书里举了一个利用了 PDCA 解决客户满意度不高的例子\n\n发展\n执行靠闭环，提高靠循环，其实讲的是短期和中期的事。短期内完成任务靠闭环，管理者有无数需要执行的事在手边，要靠闭环，不能有漏洞；中期的团队成长靠循环，就是闭环之间首尾相连，业务水平不断提高，团队能力才能不断增长。那长期呢？长期的做大做强靠规划\n不要用战术上的勤奋掩盖战略上的懒惰，因为仅靠战术上的勤奋，打不下明天的山头\n一个常见的管理者的现象：从早上 8 点到公司，一直到晚上 7 点多离开公司，他的时间被塞得满满当当。而且这种情况几乎是日复一日，每天都疲于奔命，只顾着处理这些眼前的事情，而忘了抬头想一想以后的发展规划\n时间管理的基本原则，首先要做重要又紧急的事，其次做重要但不紧急的事\n但很多人把重要又紧急的事做完之后，会接着去做紧急但不重要的事，因为它们迫在眉睫。做紧急的事会让自己获得一种付出的安全感 —— 快速完成任务后的安心与放松；做重要但不紧急的事，比如每天的学习任务，你很可能会觉得焦虑，因为它的效果没有完成紧急任务那么立竿见影。为了逃避焦虑，你会让紧急的事情塞满自己的时间，但它们基本都和发展、成长无关\n管理者是通过员工来执行具体任务和完成目标的，执行者活在当下，眼中看到的可能都是紧急的事；可是成为管理者之后，你要培养管理者思维，活在未来，眼中看到的应该都是重要的事\n\n因此管理者要经常问自己的几个问题：你要明确明天的目标是什么，打下明天的山头需要什么样的武器，带领明天的部队需要什么样的能力提升\n具体规划的三步有三步:（1）练习做三年规划（2）确定最近一年要做的三件事（3）设定一年三件事的衡量标准\n\n（1）练习做三年规划\n问三个问题\n问题一：三年之后，我的业务是什么样子的？即三年之后，我们获得了什么样的成就；我们为公司创造了多少利润；我们在别人心目中会获得什么样的认可\n问题二：三年之后，我的同事是什么样子的？即三年之后，他们达到什么样的水平，大概有哪些方面的能力，有什么样的背景\n问题三：三年之后，我自己是什么样子的？答案因人而异，作者的答案是公司一半以上的收入是与我自己的时间无关的，我可以去开拓公司的第四增长曲线、第五增长曲线\n（2）确定最近一年要做的三件事\n虽然事情多如牛毛，但真正对你和你的部门有重大影响的，一般不会超过三件事。贪多嚼不烂，一年聚焦三件事，足以让部门有所成长\n（3）设定一年三件事的衡量标准\n设定衡量指标的第一原则：有总比没有好。任何管理都需要评估，评估就需要衡量指标。没有衡量指标，就没办法客观地评价团队、评价自己。你可以不用衡量指标来考核，但一定要有衡量指标。它们就像汽车的仪表盘一样，指示着车辆的运行情况\n设定衡量指标的第二原则：可衡量、数据化\n设定衡量指标的第三原则：分清楚前置指标、后置指标\n前置指标就是结果还没有发生，但你一看到前置指标，就能预测到这件事会出问题。比如说每天必须见 20 个客户，才会产生销售，这就属于前置指标；后置指标通常是财务指标，比如公司今年完成的销售额是 2000 万元。应该把关键的前置指标而非后置指标作为 KPI 考核指标（其实就是找好中间指标和过程指标）\n文化与流程\n管理者需要逐步降低自己的重要性，让自己在团队里越来越不重要。反之要让文化和流程变得更加重要，因为这两部分跟前面提到的穿越时间的沟通机制一样，是能够降低 “人” 的不确定性的重要方式，是提高效率的重要途径，是组织规模变大的必经途径\n健康靠文化\n再小的公司，再小的团队，都是一个共同协作体，就像整个人类社会是共同协作体。要理解人类的协作，我们要先理解三件事：人性、道德和法律\n人类社会是靠人性、道德和法律这三个因素来协作运行的。公司作为一种组织，带有人类社会的基因，也有三个类似的因素：利益、​（企业）文化和制度，它们与人性、道德和法律一一对应\n\n文化不是刚性的约束，它是大家要倡导的东西，以此让整个公司运行得更好；文化某种程度上也可以理解为集体主义精神\n打造健康文化的三个建议：（1）鼓励白、压缩灰和禁止黑\n（2）用人不疑，事情要查 （3）提高可预测性\n\n（1）鼓励白、压缩灰和禁止黑\n鼓励白。就是用 “利益” 来鼓励那些公司认为正确的事。比如，业绩好就给奖金；对公司的发展有巨大贡献，就给股份\n压缩灰。比如员工只顾自扫门前雪，不去帮助别人，这就是灰度的。你不能说他是坏人，也不能因为这种行为把他开除，这时就要用 “文化” 来管理\n禁止黑。禁止黑就是用 “制度” 来明确有些事绝对不能做。比如说做新媒体，抄袭就是一条刚性边界，绝对不能碰，哪怕抄袭一点点也是黑，必须严惩。要从第一天开始就依照制度禁止黑，否则以后很难往回收\n（2）用人不疑，事情要查\n“不疑” 的是一个人的用心，这是对人的一种信任；​“查” 的是这件事情本身的完成度。抱着 “不疑但查” 的心态，你才能用利益、文化和制度来鼓励和规范人们的行为\n（3）提高可预测性\n有的管理者认为不要让下属猜透自己的想法，要把话说得模棱两可，这样自己就不会犯错，就能保住自己的权威性\n但实际上管理者提升自己的透明度和确定性，让员工可预测（包括管理者的决策可预测，自己的升职加薪可预测），都有利于提升协作效率。因为如果管理者的决策不可预测，所有的决策都要经过管理者才能拍定，这无疑是非常低消的\n管理者的可预测性，带来决策的一致性，带来授权的可能性。只有可预测，管理者才能让自己变得越来越不重要，让利益、制度和文化变得重要，公司才能演变为生命体\n效率靠流程\n流程就是把复杂的事情、类似的决策标准化。流程往往是是一个或一系列连续有规律的动作\n流程有三点重要作用\n（1）流程能够减少不必要的重复沟通\n（2）流程可以降低 “人” 的不确定性带来的系统性风险，对于企业已经探索过无数遍并已经找到最优路径的事情，就没有必要让员工再去自行探索，以免掉进陷阱带来系统性风险\n（3）企业可以让员工凭借流程，获得可预期的、持续的、稳定的产出，跟前面提到的可预测性是一致的\n跟流程出现的往往还有愿景和价值观；如果说流程是 “怎么做” 的标准化，那愿景是 “为什么” 的标准化，价值观是 “是什么” 的标准化\n\n利用流程来提高效率的三个步骤：建立流程、优化流程、固化流程\n\n授权\n管理的本质是通过他人完成任务，所以管理者是要依赖大家的，而不是大家依赖于管理者，那样管理者肩负重任，就走不了很远\n一个反面例子是当看到员工做错事时，他会说，​“你放着，我来”​。当他熬夜帮下属做好，下属特别感激时，他自我感觉非常好：这才是当老大的样子，老大就是要有担当啊！其实这是不对的。在这种状态下，他享受的不是 “管理者” 的成就感，而是 “被依赖者” 的成就感\n授权有五个级别：指挥式、批准式、把关式、追踪式、委托时\n\n（1）指挥式授权：员工按照命令和指示工作\n自己是头，下属是手，完全不需要思考，自己说什么下属就做什么，这就叫指挥式\n指挥式是很多管理者一开始最常用的授权方法，至少他把要做的具体事情交出去了，事情不是自己做了，而是别人做了\n（2）批准式授权：员工在取得上司批准后工作\n员工思考，但是决定由经理来做，这是批准式\n员工拿着两个方案过来找经理，经理问他：​“这个方案和那个方案的缺点分别是什么，怎么改进？​” 员工说，应该这么改进…… 经理听完，说：​“不错，你去干吧。​” 这就叫批准式\n（3）把关式授权：员工在关键环节请示批准\n跟前面比较相似，只是员工请示的频率不太一样\n（4）追踪式授权：员工在过程中先斩后奏\n在一件事的整个流程中，员工可以先斩后奏，可以不请示，但是做完之后还是得向经理报告\n（5）委托式授权：上级只关注结果\n“你想怎么做就怎么做，别来问我，我只看最后有没有把‘城池’拿下”​，这就是委托式授权\n","categories":["管理"],"tags":["读书","管理"]},{"title":"出海、选择与 “弯路”","url":"/2025/10/05/%E5%87%BA%E6%B5%B7%E4%B8%8E%E9%80%89%E6%8B%A9/","content":"许多人大概都读过这段充满宿命感的话：\n\n当你老了，回顾一生，就会发觉：什么时候出国读书、什么时候决定做第一份职业、何时选定了对象而恋爱、什么时候结婚、其实都是命运的巨变。只是站在三岔路口，眼见风云千樯，你作出抉择的那一日，在日记上，相当沉闷和平凡，当时还以为是生命中普通的一天\n\n这段自出《杀鹌鹑的少女》的文字，具象化了选择的厚重感。而事实上，很多 “当时只道是寻常” 的瞬间，确实会这个名为 “人生” 的游戏开辟很多新的分支。最近几天在玩\nDetroit:\nBecome Human\n这款游戏，通过三个仿生人（androids）徐徐展开讲述了一个有关人工智能、科技、伦理、人性的故事，而故事的结局是由你在游戏中的种种抉择所决定的，你的一个不经意的选择，可能会掀起一场人类与仿生人的流血革命\n\n都说选择大于努力，我们特别擅长拿着后视镜去度量一个选择的 “好” 与 “坏”。因为我们没有上帝视角去获取整个游戏的\nFlowChart，去看到那些未选择之路种种；更要命的是，我们在当前已选择的道路上的任何操作无法撤销或回滚，当最终的结果不太如人意时，就会在事后复盘时被打上一个 “走了弯路” 的标签，然后幻想那条未选择的道路所拓展出来的分支\n最近就面临着一个关于出海的选择，一直觉得出海是国内企业发展的未来，但是真正面临这样的选择时，也少不了瞻前顾后、忧心忡忡，总想小心翼翼地迈出下一步，希望少走 “弯路”。脑子里的各种观点打架打了好多天，我觉得是时候写一篇文章来梳理一些这个迷思了。本文是关于出海和选择的一些碎碎念，祝开卷有益～\n\n关于出海\n“失落” 日本的出海史\nB 站有个分析师 up 主针对日本的出海历史做了一个出海专题，从宏观角度讲述了日本的出海历史，同时提供了个人在这种大环境下的一些选择。最近又出了一本《以日为鉴》的书籍，虽说历史不会重演，但免不了会有惊人的相似性，可以读一下作为参考（下面的这些数据基本来自这些视频和书籍，但豆瓣的书评提到，这本书的一些数据可能不够权威，笔者在引用时也会尝试找到其出处）\n海外再造日本\n截止 2022 年末，日本的海外净资产达到了 418\n万亿日元。与此相比，当年日本国内总额为 545 万亿日元，由此形成了所谓 “海外再造日本” 的经济现象\n日本在进入泡沫危机后，首先遇到的最大问题就是增量减少。以房产投资和日常消费为代表的内需走向低迷。泡沫破裂前，80 年代日本雇员薪资增速约 5.5%；破裂后的 92 年，迅速降至 1.8% 左右，此后长期负增长，工资下滑进一步导致国内消费需求萎靡。（此时此刻，恰如彼时彼刻？）当时，日本庞大的内需市场快速萎缩，全社会从消费型转向储蓄型，市场陷入同质化竞争。内卷之下，服务业不得不提升质量以吸引客户，这倒逼出了极致的服务态度\n在此背景下，日本于 1994 年开启大规模出海潮，企业利润也逐渐回升。2000 年，日本企业利润重回 1989 年高位；2007 年，全国企业利润已是泡沫经济前的 1.5 倍。上世纪 90 年代泡沫破裂后，日本对外直接投资占 GDP 比例从 1990 年的 6.5% 迅速提升至 2021 年的 40%。21 世纪的第一个十年，更实现了出口和投资双轮驱动。出口额占 GDP 比重从 95 年最低的 8% 升至如今的 20%。据大和证券统计，日本上市公司海外利润占比接近 60%，多个行业龙头海外收入超 75%\n那么，这又是如何做到的？\n泡沫破裂后，日本对外直接投资的动机主要包括以下几个方面（1）拓宽本国的海外品牌影响力\n（2）在海外重塑全球分工体系（3）利用先进技术优势换取海外市场（4）寻求低成本的劳动力。而在具体运作上，基本的逻辑是：一方面寻找中国与东盟为代表的亚洲新兴经济体利用其劳动力与市场优势，另一方面通过新兴经济体的利润反哺研发，实现欧美市场所需的技术创新\n这也意味着日本对外投资活动主要以市场和技术驱动为主。日本同时需要从新兴经济体获取利润又需要和欧美企业进行竞争，事实上目前的中国也正在走上这条被追赶的路，一方面一带一路扩大在新兴国家的影响力，另一方面重复高端技术以蚕食欧美企业份额\n90 年代，日本大量投资东亚新兴经济体。自 1995 年始，索尼、东芝、丰田、大金等头部企业组成产业联盟抱团出海。此轮出海目的地不再是 80 年代首选的美国，而是中国与东南亚。1995 至 1996 年，日本对东南亚直接投资占总额 20%，中国占 10%。此后数年，日本将国内产业链集中迁至不同国家，把低端产能转移到海外廉价劳动力市场，国内保留高利润的高端产能。\n2000 年左右，日本商品迎来中国进口需求的爆发。中国迅速释放的内需，吸收了日本在华工厂的大量订单。日本在华工厂本地销售比例从 1999 年的 31% 猛增至 2010 年的近 70%。可以说，日本早期在华投资建厂，既赚取了廉价生产成本与土地红利，也吃到了中国发展的时代红利\n在投资出海的同时，日本国内产业结构也在剧烈调整。泡沫破裂前，金融业是日本市值最高板块，占比近 30%。日股市值前 30 的公司中，一半属金融行业。金融与地产股一度占全市值 38%。\n三十年后，金融股仅占 8%，地产股不到 2%。市值最高的四大行业依次为工业、消费、信息产业与医疗健康，占总额 65%。而泡沫破裂前，医疗仅占 3%，信息产业占 5%，消费品占 11%。事实上，除工业制造这一日本 80 年代的荣耀外，其余三大行业多在泡沫破裂后崛起，且基本属非制造业范畴；工业、消费、信息产业、医疗健康这几个产业，其最大市场恰恰在海外，海外收入常年高于国内。以医药龙头第一三共为例，其 2022 年日本营收仅占 35%。\n普通人的出海之路\n上面都是偏宏观的数据，落到普通人头上，在出海这个新游戏里，机遇又在哪里？回顾历史，会发现里面有一个比较核心的思想：在国内增长放缓时，将自身技能、商业头脑或资本与海外市场更早期的发展阶段相结合，从而获得溢价和机遇\n当年的普通日本人，主要有 3 个出海增量机会。分别是工作出海，商业出海与投资出海\n（1）工作出海\n工作出海，或者说跟随公司出海是最主流、最稳妥的方式，也是普通人最有机会的领域\n在说出海的企业对个人的影响前，可以先看一下日人普通人的择业观和变化，也许会对我们有更多启发\n从就业选择看，泡沫前 80 年代，日本最受欢迎的企业中，除索尼与丰田外，几乎全是当时如日中天的金融和地产公司。泡沫时期，金融业普通员工工资是社会平均的 2.8 倍，中层干部达 4.1 倍。从业人员从 1976 年的 150 万，十年后增至近 265 万\n然而，泡沫经济后的四年，日股跌 60%，地价跌 50%。日本金融业模式特殊，自成一体，缺乏国际化能力，国内业务萎缩，又难以开拓国际业务，加之房产泡沫破裂导致不良率攀升，行业盈利能力骤降。证券业首当其冲，前五年主要通过裁员降薪维持，从业人数仅降 15%。但 1997 年 11 月，日本最大券商山一证券破产，行业彻底进入破产潮。当年金融从业人口减少 60 万，部分 “幸运儿” 转入号称永不倒闭的银行业\n随着 1998 年亚洲金融危机爆发，日本银行体系全面崩塌，四年间累计 150 家银行破产，日本金融业坠入深渊，大量转型银行的金融人员再度失业\n尽管日本银行业于 2000 年开启重组出海，但直到 2005 年不良率才降至 3.5%，出海收入占比开始快速提升。此时，距泡沫破裂已过去十五年。尽管此时日本金融业开始吸纳海外岗位，但 90 年代的 “天之骄子” 们大多已转行，或所学知识难以适应海外业务需求。这大概就是 “时代的一粒尘，落在个人头上，就是一座山” 了\n此外，泡沫破裂后，日本迎来了接近 10 年的考公热，公务员报录比例超过\n1:30，但随着此后持续的低薪与工作量升高，公务员逐渐被日本年轻人所抛弃，如今报录比仅为\n1:3\n实际上，90 年代后，日本除大企业外，有三类公司异军突起：第一类是消费降级型，如优衣库；第二类是隐形冠军，如基恩士；第三类是出海抢单的非制造业企业。第二、三类企业创造了大量就业，甚至在 90 年代日本 “就业冰河期” 形成了 “出海暖流” 之说。日本出海一代，既躲过了国内经济下行的内卷，又凭借海外增量机会，摇身成为 2000 年后日本中产的主力之一\n一个典型例子是技术人才。当丰田、本田等在泰国、美国设新厂时，一名在日本本土有经验的产线班长或质控员，可被派往海外，培训当地员工，确保流程与品质标准与本土一致。他们能获得可观的外派津贴，并享受当地较低的生活成本。另一种模式是日企海外子公司本地化运营，催生了对既懂技术又了解当地市场人才的需求，为日企员工加入本土企业创造了条件\n工作出海这种方式的特点往往是大企业先行，中小企业随后成为主力；从配套生产到独立经营；对技术、营销人才需求大都比较大。最常见的是日本大企业在海外设厂，催生对配套供应链的需求，许多中小企业和个人创业者会为这些海外日企提供零部件和生产服务，而结果就是日本破产中小破产企业数量在 1998 年登顶后一路下滑，10 年后基本降到泡沫前水平，保住了就业的基本盘，与之相反的是韩国在遭遇金融危机后，中小企业集中倒闭后大企业一家独大的局面\n（2）商业出海\n商业出海指的是那些更具冒险精神的人，利用海外市场对日本产品和生活方式的好奇，开创个人事业\n例如，一个曾在餐饮业工作的日本人，看到曼谷或雅加达的日资企业员工和当地富裕阶层对正宗日料的需求。他带着积蓄，在当地开一家小而精的居酒屋或拉面店，从日本进口关键食材，严格复制日式烹饪与待客之道，靠 “正宗” 招牌站稳脚跟\n另一种方式，类似今天的 “跨境电商”。例如，一位熟悉日本消费品市场的主妇或上班族，发现中国富裕家庭开始追求高品质日货。她小批量采购日本母婴产品（如尿不湿、奶粉）、美妆（如 SK-II）、家电（如虎牌电饭煲），通过早期电商平台或线下渠道销售，赚取差价\n（3）投资出海\n投资出海，更多是通过海外金融产品和不动产实现。这也是当下国内投资者常见的模式 —— 身在国内，投资全球\n典型的例子是，一个普通的东京上班族，看到国内利率近乎为零，股市低迷。他将每年的一部分奖金投入到银行或证券公司推荐的 “国际分散投资” 基金中。这些基金专门投资于正在增长的海外市场（尤其是东南亚和美国）的股票和债券，让他能间接分享海外经济增长的红利，实现对冲国内经济风险的目的\n实际上，有一个诞生于日本的概念，精准描述了这一现象 —— 渡边太太：指的是以进行借贷利率低的日圆，兑换成外币后向海外高利资产投资做为理财持家手段的炒汇散户，渡边是一个常见的日本姓氏，里面有有一大部分炒汇散户为家庭主妇，《经济学人》便以渡边太太来称呼这些进行外汇操作的个人投资者\n这其中，蕴含着一个核心理念：在通胀国挣钱，在通缩国花钱。这个理念时至今日其实也不过时\n在这个问题里《北京土著中产要润么？》，有回答提到了相似的理念\n\n如果回到 1990 的日本，东京土著中产要润吗？卷又卷不过，躺平不甘心，重启人生不确定性太强。靠长辈托举，在东京衣食无忧，到底要不要润到欧美、加拿大和澳大利亚？\n正确回答是：把东京的资产卖了，换成美元资产进行投资，然后继续生活在日本，东京大阪名古屋，自己爱住哪住哪。投资海外，生活日本。即使搬去北海道夕张都没事，人跑光了和你又有什么关系\n以上策略后来被总结成一个经济学术语：渡边太太。至于那些觉得日本能走出通缩，死扛不卖，投资日本固定资产的，在 30 年以后日本终于开始走出通缩时，已经熬成老头老太太，成功过上了生活费不足需要继续工作的退休生活，被东大人挂在网上嘲笑\n以上策略过去 30 年回测适用于东京。至于能不能用在北京，你自己考虑。你们都可以拿来回测一下当年的东京中产，看看策略能不能跑通。我们做金融的一般来说有个原则：回测跑通的策略不一定对，但是跑不通的那些直接 pass\n\n出海这一站，我们身在何处\n观察当下中国，与 90 年代初的日本宏观环境，会发现对于普通人的影响因素高度相似\n人口因素上，虽总人口下降，但适龄劳动人口尚未减少，简单说，就是岗位少了，干活的人却没少。就业市场上，房地产与消费均陷低迷，原吸纳大量就业的行业普遍收缩。外部环境上，90 年代日本同样面临与美国贸易摩擦，出口受限\n前述大和证券统计显示，日本上市公司海外利润近总利润 60%，多家龙头海外收入超 75%。反观中国，2023 年上半年，中国境内上市公司海外总利润 3398 亿元，仅占 8.8%。这表明当前中国上市公司高度依赖国内市场。若能实现全民出海，海外利润端或有 5 倍以上增长空间，进而将海外利润增长转化为经济承压期的新动能。我国倡导全球贸易与一带一路，也正是鼓励中国企业出海\n从 2021 年开始，中国就进一步加大了对越南的投资，在视频《 【全球经济】为何 2023 越南经济增速下滑\n欧美撤资 中国加仓\n越南为何选择与中国修补关系》里提到了这个现象。中国此轮加大越南投资，主要是 &gt; 换取越南内需市场，以及利用越南劳动力优势的三角循环，另外也是希望中国的品牌进入到亚非拉国家的民众心智中，实现早期的心智占领，从而为后续占领这些国家的市场打下基础。这套逻辑跟前面的提到的日本在东南亚尤其是中国出海投资的逻辑类似：以市场和技术驱动为主，需要同时从新兴经济体获取利润以及和欧美企业进行竞争\n尽管 2018 年后因贸易战等因素，上一轮出海热戛然而止。但根据 普华永道的统计，2023 年中国民营企业的海外并购市场结束了连续 3 年的下滑，并购额触底反弹重新回到 120 亿美元大关。虽然 2018 年以后，因为众所周知的各种因素叠加，上一轮出海热戛然而止。但随着 2023 年以后，以比亚迪与阳光电源等新能源企业出海为起点，产业资本成为了这一轮出海热的先驱。而在\n2024 年，我国上市公司海外收入在去年已经两位数增长的情况下，再次出现近\n10% 的增长，历史上首次突破 10 万亿收入大关，海外已经能占到总收入的\n13.8%。与海外市场高歌猛进不同，国内市场则呈现持续低迷，2024 年上市公司国内收入出现 10 年来首次负增长。目前我国境内外收入增长差已经接近 10%，非常接近日本当年全面出海潮启动的指标\n随着企业出海的加速，必将带动的是国内人才向海外流动的加速\n若以日本为参照，伴随 2023 年中国新一轮企业出海潮开启，我国普通人的出海机遇期也已到来。以美世与领英的报告为参考，2015 年至今，中国外派岗位数量增长约 640%，2023 年更增长近一倍。可以说，中国普通人正面临工作出海的机遇。截至 2023 年，中国财富 500 强企业中，91% 拥有不同形式的海外业务。据统计，中国已有 5.9 万家企业开展出海招聘，近两年岗位数量呈爆发式增长，领英预计到 2026 年，中国海外招聘岗位将达 450 万个\n分行业看，新能源车行业是近两年出海招聘岗位增长最多的领域，增幅达 412%。互联网与消费电子行业紧随其后。以游戏赛道为例，因东南亚和拉美人口基数大、获客成本低，其海外游戏利润率通常高出国内 30% 以上，而高利润率往往意味着高收入。可以说，在新能源车、互联网与消费电子这三个领域，是否出海工作已非判断题，而是一道人生的选择题\n除了行业，具体岗位需求也有了较大的变化\n不同于过去 10\n年中海外招聘主要以销售为主的固有印象。根据美世的报告，海外企业对于运营岗位的需求开始变的更加强烈，因部分早期出海企业已具备一定市场基础，经营开始向本土化与再国际化深化，此过程需大量运营人才支撑。如何培育足够多的运营人才维系已开拓的海外市场，成为许多企业新阶段的出海难题\n日本在进入到 1998 年以后，其出海行业也逐渐转向多元化。在这个过程中，日本有两类人极大的受益：第一类是在多元化转型初期就进入到海外市场的从业者，通过将日本的经营模式与本土化进行结合，利用当时日本强大的供应链从而快速形成成本效应，由此赚到了丰厚的加工利润并进行产业升级 --90 年代末的日本制造业企业出海基本都是这个路子。第二类则是在 2000 年以后通过不断进行本土化改造，将日本产品彻底中国化的各类企业，这其中即包括丰田，也包括无印良品等零售企业海外从业者，一般称之为日本的品牌出海企业\n日本的这两类人，恰与 2023 年中国出海人才的需求画像开始重叠。一方面，我们的新能源、电子类企业开始大规模海外设厂，海外技术类工程师紧缺；另一方面，前述海外运营人才紧缺，正是海外企业深化本土化改造所催生的需求。据霞光智库报告，2024 年我国出海招聘岗位中，对本科以上学历要求占 86%，要求工作 5 年以上岗位超一半。企业对人才要求如此之高，是因部分中国企业已进入深度本土化运营时期，急需高学历人才支援，而非简单劳务工种\n但根据统计，超过一半的企业都表示海外人才招聘十分困难。主要原因之一是目前多数 30 岁以上的从业者较难接受出海工作，该年龄段出海意愿普遍低于 40%，且年龄越大意愿越低。家庭因素与水土不服是两大主因。另一方面，95 后出海意愿断崖式领先所有年龄段，多家机构统计其意向度超 75%。以猎聘海外岗位投递数据为例，95 后群体投递次数是全年龄段平均的 4 倍\n这就引发了一个有趣的人才倒金字塔现象：尽管多数出海企业在问卷中表示希望招聘 5 年以上、具国际视野的复合型人才，但因早期合适应聘者太少，企业只能先招聘 95 后慢慢培养。随后几年，因这些企业快速发展，早期培养的 95 后反而成为最快具备国际化视野的群体。目前，在海外人才市场中，95 后是所有年龄段占比最高群体，在互联网等新兴行业占比超 60%，90 后占比超 82%。可以说，30 岁以下年轻人吃到了中国近几年互联网出海的第一波红利\n虽然说了这么多出海的潜在益处，但是不得不承认的是，当下全球的复杂形式也给中国未来的出海之路带来了阻碍。比如说美国的各种贸易战、关税战。然而，或许就如中国经济这\n40\n年崛起之路一样，道路是曲折的，但前途也是光明的。毕竟，日韩在其全面出海之路上，亦曾遭遇不同国家的打压。但中国出海浪潮的趋势应不会改变，产业与文化双出海的战略机遇期，将配合 “一带一路”，成为我国未来二十年新的巨大红利。而这一红利，往往将献给最先行动的勇敢者。毕竟，若已是确定无疑的机会，又怎会轻易落到普通人手中？\n选择与努力\n都说选择大于努力；我们身边总不缺少 “人生赢家” 的故事，故事的版本往往类似：在关键时刻做出了一个 “英明” 的决定，于是乎就享受了时代的红利，从此顺风顺水，功成名就。这很容易让我们产生一种感觉：存在一个 “好选择”，一旦选中，人生就能开启简单模式\n那什么样的选择才叫做好选择？什么样的努力才叫做有意义的努力？而在出海这个问题上，选择和努力又应该如何权衡？\n什么样的选择才叫做好选择\n什么样的选择才叫做好选择？比如说在 2008 年入大 A 然后 2016\n年清仓？比如说用卖出的钱买入房市和比特币并在 2021 年出？比如说在 2016\n年加入字节跳动然后把期权比例要满并且持着不卖？\n亲爱的，这不叫好选择，这叫做好运气，或者说好梦（白日梦的梦）。在真实生活的选择里，我们无法拿着后视镜去踩中每一个风口和好趋势。而且盈亏同源、祸福相依，当前的后视镜看到的上行期的美，在另一个平行宇宙里，也许就是下行期的痛了\n赌博也许算选择频次最高的行为之一了，一盘接着一盘，买定离手，需要在短时间做出选择，然后或赢或输，再开启下一盘。我很喜欢于宙的这篇演讲里\n《我们这一代人的困惑》关于赌博、或者说 “选择” 的故事\n\n上大学的时候，我热衷于各式各样的赌博游戏，是学校旁边赌场的常客。我赌徒生涯的起点源于赌场里最基本游戏轮盘赌，轮盘上\n1 到 36 个数字和两个 0，赔率是 1 赔 36。1 到 36\n分为红黑两色，押注红黑的赔率是 1 赔 1\n作为一个合格的接受过九年义务教育的人都知道，每一次轮盘开始转动的那一刻，都是一次纯粹的独立随机事件。但是赌博这件事情的魅力就在于，当你真正身处赌场，看到已经连续\n4\n次开出红色的时候，几乎所有人都会想把筹码压在黑色的那一面。而我当时的梦想，就是破译这其中的奥秘\n我最初的策略非常简单，当连续三次开出奇数，就押注偶数，连续三次红色，就押注黑色。难以置信的事情发生了，在我严格地执行这个策略的情况下，前几次去赌场不但全身而退，每次都还赚了不少，以至于我产生了一种幻觉，也许游戏是有规律可循的，我已经看到了人生巅峰就在不远处向我招手。当然最终的结尾你们一定想到了，在经历过连续\n18 个偶数，连续开出 21 次黑色后，我把之前赚到的钱都乖乖地还给了赌场\n后来我知道，我那个愚蠢的梦想叫做赌徒谬论，就不具体展开讲了。但它对我意义深刻，我终于明白了在纯粹的随机事件面前，一切规律都是无谓的\n\n我们生活中遇到的所有事情基本可以分为三类，第一类纯粹由随机性决定，比如布朗运动和轮盘赌博；第二类纯粹由能力决定，比如英语六级考试、110\n米栏之类；第三类，也是我们最常遇到的，由能力和随机性共同决定，比如创业、投资、恋爱或是梦想\n当我们回头去看，那些看似有规律可循的 “机会”，其核心充满了随机性。一个选择在当下看来是 “好” 是 “坏，往往不取决于选择本身，而取决于它背后那套由能力和随机性共同作用的复杂系统\n如果说 “一命二运三风水”、“生死有命，富贵在天”，在随机性面前我们确实会无能为力，如果是这样，那什么样的选择才是 “好选择”？\n也许，当你能够清晰认知到所作出的决策里，随机性占比有多高，这就是一个 “好选择”。当你有这种清晰的认知后，你才不会把所有的希望都寄托于运气，也不会在失败后，完全归咎于自己 “选择失误”。就像你决定是否要\nAll In\n去闯荡演艺圈，你得明白，黄渤的成功里实力固然重要，但那万里挑一的运气，可能才是决定性的。认识到这一点，你才能心平气和地接受任何一种结果。而一个人在年轻的时候，做的每一件事情，能清楚地区分其中随机性所占的比例并能心平气和地接受它，也许就是最宝贵的财富\n人类的大脑中有一个名为 “杏仁核”\n的器官，主要帮助我们应对生活中的危机事件，所以对 “坏消息” 极其敏感，坏消息也更容易让我们感同身受。在必要时，杏仁核可以救命。但是在日常生活中，我们也很容易被\n“杏仁核” 挟持，让我们倾向于只看到负面消息，同时做出灾难化想象\n选择、或者说随机性，往往会让人恐惧，其原因是只看到了负面部分。但我们需要记住不确定性是两面的，同时决定了上行风险与下行风险。我们很容易过于在乎「下行风险」，以至于忘记自己完全有能力、有可能做出更好的结果\n所以，也许一个好选择，是你能同时看到随机性带来的上限和下限的选择，去看数据、看事实，从数据和第一性原理出发去判断一件事情的好坏，而不是原始的恐惧情绪\n那当外部有很多负面的声音的时候，我们又应该如何应对呢？《钝感力》这本书有一句笔者比较喜欢的话\n\n在人缺乏自信或犹豫不决的时候，无论怎样的左思右想都于事无补。因此在这种时候，就要摒弃杂念，更为大胆、充满自信地向前迈进才行。\n犹豫不决，不仅根本无法前行一步，还有可能往后倒退。\n对于你的犹豫不决，很多人当然会有他们各自的看法。而这时，我们要做的就是从中选出自己听起来最为顺耳，最能够使自己振作并快乐地努力下去的话语，从而坚定不移地向前迈进\n\n虽然上面说的都是有点偏心态层面有点虚，但这的确非常重要，至少不会让你在某一次结果不如人意之后怨天尤人，失去做下一次选择的勇气和持续向前的魄力。在我们心里能接受随机性把我们置于最差的结果后，也许我们能开始尝试 “预测和计算” 每个选择的优劣了\n从统计学的角度，每次选择的期望是可以形式化演算出来的，以最简单的压大小为例。如果本金是\n\\(a\\)，胜出后得到的总奖金是 \\(b\\) (\\(b \\gt\na\\))，胜率是 \\(p\\)，那这这选择收益的期望就是\n\\[E = p*(b-a) - (1-p) * a = pb -\na\\]\n只要保证这个期望为正，那根据大数定律，在重复多次的试验中，实际的收益的均值就会趋近这个期望\n但生活的奇妙之处在于，一是我们是没法知道那个分布的，也就是上面 \\(p\\)\n的这个值我们是无从得知的，而且每次选择都是不同分布的。二是生活中绝大部分的选择是没法像大数定律那样，可以重复抛硬币去让最终平均收益逼近期望收益\n即便如此，在我们每一次的选择里，可以尝试问一下自己这几个问题：我们关注的收益有哪几方面（健康、金钱、技能等）？我们 “赢” 的概率（能获取到这部分的收益的概率）有多大？如果我们 “输” 了，损失的东西我们是否能接受的？而在这个选择 “赢” 的概率中，随机性又扮演了大的角色？\n这些问题也许不能很好做量化，甚至做量纲对齐本身就是一个比较千人千面的答案。但也许当我们能够对自己诚实，去一遍遍的询问拥有着 “一颗不断强大的大脑和一颗充满恐惧、贪欲和脆弱的玻璃心” 的自我时，也许我们就能知道哪些是当下对你最重要的是什么；也许我们就能够学会理解和接受人生的偶然性；也许我们就能允许自己恐惧，因为未知自然让人恐惧，这是你心灵的自然反应；也许我们就可以相信：每个人都有一条路，这条路有很多错综复杂的因素促成，但一定由一个人在当时当地能够做的最好的选择组成。所以你提前恐慌或者不恐慌，并不能改变什么。走上去以后，就好好地走，才能精彩\n什么样的努力才叫做有意义的努力\n既然生活充满着随机性和不确定性，没有绝对的 “好选择”，那该朝着哪个方向努力？还有什么事情值得努力？或者说什么样的努力才叫有意义的努力\n这里我还是想借用《我们这一代人的困惑》里的例子，因为实在是写得非常好\n\n去年这个时候，我和朋友在琢磨去大庆做点服装生意，决定去考察几个商场。我当时住在北京，因为之前晚上和朋友在外面玩得比较尽兴，回到家里已经比较晚了，担心睡觉睡过头会错过航班，那晚上就直接在沙发上靠了一晚。那是我第一次去哈尔滨，十一月份已经很冷了，衣服拿得不足，下了飞机冻得头疼。又因为没有提前订票，到了哈尔滨之后才买的火车票，发现就只剩站票了。于是，当我一晚上没睡，冻得头晕眼花，又在绿皮火车上站了两个多小时之后，抵达大庆的那一瞬间我觉得自己实在是太不容易了，将来必须要写进回忆录里面。可是，回头仔细一想，这些所谓的\n“努力”\n对我最终把那个服装生意做好，没有半毛钱关系。更何况，如果我前一天晚上能早点上床睡觉，多准备点衣服，提前在网上把火车票订好，完全可以舒舒服服地达到同样的目的\n我的那次经历像是自己二十多年生活中很多事情的缩影，沉溺在对结果没有直接帮助只是因为自己遭受了一些痛苦的行为中，误以为那就是努力\n当我终于意识到我并不是唯一曾经把无意义的消耗当作努力的时候，忽然发现，原来生活中我觉得很努力的人，也许没那么勤奋，如果在正确的方向上坚持行动，超过他们也并不困难\n\n我们这一代人对于勤奋和努力的理解，几乎清一色地来自于学校，更精确地说，在前二十多年的生活中，我们眼中最努力的人，就是那些最能拼命看书和做题的人。实际上，这种理解是极其片面而幼稚的，因为看书和做题本身，都是为了一个极其鲜明的目的而存在的，就是通过考试。这种勤奋的付出极其纯粹，更多的复习时间，更高的复习强度，一般而言，都可以直接地提高考试的分数\n但生活的美妙之处却在于，很多事情在我们没做到一定程度之前，是完全没法理解的。生活很多事情的回报曲线往往是非线性的，需要经过漫长的潜伏和积累，才会有质的突破。就好像作者在演讲里提到的学英语的例子\n\n这就好比学英语，十几年漫长的岁月里我都在幻想，要通过多么复杂的流程，多么精密的设计，多么全面的涉及和多么不可思议的努力，终于有那么一天，或许我就能因为前期的那些无懈可击的学习，说一口比较流利的英语，像说中文一样，可以边说边想，而不是说每一句话之前设计它的句式时态词汇然后在心里复述几遍再看上去流利地背诵出来。谁不是这么设想的呢？可惜，它不仅从来没有实现，并且让我看不到有任何实现的趋势，对于每一个设立目标的人来说，没有比这更痛苦的感受。\n但是在去了美国两年左右的时间之后，我忽然发现自己已经可以毫无障碍地说一口流利的英语了。这并非我采用了什么新的学习方法，而是因为去了印第安纳之后身边中国人很少，在没有选择的情况下，只能被迫用英语去交流和表达，在这个过程中，我并没有认真想过自己每天进步了多少，也没有阶段性的检验学习效果，只是不停地去听和说，因为没有选择嘛。直到两年多后的忽然有一天我才意识到，咦，自己好像真的已经可以了。但是我确实无法总结出来是如何一步一步做到的，只是那两年的时间，我一直都在很不情愿地用英语去生活嘛。\n\n一个人能获得的最可贵的能力，也许和掌握一门语言一样。你所付出的努力不是能够获得即时回馈的，甚至在很长的一段时间内没有任何收获，直到积累到了一定的阶段后，忽然爆发出惊人的力量，连你自己都不清楚这一切是如何发生的\n当你经历了足够多的量变后引起质变时所拥有的技能，大部分人是终身难以企及的，不是因为他们太笨，恰恰相反，因为他们都太聪明了。触发人类行动的最基本原理被称为反射，我们是需要即时回馈的物种。所以绝大多数人对于世界的理解度是线性的，但更多情况下，事物却是以漫长的潜伏震荡后爆发突破的形式发展的，比如说学习语言、读书写作、锻炼身体\n那回到最开始的问题，什么样的努力才是有意义的努力？\n我想在离开学校之后，在现实生活中，当我们遇到的很多事情不再像做题和考试之间联系得那么紧密时，当付出和结果之间往往没有那么立竿见影时，很多人的付出都是浅尝辄止时，而最可贵、最有意义的努力，也许是在基于上面反复询问自己内心后而做出的 “正确” 方向上，对那些无法立即获得回报的事情，依然能付出十年如一日的专注和热情。最终的结果也许不足以让你独孤求败，但足以出类拔萃\n出海中的选择与努力\n说到 “出海”，无论是留学还是工作，这大概是当代很多年轻人面临的最典型、也最纠结的选择之一了。它完美地融合了能力、随机性、梦想和现实的冰冷计算\n我们很容易被两种极端叙事裹挟：一种是 “出海即天堂”，描绘了一幅远离内卷、高薪自由的生活图景；另一种是 “出海即深渊”，充斥着文化隔阂、玻璃天花板和难以言说的孤独。这两种叙事都像轮盘赌上连续开出的红与黑，让我们产生一种错觉，仿佛押中颜色就能决定命运\n但正如我们前面讨论的，一个 “好选择” 不是去赌一个绝对正确的答案，而是基于多维度期望值的理性计算。我们可以尝试用 “期望值” 模型来掂量一下 “出海” 这个选择\n我们不能只盯着一个维度的得失，得把几个关键维度打包在一起算个总账，比如说对于笔者，健康、金钱、体验多样性、复利性是几个重要的维度\n\n健康\n\n健康是所有 0 前面的那个 1，没有了健康，其他一切都没有意义；而成功的道路有很多，但没有任何一条是必须要以健康为代价的\n很多人出海的理由是国内太 “卷” 了，为了不让过高强度的工作影响到身体的健康而选择去海外。但工作过的朋友都知道，不同团队或部门的工作强度的方差极大；虽然都叫出海，但去到哪个公司，哪个部门，哪个团队，会极大影响你实际的工作强度，所以在选择前，需要做好背调\n此外，你需要评估自己能否承受异国他乡的孤独、文化冲击带来的长期心理压力。有些人天性喜欢新鲜事物，社交能力强，那这个选择的健康期望值就是正的；反之，对于一个极其恋家、内心脆弱的人，强行出海可能意味着巨大的身心健康损耗\n\n金钱\n\n这里需要算两笔账。一是直接的成本和收益，学费、生活成本与未来可能收入之间的概率。别光看 “华尔街高薪” 或 “硅谷大包” 这种小概率事件，也得算算找不到工作、或者只能找到一份普通工作的概率和收益。二是个人的宏观策略，其实就是前面提到的 “通胀国攒钱，通缩国花钱” 的概念；这个回答针对这一点也给出了一些观点\n\n所谓生活、所谓移民，就是一个赚钱、一个花钱问题，这俩问题不解决其他都是次要\n你问题中没谈美国，但我最熟美国，先拿它举例：美国物价最高、移民难度最高最折腾、而且这国家细想也很奇葩，但移民来的最多尤其中国人和印度人。因为容易赚钱，对就因为容易赚钱这么简单就这一个因素，导致美国最大的优势是即使有一天不想在这住了随便去世界哪里都觉得便宜，带着攒的钱跑路去哪里不是吃香喝辣\n我个人认为，如果类比起来：中国三线城市之于北京，便如北京之于美国 —— 消费低、通缩等\n按 “渡边太太” 理论，确实更好的策略是：钱出去，人留下，钱跟着通胀去，人享受通缩去\n说得简单点，人生需要钱时要卷起来时，去通胀国 / 地区打工；人生开始走下坡路准备退场时，已有的资源就得加倍呵护因为是 “有限资源”，回通缩国 / 地区消费\n\n\n体验多样性\n\n这一点上，“出海” 绝对能给你提供一种全新的体验，你被迫用另一种语言生活，理解另一套社会规则和新的文化，也许让你看到，生活不止一种活法。这种价值，无法用短期金钱衡量。却能在你出海期间，甚至未来几十年的人生里，提供看问题的不同角度和解决问题的不同思路。这种体验本身就是一张未来可以兑换其他机会的彩票\n生活，最终还是活的一个体验的多样性。可以问一下自己，当前的这一次出海的机会如果放弃了，后续是否还会再有？到那时候对自己而言，是否还会是一个出去的好时机？如果都没有这样的机会，10\n年后自己会后悔当初的这个抉择么？\n\n复利性\n\n如果说中国的企业出海是未来，那未来大量的岗位必然对语言甚至海外经历有一定的要求。如果这个趋势是必不可挡的，那最好的选择也许是及早拥抱这个趋势\n抓住这个 “没有选择” 的环境，去真正掌握语言、建立跨文化的适应能力、构建在全球范围内都具备竞争力的专业技能，那么这种能力就是带复利的。它能在你未来的人生里，像滚雪球一样，持续带来新的机会和增长\n但如果你只是去混个文凭或者只是赚两年的快钱，把自己封闭在华人圈子里，重复国内的生活模式，那复利效应很弱\n那么，努力在出海里又扮演什么角色？\n当你基于这个模型，做出了出海的选择后，努力的方向就变得无比清晰：你的所有努力，都应该服务于最大化这个选择中具有复利效应和体验价值的维度，同时管理好健康和金钱维度的风险\n需要谨记的是，你的努力不是去赌场里试图破译轮盘的规律 —— 比如幻想靠一次投机就能实现阶层跃迁。而是要最大化你能力的概率，让你成为那个在 “能力和随机性共同决定” 的游戏中，能力占比更高的人\n你的努力，是清晰地认识到 “通胀国攒钱” 的阶段目标。在这个阶段，你需要的是卷起来，是把你的专业技能和汗水，高效率地兑换成未来的筹码和选择权。此时的辛苦，如果是指向明确的能力提升和资本积累，那就是有意义的努力\n你的努力，是构建你的 “复利引擎”。学好语言，建立有价值的专业网络，培养跨文化解决问题的能力。这些能力，无论你未来是选择继续留在 “通胀国”，还是带着筹码去 “通缩国” 享受生活，都是你赖以生存和发展的根本\n你的努力，是主动跳出舒适区，去最大化 “体验多样性”。不是把自己活成一个孤岛，而是强迫自己去和当地人交流，去理解他们的文化，去参加那些看起来 “没什么用” 的社交活动。这正是在为你未来的 “选项价值” 充值\n这是个需要花力气做出改变的过程，过程中必然是会有痛苦的。但人生嘛，宁吃自己的苦，不吃生活的苦。有得选的时候尽量做选择，虽然苦点累点，不要没得选了开始内耗。比如选读书的苦，不选无业的苦；选运动的苦，不选生病的苦\n没有弯路\n虽然《杀鹌鹑的少女》把选择这个命题描述的比较沉重，但真的是所有选择都会影响巨大么？我们没法回答这个问题，因为 “未选择的道路” 就如同反事实数据一样，我们永远无法得知那条未选择之路的好坏，虽然我们常常会去美化这条所未曾选择的道路，然后觉得自己走了 “弯路”\n这种选择的沉重感，归根结底是源于三件事：\n（1）人生的有限性。“我走这条路是不是多浪费一两年的时间？”“我会不会走弯路？”\n这是人生的有限性在摆弄你\n（2）未来的不确定性。“我要如何准备才能胜出？”“我会失败吗？”\n这是未来的不确定性在你的内心敲打\n（3）过去的不可更改性。“我的选择是不是错了？”“这么大的投入最后回报得来吗？”，你们患得患失，因为没有后悔药可以吃。\n我们之所以在选择的时候有这些焦虑、恐慌、敏感的情绪，害怕走 “弯路”，很多时候就是源于这三个因素。但吊诡之处在于，如果你感到的恐惧是由人生某些不可改变的特点带来的，那么即使你拼命想抓住写什么，甚至于你终于完成了一个目标、得到了一块暂时的喘息之地，你仍然无法解决内心的痛苦\n这也许就是我们这一生都在实现目标中挣扎着度过的原因：上初中的时候，老师告诉你，中考的淘汰率是最高的，只要闯过去，上了高中一切就好了。但上了高中的时候，高中老师又说了，考上大学就进了天堂。当你考上了大学，依然空虚迷茫，父母老师又告诉你，找到工作就好了。工作之后发现烦恼和忧虑依然都在，大家都告诉你等你事业有成就好了……\n我们这一生似乎都在试图让自己走在社会告诉我们的 “正道” 上，少走 “弯路” 是朋友、老师、父母告诫我们要避免的，可是，这个 “弯路” 又该怎么定义？排除掉那些明显不合法的伤害他人、伤害自己的行为，又有多少的路是真正的 “弯路”\n每每这时，我都会想起 Steve Jobs 的那个我非常喜欢的 connect the dots\n的故事，虽然已经是 20 年前的演讲，但我一直奉为圭臬，每次重听也总能激励到我\n\nNone of this had even a hope of any practical application in my life.\nBut ten years later, when we were designing the first Macintosh\ncomputer, it all came back to me. And we designed it all into the Mac.\nIt was the first computer with beautiful typography. If I had never\ndropped in on that single course in college, the Mac would have never\nhad multiple typefaces or proportionally spaced fonts.\nIf I had never dropped out, I would have never dropped in on that\ncalligraphy class, and personal computers might not have the wonderful\ntypography that they do. Of course, it was impossible to connect the\ndots looking forward when I was in college. But it was very, very clear\nlooking backwards, ten years later.\nAgain, you can’t connect the dots looking forward. You can\nonly connect them looking backwards, so you have to trust that the dots\nwill somehow connect in your future.You have to trust in something: your\ngut, destiny, life, karma, whatever. Because believing that the dots\nwill connect down the road will give you the confidence to follow your\nheart, even when it leads you off the well-worn path. And that will make\nall the difference.\n\n事实上，真正的人生不是一场竞赛；至少也不是一场跑向同一终点的竞赛。那条笔直的、被无数人验证过的 “跑道”，其实是高考给我们养成的思维惯性。一旦进入大学、迈入社会，这套逻辑就渐渐失灵了\n如果你去读大多数成就一番事业之人的传记，会发现他们的人生轨迹，极少是一条昂扬向上的直线，反而充满了意想不到的波折与迂回，有的甚至是几起几落\n比如说比如说奥巴马和克林顿都是穷小子，好不容易考上了个东部的常青藤，结果毕业后一个回老家干社区工作，一个回老家在名不见经传的大学教书；比如说如今被誉为 “寿司之神” 的小野二郎，年轻时曾在市场鱼贩那里做了多年的学徒，每天只是处理鱼货、打扫卫生；比如说爱因斯坦在提出划时代的相对论时，只是瑞士专利局里一个默默无闻的三级技术员。他的研究工作与他的本职工作几乎毫无关系，完全是在 “业余时间” 进行的。按照前面的逻辑，他们都 “走了弯路”\n可是从后往前看，哪里又有什么弯路？\n你当过社区工作者，深入了解了基层的脉搏与民众的真实诉求，这份独特的阅历，是那些直接进入精英机构的同学所没有的。几十年后，当你需要展现对民生的深刻理解与共情能力时，这就成了你无可替代的优势；你曾在鱼市里磨砺过，对每一种食材的时令、质地、处理方法了如指掌。几十年后，当你追求极致的料理时，这段经历便化作了你区别于所有厨师的、最深厚的底蕴；你在专利局里安静地演算，避开了学术界的浮躁与成见。恰恰是这份 “孤独”，保护了你最颠覆性的思想得以孕育成型\n当你从后往前看，把人生的点连成线，会发现所有的努力和经历都不会白费，在哪个领域、哪个岗位努力都可能在未来产生回响。\n那些看似八竿子打不着的知识和技能，不知在哪个命运的拐角，就会悄然融合，迸发出惊人的力量\n当你从后往前看，你会发现，真正的弯路从来不是哪一次具体的方向选择，而是内心的怨天尤人与行动上的停滞不前。是那种不断回头张望、懊悔 “如果当初”，却让 “现在” 和 “未来” 在无尽的内耗中白白流逝的状态\n人生的路，不是规划出来的直线，而是一步步走出来的轨迹。\n我很喜欢这句话：“没有什么事情或节点，能把我们的人生分成两段”\n我们总是不自觉地给人生划分阶段，并寄望于某个里程碑之后，生活能焕然一新：考上大学就好了，找到工作就好了，结婚生子就好了，财务自由就好了…… 仿佛人生是一部可以分集的连续剧，过了某个关键情节，基调就会彻底改变\n但现实是，从出生到生命尽头，生活是一条连绵不绝的河流。它不会因为你拿到了某个学位、某份聘书，或戴上了某枚戒指，就从湍急变得平缓，从浑浊变得清澈。生活的每一个阶段都自带其独特的痛苦与焦虑，它们周而复始，只是换了一副面孔出现。\n你不会因为考上大学，就告别了青春的迷茫；不会因为事业有成，就消除了中年危机的阴影；更不会因为迎娶了女神，就免疫了婚姻中琐碎的摩擦与漫长的磨合\n反过来，每一个阶段也镶嵌着独属于那个时期的、无法复刻的快乐。二十岁时，和兄弟们在夏夜操场打完篮球，瘫坐在场边，汗水淌进眼里有些刺痛，仰头猛灌一口冰可乐，那气泡在喉咙里炸开的简单酣畅，是你未来坐在米其林餐厅里，从容品鉴一杯复杂红酒时所无法体会的。年轻时可以为了一个灵感通宵达旦，可以来一场毫无计划、说走就走的 “特种兵” 旅行，那种由充沛精力和对世界的好奇所驱动的纯粹快乐，与中年后精心规划的、追求舒适与品味的旅行，是两种截然不同的生命体验\n每一段岁月都有它存在的价值，没有高低贵贱之分，都不应该被辜负。试图用未来的某个 “黄金时代” 来否定当下的 “青铜岁月”，或者沉溺于过去的 “美好时光” 而逃避眼前的现实，都是对生命本身的浪费\n因此，你要记住，没有弯路，因为\n“发生在你身上的事，要么是个好事，要么是个好故事”。顺利的成功，自然是 “好事”；而那些看似失败的、痛苦的经历，在事后回望，往往会沉淀为你人格的一部分，成为你酒桌上、文字里、与晚辈交谈时，那个最有力量的 “好的故事”。它定义了你的韧性，丰富了你生命的层次\n当然，这话听起来，在某种程度上确实有点阿 Q 精神的嫌疑。它似乎轻描淡写地忽略了过程中真实存在的、甚至对一些人而言是生命无法承受之重的苦难\n我非常喜欢《三傻宝大闹宝莱坞》这部电影，十多年前第一次看的时候被电影的观点震撼到，这次国庆重新看的时候依旧觉得获益良多，电影里有个情节是这样的\n当 Raju 在对繁重学业和即将到来的期末考试而焦虑不安时，Rancho\n告诉他，尝试把手放到自己的胸口对自己说 “All is\nWell”，并说我们的心是很容易动摇的，你得学会哄它，无论遇到多大的事情都可以对自己这么说。Raju\n反问到 “这能解决问题么”。Rancho\n说并不能，但是这能够给你解决问题的勇气和决心\n这句简单的 “All is Well”，和我们在播客《无人知晓》里都听到的 “咒语” 这一概念一样，它们不是解决问题的具体工具，而是一种 “心法”\n比如说《此时此刻》中提到的 “刺激和回应之间存在一段距离，成长和幸福的关键就在那里”，“平静是意识到世界并非我们想象的那样”，还是《哦》中提到的\n“内心自我否定、攻击、怀疑，99%\n都是假的”、“最重要的就是走，往哪走都行”\n我想，这些 “咒语” 和 “All is\nWell” 一样，它们或许不能像螺丝刀一样直接拧紧松动的零件，也无法像导航一样为我们规划出最优路径。但它们能为我们疲惫的心灵 “充电”，在我们踏入未知的黑暗时，像一支火把，虽不能照亮整个前程，却足以给予我们迈出下一步的勇气和决心，让我们能够轻装上阵\n不确定的赌局，与我们的下注\n聊了这么多，从日本出海的宏观叙事，到个人抉择的微观权衡，我们仿佛在迷宫中绕了一大圈，最终却回到了那个最原始、也最核心的困惑前：在一个由能力、努力与巨大随机性共同主宰的世界里，我们究竟该如何自处？\n回望日本出海的历程，它并非一份精心绘就的宏伟蓝图，更像是一个经济体在撞上内需天花板后，被逼出来的一种集体生存智慧。企业与个人在时代的巨浪中扑腾，摸索出 “工作、商业、投资” 三条路径，其底层逻辑出奇地一致：将自身已有的技能、资本或头脑，与海外市场更早期、更饥渴的发展阶段相嫁接，从而兑换一份 “时间差” 带来的溢价。\n这背后，是 “通胀国攒钱，通缩国花钱” 的冷静算计，也是对全球资源进行朴素配置的直觉\n当我们把这份宏观的镜子转向自身，所谓的 “好选择”，从来不是那个能让你一劳永逸、开启人生简单模式的 “标准答案”。它更像是在信息不完备的牌局上，一次基于多维度期望值的下注。这个决策模型里，装着健康、金钱、体验的广度与能力的复利等重要的、千人千面的维度。一个 “好选择” 最关键的标志，在于你清醒地知道随机性在其中占了多大分量，并且，无论牌面如何翻转，你都能心平气和地摊开双手，接受任何结果\n而 “有意义的努力”，则是在你下注之后，不再沉溺于 “没有功劳也有苦劳” 的自我感动型的无效消耗，而是将所有的行动与心力，都聚焦于一件事：如何提升我们 “赢” 的概率。它是在基于前面 “好选择” 选定的方向上，对那些无法立即兑现、甚至漫长岁月里都看似石沉大海的事情，付出持久的专注与热情。无论是被迫掌握一门语言，还是构建一个能产生复利的能力网络，其价值都在于，它们让你在这个充满随机性的牌桌上，资格更老，筹码更多，活得更久，从而能等到那场属于你的、非线性回报的爆发\n那么，回到最初那个纠缠我们的迷思 —— 关于 “出海”，关于所有人生十字路口的抉择：它们究竟是那 “命运的巨变”，还是我们当时所以为的 “生命中普通的一天”？\n答案是：它两者都是，它取决于你站在哪个时间点回望\n站在抉择的当下，它平凡、沉闷，甚至充满了患得患失的焦虑与自我怀疑。我们惧怕 “一步错，步步错”，恐惧有限的青春被 “浪费” 在弯路上，懊悔没有吃到后视镜里清晰可见的红利；这种感受，真实而具体。但当我们走过一段，回首来路，用 “连点成线” 的眼光去审视时，每一次选择，无论当时被贴上 “正确” 或 “错误” 的标签，都深深地编织进了我们独一无二的生命纹路里，构成了无法替代的路径。你会发现，那些所谓的 “弯路”，在生命的宏观图景中，恰恰提供了最独特的风景与给养\n所以，人生这场游戏，或许根本就不存在一条被社会严格定义的 “正道”，也因此没有什么绝对的 “弯路”。真正的弯路，不是你在某个路口选择了左转而非右转，而是你因为恐惧而停留在原地，不停地抱怨与内耗\n就像《三傻大闹宝莱坞》里那句简单的 “All is\nwell”，它并不能解决实际问题，却能给你直面问题的勇气。在面对出海，或是任何人生命题时，我们最需要的，也正是这份勇气 —— 在计算之后行动的勇气，在行动中拥抱不确定性的勇气，以及在不确定性中始终相信 “发生在自己身上的事，要么是好事，要么是好的故事”\n生命就在每天的生活里，不在某个遥远的、过了之后一切都会好起来的 “节点” 上。认真评估，果断下注，然后专注于打好你手中的牌\n走上去，好好地走，本身就已经是一种精彩\n\n本文一些参考资料\n\n日本企业 “出海” 启示录\n| 逐潮向海\n\n日本出海之路给我们的启示\n\n泡沫破裂后，日本全民出海潮对普通人的借鉴\n\n2023 年中国企业并购市场回顾与前瞻\n\n高度内卷下，日本全民出海潮中普通人的抉择与借鉴\n\n我们也会有如同日本一样的全民出海潮吗？\n\n复盘中国 2024 出海之路，对比日本出海潮，我们走到了哪一步？\n\n泡沫破裂后，日本全民出海潮中普通人的机遇\n\n我们这一代人的困惑\n\n写给大四\n\nConnect\nthe Dots\n\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"分布式机器学习 (1)-A New Era","url":"/2018/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(1)-A%20New%20Era/","content":"这个分布式机器学习系列是由王益分享的，讲的是分布式机器学习。正如作者在分享中所说，分布式机器学习与我们今天常听到的机器学习存在比较大的差异，因此分享中的很多观点跟我们从教课书上学到的机器学习是背道而驰的。作者在这方面具有丰富的经验，虽然是三年前的分享，或许分享中提到的部分技术改变了，但是其中的一些观点还是具有一定参考价值的。\n笔者对于分享中的一些观点也是存在疑惑的，这里还是按照分享中作者表达的意思记录下来，\n也许等到笔者工作后，才有机会去验证这些观点的正误。\n本文主要介绍了分布式机器学习中的一些重要概念，如互联网的真实数据是长尾分布的、大比快要重要、不能盲目套用一个框架等，本文对应的视频在这里，需要自备梯子。\n\n机器学习与分布式机器学习\n机器学习最重要的是数学知识，而这里的分布式机器学习更关注的是工程技法。且一般的机器学习模型假设数据是指数簇分布，而实际的数据是长尾分布的，分布式机器学习需要去对长尾的数据进行建模，对于这点，后面会有详细的描述。\n\n\nml and distributed ml\n\n长尾效应\n根据维基百科，长尾指的是指那些原来不受到重视的销量小但种类多的产品或服务由于总量巨大，累积起来的总收益超过主流产品的现象。在互联网领域，长尾效应尤为显著。下图所示黄色的部分就是长尾部分，之所以要关注长尾的数据，是因为长尾的数据的量并不小，蕴含着重要的价值\n\n\n长尾\n\n以互联网广告为例，长尾现象指的是有大量频次低的搜索词，这些搜索词并非为大多数人所了解。如搜索 “红酒木瓜汤” 时不应该出红酒、木瓜或者汤的广告，因为这个是一个丰胸健美类的产品，这就是一个长尾的搜索词。\n另外一个例子如下所示，下图是从 Delicious\n的网页中挖掘出的一些频繁项集，最左边便是这个频繁项集，最右边表示这个频繁项集在所有网页（数亿）中出现的次数，可以看到这些频繁项集是长尾的数据了，然而，这些数据都是有具体意义的。\n\n\nlong tail\n\n大比快重要\n并行计算关注的是更快，而分布式机器学习关注的则是更大，因为数据是长尾的，要涵盖这些数据中的长尾数据，首要的处理的是首先是大量的数据。\n在讲述下文前，需要先说明几个概念，Message Passing 和 MapReduce\n是两个有名的并行程序编程范式（paradigm），也就是说，并行程序应该怎么写都有规范了 —— 只需要在预先提供的框架（framework）程序里插入一些代码，就能得到自己的并行程序。Message\nPassing 范式的一个框架叫做 MPI。MapReduce 范式的框架也叫 MapReduce。而 MPICH2 和 Apache\nHadoop 分别是这 MPI 和 MapReduce 两个框架的实现（implementations）。\n对于框架而言，其中重要的一点是要支持 Fault\nRecovery，简单来说就是要支持失败的任务的回滚。MPI 无法实现 Fault\nRecovery，这是因为 MPI 允许进程之间在任何时刻互相通信。如果一个进程挂了，我们确实可以请分布式操作系统重启之。但是如果要让这个 “新生” 获取它 “前世” 的状态，我们就需要让它从初始状态开始执行，接收到其前世曾经收到的所有消息。这就要求所有给 “前世” 发过消息的进程都被重启。而这些进程都需要接收到他们的 “前世” 接收到过的所有消息。这种数据依赖的结果就是：所有进程都得重启，那么这个 job 就得重头做。\n虽然很难让 MPI 框架做到 fault\nrecovery，我们可否让基于 MPI 的应用系统支持 fault\nrecovery 呢？原则上是可以的 —— 最简易的做法是做\ncheckpoint—— 时不常的把有所进程接收到过的所有消息写入一个分布式文件系统（比如 HDFS）。或者更直接一点：进程状态和 job 状态写入 HDFS。\n与 MPI 相反的框架是\nMapReduce，MPI 容许进程间在任意时刻互相通信，MapReduce 则只允许 进程在 Map\n和 Reduce 间的 shuffle 进行通信，MPI\n几乎能够将所有的机器学习算法并行化，而部分复杂的算法无法通过 MapReduce\n实现。\n一些坑\n这里是作者认为一些需要避开的坑， 下面选择几个展开说明\n\n\n坑\n\nDe-noise data\n这里的 noise data\n指的是将数据中出现频率不高的数据，将这些数据去掉，根据前面讲的长尾，这相当于是将长尾的尾巴截掉了，因此会丢失大部分有用的数据。\n为什么不能将长尾的尾巴割掉，作者在 描述长尾数据的数学模型\n这篇文章是这么说的\n\n那条长尾巴覆盖的多种多样的数据类型，就是 Internet 上的人生百态。理解这样的百态是很重要的。比如百度和 Google 为什么能如此赚钱？因为互联网广告收益。传统广告行业，只有有钱的大企业才有财力联系广告代理公司，一帮西装革履的高富帅聚在一起讨论，竞争电视或者纸媒体上的广告机会。互联网广告里，任何人都可以登录到一个网站上去投放广告，即使每日广告预算只有几十块人民币。这样一来，刘备这样织席贩屡的小业主，也能推销自己做的席子和鞋子。而搜索引擎用户的兴趣也是百花齐放的 —— 从人人爱戴的陈老师苍老师到各种小众需求包括 “红酒木瓜汤”（一种丰胸秘方，应该出丰胸广告）或者 “苹果大尺度”（在搜索范冰冰主演的《苹果》电影呢）。把各种需求和各种广告通过智能技术匹配起来，就酝酿了互联网广告的革命性力量。这其中，理解各种小众需求、长尾意图就非常重要了。\n\nParallelize models\nin papers and textbooks\n教科书或 paper 中的模型无法应用到大数据环境下，原因是教科书上的模型假设数据是指数簇分布（Exponential\nfamily）的，而真实的数据是长尾分布（Long tail）的。\n用指数簇分布的模型去拟合长尾分布的数据，会有什么后果？答案就是会将长尾数据的尾巴给割掉了。比如说对于 pLSA 和 LDA 这两个 Topic\nmodel，如果大家尝试着把训练语料中的低频词去掉，会发现训练得到的语义和用全量数据训练得到的差不多。换句话说，pLSA 和 LDA 模型的训练算法没有在意低频数据。\n目前大部分的数学模型都假设数据是指数簇分布的，如 Topic model 中的 LDA\n和 PLSA 其先验分布和后验分布均是指数簇分布，像 SVD\n这种矩阵分解的方法也做了高斯分布的假设，而像 Linear Regression\n这类方法，从损失函数可以看到也倾向于优化数量多的样本。\n\n\nmodel pitfall\n\n既然如此，为什么这类模型还要假设数据是指数族分布的呢？，作者在\n描述长尾数据的数学模型\n中是这么说的\n&gt; 这么做实在是不得已。指数族分布是一种数值计算上非常方便的数学元素。拿 LDA 来说，它利用了 Dirichlet 和 multinomial 两种分布的共轭性，使得其计算过程中，模型的参数都被积分给积掉了（integrated\nout）。这是 AD-LDA 这样的 ad\nhoc 并行算法 —— 在其他模型上都不好使的做法 —— 在 LDA 上好用的原因之一。换句话说，这是为了计算方便，掩耳盗铃地假设数据是指数族分布的。\n因此，要对长尾建模，作者提到，Dirichlet\nprocess 和 Pitman-Yor\nprocess 是一个可能的新方向。\nUse existing frameworks\nMPI 和 MapReduce 是两个极端，介于两者之间的是 Pregel（google），spark\n等框架，这些主要思想就是在磁盘上做 checkpooint\n从而实现灵活的通信和有效的 Recovery。\n像 PageRank 这种算法在这些框架上能 work，但是像 LDA\n这种通信量大的算法，做 checkpoint 时需要写入磁盘，会导致 buffer\n过大，进而出现 out of memeory\n的情况。因此，并不能通过一个通用的计算框架来解决所有问题。\n\n\nexisting model\n\n下面是一些被广泛使用的开源框架，需要注意的是，在设计机器学习系统时，需要权衡效果与代价，选择甚至是开发合适的框架\n\nMPI\nMapReduce\nPregel\nGraphLab\nSpark\n\n另外一个需要注意的问题就是不要将计算框架和分布式操作系统混在一起，如\nHadoop 1.0\n设计中将集群管理系统和分布式计算框架混合在一起就显得非常混乱，因此在 2.0\n中出现了 Yarn 这个分布式操作系统专门负责任务的调度。\n分布式机器学习技术栈\n最后这张图是分布式技术栈，从下到上依次涵盖了分布式文件系统，分布式操作系统，一些中间件，分布式计算框架，以及构构建在这套分布式系统上的应用。由于是 15 年的视频了，因此相应的技术也会有所改变，至少我所知的\nDNN 的 framework 已经是 tensorflow，pytorch，caffe，mxnet\n等占主流了。\n\n\n参考\n分布式机器学习系列讲座\n- 00 A New Era 分布式机器学习的故事：评价标准\n分布式机器学习的故事：pLSA 和 MPI\n描述长尾数据的数学模型\n","categories":["机器学习"],"tags":["机器学习","分布式"]},{"title":"分布式机器学习 (2)-Infrequent Pattern Mining using MapReduce","url":"/2018/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(2)-Infrequent%20Pattern%20Mining%20using%20MapReduce/","content":"这一讲主要介绍了挖掘频繁项集中的经典方法 FP-growth，以及如何通过\nMapReduce 实现这个算法，通过 MapReduce 实现的 FP-growth 也称为 PFP，这个方法不仅能够挖掘频繁项集，还能够挖掘非频繁项集。原视频在这里，需要自备梯子。\n\n频繁项挖掘\n频繁项集挖掘（Frequent Itemset\nMining）指的是将那些共同出现较为频繁的项集找出来，其中一个经典的例子就是啤酒和尿布。下图展示了频繁项挖掘中的一些基本概念\n\n\n基本概念\n\n\n Transaction：每一行就是一个 Transaction，表示一个订单\n Item &amp; ItemSet：每个 Transaction 包含了若干个\nItem，表示订单中的商品，任意数量的 Item 可组成一个 ItemSet\nSupport：表示某个 ItemSet 出现的次数\n\n挖掘频繁项集有两个基本方法：Apriori 和 FP-growth，其中 Apriori\n需要对原始数据进行多次的扫描，导致速度很慢。这里不展开讲述，具体可参考这篇文章。\n这里讲述的是 FP-growth 算法，该算法通过构建一棵 prefix\ntree，使得只需要遍历两次原始的数据，下面先介绍这个算法的流程，再介绍如何通过\nMapReduce 实现。\nFP-growth 算法\nFP-growth 算法首先需要构建一棵 FP-tree ，FP-tree 是一棵 prefix\ntree，构造的方法跟一般的 prefix tree 一样，但是在构造前需要对每个\ntransaction 中的 items\n按照统一的规则排序（可以按照 item 出现的频率，也可以按照 item 的名称，总之各个 transaction 保持一致即可）\n由于原始的 items 已经按照 items 名称排序，因此这里直接按照构建 prefix\ntree 的方法构建 FP-tree 即可，如下图所示是读入前两条记录的构建结果\n\n\nprefix tree 1\n\n按照这种方法可以构建出整棵 FP-tree 如下图所示\n\n\nprefix tree 2\n\n这里需要注意的是上述的过程忽略了一步，就是在构造 FP-tree\n前还还需要设定一个 min-support 值，这个值表示某个 item 至少要出现\nmin-support 次，如果小于 min-support 次，则将这个 item 从所有的\ntransaction 中剔除，再按照上面的方法构造 prefix tree。\n这么做的原因一是出现频率不高的 itemset\n中的 item 被认为是相关度较低的，二是当输入很大的时候，构造出来的树也会很大，因此可通过设定阈值，去掉\ninfrequent\nitem。但是从我们前面讲的内容可知，这样做其实已经去掉了部分长尾数据。\n构造完 FP-tree 后，挖掘频繁项集时需要先指定一个\nitem，再挖掘与这个 item\n相关的频繁项集，挖掘与这个 item 相关的频繁项集需要构造\nConditional FP-tree，所谓的 Conditional FP-Tree，就是与该 item\n相关的所有 transaction 所组成的 FP-tree。如要挖掘与 e 这个 item\n相关的频繁项集，则构造 Conditional FP-tree 过程如下\n首先需要找到以 e 为叶子节点的所有 branch 组成的 tree，如下图 (a)\n所示\n\n\nconditional FP-tree 1\n\n接着更新父节点的计数为子节点的计数之和，并且去掉 e\n这个叶子节点，如下图所示\n\n\nconditional FP-tree 2\n\n在这个过程中需要去掉那些 support 数不满足 min-support 的 item，这里令\nmin-support 为 2，, 则由于上图中的 b 的 support 数只有 1，需要去掉 b 这个\nitem，如下图所示\n\n\nconditional FP-tree 3\n\n构造出 e 的 conditional FP-tree 后，只需要挖掘出 conditional FP-tree\n中的频繁项集，并在所有的频繁项集后加上 e 即可，如从 conditional FP-tree\n中挖掘出的频繁项集为 {a:2, d:2} 和 {c:2},\n则关于 e 的频繁项集为 {a:2, d:2, e:2} 和\n{c:2, e:2}。这样实际上是在解决一个递归问题了。\nMapReduce 实现 FP-growth\n一般来说，挖掘频繁项集的原始数据都相当庞大，因此构造出来的 FP-tree\n也会很大，当内存无法存储 FP-tree 时，一种解决方法就是通过提高\nmin-support 的值，从而将 FP-tree\n的大小降低，但是根据之前讲的长尾效应，这样做会将长尾数据切掉。\n另外一种方法就是通过分布式的方式实现该算法，通过分布式使得能够 构建\nFP-tree 时支持更小的\nmin-support，从而能够挖掘非频繁项集。这里采用的分布式框架就是\nMapReduce，下面讲述的是这个方法实现的细节。\n下面首先介绍一下 MapReduce 的编程范式，输入、中间结果和输出均是\nkey-value pairs, 中间的 shuffling 过程目的就是要将 key 相同的 pairs\n交给同一个 worker 处理。\n\n\nmapreduce\n\n在具体的实现上，shuffling 过程可通过对 key 做 hash 后，再取模（worker\n的数量）决定这个 kv 对由哪个 worker 来负责。\n另外数据的传输是通过分布式文件系统实现的，也就是需要通过反复写磁盘来进行数据的传输，这导致了\nMapReduce 的速度无法快起来。\n从上面可知，找到 item A 和 item B\n的频繁项集这两个任务是不相关的，只需要分别找出 item A 和 item B 的\nconditional FP-tree 即可，因此可以分别交给两个 worker 进行处理。\n通过 MapReduce 实现的 FP-growth，每一次的 map + reduce\n过程能够输出 item 对应的 conditional FP-tree，下图所示的是 map\n过程，能够从 transaction 中输出各个 item 对应的 conditional\ntransaction\n\n\nPFP map\n\n上图中最左边的 map input 是最开始的 transactions,\n经过排序和过滤掉非频繁项后能够得到各个 item 对应的 conditional\ntransaction，作为 map output 输入到 reduce 过程中，reduce\n过程如下图所示，reduce 能够将相同的 key (item) 的 conditional transactions\n聚合在一起，进而构建出这个 item 对应的 conditional FP-tree。\n\n\nPFP reduce\n\n得到这个 item 的 conditional FP-tree 后，可对这棵 conditional FP-tree\n进行相同的 MapReduce 操作，因此，原始的 FP-growth\n算法递归一次在这里就是一个 MapReduce Job。\n上面的实现方法实际上就是 PFP(Parallel\nFP-growth) 算法，目前在 spark 框架中有相应的实现，可以直接调用。\n最后需要注意的一点是，PFP\n虽然能够挖掘出非频繁项集，也就是长尾数据的尾巴部分，但是却无法判断出这些非频繁项集中那些项集是有效的，那些是噪声。\n综上，本文主要介绍了频繁项挖掘中的 FP-growth 算法以及如何通过\nMapReduce 实现这个算法，通过 MapReduce 实现的版本能够支持更小的\nmin-support，因此能够挖掘出非频繁项集，也就是长尾数据中的尾巴部分，但是这个方法不能判断出那些非频繁项集是有效的。\n\n参考\n分布式机器学习系列讲座\n- 01 Infrequent Pattern Mining using MapReduce FP\nTree 算法原理总结\n","categories":["机器学习"],"tags":["机器学习","分布式"]},{"title":"分布式机器学习 (3)-Application Driven","url":"/2018/02/18/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(3)-Application%20Driven/","content":"本文主要介绍了互联网几项重要业务（在线广告，推荐系统，搜索引擎）背后所需的一项共同技术：语义理解 (semantic\nunderstanding)，同时介绍了实现语义理解的若干种方法：包括矩阵分解，主题模型 (Topic\nModels) 等。原视频见这里，需要自备梯子。\n\nsemantic understanding\n支持的业务\n在线广告\n作者以其在腾讯参与开发的 Peacock\n系统为例介绍了在线广告所需的语义理解技术，如下是 peacock 系统对\n“红酒木瓜汤” 这个查询关键词返回的语义理解的结果\n\n\nonline advertising\n\n最下面的 topics 是从 query 中推断出来的主题，每行为一个 topic，weight\n是 query 属于这个 topic 的概率，topic_words 则是这个 topic\n中的词语，且按照与 query 的相关度从高到低进行了排序。\n如果将广告主的广告描述生成同样的 topics 分布，则可以将 query\n和广告对应起来。Peacock 系统是基于 LDA(Latent\nDirichlet allocation) 开发的，发表的具体论文是 Peacock: Learning\nLong-Tail Topic Features for Industrial Applications.\n除了用户的 query，同样可得出用户浏览的内容的 topic\n分布，进而根据用户的浏览内容为其推荐关联的广告。\n推荐系统\n推荐系统中最耳熟能详的算法是协同过滤，通过 users-items 矩阵中的 user\n向量衡量 users 之间的相似性，通过 item 向量衡量 items\n之间的相似性，具体可参考这篇文章。但是遇到下面这种情况时，协同过滤是无能为力的\n\n\nrecommendation system\n\n上图所示的状况是不同 user 向量之间没有共同出现的 item，这样无法计算\nuser 向量间的近似性；而实际中的数据往往是非常稀疏的，不同用户间相交的\nitems 很少，因此协同过滤在推荐系统中几乎不再被使用。\n那么，如何将上面提到的 semantic understanding\n应用到推荐系统中呢？如下图所示，假如将原来的 users-items\n矩阵划分为两个大的 topics，则每个 user 和每个 items 都可以被映射成一个\ntopic 向量，这样通过比较 topic 向量即可比较两个 user 或 item\n的相似性\n\n\nrecommendation system 2\n\n在实际中用得更多的是矩阵分解这一类方法，即无须事先对 users-items\n矩阵进行划分，而是通过矩阵分解的方式得到每个 user 和每个 item 的 topic\n向量。常见的 SVD、NMF 等都是这一类方法。\n搜索引擎\n搜索引擎跟在线广告有点类似，都会根据用户的 query\n返回关联的内容，下图中的 text\n可以理解为用户的搜索关键词和页面的关键词\n\n\nsearch engine 1\n\n图中有几个 topic，最简单的是两个，如下图所示\n\n\nsearch engine 2\n\n但是也可以认为有四个，如下图所示\n\n\nsearch engine 2\n\n所以这里有个重要问题，就是应该学习什么样粒度的 topic\n？作者的观点是学习更小粒度的语义的好处更多。比如说按照第一种大粒度的语义分法时，第一行和第三行是有关联的；而按照第二种小粒度的语义分法时，第一行和第三行直观上没有联系，但可通过第二行联系起来。即更小粒度的语义能够完成大粒度的语义所完成的事情，但是大粒度的语义未必能完成小粒度语义所完成的事情，如从搜索的精确性考虑，肯定是小粒度的语义比大粒度的语义搜索要来得精准。\n实现 semantic understanding\n的方法\n实现 semantic understanding 的方法可以分为 unsupervised 和 supervised\n两大类，supervised 其实就是分类（即将上面的 user 或 item 分类），但是\nsupervised\n不仅需要人工标注的数据，而且还难以确定准确的类别数；因此实际中往往是采用\nunsupervised 的方法，如下所示是一些 unsupervised 方法\n\n\nunsupervised method\n\nFrequent itemset mining 在上一讲中已经讲过，实际中，Frequent\nitemset mining 和 collaborative filtering 使用得并不多\nLSA(Latent\nsemantic analysis)\n实际上是对文本矩阵 (每行是一篇文本，每列是一个单词) 进行 SVD\n分解，且得到的 U 矩阵 和 V 矩阵中分别含有每篇文本或每个单词的 topic\n向量。其他矩阵分解类的方法也都是遵循着这个思路。\nNMF(Non-negative\nmatrix factorization) 是受限的\nSVD，原因是这种方法分解后的矩阵中的元素必须要为正数。\npLSA(Probabilistic\nlatent semantic analysis)\n在 LSA 的基础上加入了概率，使得结果有可解释性\nLDA(Latent\nDirichlet allocation) pLSA\n只能解释输入的数据，对于新来的数据无能为力，因此无法做到实时；因此有了\nLDA 的出现，LDA 能够推断出新来的数据的 topic 分布，而 smoothed pLSA\n指的是 LDA 在 pLSA 基础上加入了先验概率，具体为狄利赫里先验分布。\nHDP(Hierarchical\nDirichlet process) 与 LDA 效果类似，好处就是训练前不需要指定的\ntopics 的数量\n作者认为，上面虽然列出了很多模型，但是不少模型之间是等价的，如 NFM 和\npLSA 的等价性可参考这篇文章，pLSA\n和 LDA 之间的等价性可参考这篇文章；\n最后 semantic understanding\n应用在上面所述三个互联网业务 (在线广告，推荐系统，搜索引擎) 时，基本的步骤是一致的，均为\n\nRelevance: information retrieval\nRanking: click-through rate prediction\n\n第一步就是借助 semantic understanding 找到与 query\n相关的结果，第二步要根据点击率等于具体业务相关的指标对这些相关结果排序。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"写给大四","url":"/2015/11/18/%E5%86%99%E7%BB%99%E5%A4%A7%E5%9B%9B/","content":"\n文章为转载，作者不详\n\n无论你是找工作了，还是保研了，我们或多或少对未来的不确定性感到彷徨和焦虑，看完希望对你有用。\n昨天有个同学来找我，谈毕业选择的事情。谈完以后她突然过来拥抱了我一下。为此我深受鼓舞，决定给你们写一点东西。你们正处在一个艰难的阶段。我想以下面这段文字，争取给你们提供点正能量。虽然写了很长，但不会谈如何申学校、找什么工作、要不要保研这种具体的问题。我想谈的是，如何排解你心中的恐慌。\n\n焦虑的本质\n大四是一个焦虑的时期。你们的烦恼有时候是具体问题带来的压力，有时候却是无端的、莫名其妙的，有时候还极易受到外界的影响，别人一句话就会激起内心难以遏制的波澜。所有这些焦虑、恐慌、敏感，在我看来，其实归根结底是源于三件事：\n\n人生的有限性\n\n未来的不确定性\n\n过去的不可更改性。\n\n这三件事是人生不可回避的三道阴影。每当想到它们我就想写诗。\n\n“我要如何准备才能胜出？”“我会失败吗？” 这是未来的不确定性在你的内心敲打。\n\n“我走这条路是不是多浪费一两年的时间？”“我会不会走弯路？” 这是人生的有限性在摆弄你。\n\n“我的选择是不是错了？”“这么大的投入最后回报得来吗？”，你们患得患失，因为没有后悔药可以吃。\n\n怎么样？其实每天让你感到痛苦的，不是那些具体的事，而是上面这些萦绕不去的疑问。我开宗明义地拔高到人生的高度来剖析，不只是因为我一以贯之的文艺与深沉，更是因为我觉得下面这个前提非常重要：如果你感到的恐惧，是人生某些不可改变的特性带来的，那么你拼命地想抓住什么、甚至你终于完成了一个目标、得到了一块暂时的喘息之地，你仍然解决不了你内心的痛苦。\n这就是为什么你们没拿到 Offer 的时候很煎熬，拿到了 offer 的又患得患失；等到 6 月份还没出路的痛苦无比，早在 9 月份就保研的同样不开心。痛苦最多可以缓解，永远得不到解决。这次痛苦比较小的人，下次痛苦没准会很大。\n由于人生的规律没法改变，所以要想解决内心的痛苦，外面如何挣扎和抓取都没用，最终只能靠内心的成长。\n怎么破？\n如果现在要打的是人生的终极大 boss—— 所谓人最大的敌人就是自己。那么我这个水平，当然不知道怎么破。不过我已经感觉到了人内心成长的历程，可以跟你们分享。\n首先你们要重视自己的理性。人有一个不断强大的头脑，和一颗充满恐惧、贪欲和脆弱的玻璃心。人的大多数行为仿佛更倾向于受到情绪的驱动，而不是头脑的指示。你的头脑作为弱势群体，要学会和内心对话，了解你内心深藏的力量和驱动，并争取引导它，逐渐达到心智的合一。\n这是一个漫长的成长过程，一些人最终变得更有智慧，大气、从容、坚定，能够更好地把握自己的人生。但绝大多数人终此一生也无法主宰自己，像浮萍一样地随风浪摇摆。这其中可能包括你的父母。\n但我常常感觉到，情绪的力量非常强大，理性的作用一开始往往有限。你可以很容易明白很好的道理，但是你控制不了自己，尤其在受到外界干扰的时候。你内心成长的心智就像一颗小树苗，在它尚还柔弱的时候，恐怕经不起太严酷的风吹雨打。外部的环境太恶劣了，再强大的内心都有可能被压倒。\n所以，我的经验是：内外双修。\n跟自己的内心对话\n深感艰难的阶段，我们要多跟自己讲讲道理。听别人讲也行，自己跟自己讲也行。其实被情绪所操纵的心灵很傻的，说的多了，它就会慢慢相信。这就是为什么要多跟女人说我爱你的原因。你们可以经常给自己说说这些道理：\n相信命运\n我这么说不是要你们去算命。而是说作为受过大学教育的人，你们要明白事物的不可操控性。我们高中学习的知识和成长的经历，容易给我们塑造一种观念，就是万事是因果相连的。有了条件，就可以推算出答案。这个世界由根本的、简明的自然规律和公理推动运转，一切都可以预测和解释。好好学习，一般来说就可以考上好大学。能不能达到预设的结果，全在于能不能满足一定的条件，比如说有没有努力。\n可惜由于我们的大学教育整体比较失败，所以前面形成的观念基本得不到修正和挑战。如果你大学继续学习数理化，你就知道你高中的那些定理和公理都是 17 世纪牛顿时代的东西。20 世纪出现的相对论、量子理论、混沌理论，说明连自然界都不是那么确定无疑的。微观来看，因果关系固然牢靠，但是一旦到了宏观，变量可能太过复杂，复杂到超越人在有限时空里可以理解或掌握的程度。\n自然是这样，人生也是这样。有的事情变量更简单，可把握性更强；有的事情变量更复杂，可把握性更小。高考就算可把握性比较强的事情，虽然也有个别点背的。但是学习这种事情，绝对是社会现象中的特例。你一生里遇到的其他事，其可把握性都远远低于在学校里的生活。\n现在你们即将踏入社会，社会给你们上的第一课，就是全然不同于学校学习的 “低把握性逻辑”。你想要找一条好的出路，只能尽量为之准备，但结果如何，非常地 “混沌”。我给你们讲过很多我身边的故事。我大学同班，学习最好没有留在北京，最像共产党员的去了外企，最不上课的当了公务员，最北京味儿的出了国，最学术的之一当了导演，最不像搞学术的一个如今在大学当老师，就是我。\n我不是想告诉你们世事无常，躺下来等着天上掉铁饼就好了。我有一个同学进了外交部，她一直想进外交部；还有一个同学也在当大学老师，他当年确实是一个学霸。但这两个同学当时并不比别的同学更努力，他们现在也并不比别的同学更成功。\n你们必须学会理解和接受人生的偶然性。未知当然让人恐惧，但这是你心灵的自然反应。你的头脑可以相信：每个人都有一条路。这条路有很多错综复杂的因素促成，但一定由一个人在当时当地能够做的最好的选择组成。所以你提前恐慌或者不恐慌，并不能改变什么。走上去以后，就好好地走，才能精彩。\n没有弯路\n踏上社会之前的恐慌，很大程度上来自于 “输在起跑线” 上的幻想。真正的人生不是一场竞赛；至少也不是一场跑向同一终点的竞赛。这也是高考养成的逻辑。从进大学开始就已经不管用。\n我也给你们讲过很多故事。习近平青年时候被下放到农村种了七年地，22 岁才回城读大学。胡锦涛大学毕业以后到西北修水坝去了。奥巴马和克林顿都是穷小子，好不容易考上了个东部的常青藤，结果一个回老家干社区工作，一个回老家在名不见经传的大学教书。按照前面的逻辑，他们都输在了起跑线上。\n如果你们真要有志向当个人生的赢家，可以多看看那些有成就的人。大多数伟人的人生都充满了波折，有的还几起几落。可是从后往前看，哪里又有什么弯路？你当过村支书，别人没有当过村支书，几十年后，竞争国家主席你就有优势。\n你可能不想当伟人，只不过不想吃苦。但前面已经说过，你们的人生总有一条路，有时候会是弯的，有时候是直的。不要只想走直路。直路开的太快，便无心欣赏风景，难有宽广的眼界和丰富的心灵，于是你缺少快乐的潜质。总之所有的努力都不会白费。在哪儿努力都行。\n我自己定\n大四这种时候，你们每天最好给自己打打气：凡事我做主。\n我不知道你们会有一个什么样的爸妈，总之他们很有可能会很烦。有的平时很开明，到了这种时候，也偶尔冒出一两句不懂事的话，让你平添无意义的难受。七大姑八大姨都会打电话。放个假回家就会不停地问。哥们闺蜜死党什么的纷纷提建议。同班同学这呀那的，你努力从每一个传闻中获得给自己的启示。\n最后，你接收了很多信息，询问了很多建议，却迷失了自己。其实你的事情只有你最懂，别人都不行。从今天开始，你要立足于自己。你希望别人帮你，其实是害怕对可能的失败负全责。可是，你的前途，谁能替你负责？真正的自信，就从自负其责开始；真正的自由，也是从自负其责开始。\n改变外围环境\n除了常常与自己的内心对话，你们还要争取为这种对话营造一个好的外在环境，靠近阳光雨露，远离风吹雨打。\n### 不要来烦我\n我好像试图挑拨你们与父母的关系。其实我的意思是：到了这个年龄，你们必须开始重塑你们与父母之间的关系。在这个特殊的阶段，最关心你们前途的是你们的父母；最影响你们心情往往也是他们。你们需要父母的关心。但如果他们跟你一样被情绪所驱动，缺乏坚定与理性，时常而来的关心不过是一种情绪发作后的排解。那你就要明确告诉他们：请克制自己的恐慌，多给我传递正能量。\n父母的建议可能有用。但多半用处不大。如果生活在农业时代，你基本上只需要按照父母说的做就行了，因为你们两代人的人生没有什么大的区别。可是现如今，你在外地的父母怎么知道北京的事？又怎么能知道美国的事？如果要沟通，让父母多给你分享一些人生感悟，而不是具体建议。\n同样的的道理，远离负能量的种种议论，除非你们能相互鼓励。远离打扰你内心平静的环境，为内心的对话保留空间。如果可能的话，尽量地给周围的人一些正能量。如果你确定你知道什么是正能量的话。\n不要拖沓\n这个时候你们都在为了各种事做准备。不管在追求什么，制定好计划，有规律地生活，该做的事不要拖沓。当你感到愈发恐慌的时候，往往是有该做的事却没做的时候。那件事在你的心底大声地发出嘲讽，让你越发地焦虑和没有自信。如果你所努力的事情按部就班、取得进展，你的情绪化的心灵会驯服很多，讲的道理它才会耐心去听。\n完成小心愿\n一个年轻的心灵，最需要灌溉的泉水，就是自信。做一些能够让你提升自信的事情。这些事情可能都很小，但是短期能看到效果，取得进步，或者得到结果。你们如果排除无谓的焦虑，其实有相当的时间可以做这些事情。\n\n出去短途旅游。独自计划、独自出行、独自完成，回来后你会觉得自己成长了。\n\n读一些书，讲给没读过的人听。那人最好不是大四。\n\n练习一项体育运动，看到自己的进步。\n\n做任何你想做、靠相对简单的努力能够做成的事情。\n\n希望有用\n我花了两天的时间来构思，又用一整天的时间把这些话写出来，中间鼓起了很多次的勇气，要自己坚持下去。一方面我很不愿意说教，虽然我经常说教。我对你们说教，实际上我自己和内心对话的一个形式，所以无比真诚。你们叫我人生导师，可我的人生也亟需引导。目前只能进行\n人生有的阶段不可逾越，有的痛苦必须体验。身处其中的人很难解脱。评论的人常常是站着说话不腰疼。比如现在如果有个老同志对我说：“你年轻人不要为买不起房、排不上车号、每个月存不下钱而焦虑。这些都是虚妄的。” 我一方面觉得他说的有道理，另一方面心底有一个强大的声音在呼喊：“把你的给我嘛”。\n　　许多道理，只有走过了，回转来才能真正体会。上学期我在毕业典礼上做的发言，在社会上有不少媒体刊登和转载，光各种稿费单子就收了一堆。可是在学校里的同学们，感受就隔了一层。他们找到了一个黑我的新办法。一度我只要问 “为什么”，他们就会说 “因为我们来自山顶”。\n所以，接下来我再问为什么，你们可以说：“请先跟你的内心对话。”\n我现在内心里的想法，就是希望你们接下来这一年不被虚度。这是你们人生中最美好时代中的一年，它不是拿来过渡、等待或者牺牲的。现在这一年才刚刚开始。\n大四加油！\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"分布式系统笔记 (1)-MapReduce","url":"/2019/01/14/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(1)-MapReduce/","content":"一直都想系统性地学习一下分布式系统的一些理论，所以打算开个坑学习一下\nMIT 的课程 6.824:\nDistributed Systems 。本文主要是 LEC 1\n中的内容，简单介绍了分布式系统的几个核心问题，以及经典的分布式计算框架 - MapReduce,\n虽然这是耳熟能详的一个框架（或者说是编程范式）了，但是其设计思想至今还是非常值得参考的。\n\n分布式系统概述\n简单来说，分布式的目的就是通过多台机器进行协作来提供一台机器所无法提供的运算能力和存储能力。除了通过增加机器来拓展运算能力和存储能力的伸缩性，分布式会额外带来机器隔离后的安全性、多份数据副本的错误容忍性等。\n性能 (performance)，一致性 (consistency), 容错 (fault tolerance)\n是分布式系统中比较关注的问题。\n如何让总体的运算能力随着机器数量的增长而线性增长？这是 performance\n所关心的，各台机器的负载差别大吗 (load balance)?\n网络能承受得住随着机器数量增加而增加的通信吗 (共享资源的瓶颈)? 所有的代码都能够被并行化吗？\n如何让多台机器上的同一份数据副本被多个进程读写后仍然保持一致？这是\nconsistency\n所关心的，这个过程需要考虑的问题就太多了，因此也产生了很多一致性协议 (Paxos,\nRaft 等) 专门处理这个问题。比如说当 client 或 server\n在写数据的不同阶段时宕机该怎么办？网络的抖动导致了\nserver 的假死 (脑裂) 怎么办？性能 (performance) 与一致性 (consistency) 总是相悖的，也就是说强一致性往往会导致比较慢的系统，高性能的系统通常会以牺牲强一致性为代价，需要根据具体场景进行\ntrade-off。\n如何让不可避免的宕机不影响总体的服务？这是 fault tolerance\n所关心的，在一台机器宕掉后，其执行过的 tasks\n该怎么办？其他与这台机器发生过的 communication\n的机器的依赖性该怎么解决？\n可以说任意的分布式系统都会涉及到这三个问题，只是会各有侧重；目前常见的分布式系统从功能上可分为分布式计算框架 (MPI,\nMapReduce, Spark 等)、分布式文件系统 (GFS, HDFS 等)、分布式调度系统 (Mesos,\nYarn 等)。\nMapReduce\n很久之前写过一篇关于 MapReduce 使用的文章：分布式机器学习 (4)-Implement\nYour MapReduce，这里则主要是根据 MapReduce 论文着重讲述 MapReduce\n的一些原理。\n下图是从 MapReduce 的论文 MapReduce:\nSimplified Data Processing on Large Clusters 中摘取的\n\n\nExcutation\n\n通常 MapReduce 被人所了解的是上图中 (3)(4)(5)(6)\n的过程，也就是输入文件被分成 M 个小文件，每个小文件分别在 Map phase 被\nMap 函数处理后输出一系列的（key,valu) 对，然后对 key\n进行哈希取模找到除了这个 key 的 reducer worker，reduce worker 在 Reduce\nphase 会对通过 reduce 函数处理相同的 key，两个函数的输入输出如下所示\nMap: (k1,v1) → list(k2,v2)Reduce: (k2,list(v2)) → list(v2)\n这个过程理解起来很简单，框架使用起来也不麻烦，这是因为 MapReduce\n框架隐藏了上图中通过 master 追踪各个 task 是否顺利完成、以及如何进行\nfault tolerance，而这也是论文最值得关注的点之一。\nExecution Overview\n上面讲的 MapReduce\n过程可以说是一个编程范式，下面主要根据原始论文讲述整体的执行流程，各个步骤的编号跟上图保持一致\n\nUser Program 中的 MapReduce 库会将输入分成 M 个小文件（通常是 16MB\n到 64MB），然后 fork 出多个子进程\n子进程中有一个作为 master，其他作为 worker。假设有 M 个 map task 和\nR 个 reduce task（M 和 R 都远大于机器数量），master\n会分配挑选处于 idle 的 worker 分配 task\n 被分配了 map task 的 worker 读入对应的输入文件，从输入文件中解析出\nkey/value pairs，并将每个 pair 输入用户自定义的 Map 函数，Map 函数输出的\nintermediate key/value pairs 被缓存在内存中\n缓存在内存中的 intermediate key/value pairs 会被周期性地写入\nmap worker 本地的磁盘，根据 key 的不同分别写入到 R\n个本地文件中，然后这些文件在本地的路径会传输给 master，从而\nmaster 可以告知 reducer worker 到哪里读取数据\n直到所有的 Map 过程完成，Reduce 过程才能开始；当\nreduce worker 被 master 告知要读取的文件的位置时，会通过 RPC 从 map\nworker 的磁盘读取这些数据；reduce worker 读取完所有的 intermediate\nkey/value pairs 后会针对 key 进行排序 **，从而让所有相同的 key\n汇聚在一块\n reduce worker 遍历排序后的 pairs，并将每个独立的 key 及其对应的\nvalue 集合传输给 Reduce 函数，Reduce\n函数则会将输出添加至最终的文件中。\n\n在完成一个 MapReduce 计算过程后会产生 R\n个文件，但是一般不需要对这 R\n个文件进行合并，因为这些文件可能会被作为下一个 MapReduce\n计算的输入，或者被另外的分布式引用处理，而分布式应用往往能够处理这样被分隔后的小文件。另外，这里的\nMapReduce 配合了 GFS 的使用，所以从磁盘读写直接使用的是 GFS\n文件系统。\nMaster 与 Fault Tolerance\nmaster 会记录每个 map task 和 reduce task 的状态 (idle, in-progress\n或者 completed), 对于那些不是 idle 的 task，master 还会记录执行这个 task\n的机器。\n从上面描述的 MapReduce 流程可知，master 是 map worker 和 reduce\nworker 的桥梁，每个 map task 完成后都会将其产生的 M\n个文件的路径和大小传输给 master，master 则会将这些信息 push 给那些有处于\nin-progress task 的 reduce worker。\nFault Tolerance 可分为两大类：worker 的 Fault Tolerance 和 master 的\nFault Tolerance；而 worker 又可分为 map worker 和 reduce worker\n两种，下面分别介绍针对这三种角色的 Fault Tolerance 的策略。\n首先，master 会周期性地 ping 各个 worker，并根据是否收到回复来判断\nworker 是否发生了宕机。\n当一个 map worker 被 master 判为宕机后，这个 worker 所有的\ntask (包括 in-progress 和 completed 的) 都会被重置为 idle\n状态，从而让这些 task 能够被重新分配给其他的 map worker\n来重新执行。而发生这种重新执行的情况时，所有的 reduce worker\n都会被告知重新执行的这个 task 的是哪个 map worker，从而让 reduce worker\n从新的 map worker 那里读取数据（针对那些还没从已经宕机的 map worker\n读取数据的 reduce worker，原来那些已经读了数据的 reduce worker\n不需要）\n当一个 reduce worker 被 master 判为宕机后，这个 worker\n那些未完成的 task (也就是处于 in-progress 状态的) 会被置为 idle\n状态，而已完成的 task 不会。这是因为 reduce worker 将输出写入\nGFS，worker 宕机后数据仍然可以被读取，而 map worker\n则是将输出结果写入到本地的磁盘，宕机后数据无法被读取\n这里有两个问题值得思考：\n\nmap worker 宕机后，reduce worker 从新的 map worker\n读取的结果与从原来宕机的 map worker 读取的结果是否一致？\n如果 reduce worker\n在数据写入一半的时候宕机了，已经写入的数据怎么办？\n\n第一个问题的答案是肯定的，因为 map\n函数不记录状态，也就是对固定的输入有固定的输出，此外，reduce\ntask 会在所有的 map task 完成后才开始执行，因此也保证了 reduce\nworker 总能读到 map worker 完整的输出文件。\n第二个问题与 GFS 提供的 atomic rename\n特性有关，reduce worker\n会先将结果写入到临时文件中，直到所有的结果都完成后才将临时文件重命名为最终文件并写入\nGFS 中；这个特性也让多个 reduce worker\n重复执行一项任务时最终只产生一个文件。\n针对 master 宕机的情况，论文的做法是令 master\n周期性地往磁盘写入 checkpoint，重启 master 后从上次的\ncheckpoint 重新执行。\nLoad Balance，Backup Task\n与 Locality\nload balance 指的是某个 worker 执行 task 时间过长，导致其他已完成\ntask 的 worker 都在等待这个 worker\n完成（因为任务之间有依赖性），这一执行时间过长的 worker 也被称为\n文章针对这一问题的做法是把每个 task 分得尽量小，即 M (map task\n的数量) 和 R (reduce task 的数量)\n的值要远远大于机器数量。这样就不会出现某个 task\n执行过的时间过长，不仅解决了上面的问题，还加速了 fault tolerance 后的\nrecovery。\n除了 task 过大，出现 straggler 的原因也可能是机器本身的硬件问题，哪怕\ntask 已经分得很小了。文章解决这一问题的做法是当 straggler\n出现的时候，master 会把 straggler 在做的 task 分给另外一个 worker\n做，谁先做完就汇报给 master，而 master\n也只会接收其中一个的完成的消息，这在文中称为 Backup Task。\n在任意分布式系统中，当 worker\n数量增多，网络通信的负载都会变大。文章利用了 MapReduce 和 GFS\n架设在同一组机器上的特性，从而让 Map 过程从 GFS\n读取文件时尽可能读取处于本地磁盘的的 copy（GFS\n为每份数据创建了三份 copies），在本地磁盘找不到时才读取其他 worker\n磁盘的数据，这样就大大减少了网络的开销，而文中又称这一特性为\nLocality。\n小结\n回到文章开头提到的分布式系统中三个比较关注的问题，MapReduce\n这篇论文中主要是关注其中的 performance 和 fault tolerance。\n为了提升 performance，通过把 task 分得更小来获得更好的 load\nbalance，通过 backup task 来降低 straggler 对整个系统的影响，通过\nlocality 来减少网络的负载。\n针对 fault tolerance，则通过为 task 设定状态，失败的 worker 的 task\n被重置为 idle 状态，从而找到新的 worker 重新执行这些 task。\n每种框架都有其适用场景，而对于\nMapReduce，首先就是任务要能够表达成一个或多个 MapReduce\n过程，文中提到的一些任务包括: Distributed Grep、Distributed\nSort、Count of URL Access Frequency、ReverseWeb-Link Graph、Inverted\nIndex 等；其次数据量要足够大才能显示出 MapReduce\n的效率 (其实对于任意分布式系统基本都是这样，\n否则整个系统调度的开销比计算的开销还要大)；还有就是涉及到多次\nshuffle (也即是多个 MapReduce\n过程) 时，由于要与磁盘多次交互，因此虽然能够实现，但是效率很低，这时候就要考虑其他更灵活的框架了。不要局限于一定要把算法表达成\nMapReduce 过程，而是可以考虑更加灵活的框架，如 spark 等。\n","categories":["分布式"],"tags":["分布式"]},{"title":"分布式系统笔记 (2)-RPC and threads","url":"/2019/01/16/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(2)-RPC%20and%20threads/","content":"本系列文章是学习课程 6.824: Distributed Systems\n时的一些学习笔记，整个课程的相关材料已整理至 DistributedSystemInGo。本文是\nLEC2 的内容，主要介绍了 RPC 的概念并通过 RPC 实现了一个简单的 c/s 架构的\nkv 数据库；同时介绍了多线程编程并通过两种方式实现了一个多线程爬虫。\n\n这门课程采用的语言是 go，原因是 go 对 concurrency、RPC 和 gc\n等有较好的支持，且上手较快，可以把问题集中在分布式系统而不是由于对语言不熟悉而带来的\nbug。因此，上面提到的两个 demo 也是采用 go 实现。\nRPC\nRPC 基本概念\nRPC (Remote Procedure Call)\n的概念很好理解，类比函数调用，只是两个函数不在一个内存空间，不能直接调用，需要通过网络进行远程调用。RPC\n的调用过程如下，图片摘自 Remote\nProcedure Calls\n\n\nRPC\n\n需要明确的一点是 RPC\n只是一个概念，因此广义上任意实现远程调用的方法都可称为 RPC（如\nhttp），而区别于各个 RPC 的实现 (RPC\n框架) 在于其实现的协议的不同，而最基本的协议包含编码协议和传输协议。\n编码协议表明了该如何将要传递的参数等信息打包好；常见的有基于文本编码的\nxml、 json，也有二进制编码的 protobuf、binpack，也可自定义协议。\n而传输协议则表明如何将打包好的数据传输到远端；如著名的 gRPC 使用的 http2 协议，也有如 dubbo\n一类的自定义报文的 tcp 协议 (精简了传输内容) 等。\n类似于计算机网络中的各种协议一样，这些协议是比较繁琐的且通用的，因此产生了很多\nRPC\n框架来完成这些协议层面的东西，而除了上面提到的最基本的编码协议和传输协议，成熟的\nrpc 框架还会实现额外的策略，\n如服务注册发现、错误重试、服务升级的灰度策略，服务调用的负载均衡等。上面提到的\ngRPC 和 dubbo 就是两个比较有名的 RPC 框架，通过 RPC\n框架，在编码时能够像本地调用一样使用 RPC。\n对于一个 RPC 框架，实现中最关注以下三点 (1) Call\nID 映射：即告诉远程服务器要调用的是哪个函数或应用 (2)\n序列化和反序列化：即上面的编码协议 (3)\n网络传输：即上面的传输协议\n更详细的解析可参考这个回答，谁能用通俗的语言解释一下什么是\nRPC 框架？ - 洪春涛的回答，\nRPC in go\ngo 提供了自己的 rpc\n库，这里通过这个库来实现一个简单的 c/s 架构的 kv 数据库\nserver 端的核心代码如下，KV 这个 struct 提供了 Put 和 Get\n这两个存取方法，且存取时通过 sysc.Mutex\n进行加锁来保证一致性。通过 go 内置的 rpc 框架启动一个 server 且将 KV\n注册在 1234 端口，网络传输采用的是 tcp 协议；每当 server 与一个 client\n建立一个连接时，server 会启动一个线程 (goroutine)\n去处理这个连接对应的请求\ntype KV struct {\tmu       sync.Mutex\tkeyvalue map[string]string}func (kv *KV) Get(args *GetArgs, reply *GetReply) error {\tkv.mu.Lock()\tdefer kv.mu.Unlock()\treply.Err = \"OK\"\tval, ok := kv.keyvalue[args.Key]\tif ok {\t\treply.Err = OK\t\treply.Value = val\t} else {\t\treply.Err = ErrNoKey\t\treply.Value = \"\"\t}\treturn nil}func (kv *KV) Put(args *PutArgs, reply *PutReply) error {\tkv.mu.Lock()\tdefer kv.mu.Unlock()\tkv.keyvalue[args.Key] = args.Value\treply.Err = OK\treturn nil}func server() {\tkv := new(KV)\tkv.keyvalue = map[string]string{}\trpcs := rpc.NewServer()\trpcs.Register(kv)\tl, e := net.Listen(\"tcp\", \"ServerIP:1234\")\tif e != nil {\t\tlog.Fatal(\"listen error:\", e)\t}\tgo func() {\t\tfor {\t\t\tconn, err := l.Accept()\t\t\tif err == nil {\t\t\t\tgo rpcs.ServeConn(conn)\t\t\t} else {\t\t\t\tbreak\t\t\t}\t\t}\t\tl.Close()\t\tfmt.Printf(\"Server done\\n\")\t}()}\nclient 端的程序如下，client 首先通过 Dial 函数 与 server\n建立连接，Get 和 Put 则分别调用了 server 端对应的 Get 函数和 Put\n函数（在 rpc.Call 中声明), 可以看到，进行 RPC 就如同本地调用一样\nfunc Dial() *rpc.Client {\tclient, err := rpc.Dial(\"tcp\", \"ServerIP:1234\")\tif err != nil {\t\tlog.Fatal(\"dialing:\", err)\t}\treturn client}func Get(key string) string {\tclient := Dial()\targs := &amp;GetArgs{\"subject\"}\treply := GetReply{\"\", \"\"}\terr := client.Call(\"KV.Get\", args, &amp;reply)\tif err != nil {\t\tlog.Fatal(\"error:\", err)\t}\tclient.Close()\treturn reply.Value}func Put(key string, val string) {\tclient := Dial()\targs := &amp;PutArgs{\"subject\", \"6.824\"}\treply := PutReply{\"\"}\terr := client.Call(\"KV.Put\", args, &amp;reply)\tif err != nil {log.Fatal(\"error:\", err)\t}\tclient.Close()}\n完整的代码可参考这里\nfailure in RPC\n上面只是一个简单的 RPC\n例子，没有考虑到这个过程中可能出现的异常情况。而最常见的异常个情况就是\nclient 发出 request 后收到不 server 的\nresponse，引起这种问题的原因有很多：网络断了、server\n宕机了等。针对这个问题有什么解决方法呢？\n最直观也是最简单的方法是让 client 等待一段时间收不到回复后的重新发送\nrequest，且设置重复发送的次数的上限，如果超过这个上限，则先调用的应用程序返回\nerror 异常信息。\n但是，当 client 重复发送请求时，server 有可能已经收到了 client\n的前一个请求，只是网络的延迟使得 client 还没收到 response，那这时候\nserver 就会收到重复的 request。如果 client\n发出的是读请求，那么问题不大；但是如果是写操作，server\n端就需要处理这些重复的写请求从而使得最终只有一个被执行。\n这里可针对 server 端采取 at most once\n的策略，即同一个 request 最多只能在 server\n端被执行一次，如果收到了重复的\nrequest，那么就将之前的结果返回。这样需要解决的问题就是为每个 request\n生成一个 unique id，生成 unique id 也有很多方法，比如说可以利用 client\n的 ID 及其 request 编号的组合等方法。\ngo 的 RPC 库采用的就是 at most once\n的策略，库已经在传输层进行了过滤重复 request\n的操作，因此在代码中无需体现这一操作。\nthreads\n多线程是 concurrency 的重要手段，在 golang 中的 thread 也被称为\ngoroutine，一般多线程都能够充分利用 CPU 的多个核 (Python 的 Cpython\n解析器除外，GIL 的限制)\n进行多线程编程时有以下几点值得注意：\n（1）同一个进程内的线程是共享地址空间的，因此在对共享数据进行写操作时需要加锁\n（2）当任务间有依赖性，一项任务拆分给多个线程去完成时，线程间往往需要先共完成任务\nA 才能开始任务 B\n（3）要确定线程并行的粒度，比如说多线程爬虫，每个线程是负责一个站点？还是站点下的一个目录？一般粗粒度的实现会很简单，但是并行性不高；而细粒度的并行化程度会更高，但是会更容易出现死锁等问题。\n下面会通过 golang\n实现一个多线程爬虫，分别采用了两种方式，第一种是通过经典的队列方法（channel)，这种防范没有加锁；第二种则通过加锁 (mutex) 和设定任务完成的\nthreshold (waitgroup)；在这个例子中两种方式的并行化粒度均是网页\nchannel\n队列是很常见的多线程编程采用的方式，将需要执行的任务送入队列，然后线程从队列中取出任务执行，并将新的任务入列 (在这个例子中就是当前网页所含有的其他网页的链接)，这里还需要额外检查网页是否已经被抓取过原因有两个\n（1）网页间的指向有可能形成闭环，不判断会导致死循环\n（2）效率问题，不希望执行重复工作\n因此，golang 通过 channel 完成的多线程爬虫如下所示，master\n从队列头读出一个网页并判断其是否已经被执行，如果没有执行，就启动一个\ngoroutine 来执行 dofetch 任务，dofetch\n会将其当前网页所指向的其他网页入列\nfunc dofetch(url1 string, ch chan []string, fetcher Fetcher) {    // body is content of url1, urls are those to which url1 refer\tbody, urls, err := fetcher.Fetch(url1)\tif err != nil {\t\tfmt.Println(err)\t\tch &lt;- []string{}\t} else {\t\tfmt.Printf(\"found: %s %q\\n\", url1, body)\t\tch &lt;- urls\t}}func master(ch chan []string, fetcher Fetcher) {\tn := 1\tfetched := make(map[string]bool)\tfor urls := range ch {\t\tfor _, u := range urls {\t\t\tif _, ok := fetched[u]; ok == false {\t\t\t\tfetched[u] = true\t\t\t\tn += 1\t\t\t\tgo dofetch(u, ch, fetcher)\t\t\t}\t\t}\t\tn -= 1\t\tif n == 0 {\t\t\tbreak // or close(ch)\t\t}\t}}func CrawlConcurrentChannel(url string, fetcher Fetcher) {\tch := make(chan []string)\tgo func() {\t\tch &lt;- []string{url}\t}()\tmaster(ch, fetcher)}\n那终止的条件是什么呢？队列为空，但是在 golang 中没有显示判断 channel\n为空的方法，且通过 for 遍历 channel 时，只有关闭了 channel\n后循环才能正常退出，否则会出现 deadlock\n的错误，但是显然无法随意关闭 channel，因为每一个 goroutine\n都不知道是否还有其他的 goroutine 要写入这个\nchannel；上面的解决方法是通过 n 来记录当前队列的长度，如果 n == 0 就关闭\nchannel 或退出\nmutex 与 waitgroup\n上面只是在 master 中判断某个 url\n是否已经被访问过了，那么每个独立的 goroutine 能否自行判断某个\nurl 是否别访问过了呢？答案是肯定的，只是需要对存储 url 是否被访问的\nhashmap 进行加锁，在 golang 中可通过 sys.Mutex\n对某个数据进行加锁和解锁操作\n同样，我们需要设定终止条件。该如何衡量所有任务都完成，这里我们可以想像一颗多叉树的结构，每个节点表示一个网页，而每个节点的子节点是其网页中指向的其他网页，那么一个节点被判为完成当且仅当其所有的子节点都完成，这就涉及到了上面提到的任务间的依赖性问题，实际上就是要等待\ngoroutines 共同完成当前节点的所有子节点，这要用到\nsys.Waitgroup 实现的具体代码如下\ntype fetchState struct {\tmu      sync.Mutex\tfetched map[string]bool}func (f *fetchState) CheckAndMark(url string) bool {\tdefer f.mu.Unlock()\tf.mu.Lock()\tif f.fetched[url] {\t\treturn true\t}\tf.fetched[url] = true\treturn false}func mkFetchState() *fetchState {\tf := &amp;fetchState{}\tf.fetched = make(map[string]bool)\treturn f}func CrawlConcurrentMutex(url string, fetcher Fetcher, f *fetchState) {\tif f.CheckAndMark(url) {\t\treturn\t}\tbody, urls, err := fetcher.Fetch(url)\tif err != nil {\t\tfmt.Println(err)\t\treturn\t}\tfmt.Printf(\"found: %s %q\\n\", url, body)\tvar done sync.WaitGroup\tfor _, u := range urls {\t\tdone.Add(1)\t\tgo func(u string) {\t\t\tdefer done.Done()\t\t\tCrawlConcurrentMutex(u, fetcher, f)\t\t}(u) // Without the u argument there is a race\t}\tdone.Wait()\treturn}\n上面可以说是 sync.WaitGroup\n的经典用法，在为每个线程分配任务时通过 done.Add(1)\n增加未完成任务， 在线程完成任务时通过 done.Done()\n表示当前子任务已完成，通过 done.Wait()\n阻塞直到所有的子任务都完成。\n小结\n这一课介绍了 RPC 和多线程编程的基本概念，并分别用 go\n语言实现了一个简单的例子，主要是为后面的几个实验做准备。\nRPC 是个广义的概念，RPC\n需要解决最基本的通信协议和编码协议；除此之外，一些高级的 RPC\n框架还帮我们处理了、服务注册发现、错误重试等细节，让远程调用如同本地调用一样。\n关于多线程编程，给出的爬虫例子实现了两种形式的多线程编程，一种是\nMutex + WaitGroup 的方式，一种则是 Channel\n的方式；需要注意的是，这两种方式不是非此即彼，而是可以混用的，可以参考\nMutexOrChannel\n的介绍，比如说通过 Mutex 来让每个单独的线程判断某个 url\n是否被访问过，通过 Channel 来将未完成的队列入列，通过 WaitGroup\n来分类下载资源（其实这也涉及到了并行的粒度的划分，即并行地下载某一类的资源）。回想起来，很久之前写的一个爬取几个输入法的词库的程序\nThesaurusSpider\n就是这么做的，只是通过 python 实现而已，有兴趣可参考。\n\n参考：\n\n既然有 HTTP\n请求，为什么还要用 RPC 调用？\n谁能用通俗的语言解释一下什么是\nRPC 框架？\n\n","categories":["分布式"],"tags":["go","分布式"]},{"title":"分布式系统笔记 (3)-GFS","url":"/2019/01/20/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(3)-GFS/","content":"本系列文章是学习课程 6.824: Distributed Systems\n时的一些学习笔记，整个课程的相关材料已整理至 DistributedSystemInGo。本文是\nLEC3 的内容，介绍了分布式文件系统 GFS，GFS 为 MapReduce\n提供了存储，同样是出自\nGoogle，同样是年代久远，但是其中的一些设计思想同样值得我们参考。\n\n设计理念\nGFS 的设计理念也可以理解为其适用场景\n\n整个分布式系统是由普通的商用机器构成的，因此故障会比较频繁\n系统存储着数百万级的大文件，每个文件的大小基本都大于\n100MB；小文件也存在，但是不是优化的目标\n文件系统读操作主要有两种：large streaming read 和 small random\nread，且前者占主导；区别在于 large streaming read\n是顺序访问，读取量大；small random read 则刚好相反\n文件系统的写操作主要是对文件进行追加 (append) 操作，追加的数据是\nlarge、 sequencial\n的；且写入后文件就很少会被修改；同样的 small random\nwrite 也支持，但是不是优化的目标\n系统需要支持多个 client 同时写一个文件\n相比低延迟，更看重高吞吐量\n\n系统结构\nGFS 总体的系统结构如下图所示\n\n\nGFS\n\n整个系统包括一个 master 和若干个 chunkservers，master\n存储着文件系统的 metadata，chunkservers 则存储着真正的文件内容，client\n会先从 master 获取文件存储在哪一个的 chunkserver，然后从这个 chunkserver\n直接读取。该过程有以下几点需要注意:\nmetadata\nmaster 存储的 metadata 主要包括文件系统的\nnamespace、文件与 chunk 的映射、chunk 的位置等。\n文件系统的 namespace\n在 GFS 中存储为 B + 树。树上的每个叶子节点代表普通文件，而中间节点则代表目录文件。根节点是文件系统的根目录。\nmaster\n启动时会将所有元数据加载至内存中，优点是元数据操作速度很快，但是由于采用的是\nsingle master，master 的内存会限制了文件系统的可扩展性，由于每个 64MB 的\nchunk 会占据 64B 的 metadata，则 64GB 内存的服务器最多可支持的 chunk\n的数量约为 64GB/64B =\n10 亿。但由于 GFS 应用场景是大文件，所以这个问题并不是严峻\nchunk\n每个大文都件被划分为若干个固定大小的 chunk，每个 chunk\n在创建的时候都会被 master 赋予一个唯一的 ID，称为 chunk handle\nchunk 的大小理论上可为任意值，GFS 中为 64MB)，大的 chunk size\n有以下优点与不足\n\n减少了 client 与 master 的通信次数，从而减少了 master 和 network\n的负载；对于 sequential read，chunk size 越大，数据在同一个 chunk\n上的概率就越大，因此避免了读取多个 chunk\n 减少 master 存储的 metadata 的大小，因为 chunk size 越大，chunk\n的数量就越小\n可能会浪费空间，如一个 65MB 的文件会被分成两个 chunk，但是第二个\nchunk 只有 1MB 的数据却占了 64MB 的大小\n可能会导致 load imbalance，如一个小文件只有一个 chunk，因此存储这些\nchunk 的 chunkserver 会成为 hot spot；但是在文章提到的应用中，hot spot\n并不是一个大问题，因为文章内的应用应用场景是 read large multi-chunk\nfiles sequentially\n\n此外，在每个 chunk\n都会有另外的两个副本，分别存储在三台机器上，其作用有两个：high\navailability 和 load\nbalancing；在这个机制中，副本位置的选取是一个比较关键的问题，一个好的副本位置定义算法满足下面特性：\n\n保证足够的可靠性，例如，不能将所有副本存放在同一个磁盘或者物理机器上；\n保证写入高效性，多副本位置尽量靠近，降低写入延迟，提高读写性能\n\n论文中创建 chunk 时副本位置的选择方法如下：\n（1）选择存储空间利用率最低的节点和磁盘 （2）选择最近一段时间内新建\nchunk 数量较少的节点和磁盘； （3）将多个副本分散在不同的机架上\n1 和 3 比较容易理解，2 是为了保证一个节点 / 磁盘不会被频繁新建 chunk（新建完接下来就是数据写入了），否则很容易沦为\nhot spot，导致磁盘 IO 和网络带宽被占满，影响效率。\noperation log\n由于 metadata 存储着 GFS 的核心信息，因此 GFS 还设置了日志记录\nmetadata 的变更信息，这个日志就是 operation log\noperation log 中一个关键信息是时间信息，包括 chunk\n的版本、创建时间等，从而能够处理 concurrent opration\nclient 请求的 operation 首先会被 master 接受，然后 master\n会先写日志，在修改内存中的\nmetadata，这样即使出现断电等异常，也不会丢失更新，因为可以在重启时通过\noperation log 重新构造内存的 metadata\n如果 operation log 记录着 GFS 自使用以来的所有 operation，那么 log\n会非常大且重启时构建耗时会非常长，GFS 采用的机制是当 log\n达到一定大小时，将当前内存的 metadata 持久化到硬盘上，称为\ncheckpoint；则 operation log 只需要存储创建 checkpoint 的时刻后的所有\noperation，恢复时根据 latest checkpoint 恢复最新状态，且重新执行一遍\nopration log 里面的操作即可\nsingle master\nsingle master\n的设计显然存在着单点故障的问题，但是论文表明这么做两个理由\n\n(1) 这样大大减化了设计和实现\n (2) 实际数据直接在 client 和 chunkserver 间交流，所以 single master\n不会成为 bottleneck\n\nmaster 与 chunkserver 通过周期性的 HeartBeat 通信，用于动态搜集\nchunkserver 的状态：如 chunkserver 上有哪些 chunk，从而 master\n能够及时更新而无需长期存储这些信息\n读写操作\n下面主要讨论 GFS 中的读写操作\n读\n读操作比较简单，上面的系统架构图也显示了这一过程，其步骤如下\n\nclient 告诉 master 需要读取的具体文件及其位置 (chunk index)\nmaster 返回这部分文件所在的 chunservers 以及 chunk 的 version\nclient 缓存这些信息 (信息有一个过期时间，client\n会等到这个信息过期后才会再次向 master 请求，从而缓解了频繁读写时，向\nmaster 请求次数过多从而导致 master 负载过大)\nclient 请求最近的 chunkserver，然后检查 chunkserver 上 chunk 的\nversion 是否与从 master 获取的相同；如果相同则读取数据，否则重新向\nmaster 请求这些信息\n\n写\n写操作主要分为两种：write 和 append，write\n是修改数据，append 则是在文件末尾添加数据\n首先是 write 的过程，为了让同一个 chunk 多个副本数据保持一致，\nmaster 将存储 chunk 的其中一个 chunkserver 作为 primary，其他是\nsecondary，primary 用于确定数据写入这个 chunk 的顺序，secondary 则复制\nprimary 的写入顺序即可，下图是一个写操作的流程\n\n\ngfs_write\n\n各个步骤的具体操作如下\n\nclient 询问 master 要写入的 chunk 所对应的 primary 和 secondary\n位置（如果这时没有 primary，master 就会指定一个\n master 返回相关信息 (chunk locatioin，chunk version\n等)，client 会把信息存入 cache，以后就不再重复询问 master\n以节省开销，直到该 primary 的身份失效\n客户端把数据发给所有 replicas (包括 primary 和 secondary)，replicas\n们会把数据暂存在 LRU buffer cache\n中，但是此时还并没有真的写入磁盘\n当所有 replica 都确认收到数据后，client 发写入指令给\nprimary；primary 会给这个指令定一个序列号 (当同时收到多个 client\n的请求时，在 primary 这里确定顺序)，primary\n依序列号修改本地的数据\n primary 把写入指令和序列号发给\nsecondary，secondary 都依同样序列号修改自己的数据\n当 primary 收到 secondary 的回复时，返回成功信息给客户端\n如果有 secondary 失败了，primary\n会返回失败信息给客户端。此时数据就是不一致的。客户端会发起重试。\n\n另外一个写操作是 record append，其过程与上面相似，但是在这里\nclient 不会指定 offset，而是只提供数据，GFS 会把数据 append\n进去后再返回 offset 给 client。\nrecord append 还在 primary 添加了以下逻辑：\n\n每次 append 时会针对待写文件的最后一个 chunk; 如果发现该 chunk\n剩余空间不足以写入，则把当前 chunk\n用空白补齐（padding)，然后把数据写入新的 chunk\n 数据的写入是 at-least-once，如果写失败了（即只在部分 secondary 上写成功），则会在新的末尾重新写一次，这样就会导致上次已经写成功的\nreplica\n上数据出现两次，而上次写失败的 replica 上会有一段空白。返回给客户端的是成功写入的 offset 位置\n客户端程序需要能正确处理这些情况：对于不正确的数据，可以在每个记录开头加一个 magic\nnumber，或者加一个 checksum 之类；对于重复的数据，需要客户端判重，比如在记录里加一个 unique\nid。\n\n其他策略\n\nchunk version\n\n 上面提到读写过程中，master 均会告诉 client 对应的 chunk 目前的\nversion，从而保证 clinet 读取的是最新的数据\n chunk version 会在 master 为这个 chunk 选择新的 primary\n时增加，并通知包含这个 chunk 的所有 chunkservers 更新这个 version\n 如果 client 在某个 chunkserver 读到的 chunk 的 version 与从 master\n读取的不同，说明这个 chunkserver 的数据不是最新的\n\n snapshot\n\nsnapshot 是对系统当前状态进行的一次拍照。用户可以在任意时刻回滚到快照的状态\n采用 COW(copy-on-wirte)\n机制实现 snapshot，即如果被 snapshot\n的文件有更新操作时，就将文件的要被更新的 chunk 复制一份，然后对复制的 chunk 进行更新，而原来的 chunk 作为快照数据被保留，以后要恢复到该快照时，直接将该 chunk 读出即可\n当 GFS 的 Master 节点收到 Snapshot 请求时，其处理逻辑如下\n\n\n\n回收 snapshot 请求覆盖的文件的 chunks 上的租约 (即撤销\nprimary)，这样的话接下来客户端要对文件修改时，就必须向 master\n申请，而此时 master 就可以对 chunk 进行复制\n master 在日志中记录本次 snapshot 操作，然后在内存中执行 snapshot\n动作，具体是将被 snapshot\n的文件或目录的元数据复制一份，被复制出的文件与原始文件指向相同的\nchunk；\n假如客户端申请更新被 snapshot 的文件内容，那么找到需要更新的\nchunk，向其多个副本发送拷贝命令，在其本地创建出 chunk\n的副本\n，之所以本地创建是因为可以避免跨节点之间的数据拷贝，节省网络带宽；\n客户端收到 master 的响应后，表示该 chunk 已经 COW\n结束，接下来客户端的更新流程与正常的没有区别。\n\n\nchecksum\nchecksum\n解决的是数据完整性问题，即磁盘损坏从而导致数据损坏的问题\n每个 chunk 会被划分为大小为 64KB 的 block，每个 block 有一个 32 位的\nchecksum\nclient 从某个 chunkserver 读取数时，chunkserver\n首先会验证要读取的数据的 checksum，如果 checksum\n不符合已知记录 (写入时的记录)，会返回错误，从而让 client 去读取其他\nchunkserver 上的 chunk\n\n一致性\n由于 GFS 中的 metadata 只在 master 可写，因此通过加锁保证修改的\natomicity；而对于修改 metadata 的 concurrent operation，operation log\n中定义了这些 operation 的顺序， metadata\n的一致性能够得到保证。\n因此这里的一致性着重讨论的是文件的一致性，GFS\n针对文件定义了以下的一致性状态\n\n\nconsistent state\n\n上图中的四种状态含义如下\n\ndefined：从客户端角度来看，客户端完全了解已写入集群的数据的\noffset，例如，客户端串行写入且成功（serial\nsuccess），此时的状态是 defined\nconsistent：客户端角度来看，chunk 多副本的数据完全一致，但不一定\ndefined (defined 包含了 consistent)\ninconsistent：多副本数据不一致\n undefined：数据未定义\n\n下面分别以几个案例介绍上面的状态，这部分内容主要摘自这里\nserial write\n当 client\n串行更新时时，客户端自己知道写入文件范围以及写入数据内容，且本次写入在数据服务器的多副本上均执行成功。因此，本次写结果对于客户端来说就是明确的，且多副本上数据一致，故而结果是\ndefined。如下图：\n\n\ngfs-defined\n\nconcurrent write\n多个 client 同时写入时，\n由于多个客户端由于写入范围可能交叉而形成交织写。这时候，由于单个客户端无法决定写入顺序（只有\nprimary\n才能决定谁先写谁后写），因此，即使写入成功，客户端仍无法确定在并发写入时交叉部分最终写入结果，但是因为写入成功，所以多副本数据必然一致，\n如下图所示\n\n\nconsistent_undefined\n\n图中红色部分代表并发追加的部分，这部分数据由于无法确定谁先谁后执行，因此结果不确定；但由于跟新成功，因此，副本间数据是一致的，这就是 consistent\nbut undefined。需要注意的是，consistent but undefined\n只会出现在原始的 write 操作被划分为几个子操作时，原文解析如下\n\nIf a write by the application is large or straddles a chunk boundary,\nGFS client code breaks it down into multiple write operations. They all\nfollow the control flow described above but may be interleaved with and\noverwritten by concurrent operations from other clients. Therefore, the\nshared file region may end up containing fragments from different\nclients, although the replicas will be identical because the individual\noperations are completed successfully in the same order on all replicas.\nThis leaves the file region in consistent but undefined state as noted\nin Section 2.7.\n\n而无论是 serial write 还是并行 concurrent write，一旦失败，多个 chunk\n副本上的数据可能都不一致了，因此便是\ninconsistent，必然也是 undefined。\nappend\n上面提到，client 的 append 操作无需指定 offset，由 chunk 的 primary\n根据当前文件大小决定写入 offset，在写入成功后将该 offset 返回给客户端。因此，客户端能够根据 offset\n确切知道写入结果，无论是串行写入还是并发写入，其行为是 defined。如下所示\n\n\ndefined append\n\n假设上面的 append 经历了一次重试，那可能实际 chunk 的布局如下\n\n\ninconsistent append\n\n由于第一次写失败（错误可能发生在任意一个副本），导致了多副本之间从 50 至 80 的数据可能不一致。但接下来重试成功，从 80 至 110 之间的数据一致，因此，其状态是\ninterspersed with inconsistent。\n小结\n本文主要介绍了 GFS 的适用场景、系统架构、读写过程以及\n一致性的保证，GFS 可以说是为 MapReduce 量身定做的文件系统:\n大文件、文件写入后基本不修改、更着重吞吐量等，也有人说认为 GFS 也就没有\nMapReduce 了，其实与其说 MapReduce 多么牛，不如说是 GFS 牛，因为 MapReduce\n模型早就是数据库领域几十年前玩剩下的了，但是只有 Google\n做出了那种廉价高效的分布式系，主要是因为 Google 的下层的支持系统也就是\nGFS 做得好。\n\n参考\n\nThe\nGoogle File System\nGoogle File\nSystem\nPaper Reading:\nGFS\n\n","categories":["分布式"],"tags":["分布式"]},{"title":"命令行编译 Java 源文件","url":"/2016/01/13/%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BC%96%E8%AF%91Java%E6%BA%90%E6%96%87%E4%BB%B6/","content":"众所周知，Java 是一门编译型语言，需要编译成字节码才能在 JVM 上运行。常用的 IDE 如 Eclipse 等将编译、运行等步骤结合起来一起执行，只需要按下 Run 即可完成编译和运行的工作。但是实际上编译 java 程序的核心是 JDK。本文主要讲述了只安装 jdk 通过命令行来编译运行 Java 程序。\n\nJDK 与 JRE\n说到 JDK 和 JRE 的关系。可以简单认为 JDK 是用来编译 java 文件的，而 jre 是用来运行编译生成的 class 文件。JDK 中包含 JRE，在 JDK 的安装目录下有一个名为 jre 的目录，里面有两个文件夹 bin 和 lib，在这里可以认为 bin 里的就是 jvm，lib 中则是 jvm 工作所需要的类库，而 jvm 和\nlib 和起来就称为 jre。\n另外在 windows 下安装 jdk 是会有选择是否安装单独的 jre，如果选择安装，那么就会安装了两套 jre，一套包含在 jdk 安装目录中，另外一套则是独立安装的 jre。那么这两套 jre 有区别吗？\n答案是有的，因为 jdk 是作为开发使用的，而 jre 仅仅是作为运行 java 程序的环境。因此具备开发功能的 jdk 自己的 jre 下会同时有 client 性质的 jvm 和 server 性质的\njvm，而仅仅作为运行环境的 jre 下只需要 client 性质的 jvm.dll 就够了。\n可根据需要编译的源文件的数目和位置将通过命令行编译运行 java 程序分成两大类。一类是只有单一的源文件，另外一种是有多个源文件。\n单个源文件\n单个原文件是最简单的了，编译命令为：\njavac  source.java\n运行的命令为\njava source\n上面没有考虑单个源文件中引入的 jar 包，假如引入了不在 classpath 环境变量中的 jar 包时，需要在编译时加上 -classpath 参数。即命令为：\njavac -classpath  *.jar  source.java\n多个源文件\n编译多个源文件是指其中一个源文件引用了另外一个源文件里面的类。这里可以根据源文件是否在同一个包进行以下分类。\n源文件在同一个包\n这时候只需要编译 “最顶层” 的类的源文件即可，比如说有以下三个类：\n类 E 源文件如下\npublic class E{  \tpublic E(){  \t\tSystem.out.println(\"this is E\");  \t}  }  \n类 C 源文件如下\npublic  class C{  \tpublic C(){  \t\tE e=new E();  \t\tSystem.out.println(\"this is C\");  \t}  }  \n类 D 源文件如下\npublic class D{  public static void main(String [] args){  \t\tC c=new C();  \t\tSystem.out.println(\"this is D\");  \t}  }  ```  现在这三个源文件在同一个包，也就是同一个文件夹下，此时**只需要编译类D的源文件即可**，执行时也只需要执行类D的class文件。即依次执行下面命令```  javac D.java  java D  ```  得到的结果如下：```  this is E  this is C  this is D  \n源文件不在同一个包\n这种情况下又可分为两种情况，详见下面例子\n第一种情况：\n类 B 的源文件如下：\npackage tcp;public class B{  public B(){  \tSystem.out.println(\"this is B\");     }}  \n类 A 的源文件如下：\nimport tcp.*;public class A{  \tpublic static void main(String [] args){  \t\tB b=new B();  \t\tSystem.out.println(\"this is A\");  \t}  }  ```  这时候需要建立一个名为`tcp`的目录,将类B的源文件放到tcp目录中，然后类A的源文件与tcp目录在同一个目录下，这时候只需要进入到类A所在目录下执行`javac A.java`即可生成`A.class`和`B.class`两个类文件，且`B.class`在tcp目录下生成。再执行命令`java A`便可得到以下结果：```  this is B  this is A  \n第二种情况\n在第一种情况下类 A 并没有在任何包中，那么假如类 A 也在一个包中该怎么编译运行呢？\n在类 A 的源文件的开头添加多一行 pacakge udp; 即类 A 在包 udp 中，这时候通过命令行该怎么编译？\n首先建立 tcp 和 udp 两个文件夹且将他们放在同一目录，tcp 目录中放置 B.java,udp 目录中放置 A.java, 编译的方法有两种：\n第一种：进入到 udp 目录，执行命令 javac -classpath .. A.java\n第二种：不进入 tcp 或 udp，在 tcp 和 udp 所在的目录执行命令 javac udp\\A.java\n第一种方法的 - classpath 参数指出了 tcp 包所在目录，.. 表示上一层目录，这里采用了相对路径，也可采用绝对路径。\n第二种方法利用了 jdk 编译时遇到 import 的包时会先在当前目录寻找的特性。\n执行 A 程序的步骤则只有一种方法，就是在 tcp 和 udp 目录下执行命令\njava udp.A\n","categories":["Java"],"tags":["Java"]},{"title":"创作者变现与加热","url":"/2024/05/03/%E5%88%9B%E4%BD%9C%E8%80%85%E7%9A%84%E5%95%86%E6%9C%BA%E4%B8%8E%E5%8A%A0%E7%83%AD/","content":"最近一段时间都在做一些跟创作者相关的业务，相较于商业传统的三方（平台、用户、广告主），创作者是随着内容平台崛起而诞生的第四方，与其他三方的关系可以参考笔者之前文章\nYet\nAnother Overview of an AD System\n本文也是对之前的文章里创作者相关的部分做进一步的展开，主要是商机和加热两大块，前者主要是包括对商机部分中涉及到的各个链路的职责，以及各个链路之间的联动关系；加热中的自投与代投的产品形态，以及在流量上与广告流量的协同关系等。\n\n商机\n市面上星图、蒲公英、聚星、花火这一类合作平台，就是为创作者提供商机的官方平台（也有不走官方平台达成合作的，后文统一称为水下）\n在用户的角度，这些产品连接着客户 (B) 和达人 (K)，是一个供需撮合平台；B\n在平台通过一口价或招募的方式与 K 达成合作，付费后让 K\n创作与发布内容，相当于 B 付费购买 K 的内容和流量\n在平台的角度，这些产品连接着社区和广告，一端连接着社区，是创作者运营和成长的重要工具，另一端连接广告，是广告素材的一种重要的内容供给；因此这些产品往往需要同时考虑\nK 端和 B 端两侧业务目标\n在具体的实现上，这些产品一般会涉及到三条链路：撮合链路、推荐链路和广告链路，如下图所示，各个链路的职责如下：\n\n撮合链路：B 找到对应的 K 创作相应的内容（视频，图文，直播等）\n\n推荐链路：K 发布对应内容，推荐链路将 K\n发布的内容分发，获得自然流量\n\n广告链路：B 或 K\n决策是否要额外付费加热相应的内容，或利用这些内容来投广\n\n\n这部分会针对这三条链路的细节展开\n撮合链路\n撮合链路最基础的职责就是让 B 和 K 达成合作（通过 1v1 和 1vN\n的模式），即为 B 在平台上推荐的达人，类比常见的 C\n端的推荐系统，这其实算一个 B\n端的推荐系统，只是候选集和系统的复杂度是远比不上 C 端的推荐系统\n这个系统最终的优化指标就是 BK 的撮合流水，但除了流水，K\n的商机的分布、B 和 K\n的操作体验等也是平台需要关注的部分，因为撮合链路上往往会遇到下面的问题\n（1）变现产品体验不佳，如变现模式不够丰富、易用性差、审核规则不清晰等\n（2）K 的变现经验不足，缺乏引导，不知道该如何变现\n（3）流水的行业集中度高，总体集中在某几个头部行业，其他行业供需失衡\n（4）......\n在链路上的效率优化上，撮合链路往往也是有较大的挑战的，因为不像常见的\nC\n端流量实验，由于流量比较大，指标相对来说较好观测（如广告的展点消等），这类\nB 端的实验由于流量有限（只有 B 侧用户）、成单周期长（30\n天 +），很多产品能力是比较难做效率的迭代的，或者说观测周期需要非常长\n从技术视角来看，常见做法是挖掘和提升与流水相关的中间指标，但就笔者经历来看，成单过程中的各种中间指标（如\nB 在页面上对 K 的 ctr、B\n在网页上的停留时长等），跟最终的成单行为的相关性并不是那么强，因此想通过提升这些中间指标进而带动最终流水的上涨，也是有难度的\n因此在撮合链路上，算法侧在优化链路效率的时候，没有特别好的可观测指标，这是技术优化一个比较痛的点；所以往往优化的方向是提升撮合能力多样性，即为\nB 侧找博主提供更丰富的途径和工具，如可以让 B 侧提供多样的表达，eg\n通过人群包来找博主，除了 1v1 的精准定向方式，也可以通过 1vN\n的招募方式、cpm/cpa/cps 等结算方式，共同激发 K\n的创作积极性，尤其是腰围部的 K\n参考星图，撮合产品的形态往往是比较多样的\n\n另一个在撮合较为常见的优化方向是分行业建设（跟广告类似），重点挖掘需求强 (B) 但供给不足 (K) 的行业，提供更多分行业解决方案\n推荐链路\nB 和 K 达成合作后，K\n会发布其创作的内容，这个时候笔记就会进入第二条链路：推荐链路，而这部分也是技术侧重点优化部分，因为涉及到了分发相关的流量侧机制，同时也是影响其他两条链路的关键地方\n这里先说下这类撮合产品最容易遇到的问题，即 “水下” 问题；这里有个概念需要说明，水上即是走官方合作平台的\nBK 撮合，需要给平台交一定的手续费；而水下指的是 B 不通过平台与 K\n直接建联\n站在平台的视角，肯定是希望 BK\n撮合都走水上，因为除了手续费作为利润，还能对平台的这类商机有更好的管控；但在\nB\n的视角，走水下既能省手续费，内容又能更原生（有些平台对水上的内容会明确打标），因此水下与水上的博弈，会是一个平台长期需要解决的问题\n因此为了让更多的预算从水下翻水上，平台需要做好两件事\n（1）提升撮合平台的易用性与效率，即平台需要让 B 和 K 的建联更容易\n（2）流量分发上，水上需要建立与水下有差异的分发机制，并对 B\n侧透出笔记走水上的权益\n第（1）点主要跟撮合链路挂钩；第（2）点则是跟这里说的推荐链路强相关，因为\nBK 撮合的笔记跟 K\n自己发布的笔记在形式上基本是无差异的，一般都是走大盘的分发，在这种方式下，相当于水上笔记与水下笔记的流量分发没有任何差异，这样\nB 走水上的意愿就没有那么高了\n所以要做水下翻水上，水上的流量分发需要做出差异，需要更多考虑 B\n侧的权益；而实际上，K 与 B\n的权益 / 目标并不是二元对立的，本质上是平台对 B 的钱袋子在不同 K\n的分配之间做干预，所以首先要服务好愿意出钱的 B，然后需要考虑如何在不同 K\n之间做好分配\n而 B\n侧往往是有营销诉求的，在星图、蒲公英等平台上买到的素材，会是其投广素材的一个很重要的来源，且这个素材的在\nK 发布后获取到的自然流量的效果，是影响 B\n拿这个素材投广的一个很关键因素；如下图所示，在 B\n掏钱买 “素材 + 流量” 的模式下，推荐链路获得的自然流量，会同时影响其在前后链路的效率\n\n因此，有差异的流量分发，就是指这部分自然流量的分发方式和效果，在实际中，可以考虑为这部分流量加入一些\nB\n侧更关注的营销目标作为排序公式的一部分，然后在这部分流量上也做好归因并透出给广告主\n广告链路\n前面提到，广告主会拿 BK\n撮合产出的素材来投广，实际中往往也能观测到这种现象，利用 BK\n撮合产出的素材来投广，其效果是要更好的；原因往往是；如下图是星图统计的结果（from\n2024\n巨量引擎营销通案）\n\n而决策内容是否投广的一个重要依据，就是这些素材在自然流量上的效果；从技术视角来看，这两部分流量分发是两条独立的链路，分发目标不同，因此存在流量相关性弱的问题，流量数据对\nB\n侧投广参考价值小；大白话就是客户如果拿一篇在自然流量上数据好的笔记来投广，实际的投广效果不一定好，客户缺乏选择素材投广的方法论\n从技术视角，技术上可以做功的一个方向就是增强这部分自然流量与广告流量相关性，即让客户的体感上这两部分流量的相关性是存在的；因为这部分投广素材的自然流量，某种程度上算是笔记投广的冷启动流量，利用好这部分冷启动流量，更好辅助客户的投广，进而撬动更多的这些撮合素材投广，预期是能带来撮合流水和广告流水的增长\n在具体的技术手段上，前面提到，可以在推荐链路的分发中加入一些 B\n侧更关注的营销目标作为排序公式，\n在这个基础上，对这两部分流量的特征 / 样本做互通，可以增强广告冷启动效果；而在自然流量上好的素材，在其被应用来投广后，也可以尝试更激进的广告出价策略等\n对这两部分做得更极致的联动，则是类似星广联投的产品，这个产品不再是先有自然流量，再有广告流量的顺序，而是利用客户的预算，在自然流量和广告流量上同时做营销，\n流程如下图所示\n\n\n加热\n加热往往与商机相辅相成，加热是创作者用于辅助其成长的付费工具，成长到一定体量的粉丝量后，就需要通过商机来变现；有点类似\nB 侧视角的种草和拔草\ndou+、粉条、薯条这一类产品，就是用来服务所有创作者的内容加热工具\n这些产品对创作者的内容质量有要求，往往要求营销感不能太强，有专门的内容审，也不打标；“加热” 也意味着优化目标不强，一般是阅读，赞藏，主页访问，加粉等比较浅层的目标\n同时，dou+ 也支持强营销感目标如获客、商品购买、直播间推广等，\n如下图所示；与前面的浅层营销目标的一个较大的区别是其服务对象，加热的一般是创作者在投，主要目标是自身的成长，而强营销目标则是素材制作能力强的广告主在投，这部分广告总体还是会考核成本、roi\n等目标，这里其实也涉及到下面要提的流量机制\n\n出价与预算\n因为创作者对广告竞价理解有限，所以往往在出价方式上，会选择 nobid\n类的出价方式，从而降低创作者对成本、赔付等成本类出价的理解门槛；这里有一个比较有意思的问题，就是从出价角度来看各个产品的预算增长逻辑\n先看看成本类出价，像 ocpx\n这种成本类出价，预算增长的逻辑是通过出价、模型等优化，让实际转化成本更贴近广告主的出价，然后随着大盘的竞价的能力变强、总体\necpm\n水位上涨，广告主需要提高出价才能拿到更多的量，相当于广告主愿意为一个转化付出更多的钱，平台把转化卖得更贵，进而带来大盘收入上涨（打平转化数）\n而 nobid 类出价往往是能把预算都花完的，所以 nobid\n类出价的预算增长，往往是让转化成本下降，广告主的 roi\n变好，然后广告主愿意投入更多的预算\nnobid\n的成本需要持续下降，看起来似乎跟前面的成本类出价的是矛盾的，并且从长期大盘消耗\naa 增长的角度来看，adload、ctr、cvr\n都是有上界的，即转化数是有限的，因此持续让转化成本下降，相当于是让大盘收入持续下降\n所以不能让 nobid\n类的出价产品的成本持续下降，前面提到的 “广告主的 roi\n变好，然后广告主愿意投入更多的预算”，是一个比较长期的预算 aa\n增长逻辑，在这个过程中，往往会带来成本的上涨，比如说两个 nobid\n计划，除了预算不同，其他的配置都相同，在这种配置下，往往是高预算的计划的成本比低预算的要高，因为在一段时间的流量池里，便宜的流量是有限的\n所以问题会变成：当广告主在 nobid\n类出价产品提升预算，带来的成本与跑量的同时上涨后，成本是否还在广告主心里预期的\nroi 红线以内，如果还在，那是有可能接受成本的上涨的，因此，nobid\n的消耗增涨逻辑变成了：平台通过效率优化不断优化广告主的成本，进而吸引广告主增加预算（在成本可接受范围内），进而带动总体的消耗增涨（因为\nnobid 的预算总是能花完的）\n这样看来，nobid 的增长逻辑跟 ocpx\n是类似的，也是在广告主心里预期可接受范围内，把转化卖得更贵，只是在 ocpx\n这类产品中提 bid 的操作变成了 nobid 中提预算的操作\n流量机制\n由于加热产品本质上是平台在卖流量（与广告主产品一样）；因此在流量分发上，一个很直观的做法就是将这些不打标的内容与打标的广告共同竞价，即精排基于\necpm 与广告混竞、混排有相同的约束，与广告共享 ad load\n但这样做会观察到的一个现象，就是一旦遇到 618、双 11\n之列的大促，在混竞情况下，这类加热产品的 load\n就会下降，同时各种营销目标的转化成本也会上涨\n究其原因，还是因为在混竞的情况下，这类加热内容是在广告竞争流量的，因此成本是与会随着广告竞价大盘的成本水位上涨而上涨，成本无法做得比较低；而创作者预算少、出价能力弱，在跟大广告主混竞情况下，容易受到大广告主的挤压，或者说内容优质 + 不打标带来的\nctr、cvr 优势，弥补不了 bid 的劣势\n因此这类不打标的付费加热内容（有些平台也叫原生广告），往往需要独立排序链路和\nload，同时在混排需要有独立的约束（首坑、最小\ngap），或者说作为一路跟硬广和推荐内容做混排，从而避免被硬广积压得太狠\n某种程度上这算是一个流量价值洼地，因为创作者加热产品，其实还肩负着创作者成长的，丰富平台内容的生态价值，所以这部分流量也算是对创作者成长的某种扶持\n自投与代投\n一般创作者加热产品都是自投或代投，即创作者自掏腰包为自己的作品加热，或者是创作者的粉丝自愿为创作者的笔记掏钱加热，总体的预算普遍还是小的，容易受到大广告主的挤压\n但是还有一种稍微有点不同的代投模式，即平台为大广告主优选 ugc\n的笔记，然后大广告主对这些 ugc\n笔记做加热，比如说以原神这个游戏为例，会有很多的 ugc\n笔记，而平台在中间通过 “撮合” 的方式，让广告主愿意为这些 ugc\n笔记加热，直观来看，会是一个三方共赢的模式\n\n对于用户，只要做好内容，就有机会被平台和广告主选中，给予流量补贴（品牌付费）和现金补贴（可选）\n\n对于广告主，节省了素材制作的成本，平台会从海量的 ugc\n笔记中优选出营销感更弱的笔记给品牌投放\n\n对于平台，能以这种方式引入更多预算（也有可能是转移），同时利用品牌方的钱来辅助做创作者成长\n\n但是产品本身也会存在一些限制\n\n基本只有大品牌 / 广告主才适用，因为需要品牌被用户熟知才能被有大量的\nugc 笔记\n\n大品牌方如果通过这种方式把预算较好花完，可能就不会在前面提到的撮合平台再买素材了，可能会对撮合流水有一定影响\n\n在技术实现上，总体与常规的加热差异不大，主要区别是涉及到了笔记的优选过程，包括投前的圈选和投中的优选等；因为往往\nugc\n笔记的数量都是比较大的，如果通过算法手段选择出合适的笔记供品牌选择投放，或者做成托管的方式在投中做优选，是一个需要考虑的问题\n小结\n本文主要讲了与创作者相关的两个事情：商机和加热，前者是创作者赚钱的手段，后者则是创作者花钱做成长的途径\n平台推出的撮合产品是完成商机的重要工具，一个完整的生命周期往往涉及到撮合链路、推荐链路和广告链路这三条链路，其中推荐链路中的流量分发是尤为重要的一部分，因为会同时影响\nB 对 K\n的笔记质量判断以及后续的投广决策，进而影响撮合流水，以及广告主的投广预算等\n平台推出的加热产品是辅助创作者成长的重要工具，本文探讨了这类加热产品常用的\nnobid\n类出价的预算增涨逻辑、流量机制以及代投的一种新的产品形态，在这个过程中，平台需要充分发挥主观能动性，尤其是流量机制这部分，不能只从收入考虑这部分流量的价值\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"分布式机器学习 (4)-Implement Your MapReduce","url":"/2018/02/24/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-Implement%20Your%20MapReduce/","content":"提到 MapReduce，很自然想到的是 Hadoop MapReduce ，但是 MapReduce\n只是一个编程范式，而 Hadoop MapReduce\n则是这个编程范式的一个比较出名的实现。实际上，可以通过多种方式实现\nMapReduce，本文要介绍的就是如何在 Linux 的 bash 下实现一个 MapReduce\n程序，并且分别实现了单机版本和多机器版本。原视频见这里，需要自备梯子。\n\n下面以 MapReduce 中的经典例子 WordCount 为例进行讲述，\n先实现单机版本，再实现多机版本\n单机版 MapReduce\n下面是在 bash 下通过 MapReduce 范式实现的单机版本的 WordCount\n程序\ntext=$(cat &lt;&lt;EOFThis is my CupIt is not your cupMy cup is whiteYour cup is blueEOF)echo $text\\| awk '{for (i=0; i&lt;=NF; i++) print $i, 1}' \\| sort \\| awk '{if ($1 != prev) {print prev, c; c=0; prev=$1} c+=$2}'\n上面的 bash 脚本有几个需要了解的语法细节\ncat &lt;&lt;EOF 命令在 bash\n中主要用于处理与实时多行 string 相关的任务，实时指的是多行 string 要在命令执行的时候输入（遇到\nEOF 结束），这个命令一般可用于以下几个任务\n1. 将变量的值赋为多行 string 2. 将多行\nstring 写入文件  3. 将多行 string\n传入管道命令\n上面的代码中就是将多行的 string 赋值给变量\ntext，这三个任务的例子可参考这里\nawk 是一门语言，也是\nLinux 下一个常用工具，用于处理文本相关的数据，尤其是表格类的数据。awk\n会逐行处理文本直至遍历完整个输入流（可以是标准输入流，也可以是文件流，上面的代码是标准输入流），每一行默认根据空格或 tab 分割文本为若干的\nfields，从下标 1 开始，$1 表示第一个 field\n的值，其他同理；NF 则是 awk 中特殊变量，表示这一行共有几个 field。 awk\n对每行的操作是包含在 {} 中的命令。\n因此，上面的代码中第一个 awk 实现了 map 过程，sort 实现了 shuffling\n的过程，而第二个 awk 实现了 reduce 过程。\n多机器版本 MapReduce\n上面是一个单机版本的 MapReduce，然而 MapReduce\n在多台机器上更能显示其威力。多机器版本的 MapReduce\n首先要考虑的是不同机器间的通信问题，这里采用的是 ssh 通信方式。\nssh 除了可以开一个远程机器的 shell\n外，还可以通过命令直接在远程机器起一个进程来运行指定程序。如运行下面的代码会在本地机器上显示\nhello world\necho \"hello world\" | ssh 192.168.1.10 'cat'\n其通信过程首先是本机通过 ssh 连接到远程机器上，同时将\nhello world 作为输入流传到远程机器，远程机器上的 sshd\n进程截获了输入流，同时启动 cat\n进程读取输入流，并将输出流返回给本地机器，本地机器的 sshd\n进程同样会截获输出流，然后在本地机器输出。\n这种分布式通信的模式在各个分布式系统中（yarn，mesos，k8s）都非常常见，每个节点都要有一个\ndeamon 与其他节点进行通信并进行资源管理，在这里 sshd 就相当于\ndaemon，只是没有资源管理功能，但是基本原理是一样的。\n因此，利用这种方式，可将 map 过程放到其他机器上，如下代码所示就是将\nawk '{for (i=0; i&lt;=NF; i++) print $i, 1}' 这段程序放到了\n192.168.1.10 这台机器上执行。\ntext=$(cat &lt;&lt;EOFThis is my CupIt is not your cupMy cup is whiteYour cup is blueEOF)echo $text\\| ssh 192.168.1.10 'awk '{for (i=0; i&lt;=NF; i++) print $i, 1}'' \\| sort \\| awk '{if ($1 != prev) {print prev, c; c=0; prev=$1} c+=$2}'\n因此可将 map\n过程放到其他机器上执行，并将结果存储在其他机器上，因为默认一台机器无法存储所有的数据，而输入的数据也是分布在各台机器上，这个过程具体代码如下所示\nMap = '{for (i=0; i&lt;=NF; i++) print $i, 1}'ssh worker1 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker1 ok'ssh worker2 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker2 ok'ssh worker3 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker3 ok'\n上面的代码分别令三个 worker\n处理它们本地的文件并将处理后的文件存储在本地，在处理完后返回消息。注意\nawk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker1 ok\n需要用分号括起来，表示整条命令都在远程机器执行。\n上面通过 ssh\n启动远程程序时，一般会配置密钥访问，从而避免每次都要输入密码。\n除了 map 过程，shuffling\n过程也需要分布式执行，原因是数据无法容纳在一台机器上。之前 shuffling\n操作是对所有的数据进行 sort 操作，现在这种方案显然行不通，实际上\nshuffling 的一个目的是将相同的 key\n交给相同的 worker 进行处理。因此可以采取下面的方法进行分布式 shuffling\n假设有 n 个 reduce worker，则每个 map worker 对其所处理的数据的每条\nkey,value 记录进行 hash (key)% n 操作，记取模后的值为 i (0&lt;=i &lt;\nn), 并将记录写入到本地第 i 个文件中，则最多会在本地生成 n\n个文件，然后分别将这 n 个文件远程复制 (scp 等) 到 n 个 reduce\nworker 的机器上。这样就会令相同的 key 被同一个 worker 处理。reduce worker\n只需要对复制到其机器上的若干个文件进行 sort 和 reduce 操作即可，reduce\n后的结果也是存储在各台机器上的（也可以考虑存放在一台机器，如果经过\nMapReduce 后的数据量能够存放在一台机器上）。\n上面的过程需要注意以下两点\n\nmap 和 shuffle 可以重叠，但是 map 和 reduce 不能重叠。map 和\nshuffling 的重叠方法有很多，其中的一种是每个 map worker\n通过上面的方法生成 n 个文件时，不是一次性将所有的 record 传送给 reduce\nworker，而是达到一定数量后就复制，reduce worker 端则通过插入排序进行\nsort 操作，每次接收到 map worker\n传过来的文件时，就在已排序的序列上进行插入排序\n某个 worker 的可能会被比其他的要慢很多，可能原因 load balance\n问题，也就是分到这个 worker 的 record 数量太多，可以对这些 record\n进行进一步的切分，但是要保证同一个 key 需要被同一个 reduce worker\n处理。\n\nGithub 上有个 bashreduce\n的项目就是在 bash 上实现了\nMapReduce，思路与我们前面讲的差不多，只是还考虑了很多其他细节。\n作者本人也实现了一个 C++ 版本的 mapreduce-lite，没有考虑存储问题，速度较快，感兴趣可参考。\n另外，Hadoop 项目中也有 Hadoop\nStreaming，允许用于用其他语言实现 MapReduce 操作，只要指定好 mapper\n和 reducer 即可。\n","categories":["机器学习"],"tags":["机器学习","分布式"]},{"title":"分批训练过大的数据集","url":"/2017/11/18/%E5%88%86%E6%89%B9%E8%AE%AD%E7%BB%83%E8%BF%87%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/","content":"在深度学习中训练网络时，往往会出现训练数据过于庞大从而无法全部加载到内存中的情况，这里讲述的就是如何分批训练一个庞大的数据集，下面会以 Keras\n中的训练为例进行讲述。\n\n分批处理的思路就是先将那个较大的数据处理成若干个较小的数据文件（如共 1000000\n条记录，处理成 1000 个小文件，每个小文件 1000\n条记录），然后依次读取各个小的数据文件到内存中进行训练，这里的利用了\npython 的 generator 特性来依次读取各个文件的内容。\n如下代码所示，就是每次读取 num_files 个文件并合并成\nX_train 和 Y_train\n并返回，直到整个目录下的文件都被遍历一遍。\ndef train_batch_generator(train_data_dir = './processed_data/train/', num_files = 1):    files = sorted(os.listdir(train_data_dir))    count = num_files    embeddings, labels = [], []    for file in files:        print('Reading file {0}...........'.format(file))        gc.collect()        with open(train_data_dir + file, 'rb') as rf:            data = pickle.load(rf)        embeddings.append(data['embedding'])        labels.append(data['label'])        count -= 1        if count == 0:             X_train, Y_train = np.concatenate(embeddings), np.concatenate(labels)            gc.collect()            count = num_files            embeddings, labels = [], []            yield (X_train, Y_train)\n这样读取文件对应的训练方法如下（以 Keras 中的模型训练为例进行说明）\nNUM_EPOCHS = 10BATCH_SIZE = 32for i in range(NUM_EPOCHS):    print('################{0} epochs#############'.format(i+1))    for x_train, y_train in train_batch_generator(num_files = 3):        print(x_train.shape, y_train.shape)        gc.collect()        model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 100, validation_data = (x_test, y_test))\n另一种方法不需要将大文件分成若干个小文件，而是直接打开整个大文件逐行读取，然后读取了一定数目的行后通过\nyield 返回，\n这种方法的一个问题就是训练过程中必须要保持整个文件为打开状态，此时如果发生系统故障等异常可能会损坏文件，而将大文件分为若干的小文件则能够很大程度上避免这个问题。\n这种方法的另外一个问题就是不能对训练样本进行的\nshuffle，由于这深度神经网络的训练都是基于 SGD\n模式的，因此需要对其训练样本进行 shuffle，具体可参考这个问题。但是当训练样本很大时，显然无法读取整个文件后在内存进行\nshuffle，但是如果将大文件分成小文件后，可在系统内存能够承受的范围内对几个小文件进行 shuffle (如相邻的三个小文件)，虽然这种 shuffle 是一种局部的 shuffle，但是可以通过改变进行 shuffle 的小文件的间隔并进行多次的 shuffle（如间隔分别从 0 递增），从而近似全局的 shuffle。\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"听说你觉得生活很无聊","url":"/2015/11/18/%E5%90%AC%E8%AF%B4%E4%BD%A0%E8%A7%89%E5%BE%97%E7%94%9F%E6%B4%BB%E5%BE%88%E6%97%A0%E8%81%8A/","content":"\n文章为转载，作者：马德，作家，已出版《当我放过自己的时候》、《允许自己虚度时光》等。\n\n（一）\n一辈子活下来，常常是，在最有意思的时候，没有有意思地过，在最没意思的时候，想要有意思地过结果却再也过不出意思。\n或者，换一种表述就是，在看不透的时候，好看的人生过得不好看；看透了，想过得好看，可是人生已经没法看了。\n这句话说得并不绕。其实，人生比这个绕多了。\n人生就是这样的一场游戏：在欲望浮沉中，把生命扔到很远很远，最后，只为了找到很近很近的那个简单的自己。\n\n（二）\n有一年，到大连旅游，参观旅顺日俄监狱。印象中，地牢般的监狱，只有很窄的一方窗户开在地上，可以看到人世的阳光。\n在一孔窗户周围，看到一茎绿草，小小的，嫩嫩的，在风中摇曳。我想，这应是在那里苦难度日的囚犯们，所能见到的全部蓬勃和生机了吧。但是，那么多的监牢，每一孔窗户前，会恰好有一粒草的种子落在那里吗？会有生命的绿意，落在绝望的人生里吗？\n那得多么幸运啊！\n而我们的窗外，就有蓝天白云，我们的身边，就有鲜花绿草，没有谁囚禁我们，但我们却囚禁了自己。\n常常是，在追不上的时候，才去追；在味道尽去的时候，才想品；在不得已时候，才珍惜得已；在人生的大片美好过到支离破碎后，才去捡拾一些碎片，拼凑美好。\n（三）\n生活就是一个七天接着一个七天。\n不是日子重复导致了枯燥和无聊，而是你枯燥无聊，把气撒在了日子的重复上。\n其实，都在重复。位高权重的，富可敌国的，没有谁的日子不是一个七天接着另一个七天。只不过，当你仰慕谁，就会美化对方的重复，认为人家重复得有趣味有意义。其实，这一切，都是仰慕的光环散发出的五彩。\n重复，赋予每个人的本质和意义都是一样的。\n多重复才算重复呢？你看那些一天到晚打麻将的人，每天面对的就是那一百多张牌，然后，洗牌，码牌，打牌，和牌。论理说，该盯得头晕眼花，坐得腰酸腿疼，琢磨得心力交瘁了吧，但嗜打的人从来乐此不疲，没有一个喊累的，也没有一个喊重复的。\n为什么呢？上瘾。\n其实，有瘾，才是快乐生活的关键。瘾，就是情趣，它会让每一个日子，像绽开的花朵，一寸一寸阳光踩过的花瓣，无论多重复，都会美得各不相同。\n（四）\n活得没滋味的时候，去坐坐北京地铁，从 1 号线到 15 号线，在上班的早高峰。\n你一下子就释然了。当然了，一下子也更崩溃了。\n密密麻麻的人，如雨前的蚁，簇拥着，没有喧闹，没有声响，是令人压抑的寂静。几乎不用走，“哗” 被推上车，“哗” 又被挤下车。就这样，每天，还未曾上班呢，两三个小时，先折耗在了路上。无论你蓄了多少激情和活力，也会被日复一日地磨蚀殆尽。关键是，还有下班呢，还有一个晚高峰等着呢。\n谁比谁活得更容易？\n但，即便这样，一定也有活得幸福的 “北漂”。幸福的人生活里不是没有不堪和琐碎，不是没有疲惫和失望，而是不管生活给了多大的泥淖，也要让生命拔腿出来，临清流，吹惠风，也要在心中修篱种菊，怡养内在的优雅和高贵。\n幸福是一种自我剥离的能力，以及自我生成的能力。生活中，没有多少幸福是现成的，有幸福的人，只是会幸福罢了。\n（五）\n一个整宿睡得很好的人，会嫉妒一个睡眠质量不怎么好、甚至半宿还会醒一会儿的人。乍听，简直不可思议。再解释，你就明白了。原来，那个睡得很 “好” 的人，是靠安定这种镇静药片睡过一个晚上又一个晚上的。\n如果不说透，从表面上看，应该是后者羡慕甚至嫉妒前者才是。因为，前者太好了，好得简直无与伦比。\n生活，有多少是我们看透了本质的。你羡慕的权贵，前呼后拥，看起来那么风光，可是风光背后有多少痛苦，对方不说，你不会知道；你羡慕的富有，宝马香车，锦衣玉食，看起来，是那么荣华，这荣华背后有多少痛苦，对方不说，你不会知道。\n也就是说，即便失点眠，你依然是那个睡得很好的人。即便过得平凡而宁静，你也会赢得别人羡慕。甚至，这里边，那些你羡慕着的人也在羡慕你。\n只是，你要知道，这个世界没有一个人愿把这种羡慕轻易告诉你。\n\n转载\n作者：马德，作家，已出版《当我放过自己的时候》、《允许自己虚度时光》等。\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"回归任务里的损失函数","url":"/2023/05/02/%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","content":"在搜广推相关业务中，除了 ctr、cvr 这类常规的二分类任务，还存在着预估\nstay_duration、LTV、ECPM、GMV 等一系列回归任务\nctr、cvr\n这类二分类任务常用的损失函数是交叉熵损失，基本假设是事件服从伯努利分布，最终学习的输出是正样本的比例，而回归任务中存在着非常多种的损失函数可选，如\nmse、mae、huber loss、log-normal、weighted logistics regression、softmax\n等\n每种损失函数都有其假设和适用范围，如果真实 label\n分布与假设差异较大，容易导致结果不佳，因此，本文会重点关注这些常见 loss\n的推导过程以及假设\n\nMSE/MAE/Huber\n这三个 loss 是回归任务中最直观也是最常见的\nloss，前两者都假设了误差服从某种特定分布，然后通过 MLE\n最终的 loss 形式，huber 则是前两种 loss 的混合版本\nMSE\n其假设是预估值和真实值的误差服从标准高斯分布，然后写出误差的似然 (likelihood) 函数，并通过\nMLE 推导出来最终的 loss 的形式，其推导过程如下图所示（来自 CS229 的教材）\n根据误差服从均值为 0 的高斯分布可以写出如下的似然函数\n\n然后根据 IID 的假设和 MLE，可以写出最终的损失函数\n\nMAE\n的推导跟 MSE 很类似，只是假设误差服从拉普拉斯分布，把拉普拉斯的概率密度函数替换成上面推导过程中的高斯分布，便能得到了最终\nMAE 的损失函数形式\n从 loss 函数形式，可以直观看到，对于第 \\(i\\) 个样本\n\nMSE 反向传播的梯度大小为 \\(-(y^{(i)} -\n\\theta^{T}x^{(i)})x^{(i)}\\)\n\nMAE 反向传播的梯度为 \\(-\n\\frac{y^{(i)} - \\theta^{T}x^{(i)}}{|y^{(i)} -\n\\theta^{T}x^{(i)}|}x^{(i)}\\) (其实就是 ±\\(x^{(i)}\\), 绝对值的求导可参考 绝对值的导数)\n\n从回传的梯度可知，MSE\n容易收到异常值的影响，比如说有个异常样本的 label\n非常大，则算梯度时 \\(y^{(i)} -\n\\theta^{T}x^{(i)}\\) 的值也会非常大，容易把样本带偏，而 MAE\n受到这一点影响会更小\n所以 Huber\nLoss 也是考虑到了这点，对于 label 较小的样本采用 MSE，label\n比较大的样本，则采用 MAE, 整个损失的形式如下图所示\n\nZILN(Log-normal)\n这是 google 一篇预估 LTV 的 paper 中提出来的 loss，A Deep Probabilistic Model for\nCustomer Lifetime Value Prediction\n现实任务的数据往往是长尾切稀疏的，拿 paper 里的 LTV\n任务为例，会有非常多的 0 值，也存在极端高的值，如下图所示是 paper\n里展示的 LTV 的分布（这里的 LTV 含义是首次购买后产生的价值，0\n值代表很多客户只购买了一次）\n\npaper 提到直接用 MSE 会有如下问题，其实也是上面提到了 MSE\n存在的问题\n\nMSE loss does not accommodate the significant fraction of\nzero value LTV from one-time purchasers and can be sensitive to\nextremely large LTV’s from top spenders\n\n所以 paper 提出了 ZILN (Zero-Inflated LogNormal) 的这个 loss，loss\n形式如下图所示（注意这里的 \\(x\\) 是 label)\n\\[\\begin{align} L_{ZILN}(x;p,\\mu,\\sigma)=\n-\\mathbf{1}_{x=0} \\log(1-p) - \\mathbf{1}_{x&gt;0}(\\log p -\nL_{Lognormal}(x;\\mu,\\sigma)) \\end{align}\\]\n这里的 loss 其实有 2 项，对应着 paper 里建模 LTV 分成了 2\n个任务，一个是预估购买概率（上面公式里的 \\(p\\)），另一个是预估购买金额，\n，第一个任务是常规的分类的 cross entropy\n损失；这种思想其实也比较常见，即引入中间信号建模，这样往往能让总体的效果更好，比如说把\nctr、cvr 分开建模，把 send2click 拆成 send2show 和 show2click\n各自建模等\n这里重点讲第二个任务，即预估金额的损失 \\(L_{Lognormal}(x;\\mu,\\sigma)\\)， 这个 loss\n的形式如下图所示，其实就是对 log-normal\n分布的概率密度函数取了 log 变换得到的，其隐含的假设是 LTV 服从\nlog-normal 分布，同时将 \\(\\mu\\) 和 \\(\\sigma\\) 参数化，即用一个 DNN\n来预估（注意这里的 \\(x\\) 仍然是\nlabel)\n\\[\\begin{align}\nL_{Lognormal}(x;\\mu,\\sigma)=\\log (x \\sigma \\sqrt{2\\pi})+\\frac{(\\log x -\n\\mu)^2}{2 \\sigma^2} \\end{align}\\]\n最终的模型会输出 3 个预估值 \\(p\\),\n\\(\\mu\\) 和 \\(\\sigma\\)，如下图所示\n\nlog-normal 形式的推导可以参考前面的 wiki 链接或者对数正态分布（Log-Normal\nDistribution），简单来看，当一个变量 \\(X\\) 服从 log-normal 分布时，其对数 \\(\\ln X\\) 服从正态分布，反之亦然\n相比于 MSE，log-normal 在预估值异常大时，loss\n也不会非常大，一定程度上缓解了前面提到的 MSE 的问题\n\nWeighted Logistics\nRegression\n这个 loss 在 Google 这篇 2016 的神文里被应用来预估用户的观看时长，Deep\nNeural Networks for YouTube\nRecommendations；在实际业界，也有非常多的基于这个思想的 variant\n和落地，相当于把回归任务转到了分类任务\n从名字大概能猜到 Loss 的形式了，在一个分类的 loss 上做\nreweight，具体的做法是使用 cross entropy\n这个损失函数，然后对于正样本（在建模观看时长中是点击的样本），使用具体的观看时长\n\\(T\\) 来做\nreweihgt，负样本不变\n做了 reweight\n后，相当于改变了原来的样本中的正负样本的比例（这里把观看的作为正样本，不观看 / 点击\n作为负样本），我们知道 cross entropy 最终预估的概率值是正样本的比例，而\nreweight 后，对于 label 为 \\(T\\)\n的样本，相当于在分类任务中变为了 \\(T\\)\n个正样本\n则总体样本的正样本的比例变成了 \\(\\frac{\\sum_{i=1}^k T_{i}}{N + \\sum_{i=1}^k T_{i}\n-k}\\), 其中 \\(N\\)\n是总样本数，\\(T_i\\)\n则是每个正样本的观看时长，\\(k\\)\n是正样本的数量（这里的推导过程跟 paper\n里推导过程不太一样，但原理和结果是一样的，会更直观一点）\n而根据 Logistics Regression 的预估值为正样本的比例可知\n\\[\\begin{align}\n\\frac{1}{1+e^{-logit}}=\\frac{\\sum_{i=1}^k T_{i}}{N + \\sum_{i=1}^k T_{i}\n-k} \\end{align}\\]\n由于正样本的数量往往非常少，因此 paper 把 \\(k\\) 省略掉，令 \\(P=\\sum_{i=1}^k T_{i}\\),\n则上面的式子变为了\n\\[\\begin{align}\n\\frac{1}{1+e^{-logit}}=\\frac{P}{N + P} \\end{align}\\]\n左边分子分母同时乘上 \\(e^{logit}\\),\n右边分子分母同时除以 \\(N\\),\n最终可以推导出 \\(\\frac{P}{N} =\ne^{logit}\\), 由于这里的 \\(P\\)\n是取了 sum 的，平均到每个样本上，最终 serving\n时预估的时长的值就是 \\(e^{logit}\\)\n则在训练时使用的损失函数还是 cross entropy，同时对正样本做\nreweight，serving 时得到 logit 后，取 \\(e^{logit}\\) 作为最终的预估值，paper\n里这张图很好地展示了这个过程\n\n上面的推导过程中忽略了原来样本中正样本的数量 \\(k\\)，从严格意义来讲，这会导致预估值有偏，所以最简单的方法是对于\nlabel 为 \\(T\\) 的样本，把它当做 \\(T\\) 个正样本和 1\n个负样本，相比于原来的做法，为原来每个正样本多增加了一个负样本\n则原来的公式会变成如下所示，等式右边的分母里，第一个 \\(k\\) 表示原来的正样本的数量，第二个 \\(k\\) 则表示新增的负样本的数量，这样就不用有\n“正样本的数量往往非常少可省略” 的假设了\n\\[\\begin{align}\n\\frac{1}{1+e^{-logit}}=\\frac{\\sum_{i=1}^k T_{i}}{N + \\sum_{i=1}^k T_{i}\n- k + k} \\end{align}\\]\n回到这个 loss 的假设，我们知道 cross entropy 的假设是 label\n服从参数为 \\(p\\) 的伯努利分布（这里的 p\n为成功的概率），然后通过 MLE 可求得 cross entropy 的形式\n而在 cross entropy 基础上做了如上的 reweight\n后，相当于是假设了 label 服从了参数为 \\(p\\) 的几何分布（不是非常严谨，因为这里的\n\\(p\\) 是失败的概率），reweight\n时使用的值（即 label \\(T\\)），就是几何分布的 pdf 取 log\n后提到前面的值，表示连续失败 \\(T\\)\n次（这里跟原来的几何分布的物理含义刚好相反）\nBucketing With Softmax\n在回归中，还有另一种套路就是分桶 + softmax，其思路也很直观，就是对\nlabel 的值域进行分桶，然后根据每个样本的 label\n把样本分到某个桶里，训练时转为一个多分类问题，通过 softmax\n损失函数进行训练\nserving 时利用 softmax\n预估的概率分布，对每个桶做加权求和，如下式所示，\\(n\\) 表示有 \\(n\\) 个桶，\\(p_i\\) 表示每个桶的概率，\\(v_i\\)\n则表示每个桶的值（往往会取桶的中值）\n\\[\\begin{align} pred = \\sum_{i=1}^{n} p_i\nv_i \\end{align}\\]\n这个方法的关键在于如何分桶，包括分桶的数量和每个桶的大小，这两个变量对最终的效果影响会比较大，分桶过多，容易导致每个桶的样本过于稀疏，而分桶过少，预估值有没有区分性\n一般的做法是人工根据 label\n的后验分布进行划分，且由于实际中的数据往往会比较长尾（即 label\n较小的样本会比较多），所以会在 label\n较小的时分桶数量较多，桶的间隔也会越小，目的还是让每个分桶的样本尽量均衡；但桶的数量和大小，是需要不断调整的超参\n除了直接使用桶的均值作为最终的加权求和项，还可以在每个桶都套上一个前面提到的损失如\nmse 等，不过这样就相当于是分区间的 multi-task 建模了，或者说是 ensemble\n中的 stacking 方法了\n在这类方法中，另一个常见的操作就是 ** label\nsmoothing**，这个方法的出发点是对于原来的方法丢掉了 label\n之间的大小关系，比如说把 label=50 的样本分到 [0,10] 的桶和 [51, 100]\n的桶的损失是没有区分性的，因此自然的一个想法是让原来 one-hot 的 label\n变得更加 smooth\n具体的思想是在训练时对 label\n做变换，让其变为类似高斯分布或拉普拉斯分布的形式，比如说原来的 label 是\n[0, 0, 0, 0, 1, 0, 0, 0], 对其进行 smooth 后会变为 [0, 0, 0.01, 0.03,\n0.9, 0.03, 0.02, 0.01], 而这里变换的方法也相当于一个超参了\n总的来说，这个方法涉及到比较多的先验知识，包括如何分桶，如何选择\nlabel smoothing 的函数；但是避免了对 label\n的先验假设，理论上适用于任意的回归任务，但是需要定期\nreview，防止总体数据有变，先验假设失效\nOrdinal Regression\nOrdinal\nregression\n是一种适用在关注不同值的序，但不怎么关注具体的值的场景，套用 wiki\n的话是这么说的\n\nvariable whose value exists on an arbitrary scale where only\nthe relative ordering between different values is significant.\nIt can be considered an intermediate problem between regression and\nclassification\n\n常见的任务比如说评级，例如针对图片、视频的色情程度做出评级，不直接采用分类的原因，跟上面提到的\nbucketing 方法里的 label smoothing 一样，分到不同的桶里没有区分性，\n具体的 loss 形式也是基于 MLE 来推导，但是不同于前面的算式基于\nPDF\n(Probability\nDensity Function\n) 做 MLE 的推导，这里采用的 CDF (Cumulative\nDistribution Function)，这里用了基于 CDF 的\nMLE，推导过程如下图所示，从推导过程可知，这里的推导的假设也是 “误差\n\\(\\epsilon\\)\n服从标准正态分布”，跟 MSE 一样\n\n用来区分样本处于不同区间的 \\(\\theta\\) 也是需要通过训练得到的参数，最终的\nserving 输出也是依赖 \\(\\theta\\)\n和具体的预估值， 这篇文章提供了一个实现参考：处理分级问题的利器 Ordinal\nRegression\n值得注意的是，对于评级任务，label\n是比较明确的，但是对于一般的回归任务，还是存在着划分区间，或者说给每个\nlabel 评级的过程，这个跟上面的 softmax 类方法一样，也是需要拍的超参\n小结\n本文主要描述了回归任务中常见的一些损失函数，每种损失函数背后的假设不相同，其适用范围也不一样\nMSE、MAE、Huber Loss\n是回归中比较常用和直观的回归损失，其假设是预估值和真实 label\n服从正态分布或拉普拉斯分布\nLog-normal 损失函数则直接假设 label 服从 log-normal 分布，通过 MLE\n去求解 log-normal 的概率密度函数中均值 \\(\\mu\\) 和方差 \\(\\sigma\\) 这两个参数（二分类中的 cross\nentropy\n的求解原理也是这样，只是分布改成了伯努利分布，参数改成了事件发生的概率\n\\(p\\)）\n除了直接预估，也有通过转为分类来间接预估最终的值的，主要有 2\n类方法（1）Weighted Logistics regression （2）Bucketing With Softmax\nWeighted Logistics regression 通过把一个 label 为 \\(T\\) 的样本当做 \\(T\\)\n个正样本（和一个负样本），在统计意义上推导保证了预估的无偏，在实际中应用也较为广泛，其背后的假设是\nlabel 服从几何分布\nBucketing With Softmax 则是通过对 label 进行先验分桶，然后通过\nsoftmax 预估 label\n落在每个桶的概率分布，最后对每个桶进行概率加权求和得到最终的预估值，这种方法的好处是对分布没有任何假设，理论上适用所有的分布，但效果非常依赖分桶的数量和桶大小，其中也涉及\nlabel smoothing、stacking 等改进技巧\nOrdinal Regression 则是一种关心序但不关心绝对值误差的回归\nloss，其假设跟 MSE 一样，但是使用了 CDF 而不是 PDF\n来推导最终的损失，并通过训练得到分割不同级别的阈值，一般在评级任务中较为常用；但是在更一般的回归任务中，还是会依赖人工划分区间给予不同样本不同的评级\n","categories":["机器学习"],"tags":["计算广告","机器学习"]},{"title":"大规模机器学习框架的四重境界","url":"/2018/03/10/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E5%9B%9B%E9%87%8D%E5%A2%83%E7%95%8C/","content":"文章为转载，原文链接见这里，作者是 carbon\nzhang。这篇文章主要介绍了分布式机器学习中的若干重点概念和经典论文，包括数据并行和模型并行、分布式框架的流派、参数服务器以及同步协议的演进等，非常值得一看。\n\n背景\n自从 google 发表著名的 GFS、MapReduce、BigTable\n三篇 paper 以后，互联网正式迎来了大数据时代。大数据的显著特点是大，哪里都大的大。本篇主要针对 volume 大的数据时，使用机器学习来进行数据处理过程中遇到的架构方面的问题做一个系统的梳理。\n有了 GFS 我们有能力积累海量的数据样本，比如在线广告的曝光和点击数据，天然具有正负样本的特性，累积一两个月往往就能轻松获得百亿、千亿级的训练样本。这样海量的样本如何存储？用什么样的模型可以学习海量样本中有用的 pattern？这些问题不止是工程问题，也值得每个做算法的同学去深入思考。\n简单模型 or 复杂模型\n在深度学习概念提出之前，算法工程师手头能用的工具其实并不多，就 LR、SVM、感知机等寥寥可数、相对固定的若干个模型和算法；那时候要解决一个实际的问题，算法工程师更多的工作主要是在特征工程方面。而特征工程本身并没有很系统化的指导理论（至少目前没有看到系统介绍特征工程的书籍），所以很多时候特征的构造技法显得光怪陆离，是否有用也取决于问题本身、数据样本、模型以及运气。\n在特征工程作为算法工程师主要工作内容的时候，构造新特征的尝试往往很大部分都不能在实际工作中 work。据我了解，国内几家大公司在特征构造方面的成功率在后期一般不会超过 20%。也就是 80% 的新构造特征往往并没什么正向提升效果。如果给这种方式起一个名字的话，大概是简单模型 + 复杂特征；简单模型说的是算法比如 LR、SVM 本身并不服务，参数和表达能力基本呈现一种线性关系，易于理解。复杂特征则是指特征工程方面不断尝试使用各种奇技淫巧构造的可能有用、可能没用的特征，这部分特征的构造方式可能会有各种 trick，比如窗口滑动、离散化、归一化、开方、平方、笛卡尔积、多重笛卡尔积等等；顺便提一句，因为特征工程本身并没有特别系统的理论和总结，所以初入行的同学想要构造特征就需要多读 paper，特别是和自己业务场景一样或类似的场景的 paper，从里面学习作者分析、理解数据的方法以及对应的构造特征的技法；久而久之，有望形成自己的知识体系。\n深度学习概念提出以后，人们发现通过深度神经网络可以进行一定程度的表示学习（representation\nlearning），例如在图像领域，通过 CNN 提取图像 feature 并在此基础上进行分类的方法，一举打破了之前算法的天花板，而且是以极大的差距打破。这给所有算法工程师带来了新的思路，既然深度学习本身有提取特征的能力，干嘛还要苦哈哈的自己去做人工特征设计呢？\n深度学习虽然一定程度上缓解了特征工程的压力，但这里要强调两点：\n\n缓解并不等于彻底解决，除了图像这种特定领域，在个性化推荐等领域，深度学习目前还没有完全取得绝对的优势；究其原因，可能还是数据自身内在结构的问题，使得在其他领域目前还没有发现类似图像 + CNN 这样的完美 CP。\n深度学习在缓解特征工程的同时，也带来了模型复杂、不可解释的问题。算法工程师在网络结构设计方面一样要花很多心思来提升效果。概括起来，深度学习代表的简单特征 + 复杂模型是解决实际问题的另一种方式。\n\n两种模式孰优孰劣还难有定论，以点击率预测为例，在计算广告领域往往以海量特征 + LR 为主流，根据 VC 维理论，LR 的表达能力和特征个数成正比，因此海量的 feature 也完全可以使 LR 拥有足够的描述能力。而在个性化推荐领域，深度学习刚刚萌芽，目前 google\nplay 采用了 WDL 的结构 [1]，youtube 采用了双重 DNN 的结构 [2]。\n不管是那种模式，当模型足够庞大的时候，都会出现模型参数一台机器无法存放的情况。比如百亿级\nfeature\n的 LR 对应的权重 w 有好几十个 G，这在很多单机上存储都是困难的，大规模神经网络则更复杂，不仅难以单机存储，而且参数和参数之间还有逻辑上的强依赖；要对超大规模的模型进行训练势必要借用分布式系统的技法，本文主要是系统总结这方面的一些思路。\n数据并行 vs 模型并行\n数据并行和模型并行是理解大规模机器学习框架的基础概念，其缘起未深究，第一次看到是在姐夫（Jeff\nDean）的 blog 里，当时匆匆一瞥，以为自己懂了。多年以后，再次开始调研这个问题的时候才想起长者的教训，年轻人啊，还是图样图森破。如果你和我一样曾经忽略过这个概念，今天不放复习一下。\n这两个概念在这个问题中沐帅曾经给出了一个非常直观而经典的解释，可惜不知道什么原因，当我想引用时却发现已经被删除了。我在这里简单介绍下这个比喻：如果要修两栋楼，有一个工程队，怎么操作？第一个方案是将人分成两组，分别盖楼，改好了就装修；第二种做法是一组人盖楼，等第一栋楼盖好，另一组装修第一栋，然后第一组继续盖第二栋楼，改完以后等装修队装修第二栋楼。咋一看，第二种方法似乎并行度并不高，但第一种方案需要每个工程人员都拥有 “盖楼” 和 “装修” 两种能力，而第二个方案只需要每个人拥有其中一种能力即可。第一个方案和数据并行类似，第二个方案则道出了模型并行的精髓。\n数据并行理解起来比较简单，当样本比较多的时候，为了使用所有样本来训练模型，我们不妨把数据分布到不同的机器上，然后每台机器都来对模型参数进行迭代，如下图所示\n\n\ndata parallel\n\n图片取材于 TensorFlow 的 paper[3]，图中 ABC 代表三台不同的机器，上面存储着不同的样本，模型\nP\n在各台机器上计算对应的增量，然后在参数存储的机器上进行汇总和更新，这就是数据并行。先忽略 synchronous，这是同步机制相关的概念，在第三节会有专门介绍。\n数据并行概念简单，而且不依赖于具体的模型，因此数据并行机制可以作为框架的一种基础功能，对所有算法都生效。与之不同的是，模型并行因为参数间存在依赖关系（其实数据并行参数更新也可能会依赖所有的参数，但区别在于往往是依赖于上一个迭代的全量参数。而模型并行往往是同一个迭代内的参数之间有强依赖关系，比如 DNN 网络的不同层之间的参数依照 BP 算法形成的先后依赖），无法类比数据并行这样直接将模型参数分片而破坏其依赖关系，所以模型并行不仅要对模型分片，同时需要调度器来控制参数间的依赖关系。而每个模型的依赖关系往往并不同，所以模型并行的调度器因模型而异，较难做到完全通用。关于这个问题，CMU 的 Erix\nXing 在这里有所介绍，感兴趣的可以参考。\n模型并行的问题定义可以参考姐夫的 [4]，这篇 paper 也是 tensorflow 的前身相关的总结，其中图\n\n\nmodel parallel\n\n解释了模型并行的物理图景，当一个超大神经网络无法存储在一台机器上时，我们可以切割网络存到不同的机器上，但是为了保持不同参数分片之间的依赖，如图中粗黑线的部分，则需要在不同的机器之间进行 concurrent 控制；同一个机器内部的参数依赖，即途中细黑线部分在机器内即可完成控制。\n黑线部分如何有效控制呢？如下图所示\n\n\nparameter rely\n\n在将模型切分到不同机器以后，我们将参数和样本一起在不同机器间流转，途中\nABC\n代表模型的不同部分的参数；假设 C 依赖 B，B 依赖 A，机器 1 上得到 A 的一个迭代后，将 A 和必要的样本信息一起传到机器 2，机器 2 根据 A 和样本对 P2 更新得到，以此类推；当机器 2 计算 B 的时候，机器 1 可以展开 A 的第二个迭代的计算。了解 CPU 流水线操作的同学一定感到熟悉，是的，模型并行是通过数据流水线来实现并行的。想想那个盖楼的第二种方案，就能理解模型并行的精髓了。\n\n\nparameter rely controller\n\n上图则是对控制模型参数依赖的调度器的一个示意图，实际框架中一般都会用 DAG（有向无环图）调度技术来实现类似功能，未深入研究，以后有机会再补充说明。\n理解了数据并行和模型并行对后面参数服务器的理解至关重要，但现在让我先荡开一笔，简单介绍下并行计算框架的一些背景信息。\n并行算法演进\nMapReduce 路线\n从函数式编程中的受到启发，google 发布了 MapReduce[5]的分布式计算方式；通过将任务切分成多个叠加的 Map+Reduce 任务，来完成复杂的计算任务，示意图如下\n\n\nMapReduce\n\nMapReduce 的主要问题有两个，一是原语的语义过于低级，直接使用其来写复杂算法，开发量比较大；另一个问题是依赖于磁盘进行数据传递，性能跟不上业务需求。\n为了解决 MapReduce 的两个问题，Matei 在 [6]中提出了一种新的数据结构 RDD，并构建了 Spark 框架。Spark 框架在 MR 语义之上封装了 DAG 调度器，极大降低了算法使用的门槛。较长时间内 spark 几乎可以说是大规模机器学习的代表，直至后来沐帅的参数服务器进一步开拓了大规模机器学习的领域以后，spark 才暴露出一点点不足。如下图\n\n\nspark\n\n从图中可以看出，Spark 框架以 Driver 为核心，任务调度和参数汇总都在 driver，而 driver 是单机结构，所以 spark 的瓶颈非常明显，就在 Driver 这里。当模型规模大到一台机器存不下的时候，Spark 就无法正常运行了。所以从今天的眼光来看，Spark 只能称为一个中等规模的机器学习框架。剧透一句，公司开源的\nAngel\n通过修改 Driver 的底层协议将 Spark 扩展到了一个高一层的境界。后面还会再详细介绍这部分。\nMapReduce 不仅是一个框架，还是一种思想，google 开创性的工作为我们找到了大数据分析的一个可行方向，时至今日，仍不过时。只是逐渐从业务层下沉到底层语义应该处于的框架下层。\nMPI 技术\n沐帅在这个问题中对 MPI 的前景做了简要介绍；和 Spark 不同，MPI 是类似 socket 的一种系统通信 API，只是支持了消息广播等功能。因为对 MPI 研究不深入，这里简单介绍下优点和缺点吧；优点是系统级支持，性能杠杠的；缺点也比较多，一是和 MR 一样因为原语过于低级，用 MPI 写算法，往往代码量比较大。另一方面是基于 MPI 的集群，如果某个任务失败，往往需要重启整个集群，而 MPI 集群的任务成功率并不高。阿里在 [7]中给出了下图：\n\n\nMPI\n\n从图中可以看出，MPI 作业失败的几率接近五成。MPI 也并不是完全没有可取之处，正如沐帅所说，在超算集群上还是有场景的。对于工业届依赖于云计算、依赖于 commodity 计算机来说，则显得性价比不够高。当然如果在参数服务器的框架下，对单组 worker 再使用 MPI 未尝不是个好的尝试，[7] 中的鲲鹏系统正式这么设计的。\n参数服务器\n历史演进\n沐帅在 [8]中将参数服务器的历史划分为三个阶段，第一代参数服务器萌芽于沐帅的导师 Smola 的\n[9]\n，如下图所示：\n\n\nparallel topic model\n\n这个工作中仅仅引入 memcached 来存放 key-value 数据，不同的处理进程并行对其进行处理。[10]中也有类似的想法，第二代参数服务器叫 application-specific 参数服务器，主要针对特定应用而开发，其中最典型的代表应该是 tensorflow 的前身 4。\n第三代参数服务器，也即是通用参数服务器框架是由百度少帅李沐正式提出的，和前两代不同，第三代参数服务器从设计上就是作为一个通用大规模机器学习框架来定位的。要摆脱具体应用、算法的束缚，做一个通用的大规模机器学习框架，首先就要定义好框架的功能；而所谓框架，往往就是把大量重复的、琐碎的、做了一次就不想再来第二次的脏活、累活进行良好而优雅的封装，让使用框架的人可以只关注与自己的核心逻辑。第三代参数服务器要对那些功能进行封装呢？沐帅总结了这几点，我照搬如下：\n1）高效的网络通信：因为不管是模型还是样本都十分巨大，因此对网络通信的高效支持以及高配的网络设备都是大规模机器学习系统不可缺少的；\n2）灵活的一致性模型：不同的一致性模型其实是在模型收敛速度和集群计算量之间做 tradeoff；要理解这个概念需要对模型性能的评价做些分析，暂且留到下节再介绍。\n3）弹性可扩展：显而易见\n4）容灾容错：大规模集群协作进行计算任务的时候，出现 Straggler 或者机器故障是非常常见的事，因此系统设计本身就要考虑到应对；没有故障的时候，也可能因为对任务时效性要求的变化而随时更改集群的机器配置。这也需要框架能在不影响任务的情况下能做到机器的热插拔。\n5）易用性：主要针对使用框架进行算法调优的工程师而言，显然，一个难用的框架是没有生命力的。\n在正式介绍第三代参数服务器的主要技术之前，先从另一个角度来看下大规模机器学习框架的演进\n\n\n框架演进\n\n这张图可以看出，在参数服务器出来之前，人们已经做了多方面的并行尝试，不过往往只是针对某个特定算法或特定领域，比如\nYahooLDA\n是针对 LDA 算法的。当模型参数突破十亿以后，则可以看出参数服务器一统江湖，再无敌手。\n首先我们看看第三代参数服务器的基本架构\n\n\nparameter server\n\n上图的 resource manager\n可以先放一放，因为实际系统中这部分往往是复用现有的资源管理系统，比如 yarn、mesos 或者 k8s；底下的 training\ndata 毋庸置疑的需要类似 GFS 的分布式文件系统的支持；剩下的部分就是参数服务器的核心组件了。\n图中画了一个 server group 和三个 worker\ngroup；实际应用中往往也是类似，server group 用一个，而 worker\ngroup 按需配置；server manager 是 server\ngroup 中的管理节点，一般不会有什么逻辑，只有当有 server\nnode 加入或退出的时候，为了维持一致性哈希而做一些调整。\nWorker group 中的 task\nschedule 则是一个简单的任务协调器，一个具体任务运行的时候，task\nschedule 负责通知每个 worker 加载自己对应的数据，然后去 server\nnode 上拉取一个要更新的参数分片，用本地数据样本计算参数分片对应的变化量，然后同步给 server\nnode；server\nnode 在收到本机负责的参数分片对应的所有 worker 的更新后，对参数分片做一次 update。\n这里存在的一个问题就是不同的 worker 同时并行运算的时候，可能因为网络、机器配置等外界原因，导致不同的 worker 的进度是不一样的，如何控制 worker 的同步机制是一个比较重要的课题。详见下节分解。\n同步协议\n本节假设读者已经对随机梯度优化算法比较熟悉，如果不熟悉的同学请参考吴恩达经典课程机器学习中对 SGD 的介绍，或者我之前多次推荐过的书籍《最优化导论》。\n我们先看一个单机算法的运行过程，假设一个模型的参数切分成三个分片 k1，k2，k3；比如你可以假设是一个逻辑回归算法的权重向量被分成三段。我们将训练样本集合也切分成三个分片 s1，s2，s3；在单机运行的情况下，我们假设运行的序列是（k1，s1）、（k2，s1）、（k3、s1）、（k1、s2）、（k2、s2）、（k3、s2）。。。看明白了吗？就是假设先用 s1 中的样本一次对参数分片 k1、k2、k3 进行训练，然后换 s2；这就是典型的单机运行的情况，而我们知道这样的运行序列最后算法会收敛。\n现在我们开始并行化，假设 k1、k2、k3 分布在三个 server\nnode 上，s1、s2、s3 分布在三个 worker 上，这时候如果我们还要保持之前的计算顺序，则会变成怎样？work1 计算的时候，work2 和 worker3 只能等待，同样 worker2 计算的时候，worker1 和 work3 都得等待，以此类推；可以看出这样的并行化并没有提升性能；但是也算简单解决了超大规模模型的存储问题。\n为了解决性能的问题，业界开始探索这里的一致性模型，最先出来的版本是前面提到的 9中的 ASP 模式，就是完全不顾 worker 之间的顺序，每个 worker 按照自己的节奏走，跑完一个迭代就 update，然后继续，这应该是大规模机器学习中的 freestyle 了，如图所示\n\n\nASP\n\nASP 的优势是最大限度利用了集群的计算能力，所有的 worker 所在的机器都不用等待，但缺点也显而易见，除了少数几个模型，比如 LDA，ASP 协议可能导致模型无法收敛。也就是 SGD 彻底跑飞了，梯度不知道飞到哪里去了。\n在 ASP 之后提出了另一种相对极端的同步协议 BSP，Spark 用的就是这种方式，如图所示\n\n\nBSP\n\n每个 worker 都必须在同一个迭代运行，只有一个迭代任务所有的 worker 都完成了，才会进行一次 worker 和 server 之间的同步和分片更新。这个算法和严格一致的算法非常类似，区别仅仅在于单机版本的 batch\nsize 在 BSP 的时候变成了有所有 worker 的单个 batch size 求和得到的总的 butch\nsize 替换。毫无疑问，BSP 的模式和单机串行因为仅仅是 batch\nsize 的区别，所以在模型收敛性上是完全一样的。同时，因为每个 worker 在一个周期内是可以并行计算的，所以有了一定的并行能力。\n以此协议为基础的 spark 在很长时间内成为机器学习领域实际的霸主，不是没有理由的。此种协议的缺陷之处在于，整个 worker\ngroup 的性能由其中最慢的 worker 决定，这个 worker 一般称为 straggler。读过 GFS 文章的同学应该都知道 straggler 的存在是非常普遍的现象。\n能否将 ASP 和 BSP 做一下折中呢？答案当然是可以的，这就是目前我认为最好的同步协议 SSP；SSP 的思路其实很简单，既然 ASP 是允许不同 worker 之间的迭代次数间隔任意大，而 BSP 则只允许为 0，那我是否可以取一个常数 s？如图所示\n\n\nSSP\n\n不同的 worker 之间允许有迭代的间隔，但这个间隔数不允许超出一个指定的数值 s，图中 s=3.\nSSP 协议的详细介绍参见 [11]，CMU 的大拿 Eric\nXing 在其中详细介绍了 SSP 的定义，以及其收敛性的保证。理论推导证明常数 s 不等于无穷大的情况下，算法一定可以在若干次迭代以后进入收敛状态。其实在 Eric 提出理论证明之前，工业界已经这么尝试过了\n顺便提一句，考察分布式算法的性能，一般会分为 statistical\nperformance 和 hard performance 来看，\n前者指不同的同步协议导致算法收敛需要的迭代次数的多少，后者是单次迭代所对应的耗时。两者的关系和 precision，就不赘述了。有了 SSP，BSP 就可以通过指定 s=0 而得到。而 ASP 同样可以通过制定 s = 无穷大来达到。\n核心技术\n除了参数服务器的架构、同步协议之外，本节再对其他技术做一个简要的介绍，详细的了解请直接阅读沐帅的博士论文和相关发表的论文。\n热备、冷备技术：为了防止 server\nnode 挂掉，导致任务中断，可以采用两个技术，一个是对参数分片进行热备，每个分片存储在三个不同的 server\nnode 中，以 master-slave 的形式存活。如果 master 挂掉，可以快速从 slave 获取并重启相关 task。\n除了热备，还可以定时写入 checkpoint 文件到分布式文件系统来对参数分片及其状态进行备份。进一步保证其安全性。\nServer node 管理：可以使用一致性哈希技术来解决 server\nnode 的加入和退出问题，如图所示\n\n\n一致性哈希\n\n当有 server node 加入或退出的时候，server\nmanager 负责对参数进行重新分片或者合并。注意在对参数进行分片管理的情况下，一个分片只需要一把锁，这大大提升了系统的性能，也是参数服务器可以实用的一个关键点。\n大规模机器学习的四重境界\n到这里可以回到我们的标题了，大规模机器学习的四重境界到底是什么呢？\n这四重境界的划分是作者个人阅读总结的一种想法，并不是业界标准，仅供大家参考。\n境界 1：参数可单机存储和更新\n此种境界较为简单，但仍可以使用参数服务器，通过数据并行来加速模型的训练。\n境界 2：参数不可单机存储，可以单机更新\n此种情况对应的是一些简单模型，比如 sparse logistic\nregression；当 feature 的数量突破百亿的时候，LR 的权重参数不太可能在一台机器上完全存下，此时必须使用参数服务器架构对模型参数进行分片。但是注意一点，SGD 的更新公式可以分开到单个维度进行计算，但是单个维度也是需要使用到上一轮迭代的所有参数 (即计算预测值 \\(f(w)\\))。而我们之所以对参数进行分片就是因为我们无法将所有参数存放到一台机器，现在单个 worker 有需要使用所有的参数才能计算某个参数分片的梯度，这不是矛盾吗？可能吗？\n答案是可能的，因为单个样本的 feature 具有很高的稀疏性（sparseness）。例如一个百亿 feature 的模型，单个训练样本往往只在其中很小一部分 feature 上有取值，其他都为 0（假设 feature 取值都已经离散化了）。\n因此计算 \\(f(w)\\) 的时候可以只拉取不为 0 的 feature 对应的那部分 w 即可。有文章统计一般这个级别的系统，稀疏性往往在 0.1%（or\n0.01%，记得不是很准，大致这样）以下。这样的稀疏性，可以让单机没有任何阻碍的计算 \\(f(w)\\)。\n目前公司开源的 angel 和 AILab 正在做的系统都处于这个境界。而原生 spark 还没有达到这个境界，只能在中小规模的圈子里厮混。Angel 改造的基于 Angel 的 Spark 则达到了这个境界。\n境界 3：参数不可单机存储，不可单机更新，但无需模型并行\n境界 3 顺延境界 2 二来，当百亿级 feature 且 feature 比较稠密的时候，就需要计算框架进入到这层境界了，此时单个 worker 的能力有限，无法完整加载一个样本，也无法完整计算 \\(f(w)\\)。怎么办呢？其实很简单，学过线性代数的都知道，矩阵可以分块。向量是最简单的矩阵，自然可以切成一段一段的来计算。只是调度器需要支持算符分段而已了。\n境界 4：参数不可单机存储，不可单机更新，需要模型并行\n进入到这个层次的计算框架，可以算是世界一流了。可以处理超大规模的神经网络。这也是最典型的应用场景。此时不仅模型的参数不能单机存储，而且同一个迭代内，模型参数之间还有强的依赖关系，可以参见姐夫对\ndistbelief 的介绍里的模型切分。\n此时首先需要增加一个 coordinator 组件来进行模型并行的 concurrent 控制。同时参数服务器框架需要支持 namespace 切分，coordinator 将依赖关系通过 namespace 来进行表示。\n一般参数间的依赖关系因模型而已，所以较难抽象出通用的 coordinator 来，而必须以某种形式通过脚本 parser 来生产整个计算任务的 DAG 图，然后通过 DAG 调度器来完成。对这个问题的介绍可以参考 Erix Xing 的分享。\nTensorflow\n目前业界比较知名的深度学习框架有 Caffee、MXNet、Torch、Keras、Theano 等，但目前最炙手可热的应该是 google 发布的 Tensorflow。这里单独拿出来稍微分解下。\n前面不少图片引自此文，从 TF 的论文来看，TF 框架本身是支持模型并行和数据并行的，内置了一个参数服务器模块，但从开源版本所曝光的 API 来看，TF 无法用来 10B 级别 feature 的稀疏 LR 模型。原因是已经曝光的 API 只支持在神经网络的不同层和层间进行参数切分，而超大规模 LR 可以看做一个神经单元，TF 不支持单个神经单元参数切分到多个参数服务器 node 上。\n当然，以 google 的实力，绝对是可以做到第四重境界的，之所以没有曝光，可能是基于其他商业目的的考量，比如使用他们的云计算服务。\n综上，个人认为如果能做到第四重境界，目前可以说的上是世界一流的大规模机器学习框架。仅从沐帅的 ppt 里看他曾经达到过，google 内部应该也是没有问题的。第三重境界应该是国内一流，第二充应该是国内前列吧。\n其他\n资源管理\n本文没有涉及到的部分是资源管理，大规模机器学习框架部署的集群往往资源消耗也比较大，需要专门的资源管理工具来维护。这方面 yarn 和 mesos 都是佼佼者，细节这里也就不介绍了。\n设备\n除了资源管理工具，本身部署大规模机器学习集群本身对硬件也还是有些要求的，虽然理论上来说，所有 commodity 机器都可以用来搭建这类集群，但是考虑到性能，我们建议尽量用高内存的机器 + 万兆及以上的网卡。没有超快速的网卡，玩参数传递和样本加载估计会比较苦逼。\n参考文献\n[1] Cheng H T, Koc L, Harmsen J, et al. Wide &amp; deep learning for\nrecommender systems[C]//Proceedings of the 1st Workshop on Deep Learning\nfor Recommender Systems. ACM, 2016: 7-10.\n[2] Covington P, Adams J, Sargin E. Deep neural networks for youtube\nrecommendations[C]//Proceedings of the 10th ACM Conference on\nRecommender Systems. ACM, 2016: 191-198.\n[3] Abadi M, Agarwal A, Barham P, et al. Tensorflow: Large-scale\nmachine learning on heterogeneous distributed systems[J]. arXiv preprint\narXiv:1603.04467, 2016.\n[4] Dean J, Corrado G, Monga R, et al. Large scale distributed deep\nnetworks[C]//Advances in neural information processing systems. 2012:\n1223-1231.\n[5] Dean J, Ghemawat S. MapReduce: simplified data processing on\nlarge clusters[J]. Communications of the ACM, 2008, 51(1): 107-113.\n[6] Zaharia M, Chowdhury M, Das T, et al. Resilient distributed\ndatasets: A fault-tolerant abstraction for in-memory cluster\ncomputing[C]//Proceedings of the 9th USENIX conference on Networked\nSystems Design and Implementation. USENIX Association, 2012: 2-2.\n[7] Zhou J, Li X, Zhao P, et al. KunPeng: Parameter Server based\nDistributed Learning Systems and Its Applications in Alibaba and Ant\nFinancial[C]//Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. ACM, 2017:\n1693-1702.\n[8] Li M, Andersen D G, Park J W, et al. Scaling Distributed Machine\nLearning with the Parameter Server[C]//OSDI. 2014, 14: 583-598.\n[9] Smola A, Narayanamurthy S. An architecture for parallel topic\nmodels[J]. Proceedings of the VLDB Endowment, 2010, 3(1-2): 703-710.\n[10] Power R, Li J. Piccolo: Building Fast, Distributed Programs with\nPartitioned Tables[C]//OSDI. 2010, 10: 1-14.\n[11] Ho Q, Cipar J, Cui H, et al. More effective distributed ml via a\nstale synchronous parallel parameter server[C]//Advances in neural\ninformation processing systems. 2013: 1223-1231.\n","categories":["机器学习"],"tags":["机器学习","转载","分布式"]},{"title":"大模型技术报告解读","url":"/2025/02/23/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A/","content":"今年的春节，deepseek\n把大模型的讨论热度推向了高潮，即使是在十八线小城的年味里，也藏着些意料之外的科技褶皱。表妹用方言对着手机喊 “给俺写段拜年词”，小侄子在忙着跟豆包里的声音温柔、善解人意的 “校花” 聊天，连巷口的春联摊主都学会了用生成式设计定制烫金纹样 —— 这些烟火气里的数字涟漪，像一场无声的启蒙运动，将 “大模型” 三个字编织进了这座十八线小城的毛细血管\n两年前的 AI 还像一座青铜巨像，吞吐数据时浑身震颤着算力的轰鸣，只能在北上广深的数据中心里吞吐星辰；而今的大模型已化作游走的溪流，沿着 5G 基站浸润到县城修车铺的扫码系统、揉进快手主播的方言脚本，甚至蛰伏在老人机的语音助手里咳嗽一声提醒吃药。从 “暴力美学” 的千亿参数军备竞赛，到\nMoE\n架构轻巧切开算力蛋糕的刀锋，从耗资数千万美元的实验室贵族，到 DeepSeek-R1 用 600 万美元训练成本撕开的平民入场券 —— 这场进化不仅是技术的跃迁，更是科技叙事从 “神坛独白” 转向 “人间对话” 的隐喻：当大模型学会在显卡残骸上跳成本最优化的芭蕾，技术的毛细血管终于触到了烟火人间的心跳\n\n上面这段话是 deepseek 生成的，不用多复杂的\nprompt，也不用反复调试，deepseek\n就那么 “毫不费力” 地生成了一段诗意与写实的文字；而更恐怖的是，这还不是\ndeepseek-R1 的卖点：低成本但足以媲美 openAI-o1 的推理能力\n这段时间试用下来，相较于两年前，大模型的确有了很大的使用体验的改进，不再是一个生硬的文字排列组合机器；除了众所周知的数据 + 算力的 “力大砖飞” 的技巧，技术上有了什么样的进步和革新？最近听的这期播客《89.\n逐句讲解 DeepSeek-R1、Kimi K1.5、OpenAI\no1 技术报告 ——“最优美的算法最干净”》，就介绍了三个比较有代表性的大模型报告，本文是学习后的文字版，祝开卷有益～\nOpenAI-o1\n报告是 24 年 9 月 12 日发布的，Learning\nto reason with LLMs；主要是详细介绍了在各个 benchmarks 上，o1\n效果非常好（比 GPT-4o 要好很多）\n\n其中，思维链（Chain of Thought）应该是推理模型相较于前 2\n年前改进最大的地方之一了；简单来说就是在生成答案前会进行长时间的内部思考，通过多步骤推理、错误修正和策略调整优化答案质量，这种机制类似于人类 “系统 2” 的慢思考模式，显著提高了逻辑严谨性\n报告称 “This process dramatically improves the model’s ability to\nreason”\n\nIt learns to recognize and correct its mistakes. It learns to break\ndown tricky steps into simpler ones. It learns to try a different\napproach when the current one isn’t working.\n\n思维链的底层技术就是强化学习了，报告显示这种思维链的能力随着训练时算力和推理时思考时间的增加而持续提升（如下图所示），呈现出类似 AlphaGo 的 “推理扩展定律”（Inference\nScaling Law），也有点类似 LLM 在 pretrain\n阶段的随着数据量增加而效果变好的 scaling law\n\n报告做了很多性能表现与基准测试，显示思维链的优势，但 o1 的思考过程对外不可见，仅展示摘要，OpenAI 认为这有助于监控模型的安全性（如防止用户操控）\n至于具体实现算法细节，openAI\n报告里没怎么公开；外界的当时不少猜测是基于过程奖励模型（PRM），对推理过程中的每一步进行细粒度监督，而非仅评估最终结果（ORM），PRM\n这类方法最早在 openAI 的论文 Let's Verify Step by\nStep有所提及，简单来说 ORM 仅对最终结果提供反馈，PRM\n则对每个推理步骤提供反馈。报告显示在训练模型解决 MATH\n数据集中的问题时，显著优于结果监督\n但在下面要重点介绍的 DeepSeek 的报告中，显示了 PRM\n在实际中的落地的难点，是一个不太成功的尝试\nopenAI 的报告很短，没有太多的有效信息（毕竟已经是 closeAI\n了。。）\nDeepSeek\nDeepSeek-R1:\nIncentivizing Reasoning Capability in LLMs via Reinforcement\nLearning中，介绍了 DeepSeek-R1-Zero 与 DeepSeek-R1 两个模型\nDeepSeek-R1-Zero 是直接在基础模型上应用大规模 RL 训练，而不需要 SFT\n(Supervised Fine-Tuning)，这个此前在 LLM、甚至可追溯到 16\n年刚刚兴起的深度学习领域中，被认为是 common sense\n的技巧：在通用预训练模型的基础上，使用特定任务的有标签数据对模型参数进行微调，从而优化模型在该任务上的表现\n而这也是公开的研究报告中，第一次展示了 LLM 的推理能力通过纯 RL\n也能够取得非常不错的效果，这里更重要的意义，也许是为各个团队在攻克 o1\n给世界的谜题，给出了一个更清晰的解题思路；正如 paper 里所说的 “it is the\nfirst open research to validate that reasoning capabilities of LLMs can\nbe incentivized purely through RL, without the need for SFT. This\nbreakthrough paves the way for future advancements in this area“\n在讨论报告里提到的强化学习算法之前，先看下此前被广为流传的\nRLHF (Reinforcement Learning from Human Feedback) 算法的流程，再深入了解\nDeepSeek 在此基础上的相关改进\nRLHF\nRLHF 分为 3 个阶段：SFT，RM 和 RL，如下图所示（图片来自什么是\nRLHF？）\n\n（1）监督微调（SFT） 阶段\n就是上面提到的 SFT\n过程，基于标注数据调整预训练模型，使其初步适应特定任务（如对话、指令遵循）\n具体的做法就是使用高质量的人工标注数据（如人工编写的问答范例）对预训练模型进行微调；训练方式与预训练类似，采用自回归的下一词预测目标（如交叉熵损失）\n（2）奖励模型（RM）阶段\n这个阶段是构建一个能模拟人类偏好的打分模型，量化生成内容的质量\n具体的做法是通过 SFT\n模型生成同一指令的多个回答，由人工对回答进行排序或打分（如偏好排序）；这个流程往往也是提升模型的​安全性​（避免有害内容）和​价值观对齐​（符合伦理偏好）所依赖的环节，即需要引入人工的的纠偏和勘误\n然后在原始的基座模型技术上，修改输出和损失函数（eg，对比损失，\npairwise ranking\nloss），训练一个回归模型（RM），输入指令和回答，输出标量的奖励值\n（3）强化学习（RL）阶段\n最后的阶段就是利用 RM 的奖励信号优化 SFT\n模型，使其生成更高奖励的响应，通过强化学习算法（如\nDPO、GRPO、PPO）调整模型参数，最大化预期奖励值\n以 PPO (Proximal Policy Optimization)\n算法为例，RL 阶段的主要流程就是下图的 Step3\n\n下面着重讲一下原始的 PPO\n算法流程，这也是训练算法涉及到的主要理论部分；PPO 共有 4 个模型：Actor\nModel、Critic Model、Reward Model 和 Reference Model\n\nActor Model（策略模型）\n\n这个模型的功能是生成语言模型的输出（如对话中的\nresponse），是训练的主要目标；通常会基于上面步骤（1）的 SFT\n模型初始化，然后通过策略梯度更新参数，最大化奖励信号同时避免偏离参考模型即\nReference Model\n\nReference Model（参考模型）​\n\n这个模型也是通过 SFT\n模型初始化的，且参数是固定不更新的，相当于 Actor\n的初始版本，用于计算 KL 散度约束，防止 Actor\n在训练中过度偏离原始分布，用与缓解奖励作弊（Reward\nHacking）和灾难性遗忘（Catastrophic Forgetting）\n奖励作弊主要是指模型通过偏离合理策略（如生成编造虚假信息），来提高奖励分数；灾难性遗忘则是指模型在优化新任务时，丢失了预训练阶段学到的通用知识；具体的生效方式，都是通过\nKL 散度来约束 Actor Model 和 Reference Model 的输出差异不会过大\n\n​Reward Model（奖励模型）​\n\n这个模型就是上面提到的阶段（2）中训练出来的模型，其功能是对生成的\nresponse\n进行实时打分，提供即时奖励信号；​训练方式通过人类标注的偏好对（good/bad\nresponse pairs）进行对比学习，跟 Reference Model\n类似，参数在训练过程也是被冻结不更新的\n\n​Critic Model（评论家模型）​\n\nReward Model 预测的是即时收益，Critic Model 预测的则是当前状态（如\nprompt + response）的长期收益（即价值函数），用于计算优势函数（Advantage\nFunction），指导 Actor 的优化方向\n这里提到的 “优势函数” 的计算方法因具体强化学习算法而异，但其核心思想是衡量特定动作相对于策略平均表现的优劣程度，以\nPPO 算法为例，优势函数的定义如下\n\\[A(s,a)=Q(s,a)−V(s)\\]\n\n\\(​V(s)\\)：状态价值函数 (State-Value\nFunction)，表示在状态 s 下按当前策略执行所有动作的平均预期回报（在 PPO 中由\nCritic Model 预估当前时刻的未来预期 reward 得到的）\n\\(Q(s,\na)\\)：动作价值函数 (Action-Value\nFunction)，表示在状态 s 下执行动作 a 后的预期总回报（包含未来奖励的折现），可以直观拆解为当前执行动作后的即时收益和下一时间的状态价值（在\nPPO 中是 ​Reward Model 预估的即时 reward 和 Critic Model\n预估下一时刻的未来 reward 加和得到的）\n\n\n则当 \\(A(s,a)&gt;0\\)，说明动作 a\n优于平均表现，反之差于平均\n关于这个函数为什么会是这个形式，这篇文章给了一个比较通俗的例子《人人都能看懂的 RL-PPO 理论知识》（ps，这篇文章整体也非常推荐一读）\n\nCritic Model 的角色跟 Reward Model 有点像（实际上，在后来的 GRPO\n算法中也把 Critic Model 给去掉了），但 Critic Model\n的参数是更新的，由于是预测价值，因此更新的方式也是基于 mse 这一类 loss\n来最小化实际收益和预测收益差异；\n因此， PPO 的一轮训练过程是这样\n1、Actor 根据当前策略生成 response，Critic 预测其价值，Reward Model\n计算即时奖励\n2、Actor 基于上面提到的优势函数（Advantage）\\(A(s,a)\\) 和 KL 散度约束，通过 PPO-Clip\n机制优化策略\n3、Critic 则根据实际回报（Return）和预测值（Value）的误差调整参数\n关于 Actor Loss 和 和 Critic Loss 《图解大模型 RLHF 系列之：人人都能看懂的 PPO 原理与源码解读》，文章给出了代码示例，整篇文章也推荐一读\nDeepSeek-R1-Zero: Pure RL\nwith GRPO\n虽然前面提到的 PPO 的范式中的 Actor-Critic 范式看起来似乎比较合理，但\nCritic\n训练的准确性，其实是没法保证的，所以当价值函数不够准确时，与其用一个错误的价值评估，还不如直接舍弃掉这个评估\nGroup Relative Policy\nOptimization（GRPO）就是这么做的，GRPO 是一种在 DeepSpeek-v2\n中提出的强化学习算法，前面提到 Critic Model 的角色跟 Reward Model\n有点像，而在 GRPO 算法中其实是 Critic Model 去掉了，那 GRPO\n是怎么做的？简单来说就是通过多次采样同一问题的不同回答，直接利用组内奖励的统计特性（如均值、方差）计算相对优势，GRPO\n的跟 PPO 的区别如下\n\n可以看到，GRPO 没有了 Value Model (也就是 Critic\nModel)，取而代之的是每次生成 \\(G\\) 个\noutput，然后分布算出这 \\(G\\) 个 output\n的 reward，计算方式如下所示\n\n得到了 \\(G\\) 个 reward 后，通过\nGroup Computation 计算出各个 reward 优势函数 \\(A_i\\)，计算方法就是基于 group\n内的均值和方差来计算，paper 里分成了两种类型：Outcome Supervision RL 和\nProcess Supervision\nRL（一个关注最终结果，一个关注过程每个步骤），但计算方式都差不多\n\n则 policy model 的损失函数可以写成下面的形式\n\n则通过 GRPO 训练的整体算法流程如下所示\n\nReward Modeling\n虽然取消 Critic Model 保留了 Reward Model，但在 DeepSeek-R1 使用的\nReward Model 也不像前面 PPO 方法中提到的基于模型预估的，而是采用了\nrule-based 的 reward modeling，paper 中采用了两种 rule-based 的\nreward：accuracy reward 和 format reward；而采用 rule-based\n的原因是如果还是基于预估值的 reward modeling 的方式，在这种大规模的 RL\n训练中，很容易出现 reward hacking 的问题（实际上所有基于预估来获取\nreward 的方式都会有这样的问题），同时也会让训练变得更加复杂\n\nDeepSeek-R1:\nUser-Friendly Model with Cold Start\n尽管通过 GRPO 算法训练获得了推理能力的显著提升，DeepSeek-R1-Zero\n在评测上效果还不错，但是在实际使用时，仍存在一些缺陷。一方面，R1-Zero\n生成的推理内容可读性欠佳，语言混合的情况时有发生，这使得其推理过程难以被清晰理解。另一方面，由于直接在基础模型上进行强化学习，没有前期的引导，训练初期模型的表现不稳定，收敛速度较慢\n\nDeepSeek-R1-Zero encounters challenges such as poor readability, and\nlanguagemixing. To address these issues and further enhance reasoning\nperformance, we introduce DeepSeek-R1, which incorporates a small amount\nof cold-start data and a multi-stage training pipeline.\n\n为了解决这些问题，DeepSeek-R1 在 R1-Zero\n的基础上进行了如下 4 点改进\n\n(1)Specifically, we begin by collecting thousands of cold-start data\nto fine-tune the DeepSeek-V3-Base model\n(2)Following this, we perform reasoning-oriented RL like\nDeepSeek-R1-Zero\n(3)Upon nearing convergence in the RL process, we create new SFT data\nthrough rejection sampling on the RL checkpoint, combined with\nsupervised data from DeepSeek-V3 in domains such as writing, factual QA,\nand self-cognition, and then retrain the DeepSeek-V3-Base model\n(4)After fine-tuning with the new data, the checkpoint undergoes an\nadditional RL process, taking into account prompts from all\nscenarios.\n\n（1）冷启动\nDeepSeek-R1 收集了数千条长思维链（CoT）数据来微调 DeepSeek-V3-Base\n模型，并在这之后进行上面提到的 RL\n训练过程。这些数据通过多种方式获得，如少样本提示、引导模型生成带反思验证的答案等。冷启动数据提升了模型输出的可读性，并融入人类先验知识，为模型提供了更好的训练起点；在微调后再采用上面的训练\nDeepSeek-R1-Zero 的方式，整体的效果是要优于纯 RL\n训练得到的 DeepSeek-R1-Zero\n（2）语言一致性奖励\n针对 DeepSeek-R1-Zero 推理中语言混合的问题，DeepSeek-R1\n在强化学习训练时引入语言一致性奖励。这个奖励是依据思维链（CoT）中目标语言单词的比例计算，比例越高奖励越高。虽然这会使模型性能稍有下降，但能有效缓解语言混合现象，让推理过程更符合人类阅读习惯\n（3）拒绝采样（Rejection\nSampling）与监督微调 (SFT)\n前面两个步骤都把重点放在了 reasoning\n上，在这一步则是搜集各种数据来提升模型的一般能力如 writing, role-playing\n等（可能也侧面说明了在最终商用的模型中，SFT 这一环节还是少不了的）\n具体包含了两部分数据：推理数据（Reasoning data） 和\n非推理数据（Non-Reasoning data）, 拒绝采样（Rejection\nSampling）的含义指的是生成多个候选输出，并根据特定标准筛选出最优样本，从而提升数据的质量。例如，生成 10 条可能的推理路径后，仅保留最符合逻辑或最接近正确答案的样本\n对于推理数据的搜集，一部分来自前面训练好的模型中，从中得到 reasoning\ntrajectories，然后使用基于规则的简历来做拒绝采样（eg. 答案是否符合预设格式、最终结果是否正确）；另一部分则是引入生成式奖励模型（generative\nreward\nmodel），即判断模型的推理逻辑是否与人类标注的参考答案（ground-truth）匹配，这部分是通过将模型生成的预测与真实答案（ground-truth）输入 DeepSeek-V3，评估逻辑一致性和语义匹配度；整体的推理数据过滤得到了大概\n600k 的数据\n\nWe curate reasoning prompts and generate reasoning trajectories by\nperforming rejection sampling from the checkpoint from the above RL\ntraining. In the previous stage, we only included data that could be\nevaluated using rule-based rewards. However, in this stage, we expand\nthe dataset by incorporating additional data, some of which use a\ngenerative reward model by feeding the ground-truth and model\npredictions into DeepSeek-V3 for judgment. Additionally, because the\nmodel output is sometimes chaotic and difficult to read, we have\nfiltered out chain-of-thought with mixed languages, long parapraphs, and\ncode blocks.\n\n对于非推理数据，如写作、事实性问答等，复用 DeepSeek-V3 的部分 SFT\n数据集，共收集约 200k 样本\n利用这些总计约 800k 样本，对基础模型即 DeepSeek-V3-Base 进行两个\nepoch 的微调，同时提升了模型的基础推理能力和通用任务上的能力（MoE\n的体现之一）\n（4）全场景强化学习\n这一步主要是使模型与人类的偏好更一致（如无害性、实用性），在步骤（3）的模型基础上进行的额外的强化学习的训练。\n为了达到无害性，需要通过关注推理过程（reasoning\nprocess）和最终的结果（summary），识别并移除任何可能的有害的内容）；而实用性则主要关注最终的结果（summary）的有效性；具体做法\npaper 了没提，关键点是奖励信号的设计和组合（eg.\n对于无害性，需要通过检测推理过程或最终的结果中是否含有暴力、歧视性词汇，进而确定具体\nlabel）\nDistillation\n报告也证实了一个关于蒸馏的 “常识”：即可以把 DeepSeekd\n模型的推理能力蒸馏到其他模型，得到的效果会比原来的模型以以及公开的蒸馏模型要更好，如下图所示\n\n另外，在模型参数相同情况下，蒸馏出来的模型比起只在模型上使用\nDeepSeek-R1-Zero 的训练流程得到的效果要更好，同时使用的算力要更小，\n如下图所示，这能降低模型商用的成本的一个重要途径\n\ndistilling more powerful models into smaller ones yields excellent\nresults, whereas smaller models relying on the large-scale RL mentioned\nin this paper require enormous computational power and may not even\nachieve the performance of distillation\n\n\nUnsuccessful Attempts\n这部分主要提到了两个不成功的尝试：Process Reward Model (PRM) 和 Monte\nCarlo Tree Search (MCTS)\nProcess Reward Model (PRM)\n：这部分主要是指过程奖励模型在实际中比较难落地，paper\n提到主要有以下几点原因：一是把定义步骤会比较困难（即把推理过程拆解成各个步骤），二是为每个步骤定义一个\nreward 也会比较困难（模型打分不考虑，人工标注不现实），三是如果 PRM\n基于预估得到每个步骤的 reward，不可避免地会出现 reward hacking\n地问题，原因跟上面提到的一样：所有基于预估来获取 reward\n的方式都不可避免会有这样的问题\nMonte Carlo Tree Search (MCTS) ：蒙特卡洛树搜索，这个在 AlphaGo\n中关键的方法，未被证明在大模型的训练中同样有效，其主要原因还是搜索空间过大和以及价值模型训练困难，paper\n里是这么做的\n\nMCTS involves breaking answers into smaller parts to allow the model\nto explore the solution space systematically. To facilitate this, we\nprompt the model to generate multiple tags that correspond to specific\nreasoning steps necessary for the search. For training, we first use\ncollected prompts to find answers via MCTS guided by a pre-trained value\nmodel. Subsequently, we use the resulting question-answer pairs to train\nboth the actor model and the value model, iteratively refining the\nprocess.\n\nMCTS\n需要对每一步生成的可能路径进行多次模拟和评估。在下棋中这个可能的路径数量是有限的，但在自然语言生成中，每个 token 的选择涉及数千甚至数万种可能性，导致搜索空间呈指数级增长。例如，一个包含 100 个 token 的句子，若每个 token 有 100 种候选词，总路径数为\n100^100，远超可行计算范围；当然也可以提前做剪枝之类的，但 paper\n提到了这容易导致模型学不好（getting stuck in local optima）\n另外 MCTS\n也需要训练一个价值模型来评估中间步骤，这个模型训练会比较困难，主要是奖励信号比较难定义（跟前面\nPRM\n提到的原因一样）；所以一般的经验是 MCTS 更适合封闭式、目标明确的任务（如解题、代码生成），但在开放域生成（如创意写作、多轮对话）中效果有限\nKimi\nKimi 的这篇报告 KIMI\nK1.5:SCALING REINFORCEMENT LEARNING WITH LLMS，整体的方法跟\nDeepSeek-R1 比较相似，可以说是殊途同归了\n两篇报告都显示通过 RL 推动模型自主生成思维链（Chain-of-Thought,\nCoT），出现了如自我验证、反思等行为；同时能够显著提升模型推理能力，尤其在数学、编程等复杂任务中\n同时两者均通过蒸馏技术将大模型能力迁移至小模型；Kimi 提出 Long2Short 优化（如模型合并、DPO 训练），DeepSeek-R1 直接使用大模型生成数据微调小模型\n但 Kimi 的训练流程跟 DeepSeek-R1 不太一样，Kimi\n整体还是采用多阶段训练（预训练→监督微调→长链 SFT→RL），不像 DeepSeek\n做了纯 RL 的探索；另外，Kimi 这篇报告中提到了非常多的训练细节，这也是\nDeepSeek 和 OpenAI\n中没有提及的，这部分对其他团队复现也提供了非常多有用的信息\n报告提到通过 “Long context scaling” + “Improved policy\noptimization”，就能取得不错的效果，而不用依赖一些比较复杂的技术，如上面提到的无效的探索蒙特卡洛树搜索（Monte\nCarlo tree search） 和 过程奖励模型（process reward models），在 Kimi\n中也并没有被采用\nLong context scaling\n这里的 Long，指的是将 RL 上下文窗口扩展至 128k\ntokens，支持生成更长的思维链（Chain-of-Thought,\nCoT），提升复杂推理能力\n报告里提到一个概念：RL prompt\nset，这是一组经过精心筛选的问题（如数学题、编程题、逻辑推理题等），用于引导模型在强化学习过程中生成长链推理（CoT），也是重要的训练数据来源；这部分数据需要满足以下约束\n\n多样性覆盖：覆盖 STEM、编程、通用推理等多领域，确保模型适应不同任务\n\n可评估性：答案可通过规则（如数学公式验证）或测试用例（如编程题）客观验证，排除易被猜测的问题（如选择题）\n平衡的难度：包含简单、中等、困难的问题，促进模型逐步学习复杂推理\n\n具体的构建方法如下\n（1）自动筛选；通过模型自评估（如 SFT 模型生成多次答案，计算通过率）确定问题难度；通过过滤易被\nReward Hacking\n的问题，例如排除答案格式简单或可通过随机猜测解决的问题\n（2）标签系统；使用领域标签（如数学、编程）和难度标签（易 / 中 / 难）分类问题，确保训练数据的均衡分布。如数学竞赛题标记为 “高难度”，小学数学题标记为 “低难度”\n（3）验证与排除；若模型无需推理即可在多次尝试中猜中答案（如 8 次尝试内成功），则该问题被排除；避免包含需要主观判断的问题（如证明题），以保持评估的客观性\n长链 SFT（Long-CoT Supervised Fine-Tuning）的训练数据，就是基于 RL\nprompt set\n中的问题，通过人工或自动生成高质量的 CoT 标注（例如通过拒绝采样或人工精修）得到的，这部分跟\nDeepSeek-R1 中的冷启动部分数据作用基本一致\nImproved policy\noptimization.\nPolicy Optimization\n报告提出基于 Online Policy Mirror\nDescent 的一个变种算法，其做法跟上面提到的 GRPO\n也比较相似，同样去掉了 Critic model，其中 \\(r\\) 是 reward model，\\(\\pi\\) 是 actor model (报告里也成为 policy\nmodel) 和 reference model\n\n上面公式（3）显示的最终的函数跟 GRPO 也比较相似，用作 loss 加权的\nreward 是由当前 \\(k\\) 个 采样的\nresponse 的 reward 取均值得到，通过有一个 KL 散度约束 actor model 和\ncritical model 的输出的 diff\nLength Penalty\nkimi 1.5 引入长度惩罚（Length Penalty），这是在前面 openAI 和\ndeppSeek 中都没提及的技术，简单来说，就是随着回答长度的增加，边际\nroi 在逐步下降（模型效果 / 训练成本）。因此，训练的过程引入了一个 length\nreward 来约束回答的长度，在 \\(k\\)\n个采样的 response\n中计算最大长度和最短长度，然后通过下面的方式对回答长度短的回答增加一个额外的\nreward\n\nSampling Strategies\n除了 RL 本身的采样策略，paper 也提了两种额外的采样策略：Curriculum\nSampling 和 Prioritized Sampling\nCurriculum Sampling 指的是先训练简单任务，再训练困难任务（基于 RL\nprompt set 区分问题的难度）；其原因是 RL\n模型再最开始的效果不好，如果把同等的训练资源分配到困难任务上，获得的效果不佳，导致训练效率低下\nPrioritized Sampling\n可理解为会增加对困难任务的采样概率，即如果一个问题的成功率是 \\(s_i\\)，则采样概率是 \\(1 − s_i\\)\nLong2short\n这部分的作用跟蒸馏比较类似，都是把一个大模型的能力迁移到小的模型上，这里的\nlong 和 short 分别指 long-CoT models 和 short-CoT models\n为了达到这个目标，报告中主要采用了 4 种方法： model merging, shortest\nrejection sampling, DPO 和 long2short RL\n\nmodel merging：对两个模型的参数做平均\n\nshortest rejection sampling：对同一个问题采样 n 次（n =\n8），然后选择最短的回答做 SFT\nDPO：同样对一个问题采样 n 次，然后选择最短且正确的回答作为 positive\nsample，长的回答（短回答长度的 1.5 倍及以上）作为 negative\nsample（无论是否 correct）\n\nlong2short RL：基于训练好的一个模型，采用上面提到的 length penalty\n再进行一轮 RL 的训练\n\n小结\n当强化学习的梯度在 Transformer\n架构中奔涌，一场关于推理能力的静默革命正在重塑大模型的进化轨迹。从 OpenAI-o1 在思维链中构建的 \"系统 2\" 慢思考范式，到 DeepSeek-R1 用纯强化学习突破 SFT 依赖的惊险一跃；从过程奖励模型 (PRM) 对推理步骤的原子化监督，到 GRPO 算法抹除价值函数后展现的群体相对优势 —— 这些技术突破不仅解构了 \"暴力美学\" 时代的千亿参数迷信，更在算力与算法的二重奏中谱写出新的可能性。当 DeepSeek 用规则化奖励模型破解强化学习的奖励欺骗困局，当模型蒸馏将推理能力压缩进更轻量的架构，这场技术进化正在证明：大模型的核心竞争力，或许不在于吞噬多少星辰般的数据，而在于如何用更优雅的算法，在显卡残骸上搭建起通向量产智慧的阶梯\n上面这段文字，也是让 deepseek\n基于这篇文章内容生成了一段总结；看完后还是不禁感慨大模型的能力令人惊艳。而无论拒绝或接受，这场大模型革命正以比蒸汽机更温柔的轰鸣重塑文明肌理；当 18 世纪的纺织工人惊恐地看着珍妮纺纱机，他们不会想到机械臂终将托起整个工业文明；而今面对语言模型中涌现的推理能力，我们比先辈更幸运 —— 每个人都能在开源社区的算力碎片里拾取火种，每个县城开发者都能用 API 接口编织智能神经网络。与其担忧模型参数膨胀是否会吞噬工作岗位，不如成为第一批大模型的 \"算法纺织工\"；历史从不同情被动者，当大模型的思维链已能穿透数学证明的铜墙铁壁，当 DeepSeek-R1 的推理能力正通过模型蒸馏注入老人机的芯片 —— 这场静默的认知革命，终将把迟疑者永远留在蒸汽朋克的隐喻里\n","categories":["LLM"],"tags":["机器学习","深度学习","LLM"]},{"title":"如何用数据来挣钱","url":"/2017/06/05/%E5%A6%82%E4%BD%95%E7%94%A8%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%8C%A3%E9%92%B1/","content":"本文内容主要来源于 该知乎\nlive，主要简单介绍了互联网中免费的商业模式所带来的资产（数据），以及如何通过这些资产进行变现。\n\n免费模式与变现资产\n\n\n免费模式\n\n免费模式带来的后向变现资产包括\n\n\n可变现资产\n\n其中数据依附于流量进行变现，数据提高了流量变现的价值，如下图通过数据进行人群划分后再进行广告的投放能够提高收益\n\n\n数据的利用带来的效益\n\n数据变现产品的发展历程\n数据变现可以说是经历四个发展阶段，由于广告在这方面的发展历程较为完备，因此以计算广告中的数据变现的发展历程来介绍，但是拓展到其他领域也是大同小异\n1）变现人口属性数据 2）变现行为数据\n3）变现第一方和第三方数据：在此阶段前只利用了广告平台提供的数据，这个阶段用了广告主自身的数据（第一方数据）\n4）变现场景数据\n\n\n广告商业产品发展历程\n\n变现人口属性数据（合约广告）\n这个阶段对应于计算广告中的合约广告，即广告主要求在符合某些人口属性的人群中投放一定量的广告，并与提供广告位方达成一定的合约。如下图中的需求节点所展示的是一些广告主的具体需求，供给节点则是一些媒体网站等的流量信息，所展示的整个系统称为担保式投放系统（Guarantee\nDelivery，GD）。在这个过程中要解决在线分配（Online\ndelivery）的问题。\n\n\n合约分配交易模式\n\n上述这种方法在粗略定向中有效，对更精细数据的定向力不从心，因此有了竞价广告的出现\n变现行为数据（竞价广告）\n上面提到的精细数据（行为数据）其实代表的就是长尾数据，也就是频次很低但是数量很多的数据。实际中无法去掉长尾流量，因为长尾所占的总体比例还是很大的，在广告中的通过竞价交易模式来变现这一部分数据\n以搜索广告为例， A 个广告主在争取 S 个广告位\n\n\n竞价的交易模式\n\n在这个过程中有个需要注意的点就是竞价是要采用广义第二高价的原则，就是广告位拍卖最终收的钱是出价中排第二的那个价格，这是由经济学博弈论得到的一个结论。\n变现第一方和第三方数据（程序化交易）\n产生的原因是广告主的定制化的需求，因为竞价广告中提供的定向还不能满足广告主的个性需求，需要更精细化的定向。如广告主（称为第一方）要向其流失用户投放优惠广告，这时候流失的用户的数据只存在于第一方数据中，像\nGoogle、百度这些是不可能为广告主找出其流失用户的。\n程序化交易则是将用户数据展示给广告主，让广告主自己决定是否要这个广告，大大提高了中小广告主的参与度\n。这种方式使得第一方的数据得到应用，同时也让第一方（广告主）有动力去购买其他渠道的用户数据（第三方数据），也就是促进了数据交易。\n下图展示的是一个程序化交易的过程\n2.1 用户访问媒体 2.2 媒体网站上有 ADX 的代码，将请求发送给 ADX 2.3\nADX 将该用户信息和询价请求发送给 DSP，DSP 进行竞价\n在这个过程中，DSP 需要根据广告主提供的数据来判断 ADX\n所发送的用户是否符合广告主的需求，这样广告主的数据（第一方数据）便得到了应用\n\n\n程序化交易\n\n下如所示是第一方（广告主）数据的使用的一个具体例子：重定向，也就是找到访问过广告主网站的用户并投放广告\n\n\n重定向\n\n上面的例子仅仅是使用第一方数据，下面的例子联合使用了第一方（广告主）和第三方（媒体）的数据。\nLook-alike\n指的是本来广告主的用户基数人群就不多，需要借助广告平台的数据找到可能的新的用户，该过程也被称为新客推荐。这个过程需要广告主提供一部分的种子用户，DSP\n分析这些用户的共同特点，并结合媒体网站上其他用户的数据为广告主推荐与种子用户相似的用户。\n\n\nlook-alike\n\n变现场景数据 (移动广告)\n主要是与移动端广告有关，通过手机可以了解到人当前所处的场景\n\n\n场景\n\n\n\n检测场景\n\n比如说每天 9\n点进行地点采样一次，那么一个月下来便可知道其工作地点，便可向其推送附近商家的广告。\n有比如说早上上班时间检测到手机从高速移动的状态变为低速移动的状态，那么就是从地铁出来了，这是便可向其推荐附件早餐店的广告。\n这个方向比较前沿，目前还没有成熟的方案\n从电商角度看数据利用的方法\n下面以电商为例，讲述电商将其数据变现的几种途径\n\n站内推荐\n站外推荐：重定向\n新客推荐：Look-alike\n\n\n\n 从电商看数据利用\n\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"大数据是否能够改造你的行业","url":"/2017/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E5%90%A6%E8%83%BD%E5%A4%9F%E6%94%B9%E9%80%A0%E4%BD%A0%E7%9A%84%E8%A1%8C%E4%B8%9A/","content":"本文内容主要来源于该知乎\nlive，主要介绍了深度学习为什么能在大数据的环境下有效，并描述了大数据的三个特点：行为数据、全量加工和自动化应用。\n\n深度学习为何有效\n\n\n机器学习的发展\n\n深度学习属于表示表示学习的一种，将特征提取和模型训练放到一起，消除了领域知识 (特征工程) 的影响\n深度学习有效的原因\n1）深度学习的表达能力更强，模型能够容纳更多的数据\n2）深度学习的模型很早就提出了，但是一直缺乏有效的优化方法（求解方法），无法将桶灌满，直到\nGPU 的出现，相当于图中的水管的出现\n3）能够获取的数据量变得更大（水源）\n\n\n深度学习模型\n\n行为数据、全量加工、自动化应用\n能够利用大数据改造的产业必须要有以下三个特点\n1）具有行为数据 2）需要进行全量加工 3）能够部署自动化应用\n行为数据 v.s\n交易数据\n\n\n交易数据和行为数据\n\n由于两者的特点不同，交易数据和行为数据的加工方式差别很大\n全量加工 v.s\n采样加工\n问题属性决定采用哪种加工\n\n\n采样分析和全量加工\n\n全量加工是大数据的一个根本特点\nCTR\n预估是一个全量加工的问题，但是实际中往往要对负样本抽样以解决正负样本不平衡问题\n洞察应用 v.s\n全自动化应用\n洞察指的是根据大量数据生成报表，然后通过人观察这些报表并作出决策\n全自动化指的是数据的产生，加工，交易形成闭环\n\n\n全自动化应用\n\n因此，要将大数据应用到业务中，需要回答这三个问题\n1）行为数据从哪里来？ 2）要全量加工的问题是什么？\n3）如何做到自动化\n下面是根据大数据的三个特点介绍的三个应用场景，其中广告行业是已经发展的比较成熟的了，而保险行业和医疗行业则是未来有这种发展趋势的\n\n\n广告行业\n\n\n\n保险行业\n\n\n\n大数据医疗行业\n\n自动化系统一般框架\n上面提到了在大数据的环境下需要将处理自动化，下面以发展得较为成熟的计算广告为例讲述自动化系统的一般框架\n这个系统的分解以及各部分的作用如下所示，更详细的可参考这里\n\n\n自动化系统一般框架\n\n由于开源软件的发展，搭建这样的系统难度不大，开源软件的几个优势和顾虑如下所示\n\n\n搭建系统\n\n核心业务的迭代应该是非常快而且非常重要的，不能被开源软件的开发进度控制。\n最后，在具体的业务中应用到数据时，一定要遵循以下准则：数据高于经验，让数据来决策，不能只是先入为主做假设，有些现象是想不到的\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"如何成为快速阅读高手","url":"/2021/07/25/%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%BF%AB%E9%80%9F%E9%98%85%E8%AF%BB%E9%AB%98%E6%89%8B/","content":"最近在找资料的时候意外发现一个与快速阅读相关的干货，就是在得到这个\napp 上的一门课《怎样成为快速阅读的高手》，里面介绍了快速阅读的一个比较系统的方法论，总结来说就是阅读三步法：评测、速读和精读。笔者对其中的不少观点有共鸣，因此在本文摘录一些印象深刻的地方，推荐去听原始的课程，也许会有更大收获，也算是支持一下课程的作者。\n\n为什么要快速阅读\n\n“阅读是一件很享受的事” 仅限于读小说，读散文，读一些虚构类的作品，共同特征就是没有什么目的性\n阅读大多数非虚构类书籍的时候并不享受，因为需要耗费大量脑力，不应该把阅读的时间拖得过长\n比起一本书的价格，读书时间是最大的花销；我们需要将读书的收益最大化，即降低读书时间，获取书中的有用价值\n快速阅读的好处不限于读书，在工作、生活中也是大有用途\n\n摆正阅读的三个心态\n提升阅读速度和效率，除了技巧，心态的调整也是重要的环节。课程中主要讲了三个：(1) 丢掉对书本的敬畏感\n(2) 有的书只需要你速读 （3）即便是你要精读一本书，也要先速读一遍。\n丢掉对书的敬畏感\n这一点是笔者觉得最重要的一点\n我们从小被灌输着一些这样的思想：读书可以陶冶情操，可以修身养性，能获得知识，能塑造价值观，总之读书就是千好万好，书里的知识就是珍宝。\n这自然没错，但这就容易让你对书本产生敬畏感，会让你觉得，如果我不逐字逐句地读完，那我就有可能不理解作者的思想，就有可能丢掉一些很宝贵的东西；让你觉得凡是写进书里的内容就是有价值的，就是名言警句，要是我看不懂，那是我水平不够，和作者没啥关系。\n但事实并非如此，如果你觉得一本书死活看不进去，不一定是你的问题。这其中有很多原因，包括但不限于\n\n翻译的问题，其他文字转化成汉语，总有一些语意的丢失。\n\n作者在当时的社会背景下提出了一个当时的问题，现在这个问题已经不存在了\n\n作者就为了凑字数，就一个事情翻来覆去地说，故意营造一种重要感。\n\n作者连自己想说啥自己都不知道，不顾读者的感受在那自顾自地表达，东一句西一句让人不知所云\n\n....\n\n根据作者读书的经验，\n\n一本非虚构类的畅销书，有可能 80% 的内容都是在堆积材料，20% 的内容是作者自己的观点和思考，这其中，可能只有 1% 的内容对你有所启发，不过话说回来，你能有这 1% 的收获就已经赚了。\n\n因此，你要认识到，读书是不用跪在书的面前，而是应该用很轻松的心态，就像和一个比你年纪稍微大那么一点点的老朋友聊天那样，化繁为简，听他怎么说就好。觉得好的地方就收下；觉得不好的就果断扔掉\n有的书只需要你快速的读完\n笔者觉得这里也可以翻译为读书需要有目的性，即要意识到读这本书的对自己实际用作是什么，对那些实际作用不大的书可以快速读完，比如\n\n一些包装得特别好的书，作者的背景、网友的评论好像都不错，结果一看满书都是一些大而无当的话，要么就是一句话能说清楚的事情说了 500 多页，遇到这种书当然就快速翻完就完了。\n\n一些老的经典书，书的名气非常大，写得也好，但是作者讨论的问题现在已经不存在了；再比如一些概念前些年提出来可能会很牛，但是现在提出来明显就已经过时了。\n\n于一些可能名气大，但由于各种原因，营养有限或现实意义不大的书，你为了满足自己的好奇心，可以去看看它，但没必要花太多时间投入。很多书你是需要绕过去的。\n\n...\n\n即使需要精读一本书，也先要速读一遍\n精读前的速读的必要性 (针对非虚构类作品)\n\n先预览一遍可以帮你建立一个地图，能降低你理解这本书的难度\n\n快速浏览一遍能激发出你的问题和好奇心，带着问题和好奇心去书里找答案，效率更高\n\n作者理解的速读不是单位时间内你能看更多的字，而是单位时间内看的字更少，但是抓要点地去看。所以，一本书是可以像看电影一样快进看完的，就像以前你阅读是撒网捕鱼，你可能想的是怎么把网做得越来越大，怎么一网撒下去能捞到更多的鱼，现在你知道会速读的人都不是用这个方法，他们都是用鱼枪捕鱼，一次就瞄一条，但是瞄的那条一定是最大的。\n很多人拿到一本新书就准备一遍把它读完，其实最有效的方式是读三遍。用一句话总结就是：读书读三遍，从大往小看。\n第一遍看主题，看口碑，确定自己是不是要花时间在上面。这是从一本书的价值角度来看，视野要放大，离书的距离要远一些。\n第二遍看结构，看脉络，用快进的方式整体浏览一遍，看看作者是如何设置这本书的结构，哪里是铺垫，哪里是重点。这是从内部的整体结构来看，走的就离书近了一些。\n第三遍，挑自己感兴趣的部分反复看，仔细琢磨，最后输出成自己的东西，这就是进入书中，从小处着手来阅读。\n总结起来就是阅读三步法：评测、速读和精读，也分别对应着下面要说的三个部分的内容。\n评测是为阅读蓄力\n这是阅读三步法的第一步：评测和预判；即选择 “对” 的书\n评测的过程其实也是看书，目的是确定这本书值不值得看。你可能通过个人兴趣、或者别人推荐、媒体推荐等，对一本书有了兴趣，那么怎么判断这本书值不值得花时间打开呢？\n这其实包含两个层面，(1) 这本书本身是不是有价值，是不是一本好书\n(2) 这本书是不是适合你\n如何去判断呢？作者提供了以下几种方法\n\n看主题。很多人觉的序言里基本都是推荐的话，不值得一看，其实不是。序言部分通常是作者请自己一些有点名气的朋友写或是作者本人自己写，等于是一本书的门面，他们通常都会很认真，在里面他们会对这本书的主要内容和重要观点做一个综述，把你看完能得到的收获写进去，这些东西都可以很好的帮你了解这本书。\n\n看出版社的宣传文案。虽然很多宣传文案都会过度吹捧，不过也可能会把书中提出的主要问题列出来，你可以对照一下是不是自己正感兴趣的，这里面当然也会包括作者的信息，看看作者有什么样的背景，他是不是这个领域里资深研究者，这个通过介绍也很容易了解到，作者靠谱了，书自然也不会差。\n\n看目录。通过目录可以直观地看到作者要讨论的内容和结构，这也是你后面阅读的时候需要反复看的地方。它就像你穿过一片丛林时不能丢掉的地图。\n\n看评论。上网搜一搜关于这本书的相关信息，看看别人是怎么评价这本书的。\n\n在以上四点中，作者重点介绍了如何正确参考书评，或者如何判断书评是否有价值\n没价值的是啥样的呢？就是单纯的给出判断，比如\n\n这本书很好，但是就不说哪里好\n&gt;\n这种情况，要么是读者很懒或是水平有限，他确实总结不出哪里是亮点；要么就是读者压根没看懂，很多人会对自己看不懂的，觉得很厉害的东西说好，这是很常见的一种心理现象；要么就是读者已经花了时间在里面，这时候如果承认自己看了一本烂书并且一无所获，这是一件不舒服的事，所以就安慰自己这本书其实很好。\n这本书很烂，但就是不说哪里烂，还有的负面评价是发泄式的\n\n那什么是有价值的呢？就是会提供关于这本书的额外信息，给出哪里好或是哪里不好的论证，比如\n\n书是在怎么样的大背景下写出的，在同类书目中的地位是怎么样的\n\n书里的观点有没有和他对立的存在，关于这本书的延伸阅读，等等\n\n另外，作者也提到的观点笔者非常赞同，而这其实也不限于读书，生活其实都是 “小马过河”\n&gt;\n在你亲自认真的读完一本书之后，你会有一个自己的判断，很可能你会发现很多人说的好，其实没那么好，水平很一般，也可能别人说的烂也没那么烂，很多观点是对你有启发的。\n总之，评测过程可以让你对这本书有一个大致的预先判断，还能激发出你的问题和好奇心，让你带着问题去书里探寻答案。这有点像一个蓄力的过程，蓄到一定程度后，你会发觉你对这本书充满了好奇，有一种一探究竟的冲动，那在后面进入书本的时候，你就是带着解密的心态在读，会增加很多趣味性。有了这根筋后，你就不至于漫无目的地看书了，看书打瞌睡，看一会就困的问题就解决了。\n速读就是找关键信息\n这是阅读三步法的第二步：速读；这个过程需要用到速读技巧，作者这里提供了三个技巧：（1）用视觉输入代替线性输入（2）以语义单元推进阅读（3）通过概念找重点\n用视觉输入代替线性输入\n很多人的阅读习惯是一开始就从左往右、从上往下，一个字一个字，一行一行地开始读，这其实是最原始的读书方式，刚拿到一本书，不能这么开始，不然你会发现自己好像在完成一个艰巨的任务。\n那应该怎么办呢，应该要先浏览一遍，目的迅速了解一本书的主题和框架，包括作者的写作意图，背景，搞清楚重点在哪里。你可能会说，了解一本书的框架我知道，但是我的速度就是起不来，其实这是因为你总是会忘掉快速浏览的目的，总想在快速浏览的时候追求理解。\n我们通常是把看书这个动作称做阅读是吧，它分为阅和读，这其实是两个动作。这两个动作有啥区别呢？\n看是视觉输入，就像你看一幅画一样，你不会从上往下从左往右按着顺序来看，你一眼瞄过去就能知道这是幅肖像画还是风景画。\n读是听觉输入，听觉输入是线性输入。你得按照一定的顺序描述才能把它还原成一幅画，就像我们听一些课程，你放个 3 倍的速度，那你几乎就听不清说的啥了。\n既然看一段文字就像看一幅画，你一眼瞄过去就应该能找到这一大幅图的特征，就应该能知道他画的是什么东西。可能你不知道里面的细节，但是没关系，做到这一步就够了。就像你去超市逛了一圈，让你想零食的种类你可能想不起来，但是零食区在哪你肯定知道。\n理解这一点很重要。因为所谓的快速阅读，就是指快速地找出一段话的关键信息，抓的快、准、稳，直接跳过和忽略那些不重要的辅助信息。\n以语意单元推进阅读\n那怎么用快进的方式看书呢？作者分享的方法是 “语意单元推进法”\n熟悉写作的朋友都知道，在写非虚构类的文章时，总要先列个大纲，一个大纲包含很多条想要说的事和想要表达的观点，每一件想说的事，每一个想要表达的观点，其实就构成了一个语意单元。\n既然是这样，那我们在开始快速阅读的时候，也按照语意单元来理解就好了，有时候一段话就构成一个语意单元，有时候十几页构成了一个语意单元，这都不重要。重要的是你能以这个为单位进行快速地推进，而不要一开始就陷入到细节中去。\n举个书中经常遇到的例子，比如，一个故事一开始作者总要烘托一下气氛，引出他想要说的观点，要说说自己是怎么发现这个事情的，他做了哪些工作，对他产生了什么影响，总之最后的目的就是要烘托他发现的这个道理很重要。那这些所有烘托气氛的描述就是一个语意单元。\n那语意单元要怎么找呢，在找的时候注意两点。\n（1）注意关键词的位置\n比如一段话的第一句话或是最后一句话，大多数段落的中心句就是放在第一句。有很少一部分放在最后一句。\n还要注意那些表示转折的关联词，比如 “但是”、“然而”、“不过” 等等，这些词的后面经常会跟着一段话的中心句。\n还要养成一个习惯，就是对名词和动词要格外的敏感，因为把这些词抓住了，一个中心句的主语和谓语经常就能一把抓住了，一段话的中心意思也就常常一目了然了。\n至于小标题，作者重点标出的黑体字等，也是重点要看的地方。不过大多数的书里作者是不给这样的提示性信息的，还是得靠你自己去发现。\n（2）阅读过程中要在脑海中勾勒作者的写作思路和意图\n想想作者为什么这么写？他想证明什么？想要表达什么？在看的过程中脑海中要同时有两个声音在运作，一个是作者的，一个是你的，这些问题要一直问，这样就不至于陷入杂乱无章的细节论述里去了。\n这个过程就像你坐着直升机飞过一片丛林，看过去之后，大的地貌特征你已经清楚了，手里有了一张地图，以后再走起来就方便多了。\n在快速浏览的时候你可能会遇到一些问题。\n\n一些观点的论证过程非常长，看到后面你基本上就已经忘了当初那个问题是什么，这时候你需要经常的跳回去看看作者最初的写作目的。浏览的时候你完全可以来回翻，不一定就是按照顺序从前往后看，\n\n遇到了一些难点，快速浏览的时候完全看不懂，这时候很多人就容易停下来开始死磕，其实不用。遇到这种情况先把难点放一放，继续往下推进，说不定从后面的解释中你一下就明白前面那段话的意思了。实在不行，那就等到下个阶段，精读的时候再重点攻克\n\n通过概念找重点\n以语意为单位为快速推进的目的其实就是在找东西，找啥呢？找重点。\n那什么算重点呢？例子算重点吗？不算，因为例子通常是为论点服务的。那论点算重点吗？也不一定，因为作者的论点很可能是大家都知道的常识，可能没啥特别的，你已经知道了，这样的信息就可以直接过。很多人习惯只要看到作者列出个 123 就开始划线，觉得那就是重点，其实那是作者的逻辑，你不需要按照作者的逻辑走，要按照自己的逻辑走，迅速把握一本书里对你来说最重要的那些知识点。\n作者提出在速读的时候，需要注意寻找这三类重点：\n第一类：\n你不认识的新概念。比如作者用了一个新的词解释了一种大家经常见到但是叫不出名字的现象，你第一次见到，在书中作者围绕这个概念给出了一个解释，这就属于重点之一。\n方法论也可以划分到新概念里，比如在这门课里分享的一个快速阅读的技巧，你以前不会，那快速阅读对你来说也属于新概念。但是注意有些概念通常是换了一个包装而已，如果你有点经验，通常可以一眼就辨识出来，不一定作者给出的新词你就要用，你能用自己熟悉的东西解释清楚这个概念当然更好。\n第二类：你不够重视的旧概念。作者把一个你认识的概念的重要性提升了一个高度。之前你可能经常遇见这个概念，但是没有重视，重视起来后可以带来很多好处。\n比如 “自我辩护”，大家觉得自我辩护是人之常情，平时都不怎么关注它，犯错了谁都会替自己找借口，把自己的行为合理化一些，这样能减少自己的痛苦。但是在《错不在我》那本书里，作者提出的点是，自我辩护这个行为，不是我们平时以为的那么没有危害，他几乎是一切矛盾和冲突的本源。如果把自我辩护这个很常见的行为重视起来，在平时我们就能减少很多冲突。\n第三类：等待替换的概念。就是作者试图用自己的论证来说服你，让你了解自己以前的观念是错的，是具有片面性的，是可以重新解释的。\n比如 “取悦”，大家可能觉得只要取悦别人就能让别人喜欢，实际上《取悦症》这本书告诉我们，一味取悦别人会带来相反的结果，喜欢取悦别人的人容易遭人讨厌，是不是很反常识？比如说 “智商”，平时我们都以为智商是一个固定的值，天生的，谁也改变不了。但是《绝非天赋》这本书中告诉我们，智商是一个相对值，他之所以被普遍接受，是美国政府刻意推广的结果。\n这三种类别分好之后，我们在浏览的时候把这三类信息找出来就可以了。有了这个找三类信息的目标之后，你至少就不会打瞌睡了。接下来干嘛呢，你可能觉得应该精读了吧？不是，这时候你最好把书放一边去，干点别的事情，让大脑休息一会，给自己一段缓冲的时间，大脑有一个特性，会自动过滤不重要的信息。休息一段时间回来再翻开书的时候，你会感觉这本书里的内容已经一目了然了，这本书会变得很薄，你会发现自己应该在哪里用力。\n能输出才是有效精读\n这是阅读三步法的第二步：精读；速读完一本书后需要判断这本书是不是需要精读，不是所有的书都需要或者值得精读\n如果一本书是在你的专业领域范围之外，你只是路过，特别好奇地想要朝里面望一眼，如果没有意外，你可能永远不会翻开这本书，那其实就不需要精读\n如果一本书真是对你有很大的帮助，你希望去亲自阅读、去挖掘更多的东西，你想把书本中的东西变成自己的见识、谈资，内化成自己的表达能力，那你就要做更多的工作了。\n很多人在这就会遇到一个问题了，那就是看完书脑子里只留下一团模模糊糊的概念，要是别人问你看的书讲的是啥，你说不出来，或者不能漂亮利落地说出来。其实这不是因为你表达能力不行，这是因为你没有学会正确的有效阅读。\n什么是有效阅读？作者给有效阅读的定义就是能记住、能输出，因为只有这样，你读过的东西才真正变成你的东西。\n因此，在精读细读的过程中，你要建立一个目标感，你既然是以输出为目的的阅读，你就不能像看小说一样任由作者牵着你走，你要假设待会就要给别人讲讲这本书里写了些啥，哪些东西对你是有帮助的，哪些东西对你是有启发的，哪些东西特别有意思。这样带有目的阅读就是有效阅读。\n有效阅读的这个过程大致分两个步骤，第一个是理解和互动。第二个是把内容记住和输出。\n理解和互动\n先说第一个，怎样进行深入的理解和互动。\n最简单的方法就是看完你认为好的部分之后，做笔记，写感想。\n你可能会问，我没感想怎么办，不可能的，你一定有感想，比如读完一段话后，你单纯地觉得 “好”，这个 “好” 就是感想。至于哪好你可能还说不出来，不过没关系，只要有这个感受就行了，这很重要，这说明你和这段信息做了一次轻微的互动，说明你对这段信息有感觉了。\n那怎么进行深一点互动呢？你可以写写这段信息哪里好，哪里引发你的兴趣了。比如你在其它书里看到了类似的观点；比如这段信息可能很新鲜；可能是一个比喻用得很贴切；可能是很走心；可能帮你把一个平时觉得很模糊的概念定义清楚了；可能纠正了你一些错误的观念；也许就是单纯地给了你一个实用的方法论或思想工具。\n这其实就是让新知识和你脑海中已经有的东西来一次互动，把原来的东西看得更清楚一点，经过这个过程，以后再遇到类似的信息，你会一下就想起来以前在哪里看过的。用这个方法积累下去，它会慢慢地变成一种习惯，新知识和旧知识的连接和互动速度可能越来越快，长期积累，你就会变成别人眼中那种特别有想法的人了。\n记住和输出\n笔者对这部分的很多观点都是深有体会的\n输出的方式大致分两种，一种是写作，一种是口语表达。写作要求的整合信息的能力强一点，逻辑思维能力高一点。而口语表达要求故事性、画面感高一点。\n如果你能持续地做笔记，那你会发现自己会对信息变得很敏感，什么样的信息你似乎都会本能的来一次互动，从这段信息中看出一点别的什么东西出来。这时候不让你输出都很难，你会有一种想给别人表达的冲动，你会发现随便一写就好几千字。\n把自己平时觉得有价值的文章、有价值的段落标记出来，和它互动，记一些想法，然后定期整理它们，这就是你的素材库。\n上面说的是记住，如果有自己平时的积累和搜索引擎的帮助，你会发现，输出一段内容会变成一件能给你带来巨大帮助的事。比如你本来可能对一个问题的看法是很模糊的，但是现在你要写这个主题的东西，做一次分享，这种强制性的输出会让你思路变得很清晰。在做分享的时候，你自己能理解得更加透彻，以教为学是最有效的学习方式，没有之一。\n作者认为，锻炼输出能力要学会的第一件事、也是最难的一件事就是写稿：把自己想表达的东西语言流畅、逻辑清晰地呈现出来。\n写稿的作用不单单是把你知道的呈现出来，写作这个动作本身也是一个帮助你思考的过程，可能你在敲打键盘前，对自己想要表达的东西感觉都是模模糊糊的，感觉都是云山雾罩的，没关系，开始写，一旦开始，你会发现你的思路会越来越清晰，逻辑也会越来越严谨，因为写作这个动作和口语表达不一样，本来就慢，它会启动你大脑的第二系统，第二系统就是所谓的慢思考，你是有意识地、有逻辑地在调动大脑中的资源，这时候你的想法自然会更深，输出的东西自然会更有料，这就是写稿带来的好处。\n你动手前可能想要写一篇完美的无可挑剔的稿子，脑袋里抱着这样的想法自然是动不了手的，因为你脑袋里负责创造的声音和负责评价的声音是同时开启的，就像你开车的时候一边踩着油门，一边踩着刹车，你能跑快吗？\n所以在第一遍写稿的时候，你要有意识的把评价的声音关掉，等到第一遍完成了，这时候你大脑中评价的声音就可以开启，你可以用挑剔的眼光看待自己的稿子，进行修改和补充，这个过程也是提升你写作能力和思考能力的过程。等多修改几遍后，再回头一看，你会发现也没那么烂，挺好的，是值得和别人分享的。这个过程不断重复，你的输出能力会越来越强。\n接下来就是反复练习了，这个过程没有什么技巧可言，就是下功夫，如果你能坚持，时间会带给你回报\n小结\n笔者从这门课中获取的最大的两个收获是：摆正阅读的三个心态和阅读三步法 (评测、速读、精读)\n摆正阅读的三个心态分别是\n\n丢掉对书本的敬畏感，要带着批判的眼光看书\n\n认识到有些书就是要速读的\n\n精读前需要速读\n\n而阅读三步法中每一步都有一些技巧\n（1）评测\n\n不要遗漏那些有用的信息，比如序言、目录、宣传文案等，序言和宣传文案可能有过度吹捧的嫌疑，但通常也能帮你了解这本书所讨论的问题\n\n要带着怀疑的眼光去看各种信息，鉴别书评是否有价值\n\n（2）速读\n\n用视觉输入代替线性的听觉输入：快速浏览的目的是了解一本书的主题和框架，包括作者的写作意图和背景。细节在这个过程中不是最需要关注的，你需要关注的是这本书的结构、脉络。\n\n以语意为单元推进：在找语意单元的时候，注意关键词的位置，还要注意作者的写作框架。遇到不懂的地方先略过，不要停下来，可以从前往后看，也可以从后往前看。\n\n通过概念找重点：作者有可能是提出一个新概念，可能审视一个旧概念，也有可能在替换概念\n\n（3）精读\n\n建立一个有效阅读的概念:\n能记住、能输出，我们应该带着这个目的去精读，而这可分两个步骤：(1) 理解和互动\n(2) 记忆和输出\n\n理解和互动:\n把你有感觉的段落和句子记下来，仔细想一想为什么好，哪里好\n\n记忆和输出:\n以教为学，输入时需要避免完美主义，关键是要动手开始做，然后逐步完善\n\n最后引用课程作者的一段话收尾\n\n快速阅读，并不是让你抱着不求甚解的态度博览群书，它是读书的一个环节，是你对一本书作出价值判断和寻找要点的必要步骤，而且，快速阅读和有效的精读是不冲突的，它是精读前的一个步骤。掌握了这套方法，它在以后带给你的效益是超乎想象的。\n\n","categories":["拾人牙慧"],"tags":["读书","拾人牙慧"]},{"title":"如何用数据武装运营工作","url":"/2017/06/18/%E5%A6%82%E4%BD%95%E7%94%A8%E6%95%B0%E6%8D%AE%E6%AD%A6%E8%A3%85%E8%BF%90%E8%90%A5%E5%B7%A5%E4%BD%9C/","content":"这篇文章的内容主要来源于 该知乎 live，主要介绍了利用数据获取了用户后如何运营，从而能够尽可能长时间地留存用户，介绍了这方面的三个具体方法：建立用户转化漏斗、通过多维报表找到问题和建立实验框架。\n\n本文主要关注下图左半部分，利用了别人的数据和流量获取了用户后该怎么运营以留存用户。\n\n\n商业闭环\n\n为了达到这个目标，有三个重要的方法\n\n建立用户转化漏斗：总体的数据发生变化时，到底是哪个环节起了作用。\n通过多维报表找到问题：经过上一步确定某个环节出现问题后，对这一个环节需要更细致的分解，这是需要从各个维度去分析这个环节中出现问题的原因。\n建立灵活的实验框架：上面两步是被动地发现问题，实际中更要主动的探索新方案，新的实验框架有利于测试的快速迭代\n\n建立用户转化漏斗\n把商业目标转化为用户转化的一系列步骤，下面是两个具体例子\n\n\n用户转化漏斗\n\n每一步都有一定的损失，且需要关注的点是前后两步的用户的比率，因为通过比率更容易看到具体的变化\n漏斗设计的原则与作用：整个漏斗过程用于优化一个唯一的目标，并将该目标分解为若干比率的乘积，便于发现问题并优化，下面是一个优化总用户时长的例子\n\n\n总用户时长漏斗设计\n\n设计用户漏斗需要涉及到具体的度量指标，下面是移动应用中一些常见的度量指标\n转化率 / 激活率：激活数和点击数的比\n留存率：某日激活的用户中，经过一段时间还活跃的用户所长比例；根据设定的时间不同可分为次日留存、七日留存、月留存等\n活跃用户：活跃的独立用户数，根据时间的不同可分为日活跃用户（DAU）、月活跃用户（MAU）\n用户时长：每个活跃用户平均消耗的时间\n对于网站分析，其优化目标本质上跟移动的应用一样，就是尽可能吸引并留存更多的用户，但是由于两者具体提供服务的不同，两者的度量指标也有不同的叫法，下面是网站分析中常见的度量\nUV (User View)：独立访客数 PV (Page\nView)：所有浏览量 页面停留时长：页面浏览时间\n跳出率 (Bounce\nRate)：指单页会话（用户打开了网站上的一个网页，然后就退出了网站）次数在所有会话次数中所占的比例，也就是用户在您网站上仅查看一个网页的会话次数所占的百分比\n网站热力图：热力图是指以特殊高亮的形式显示访客热衷的页面区域和访客所在的地理区域的图示，简单来说就是分析出一个页面内各个部位的点击情况，如下是一个热力点击图\n\n\n热力点击图\n\n关于跳出率，高跳出率有时是网站本身的属性决定的，这时候就没必要做更多的优化，\nGoogle\nAnalytics 描述如下：\n\n高跳出率不是件好事？\n这需要视情况而定。\n如果您网站的成功取决于用户是否查看多个网页，那么高跳出率就不是一件好事。例如，如果您的首页是通往您网站的其他部分（例如新闻报道、产品页、结帐流程）的入口，而大部分用户仅查看您的首页，那么您一定不希望跳出率处于较高的水平。\n另一方面，如果您拥有类似博客那样的单页网站，或提供预计会产生单页会话的其他类型的内容，则高跳出率完全属于正常现象。\n\n分析工具\n网站分析工具：Google Analytics、百度统计、CNZZ\n应用分析工具：TalkingData、友盟 +、Flurry、Google Analytics\n应用归因工具：Appsflyer、Tune、Adjust、TalkingData\n分析工具是用于分析已有用户的行为，而归因工具是用于分析用户的来源。\n因此，总结上面的过程为：建立漏斗并利用分析工具将漏斗的每一步的数据找出来\n下面是一个通过漏斗分析的页游的例子\n\n\n转化漏斗\n\n\n\n多维度报表\n\n通过这样的步骤可以有目的地去排查具体的问题，如上面的例子中可能是程序在\nchrome 浏览器上的不兼容导致了注册率的较低。\n通过多维报表找到问题\n上面的例子中统计各个浏览器的注册率时已经涉及到了多维报表查询的问题，因为除了从浏览器，还可能从地域、时间段、时间段和浏览器组合等其他维度去统计，这时候就需要一个灵活的查询统计工具来提供这样的多维度报表，这个工具就是数据仓库（Data\nWarehouse）\n\n\n数据仓库\n\nOLAP (Online Analytical Processing) 和 OLTP (Online Transaction\nProcessing)\n\n\nOLAP v.s OLTP\n\n软件：Saiku、Tableau\n上面的流程：\n一个商业目标 -&gt; 建立一个转化漏斗 -&gt; 用分析工具在应用或网站埋点\n-&gt; 分析工具得到的数据建立数据仓库 -&gt; 用数据仓库做精细查询\n建立灵活的实验框架\n实验框架其实就是常听说的 A/B\n测试，就是把用户分成两部分（两部分的量不一定相等），然后对这两部分的用户分别采用不同的方案，比较哪种的效果好，从而采取效果好的那个方案。这里的不同方案的不同点就是我们需要验证的产品的特性，且往往是单变量的，也就是验证某个新的特性与旧特性哪个好时需要保持其他环境一样，而仅仅改变这个特性。在这个过程中需要注意划分的用户要有代表性，也就是两部分用户的分布情况应该一致（如年龄等）\nA/B\n测试中的一个重要方法是分层实验框架，其目的是为了能够在同样的流量情况下容纳更多的\nA/B 测试，下面以一个简单的例子讲解\n\n\n分层实验框架\n\n上图所示的实验层中，在 UI 层，广告检索层和广告排序层均有 A/B\n测试的需求，假如要测试 UI 层的一个新特征，同时也要测试广告检索层的一个新特征，当需要同时进行这两个测试时，必须要确保 UI 层的流量划分不会影响到广告检索层的测试，也就是说在广告检索层中划分的两部分流量中，只存在着广告检索层的特征的差异。因此如果同时在两个层进行 AB 测试，需要将流量划分为四份，分别是\nUI 层原特征 + 广告检索层原特征、UI 层原特征 + 广告检索层新特征、UI 层新特征 + 广告检索层原特征、UI 层新特征 + 广告检索层新特征。即需要测试的特征数为\n\\(n\\) 时，需要划分的流量数为 \\(2^n\\),\n显然这样的增长级数带来了流量分割的困难。分层实验框架就是解决这个问题的。\n具体的做法就是采用正交的哈希函数来为每一层进行流量划分，正交的哈希函数避免了不同层间的干扰问题。如对于上面的问题，采用两个相互正交的哈希函数，分别在 UI 层和广告检索层将流量划分为两部分，因为两个哈希函数是正交的，因此在 UI 层所划分的两部分流量 U1 和 U2 中，U1 所包含的广告检索层的原特征流量和 U2 所包含的广告检索层的原特征流量比例相等，同时新特征的比例也相等，这样就避免了上面的指数级增长的划分方式。更详细的信息可参考这篇文章\nOverlapping\nexperiment infrastructure: more, better, faster experimentation\n上图中的非重叠测试域指的是测试的特性贯穿了三个层，就是同时测试三个层的特性组合带来的效果，这时候就用不上分层实验框架了。\n上面这种采用正交的哈希函数来为每一层进行流量划分思路同样可用在灰度发布中，进行分层的灰度发布测试。\n通过 A/B 测试能够将目前的产品的特性调到最优，但是需要注意的是 A/B\n测试并不是万能的，因为过度依赖于数据会丧失对关键创新的把握，这里有一句很形象的话：汽车无法从跑得更快的马进化而来，也就是无论我们利用\nA/B\n测试把马（现有的产品）训练成跑得更快，依然是没法比得过汽车（更有创意的产品）。\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"年轻的时候，做什么才不会浪费","url":"/2017/03/25/%E5%B9%B4%E8%BD%BB%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%81%9A%E4%BB%80%E4%B9%88%E6%89%8D%E4%B8%8D%E4%BC%9A%E6%B5%AA%E8%B4%B9/","content":"文章有点标题党的意思，但是里面的内容却很很值得一读，本文为转载，作者是孙圈圈, 侵删。\n\n最近去各个城市做读者见面会，见到很多年轻人，跟他们交流，无一例外地：焦虑，迷茫，没方向，一无所有，着急……\n但，这些 “月经式” 问题，谁年轻的时候没痛过？\n从学校到公司，这条路有多长\n刚工作的时候，我比现在有个性多了：我老板是区域总监，在她下面的一位大区经理（我非常不喜欢的一个人）让我做一份数据表格。\n实际上，每个大区经理都有自己的助理，但他说自己助理太辛苦，让我做。\n结果我不服，直接发了一封邮件给他，抄送我老板，把我的岗位说明书发给那位大区经理，跟他说：在我的工作职责里面，没有一条是需要服务你的。\n现在想想，当时老板真的挺宽容我的，因为这样一来，她会处于很尴尬的境地，那位大区经理会以为，我的行为是我老板指使的。\n反正，换做是我现在，遇到当年的自己，肯定火冒三丈。\n那时候，还经常跟同事一起抱怨：公司培训机制不行、学不到东西；老板不民主、一意孤行；上层不体察民情、听不到员工的声音；公司太看重短期利益、不考虑长期发展……\n等到后来，自己做管理者，才知道，一个团队的管理、一个公司的经营，远远不是我作为一个初级员工所想象的那样。\n有很多文章，批评年轻员工不负责、不会站在老板角度考虑问题。但要我说，这些都没说到点子上，即：从学校到公司，到底有什么不同？\n最大的区别就是：你给学校付钱，但公司给你付钱。\n所以在学校里，老师为你服务，有义务帮助你学习；但在公司，你是为老板服务，有义务帮他解决问题。\n对这个概念的模糊不清，是导致我们在职场屡屡受挫的最大障碍。\n因为如果不意识到这一点，我们就不会知道：应该站在老板的角度，去了解他需要什么，而不是要他站在我们的角度，来关心我们要什么；不应该像在学校跟老师相处那样，向老板寻求答案，而是要主动学习，帮老板找到答案。\n在中国教育体制下浸润了 20 年、习惯了向老师寻求标准答案的我们，在进入公司之后，能够多快地抛弃这种惯性，从而在没有标准答案的问题面前，去主动学习和思考、摸索解决方案，基本上决定了我们最初 5 年的职场上升速度。\n焦虑感什么时候才会消失\n科比说他见过洛杉矶凌晨 4 点的样子，我没见过，但我见过上海凌晨 4 点的样子，因为那时候我还没睡。\n不是因为在工作，而是因为我焦虑，睡不着。\n毕业那年，在没有任何实习经历的情况下找工作，所以第一份工作找得不好。\n工作三个月后，就发现这个职位几乎不可能有什么发展。\n从此就一直想要逃离，但毕业半年，谁会要你呢？于是经常焦虑、自我怀疑，总会想：我这辈子是不是就这样了？是不是就要困在这儿了？\n还好，一直都在关注外部机会，最后幸运地进了咨询公司。但马上又开始焦虑，因为进公司之后 1 个月，全球经济危机就开始了，公司在全球范围内冻薪，甚至在传有可能会裁员。\n如果被裁，那就意味着，我毕业 1 年，得第三次换工作了。\n还好，又幸运地留在了公司。但公司冻薪了两年，不能加薪不能升职。\n而且，公司还在那年取消了分析师级别，应届生一进来就是高级分析师了。这意味着，我毕业快 3 年了，还跟应届生的工资一样、职位也一样。\n知道消息的那天，躺在床上焦虑得睡不着：毕业快 3 年了，事业还停留在起点，还没了男朋友，刚到上海所以也没有好朋友，租房子一年搬了 6 次家，这样的人生还有什么希望？\n但后来，毕竟还是走出来了，经济危机终究过去了。\n人生也终于开挂了，9 个月完成别人 3 年达到的晋升、成为大中华区晋升最快的顾问、几次在 1 年内薪资翻倍、带着团队拿比赛冠军、转换到我喜欢的咨询业务线、买房子把家人接到身边、买东西开始不关注价格、遇到对的人……\n当年焦虑的那些问题，如今都一一解决了。\n其实，现在随便去问一个年轻人，都会发现，我的经历没有任何特别之处，这些问题都是大多数人年轻时候的必经之路。\n只是，回头再看年轻时候那些焦虑的经历，我唯一想分享的经验是：在我们应对焦虑的时候，需要学会一次只解决一个问题。\n比如，意识到工作没发展，就把一切精力放在寻找转行机会上；经济危机的时候，反正也没法找到工作，就干脆埋头苦干，等到危机一过，就得到了自己想要的；工作顺利之后，才开始考虑感情问题，之前都先放一边；再次遇到天花板的时候，就尽全力转换业务线。\n这些问题，几乎没有哪一个是我同一时间花时间去解决的。\n没钱、没男 / 女朋友、没地位、工作不顺、人际关系受挫…… 这些都是我们大多数人年轻时候躲不开的问题。\n但是，当我们把所有问题混为一谈的时候，就会疲于奔命、无从下手，脑子里只有 “焦虑” 二字，一团浆糊。\n只有当我们把问题掰开，逐个分析的时候，才会看到问题的本质，看到我们想要的 “解决方案”，逐个击破。\n现在，我经常会对那些看职场鸡汤的人嗤之以鼻，但倒回很多年前，那时候的自己，面对诸多问题、急于一下子解决、又没有头绪的时候，不也曾诉诸鸡汤、试图寻求一个简单粗暴的答案、慰藉自己的焦虑情绪吗？\n等我们对鸡汤的粗暴答案嗤之以鼻、开始主动思考的时候，才是我们真正认识世界、开始解决问题的时候。\n年轻的时候，做什么才不会浪费\n线下签售的时候，有读者问我一个问题：我现在还年轻，现在的职业未必是以后要从事的，而且时代变化这么快，现在的工作也随时可能被淘汰。那我现在学的东西，是不是都白费了？我要怎么预测一下，未来 10 年的情况呢？\n这个问题，几乎没人可以回答。凯文凯利在《失控》里面有句话：所有对未来的长期预测都是错的。\n但是，虽然长期无法预测，有些东西是不太会变的。\n《人类简史》里面提到，根据史学家的考证，只有智人能够表达出从来没看过、碰过、听过的事物，这种能力让他们得以聚集大批人力，灵活合作，从而战胜比智人更加强大的其它人种。\n说白了，就是讲故事的能力。\n而我们看今天，人类进化到这个阶段，讲故事的能力依然重要，不管你说服客户、说服老板还是说服投资人，甚至做自媒体，都很需要。\n所以，不管行业、职业被颠覆成什么样子，有些核心的底层能力是任何时代、任何职业都用得上的。那么，只要我们掌握这些能力，就能够以不变应万变了。\n我们现在做的事情，都未必是将来想做的，也未必是适合自己的，从功利的角度讲，确实有些事情是会白费的。但我们做以下这些事情，是永远不会浪费时间的：\n核心能力提升\n就像上面说的，核心能力是能够让我们以不变应万变的能力。\n这些核心能力，除了上面说的讲故事能力之外，还包括：快速学习能力、分析与解决问题能力、创新能力等等。\n如果一份工作只能够给你知识和技能，却无法让你提升这些能力，那么就会是一份高风险的工作，因为当某种知识和技能不再稀缺的时候，你就很难有什么立足之本，瞬间回归起点。\n而当你的工作能够帮助你提升这些核心能力的时候，即便转换职业，也只是一时下滑，很快就能够反弹上去。\n认识自己的优劣势\n过去给企业做咨询的时候，接触了很多人才研究，这些研究都试图发现：究竟是什么因素，让一些人可以脱颖而出，更容易成功？\n最终会发现，知识和技能这些，都不关键，而能力，有不小的影响，但最终起决定作用的，却是我们的天性：性格特质、动机、价值观这些。\n似乎这是个悲观的发现，因为我们在成年之后，除非遭遇重大的人生变故，否则这些因素几乎是不可改变的。\n既然如此，我们还需要白费力气去做什么努力呢？\n但实际并非如此，因为这些天性，几乎没有什么好坏之分，比如在我们的社会，似乎外向的人更有优势，因为可以更加快速地跟人熟络起来。\n然而，内向的人也有自己的优势，他们很少直接发表自己的看法，更加倾向于深思熟虑，所以往往在思考方面有自己的优势。\n由此看来，任何天性，都可以找到适合自己的成功道路。\n年轻的时候，我们不了解自己、不满意自己、不想成为自己，跟自己的天性对抗，但没有关系，这些对抗最终会让我们发现自己的特点、边界和意义。\n等到了一定年龄，就会知道，不应该再对抗自己的天性，而是发现并顺应天性，找到适合自己的定位，最大化自己的优势，成就自己。\n接受无法改变的，改变可以改变的，这才是我们得以向前的高性价比方式。\n广结善缘\n年轻的时候，要成就一件事情，靠自己苦干就行，因为那时候，我们的工作主要是 “对事”，比如写好一份报告、解决一个问题、克服一个技术障碍等等。\n而随着年龄增长、地位提升，要成就一件事情，靠苦干远远不够，因为我们的工作变得主要是 “对人”，比如管理团队、说服客户、拿到投资等等。\n所以，越往上发展，你越需要与人交往，越需要更多的资源，越需要他人的帮助。个人英雄主义，往往会成为一个人未来发展的最大天花板。\n但人际交往这样的事情，不会是一次性的利益交换，而是基于长久积累的信任。\n如果我们能够在自己力所能及的情况下，不计回报地给别人提供一些帮助，你将会在后面几年的人生中，收获意外惊喜。\n我自己在几个月前开始创业，现在圈外的课程设计负责人，是我的前同事，一位非常资深的项目经理。\n她本身对创业不感兴趣，在做自由顾问，但因为我们课程设计的要求很高，初创阶段又没有足够的钱，所以很难找到适合的人。\n她就拿了相当于自己做自由顾问 1/10 的工资，帮我设计课程、培养我们的课程教练、带团队，丝毫没有犹豫。我几次觉得不好意思，想要多给她一些回报，她跟我说：“等你做大了再说，我现在还不缺钱。”\n然后，最近因为我们产品内测，需要更多的课程迅速上线，她甚至推了报酬丰厚的咨询项目，投入了更多时间在圈外。\n我从前极少麻烦朋友帮忙，但这段时间，得到了太多人的帮助，感觉自己消耗了前 10 年积攒的人品。\n所以人际交往这个事情，真的是个长期投入，如果在你想用的时候才去培育，是来不及的。\n年轻的时候，能多帮别人就多帮，别去计较一时得失，过几年自会发现，“帮助别人” 是你在年轻时候做的最有价值的投资。\n读完这篇文章，年轻的你就能接受并照做吗？未必。\n你们可能会说我是在倚老卖老、讲大道理。然而，这大概就是人生的有趣之处，总要自己痛过，才会愿意领悟。\n年轻的时候，有的是资本去试错，也应该去试错，唯有如此，才能够从中吸取教训，通过与现实的碰撞不断探索自己的意义，最终走向心智成熟。\n还是那句话，只要还年轻，对未来的恐慌、对成功的渴望、对未知的不安、对想要而不得的不甘…… 这些弯路，该经历的都一定会经历，该焦虑的也一样都不会少，无处可逃。\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"孤儿进程和僵尸进程","url":"/2015/12/05/%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B/","content":"基本概念\n正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程 , 即父进程永远无法预测子进程\n到底什么时候结束。\n孤儿进程和僵尸进程就是由于这个异步过程没有正确执行而引起的问题。下面先看看这两类进程的一些概念。\n\n孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被 init 进程 (进程号为 1) 所收养，并由 init 进程对它们完成状态收集工作。\n僵尸进程：一个进程使用 fork 创建子进程。如果子进程退出，而父进程并没有调用 wait 或 waitpid 获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。\n问题及危害\nunix 提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息，\n就可以得到。这种机制就是:\n在每个进程退出的时候，内核释放该进程所有的资源，包括打开的文件，占用的内存等。\n但是仍然为其保留一定的信息 , 包括\n\n进程号 (the process ID)\n\n退出状态 (the termination status of the process)\n\n运行时间 (the amount of CPU time taken by the process) 等\n\n僵尸进程危害：直到父进程通过 wait/waitpid 来取时才释放。但这样就导致了问题，如果进程不调用 wait/waitpid 的话，那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程.\n此即为僵尸进程的危害，应当避免。\n孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了 init 进程身上，init 进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤\n儿进程的父进程设置为 init，而 init 进程会循环地 wait () 它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init 进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。\n寻找并杀掉僵尸进程\n僵尸进程危害场景：\n例如有个进程，它定期的产\n生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程\n退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用 ps 命令查看的话，就会看到很多状态为 Z 的进程。\n严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大\n量僵死进程的那个元凶枪毙掉（也就是通过 kill 发送 SIGTERM 或者 SIGKILL 信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进\n程，这些孤儿进程会被 init 进程接管，init 进程会 wait () 这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程\n就能瞑目而去了。\n上面是具体的思路，下面是实际的操作：\n查找僵尸进程：\n可通过 top 命令查看当前是否有僵尸进程，如下图 0\nzombie 表示没有僵尸进程\n\n假如该值不为 0，可以通过 ps 和 grep 命令查找僵尸进程，如:\nps -A -o stat,ppid,pid,cmd | grep -e '^[Zz]'\n命令注解：\n-A 参数列出所有进程\n-o 自定义输出字段 我们设定显示字段为 stat（状态）, ppid（进程父 id）,\npid (进程 id)，cmd（命令）这四个参数\n因为状态为\nz 或者 Z 的进程为僵尸进程，所以我们使用 grep 抓取 stat 状态为 zZ 进程。\n找到这些进程后，可以直接 kill 掉僵尸进程，如果僵尸进程的数量比较多，也可以 kill 掉其父进程的 pid 让 init 经常回收这些僵尸进程的资源。\n参考资料：\n[1] http://www.cnblogs.com/Anker/p/3271773.html\n[2] http://be-evil.org/linux-find-and-kill-zombie-process.html\n","categories":["操作系统"],"tags":["操作系统","Linux"]},{"title":"常用数学符号的 LaTeX 表示方法","url":"/2016/09/18/%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E7%9A%84%20LaTeX%20%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95/","content":"在 Markdown 中编辑数学公式一般是使用 LaTeX\n来渲染和排版的，但是一些数学符号的 LaTeX\n比较特殊，常常会忘掉，因此在这里特意记录这些数学符号用 LaTeX\n的表示方法。\n\n指数和下标\n指数和下标可以用 ^ 和 _\n后加相应字符来实现，如果指数或下边多于一个字符， 那么需要用\n{} 将其括起来\n\n平方根\n平方根（square root）的输入命令为：，n 次方根相应地为:\n。方根符号的大小由 LATEX 自动加以调整。也可用仅给出 符号。比如：\n\n分数\n分数（fraction）使用 排版。\n\n积分、求和、连乘\n积分运算符用来生成。求和运算符由生成。乘积运算符由生成。上限和下限用 ^\n和_来生成，类似于上标和下标\n\n表达式的上、下方画出水平线\n命令和表达式的上、下方画出水平线。比如\n\n表达式的上、下方给出一水平的大括号\n命令和在表达式的上、下方给出一水平的大括号。 比如：\n\n向量上方的箭头\n向量通常用上方有小箭头（arrow\nsymbols）的变量表示。这可由得到。另两个命令和到 B 的向量时非常有用。\n\n其他一些数学符号\n\n\n\n\n\n\n\n\n参考：http://mohu.org/info/symbols/symbols.htm\n","categories":["工具使用"],"tags":["工具使用"]},{"title":"工作、体制化与自由","url":"/2025/05/03/%E5%B7%A5%E4%BD%9C%E3%80%81%E4%BD%93%E5%88%B6%E5%8C%96%E4%B8%8E%E8%87%AA%E7%94%B1/","content":"最近一段时间很忙，忙到整个大脑带宽被打满、回到出租屋只想躺平放空，或是忍不住无意义的刷短视频；忙到觉得自己工种变了（变成了一个消防员，天天在救火）；忙到甚至没有没时间去做更长期规划；能明显感觉到自己的词汇量、语义精度、表达能力和表达欲在迅速下降，同时自我对知识、对他人、对世界的好奇心和热情，似乎正在被浇灭。人身处其中的时候也许并不觉得有什么大碍，因为已经被 “体制化” 了，但一旦有更长的空闲时间，开始做 “上帝会发笑” 的行为，就能愈发感觉到了这种状态的恐怖之处，我觉得可能是时候给自己做个诊断了，于是有了这篇文章\n本文主要是对最近一些经历和疑惑的碎碎念，以及试图求得其中的破解之法；涉及工作、情绪、短视频、以及自由的追求，文章极度发散，内容极度主观，就是一个情绪出口和以及写给自己的心理按摩，如果你愿意看，那祝开卷有益～\n\n工作与情绪\n如果说这一切的开端，也许是工作及其带来的情绪变化\n\n世人慌慌张张，不过图碎银几两。偏偏这碎银几两，能解世间万种慌张\n\n为了 “碎银几两”，我们不得不去工作，除了极少数的幸运儿能从工作中获得满足感与成就感，达到理想中的\nIkigai\n状态。对于绝大部分的 “世人” 来说，工作带来的也不仅仅是 “慌张” 的情绪，反刍过去、灾难化未来、他人眼中期待的自我与内心真实声音的矛盾，这些行为会迅速让人陷入我们称之为 “内耗” 的状态：瞻前顾后，忧心忡忡地迈出每一步，还得时刻担心着自己明天会不会被裁了，进而迅速消耗掉一个人的生命力\n该用何种心态面对工作、以及在工作中衍生出来的种种情绪，也许是每一个职场人的必修课，尤其是对于那批度过了懵懂的新手期、工作了几年开始建立了一些职场认知、对未来有一些期待但现状有不太符合预期、处于\nburntout 阶段的职场人\n内耗：自我、他人、规训\n内耗是最近几年在职场特别流行的一个词语；在心理学上，内耗并不是一个专业术语的，它描述的是一种心理状态，在这个状态当中内心充满的是挣扎和冲突的感觉\n内耗的人一般会有的两种特质：反刍和灾难化思维。顾名思义，反刍就是把过去不好的体验、情绪和经历，反复在脑海里重演，回味那种尴尬的情绪和氛围，犹如吃苦耐劳的老牛一样，让那些本来应该消化掉的情绪一遍遍重复折磨自己；灾难化思维则是把一件事情的结果和未来都想象成最差的结果，在自己的脑子里当了一回灾难片的导演，自己来恐吓自己；这两种负面的情绪导致了一个人会把一件事情的结果或者过程过度强化，所以\nta\n内心就很焦灼，那这个过程就发生了很多的挣扎和冲突，消耗了大量的心理能量，或者说生命力\n在工作中导致内耗的直接的原因，《职场心累真相：90% 的疲惫和焦虑都来自这 3 种隐形内耗》中指出有 3\n个：（1）过度的自我批评（2）过度的关注他人的评价（3）限制性思维：即你 “应该” 这么做\n前两者的关键词在与 “过度”，因为过犹不及，在其他语境下，这两点的描述的可能就是善于自省、共情能力强的这类偏 “褒义” 的特质了；第三点则是常说的社会时钟和社会规训，我们做的很多事情，也许并不是自己想做的，而是到了某个年龄，身边的亲人、朋友、媒体都在无一例外告诉自己应该去这么做的（比如说买房、结婚、生子）；在工作中，这几个原因的外显，就是当一个任务没能很好地完成时，会过度归因到自己身上，担心自己在其他人眼中会是一个 “没能力” 的员工，或者说日常的工作被一些自己不感兴趣但不得不去做的事情，工作的意义感荡然无存\n另外，在生活中也许可以不在意其他人的眼光、不在乎社会给你打的标签；但是在工作中这并不容易，因为工作就是一个需要关注他人评价的过程，是一个被公司规训的过程：你需要关注合作方的评价、需要关注上级的评价、需要遵守公司的规章制度，做什么、不做什么都是有白纸黑字写着的，你需要交付好你的价值，以此来得到一个工作机会，以及可能的一个还不错的绩效。因此在工作中，第\n2 点和第 3\n点基本就是难以避免的，我们总得关注其他人的评价，总得去做公司让我们做的事情，当然，这里的 “度” 还是可以控制好的，但大部分的人是没法很好控制的，这其中也包括笔者\n所以在工作中，更可操作的往往是第一点，即我怎么看待工作、看待我自己，或者说该用什么心态去应对工作及其衍生出来的一系列人和事，毕竟如同《男孩、鼹鼠、狐狸和马》里提到的 “我们只能看到外在，但一切发生在内在” 一样，如同《遇见未知的自己》中提到的 “外面没有别人，所有的外在事物都是你内心投射出来的结果” 一样，我们在内心塑造了世界，然后这个世界又塑造了我们，或者大白话说，同样一件事，心态会极大地影响我们的情绪和动作\n工作只是交换，意义感和爱并非题中应有之义\n对于完美主义者而言，在工作的很多时候是在演绎一个期待的自我，所以也会极度在意自己的行为，以及其他人的看法；虽然这种期待往往是不现实的\n这种演绎会让一个人在工作进展不顺利时，会反复想是不是因为自己不够努力，而导致了结果不符合预期，而忽略了这个事情其实还跟上下游的合作方的配合有关、跟老板的方向判断有关、跟当前的市场环境有关；会让一个人反复猜测自己是不是领导心里那个最好的候选，如同小时候想的 “我是否父母心里最好的孩子”，进而会怀疑自己是不是做得够了，是不是要做得更多（这里面多多少少有一些讨好型人格在里面了）\n为什么会这样？在上面的关于职场心累真相的那一期播客，以及《每个人都有至暗时刻，其实你并不孤单》中都提到了类似原因，心理学上有一个概念叫 “价值条件化”：只有符合其他人的期望才能等到其他人的爱（有条件的爱），这种一般来自早年的经历，作为儿童的我们会有一种默许的规定：如果我能够被别人爱，能够被别人肯定，那是因为我按照权威对我的评价和指点去做了某些事，这个爱是有条件的，这些条件就是我只有按照他这么做我才有价值。这里又涉及到了老生常谈的 “原生家庭”、“东亚小孩” 的问题，社交媒体每每讨论到这一类话题，似乎都演变成了停滞在原地不动的、对原生家庭的批斗，《被讨厌的勇气》中把这种行为成为目的论而非原因论，即当前的不幸不是因为原生家庭导致的，而是因为先入为主相信了自己就是不幸的，不想改变而去找的原因；而我们需要谨记的一点是：人是有选择权的，是有能力选择自己的生活，不需要、也不应该用过去的经历来解释自己的一生\n扯远了，工作很重要，但是意义感和爱并未题中应有之义，如果你是极少数能在工作中获得这两者的幸运儿，那么恭喜你；但对于绝大部分的人来说，工作就只是单纯的交换，用自己的时间精力、专业知识技能、指哪打哪的服从能力，去换取让自己维持生计的工资，仅此而已；在工业化程度越来越高的社会里，我们所做的事情会越来越螺丝化和单一化，这种标准化带来的结果是系统的鲁棒性和可持续性，因为螺丝坏了，替换一个就好，系统能够简单而快速地自我修复；而这也注定了工作对于个人的意义和价值在当今是一个奢侈品。打扮精致、穿行于灯火璀璨的高档写字楼的白领，跟流水线上、码头工地上重复着单一劳作的人，也许并无二样，即使是对于从事着看起来高大上的 “智力行业” 的程序员，不禁想到了大刘在《2018 年 4 月 1 日》那段让人拍案叫绝却、如同预言家一般的描述\n\n程序员网络工程师、数据库管理员这类人构成了 IT 共和国的主体，这个阶层是 19 世纪的产业大军在 21 世纪的再现，只不过劳作的部分由肢体变成大脑，繁重程度却有增无减\n在浩如烟海的程序代码和迷宫般的网络软硬件中，他们如两百多年前的码头搬运工般背起重负，如妓女般彻夜赶工\n信息技术的发展一日千里，除了部分爬到管理层的幸运儿，其他人的知识和技能很快过时，新的 IT 专业毕业生如饥饿的白蚁般成群涌来，老的人（其实不老，大多三十出头）被挤到一边，被代替和抛弃，但新来者没有丝毫得意，这也是他们中大多数人不算遥远的前景…… 这个阶层被称作技术无产阶级\n\n对于意义感，《每个人都有至暗时刻，其实你并不孤单》中提到，对于刚入职场的年轻人，因为对工作的新鲜感和热度还没褪去，也许还能通过给自己洗脑，赋予自己所做的事情以一些意义感，但是随着被职场毒打了几年，经历了更多的人和事后，对工作和职场看得更清楚、认知越来越清晰、再也没法给自己洗脑装睡后，会发现工作它不提供意义，它主要提供工资。但是播客也提到，一个人如果到了\n40\n岁再往后，或许可能会发现自己在做的事情会和这个世界某一部分的人群产生一些深层次的连接，ta\n能感受到一些召唤或者使命，即意义感这个东西可能是到了中年以后会慢慢浮现出来，或者至少对一部分幸运儿它会浮现出意义感，有种 “看山还是山，看水还是水” 的感觉\n同样的，工作很重要，但不值得和爱挂钩；没必要把自己在原生家庭或者小时候某种没有被满足的需要投射到了职场当中；如果你太多的把工作等同于你的个人价值，等同于你有没有被爱、被尊重，等同于有没有成为 “父母眼中最喜欢的孩子”，会把个人的很多边界模糊掉（比如说生活和工作），而模糊带来的后果是很多动作会变形，心态也会变形\n工作只是交换，为了交换我们不得不接受规训、做一些自己没那么喜欢但也没那么讨厌的事情，那在这个过程中，工作时候的 “我” 并不是真实的我，同样的，由于工作而导致别人喜欢或讨厌的 “你”，都不是真实的你。心理学上有两个词跟自我有关，一个是\nego（客体的自我），另一个是\nself（主体的自我），前者更多指的是别人眼中的 “我”，包括 “我是不是一个被爱着的人”、“你是否能从我身上看到我有能力、我有价值” 这些都是跟\nego\n有关的，它是一个跟自尊、自我评价，甚至有的时候是跟自负有关的一个自我；后者则是跟自我实现有关，“我要去创造什么”、“我要做什么样的事情”，都是跟\nself 有关的\n职场中的 “自我实现” 的自我，往往指的是前者\nego，而这其实是有点碍事的，因为工作就是去完成一个社会化的过程，去扮演一个 “拧螺丝” 的角色，但这个角色跟真实的自我\nself\n其实关系并不大，我只是去完成一个交换，在这个交换过程中，也许会让一些人喜欢，也会让一些人讨厌，但这并无大碍，因为这个角色并不是真实的自我\n所以，不要在职场寻求爱，给予爱与被爱对大部分的人来说都很重要，你可以从你的朋友圈子、从你的爱人身上、从你的家庭中寻找和体会，但不要在工作当中去感受，因为不值得，套用李松蔚老师的话是这么说的：“对于一个有一定工作经验的人来讲，工作是很简单的。工作可能会涉及到计划、涉及到执行，有时候会涉及到创造，但是跟你的 ego，跟你的自我它没有那么大的关系，就是你在你的岗位上边做这个岗位应该去做的事情。这么说起来好像有点冷酷，但是在我心里边，其实我觉得有时候我们抱着冷一点的态度去面对一份工作的时候，有可能会让他做得更准确，然后你自己更轻松”\n刀刃向内，给思想做一场手术\n侃侃而谈这么多，最终还是要寻求一些破解之法的，否则就成了烂大街的那些没有勺子的鸡汤一般\n如同前面所说，我们在内心塑造了世界，然后这个世界又塑造了我们，破解之法的关键，也许就是刀刃向内，为自己的思想做一场手术，改变自己的一些认知和想法，毕竟他人的评价和公司的规训，在职场中我们确实是不得不关注的\n（1）觉察并接纳\n能够察觉到自己的思维模式（反刍、灾难化思维等）和行为（过度自我批评、关注他人想法等）是导致了自己内耗的重要原因，有时候只是 “看见”，就能让自己的焦虑下降不少，因为再也不是如同无头苍蝇一般恐慌；其次是能够接纳真实的自己，接纳自己现状就是这么一个人，接纳事实是自己过去的经历塑造成自己就是这么一个人，但需要注意的是接纳并不是摆烂，人是有主观能动性的，是可以尝试去解决当下遇到的问题的\n（2）课题分离\n这里的 “课题” 指的是人能控制的事情，可能粗略分为个人课题和他人课题\n\n个人课题（Personal\nTopics）：指个人可以控制和改变的行为、想法和情绪。例如，一个人可以决定自己要学习什么、如何安排时间、如何应对压力等\n\n他人课题（Other-People\nTopics）：指他人的行为、想法和情绪，这些是个人无法直接控制的。例如，你不能控制别人对你的看法或者他们的行为方式\n\n课题分离指的就是，别人怎么看带我是他们的课题，我无法干预；我们不必不干涉他人的课题，也不被他人的课题干涉（雅俗共享版就是 “关你屁事” 和 “关我屁事”）；因为我们需要意识到一点，自己不用让所有人的满意，一是因为无论你怎么做，都没办法做到让所有人都满意，二是他人其实也没那么在意你，很多你以为的 “别人眼中的你”，其实是你再给自己的大脑制造幻想，自己给自己加戏\n（3）角色扮演\n如同前面提到的 “工作只是交换，意义感和爱并非题中应有之义”，工作最重要的过程是交换，某种程度上我们只是在扮演一个社会化的角色，而在这个过程中，意义感和爱并不是必须之物，如果能找到那当然很好，找不到其实也是非常正常的\n可以把自己想象成游戏中扮演的角色一样，上班就是游戏启动了，下班了退出登录回到现实世界中，游戏中的那个你，并不是真实的你；别人喜欢或讨厌的你，也不是真实的你；真实的你是回到现实生活的那个你\n（4）保持 “初” 心\n在《心理学博士怎么发疯？和李松蔚聊情绪：职场、东亚小孩、内耗、嫉妒和愤怒》中提到，职场中往往是新人的心态会比较好，因为刚工作，心里没太多负担，同时觉得未来是有盼头的；但新工作一段时间，晋升困难，努力的边际收益在初步递减，这个时候容易青黄不接，进而产生内耗等种种负面情绪，某种程度上，这是因为有了一年的工龄后，对自己的预期变得过高导致的\n保持 “初” 心，或者说保持年轻的心态，意味着需要保护好自己的好奇心，时刻提醒自己 “我的认知是有限的、需要持续学习，工资少点没关系我也是在学习中的”；而不要过度关注自己的实际年龄（社会时钟规训的在外在表现）\n（5）用好老板\n需要意识到，老板是你的 “资源”，是协助你把事情做好的 “工具”；不要害怕去使用这个工具，或者麻烦到老板，因为老板的职责之一就是要协助你拿到目标的\n虽然上面提到很多不要过度在意他人评价的原因和方法，但在职场老板的评价还是重要的，因为这决定了你的晋升和绩效等你希望交换得到的东西；但在这个过程不要去猜老板的预期，而是需要跟老板做好阶段性汇报，主动把一些进展披露给老板，老板也需要知道你的情况，同时应该根据你的情况给出建议和支持，老板真正害怕的是你一声不吭给他一个大的\nsurprise\n另外，不要压力过大，一个事情做不好，你老板也需要担责，老板有可能是误判了这个事情的预期，才把事情分配给你\n（6）多样的参考系\n不要让自己的生活只有工作这一个参考系，当你生活的支点越多，你的内核就越稳定，如同在《爬过第二座山》中提到的一样\n\n当一个人评价自己的标准过于单一，比如只有工作业绩或者是财富数字；如果有一天这个唯一的参考系崩塌了，这对\nTA 来说，无疑是一种毁灭性的打击；这点也许对那些工作狂的一种诅咒\n\n上面这 6\n个方法，篇幅很短，但是要真正做起来，其实很难，因为要在大脑中植入新的想法，要改变自己过去几十年养成的习以为常的习惯，需要我们长时间持续的刻意练习\n短视频与 “体制化”\n工作压力大、极度焦虑的时候，总忍不住刷短视频。犹如精神吗啡一般，短视频 “治愈” 着北上广深的钢筋丛林里被工作折磨了一天的打工人的身心灵，手指翻飞间，奇闻异事、家国天下，都缓缓流淌在那巴掌大的屏幕上，刺激着人的多巴胺分泌，让人流连忘返，忍不住往下刷，甚至于深夜不眠；然后第二天精神萎靡地回到工位，身心灵被工作暴揍一天后回到出租屋，继续刷着短视频疗伤，日复一日，仿佛一个无限的循环\n这个现象在很多人身上都有，包括笔者，长期在这种 “好想逃却逃不掉” 的习惯下，也会让人不禁会问：到底是什么操控着我们做这个事情，这样到底会有什么影响，以及有什么方式能打破这种恶性循环（如果真的想打破的话）\n“体制化”\n在知乎的这个回答中，提到一个很有意思的现象 “早酒文化”\n\n很多码头城市都有 “早酒文化”，也就是在早上喝几杯白酒，搭配高碳高脂的早餐，吃完以后再回家睡觉。形成这种文化的主要原因是，码头城市有很多以卸货为生的工人，而卸货通常是在晚上进行，在苦熬了一个通宵之后，已经身心俱疲，为了能睡个好觉，只好在早餐时喝点高度白酒，时间长了就形成了一种文化\n这种生活是很伤身体的，这一点码头工人肯定也知道，但喝早酒的习惯却根深蒂固。原因也很简单：不这样做日子是要过不下去的。在充满苦的环境中，早上的几杯劣质白酒是唯一可以确定的甜，是生活唯一的 “正反馈” 来源。这也就是为什么有人劝他们戒酒，他们就会来一句 “这是我唯一的爱好，戒了活着还有什么劲”\n\n仔细想想，其实我们和码头工人也没什么区别，大部分人的工作都是没有正反馈可言的，基本上就是枯燥任务的反复循环，犹如被惩罚的西西弗斯，一次又一次地推石头上山。人如果长期呆在这种负反馈的环境之中，生命力很快就会耗尽，不得不依靠 “低级娱乐” 来输入正反馈。这也就是为什么很多人下班以后，选择刷短视频，而不是学习的原因\n为什么是短视频而不是长视频呢？因为在高压社会下，个人精力被工作的过度耗竭，让个人难以拿出足够的时间精力接受整块信息，被迫在碎片化的摸鱼休息时间中，做一些不动身体也不走脑的低功耗行为\n但吊诡之处在于，当工作中的负反馈和业余时间 “低级娱乐” 带来的正反馈达到某种平衡以后，所有尝试打破这种平衡的行为，都会变得难以忍受。比如你打算告别短视频，尝试在业余时间学习新技能，容易导致工作中的负反馈无法消解。与此同时，进入陌生领域也会带来新的负反馈，两者叠加在一起，人就很难坚持下去（大多数人自我提升计划的失败，基本上都是这个原因）\n长期如此，人就会被环境 “体制化”，也就是身体、大脑、认知，甚至是性格，都会往融入当前环境的方向发展，以适应让自己不那么痛苦；毕竟 “人是环境的反应器”，毕竟 “与恶人居，如入鲍鱼之肆，久而不闻其臭，亦与之化矣”\n事实上，人的适应性是很强的，无论是对于上面的痛苦还是生活的愉悦；而这也许就是可乐只有在第一口是快乐的原因，体验过、适应了这种快感，就会觉得平平无奇，需要更强的刺激和体验才能够重新体验到新的快乐，我们追求的是 “加速度” 而不是速度本身。柯立芝效应也是在说这个事情，七年之痒，无非就是新鲜感全然殆尽，又没能发展出比新鲜感更深的维系双方关系的联结（我们往往管这叫亲情）\n扯远了，人是适应性很强的动物，但在这种情况下的适应的代价就是灵气的消失，以及学习能力的退化，简称为 “体制化”。从某种角度来说，被 “体制化” 好像也没什么不好，最起码痛苦的感受没那么强烈了。但人所有的行为都是被情绪驱使的，所有的努力都是对当下的不满，如果痛苦的不再让你感到的痛苦，难受的不再让你难受，就会选择安于现状，人生的可能性也就消失殆尽了。就如同影史经典《肖申克的救赎》对 “体制化” 的阐述：\n\n这些围墙很有趣，开始，你恨它，接着你适应了它们。时间久了，你开始离不开它们，那就是被体制化了（Institutionalized）\n\n如果你不幸被 “体制化” 了，从里面爬出来可能要花费很大力气，因为这是一个很深的泥潭，靠自己的力量挣扎出来会很难。如何避免被体制化，回答里也给了一些建议\n\n用低功耗维持工作运转。具体来说就是，把工作当成工具而不是目的，在心理上与工作脱节，不主动辞职，但也不追求进步，更不和同事争抢。把老板、上司、同事在内心 “NPC 化”，也就是不把他们当人看，无论他们多么傻 x，都不与他们发生冲突，但也不多干活\n把节省出来的精力，专注于提升自己和扩展 “弱联系”，避免认知和人脉被 “体制化”\n弱联系是什么意思？微弱的联系，比如说逢年过节群发的消息，可以理解为朋友圈的点赞之交，也就是互相知道对方是做什么的，但在生活中几乎没有什么来往。然而这种关系的作用，往往比生活中经常见面的 “强联系” 要大的多。可能在未来的某个瞬间，这种弱关系就能帮助到你\n熟人社交是无效社交。人们通常只能从 “强联系” 中获得表达性支持，简单来说就是熟人只有情绪价值，你不开心了，能抚慰你，并和你一起吐槽让你不开心的人；而从 “弱连接” 中则更容易获得工具性支持，能产生更多实质性的价值\n\n斯金纳箱的奖赏\n在这一期关于抽卡的视频中，提到 “斯金纳箱实验（Skinner\nBox Experiment）”，deepseek 给出的这个概念和实验过程如下\n\n斯金纳箱实验，给我们呈现的事实是：小白鼠在一个有一定概率产生正反馈的系统中，会不断重复尝试某一个动作，以期望能够得到正反馈，即使这个过程中的也会出现负反馈（惩罚），小白鼠还是会乐此不疲得去重复这个动作，因为\nta\n期待的是下一次未知的、有可能会出现正反馈的、持续分泌多巴胺让小鼠沉醉其中的奖赏\n这跟巴浦洛夫的狗还不太一样，斯金纳箱里的小白鼠是自发学会了不断重复尝试某个动作；前者是经典条件反射（被动关联刺激与反应），后者主动行为与结果的联系\n为什么现在但凡是个游戏都加入了抽卡机制？是因为游戏抽卡就是一个大型的斯金纳箱实验，在一次次游戏抽卡行为中，我们都在渴望着下一次的抽卡能够出现金光、能够抽到稀有角色、能够获得 “欧王” 的奖赏\n斯金纳箱的原理，也被应用在了各种短视频 app\n中，小白鼠按压开关的行为变成了我们每一次的下拉刷新，以及在下拉刷新后期待得到的概率的奖赏：经过提纯的高浓度片段。没错，短视频是经过提纯的高浓度片段，将过去需要花大量时间去观看和思考的长视频碎片化成更纯粹的刺激、每次划屏就是换内容、算法还能记住你的偏好的提纯片段；每一次的下拉刷新，都是一次未知的奖赏或惩罚，如果是前者，会让我们在屏幕上疯狂点小红心，享受这个内容的同时开始期待下一次的奖赏，如果是后者，那也没关系，只需要轻轻一划，便能够重新拥有获取这种奖赏的期待，并且短视频推荐算法的考核机制，决定了你划到奖赏的概率会越来越高\n所以，在深夜中一次次下滑屏幕刷短视频的我们，跟那只在斯金纳箱中重复按下按钮的那只小白鼠，也并无二样\n那代价是什么\n我很喜欢《钢之炼金术师 FA》这部动漫，以及里面提到的 “等价交换” 的概念：你如果想得到一些东西，必须要付出一些东西；因为 “命运所有馈赠的礼物，早已在暗中标好了价格”\n不可否认的是，刷短视频的时候，人是快乐的，虽然这个快乐有点短期，也没法回味（仔细想想刷了几十个短视频的你，还记得这些短视频的内容么）；那么，这些快乐的代价是什么\n\n表达能力\n\n如果你花了大部分时间来观看短视频，你会发现，在日常生活中，不借助常用的网络用语，而是用自己的语言把一件事情描述的足够清晰，也会变成一件很困难的事情；你会发现过自己说话时如鲠在喉，想说但又说不出来，因为自己的词汇储备量，已经是很匮乏了\n短视频平台的推荐算法考核目标，就是是内容快速传播、用户时长和完播率等，在这种模式下天然收到追捧的内容，是那些时间短、文本简洁、充斥着猎奇的观点的内容，同时导致短视频为了为了简洁，会使用大量的 “梗”，或者极度简化其中的事实与逻辑\n这个回答就提到了，长时间的过度感知劣质样本集（在短视频的喜好推送中很容易重复出现）的结果有两个\n（1）降低你的词汇量和语义精度，而这两点是思考、学习和认知的基础\n（2）对感知的取样模式变得单一化 / 简单化，并且调整脑部的功能区用于对这种拟合的计算和思考，从而挤占了进行其他计算（比如中心路径的深入思考）的功能区\n\n阅读长内容能力\n\n仔细想想，短视频刷多了后，你还有耐心坐下来把一部电影完整看完吗？你在阅读长文本的时候，还有足够的耐性和时间去把文字完整读完吗？还是说在期待有一个短视频帮你把电影内容或者这本书的内容给总结了？\n短视频通常以 15 秒到 1 分钟的短时长呈现，内容快速切换且充满视听刺激（如音乐、特效、高频率剪辑）。这种设计让大脑逐渐适应了 “快速反馈” 模式，触发大脑多巴胺的快速释放，形成 “观看 — 愉悦 — 更多观看” 的循环。这种即时满足机制降低了对延迟回报（如长视频的剧情铺垫或长文本的逻辑推导）的耐受性。长期来看，大脑对低刺激、需深度思考的内容失去兴趣，转而追求更易获得快感的短平快信息，导致注意力难以长时间集中于单一任务\n沉迷这种快速反馈模式后，容易失去阅读长文字处理的功能，或者失去看内容深刻的长视频的功能。这时候再去进行阅读或者一些深度学习行为就会由于功能区被转移，而导致剧烈的不适感\n\n系统性思考与推理能力\n\n短视频的内容多为孤立片段，缺乏系统性，用户被动接受信息而无需主动整合逻辑。这种碎片化输入模式导致思维连贯性下降\n另外，短视频篇幅过短，基本上只支持作者抛出观点和结论，但没有时间来论述这个结论或观点的推理过程，即只有 “是什么”，没有 “为什么”，而这又是验证观点是否合理的很重要的一环，同时也是能够延伸出很多其他话题的的重要过程，是锻炼思考能力的很重要的一个过程，如下图就是一个比较具体的过程\n\n而当我们长期在这种只追求一个简短精炼的结论的阅读中，造成的后果是我们不再关注观点是否有支撑，大部分人只关注观点是不是合自己的胃口，同时对于各类社会现象，更多以感性的情绪输出为主\n长期以往，你的知识体系会变得支离破碎，缺乏辩证思考能力，对于各类社会现象，更多以感性的情绪输出为主，久而久之，人就失去了识别真知的能力\n\n空虚感\n\n你是否会有过这种感觉，连续刷了几个小时的短视频后，会让人感觉特别空虚；与这种空虚相对反的，是用几个小时来专注做一件事情的充实感\n这个回答就提到了这个原因，即人觉得充实与否取决于人的意识是否和时间的流逝所匹配\n几个小时的短视频，起码得有几十个，但由于短视频本身是毫无规律可言的，所以短期记忆里没有留下什么值得大脑分析的东西；留不下东西，自然没有可分析的结论；相当于花了好几个小时，但是大脑只有大致印象，所以你的意识认为和时间流逝不匹配\n但如果在做同一件事，大脑就会不停地强化有关这件事的记忆，并且由于同一件事的普遍规律性较强，人会很快分析到一些结论，对于人的感受来说，就是记得做了什么，并且知道做了什么，也得到了一些经验，感觉就很充实\n我知道应该，但真的做不到？\n上面提到的短视频对人的危害的观点，其实早就在各种媒体上被广泛传播过，但效果就有点像烟盒上印着” 吸烟有害健康” 一样，被烟瘾者习惯性的忽视了。为什么我知道应该，但就是做不到？\n除了上面提到的 “体制化” 现象中，需要通过这类有短期 “正反馈” 的行为应对无聊的工作带来的生命力的耗损外；在《问题不大》的这期播客，就从心理学上分析了，报复性熬夜、持续刷短视频这种 “我知道应该，但真的做不到” 的现象的原因；播客内容很不错，推荐一听，这里就只摘录一些核心内容\n在心理学的图式治疗中，认为人的体内有三类角色：内在的小朋友，内在的父母，应对者\n小朋友跟弗洛伊德的本我有点像，小朋友也有很多类型，比如说脆弱的，焦虑的，愤怒的（这其实也是人很多情绪的映射）；而最常见的是脆弱的小朋友，与脆弱的小朋友同时出现的往往还有一个惩罚性的，或者叫高标准的父母\n脆弱的小朋友代表个体内心未被满足的情感需求，通常与童年创伤（如被忽视、虐待、情感剥夺）相关。有点像一个受伤的、需要被保护的孩子，承载着恐惧、孤独、羞耻等原始情绪；脆弱的小朋友会在我们的内心发出这样的声音：“没有人会真正爱我”、“我永远不够好”、“我需要别人照顾我才能生存”\n高标准的父母则是我们童年时期父母或重要他人的负面声音的具象化，这里面有分为两种类型：惩罚性父母（Punitive\nParent）：象征严厉的批评和道德压迫；要求性父母（Demanding\nParent）：象征过高的标准和不现实的期望；我们心里很多自我攻击的声音，其实就是高标准的父母说的：“你活该被惩罚”、“你必须完美”、“你真没用”、“你不配被爱”\n应对者则是为了逃避 “内在小孩” 的痛苦或满足 “内在父母” 的要求，发展出的防御策略；面对体内的矛盾，一般出现的应对者行为有三种：屈从（Surrender）、回避（Avoidance）、过度补偿（Overcompensation）\n\n屈从型应对者：认同 “内在父母” 的批判，自我贬低（如：“我就是个失败者”）。行为上的表现被动接受虐待关系，不敢争取权益\n\n回避型应对者：逃避触发痛苦的情境（如拒绝亲密关系、用娱乐、忙碌来麻痹自己）。行为上的表现就是情感麻木、社交隔离\n\n过度补偿型应对者：用相反行为掩盖脆弱（如：通过控制他人对抗 “被抛弃” 的恐惧）。行为上的表现就是傲慢自大、过度竞争、讨好他人\n\n为什么明知应该早睡，却熬夜刷手机？套用上面内在角色，可以在内心演化出这么一出戏\n（1）内在父母激活：“你必须早睡！熬夜会变丑变笨！”（严苛要求）\n（2）内在小孩反抗：“工作一整天已经很累了，我就想放松！”（需求未被看见）“早睡计划失败过那么多次，我肯定做不到……”（恐惧失败）\n（3）应对者行动：用刷手机转移对焦虑的注意力（回避）\n此时回避型的冷漠应对者为了屏蔽掉这些感受声音，会采取一种麻痹的方式，即机械地刷短视频，但此刻已经不是为了开心才这么做，单纯就是回避那种心里面很焦虑的感觉：不想再听小孩和父母吵了，然后就会用刷短视频的方式麻痹自己（有多少人中枪了）\n“我知道应该，但真的做不到” 的本质，是内在父母用恐惧驱动改变，而内在小孩用抗拒保护自我，那我们能做些什么，来缓解这种矛盾，播客里提到方法可以总结为：识别出三类声音，然后引入一个健康的成年人角色，来疗愈内在小孩、修正内在父母，并做出循序改变\n第一步：识别三类角色的声音\n写下冲突时的内心对话，一般的模式是这样的\n\n内在父母：“你应该……”\n内在小孩：“可是我害怕 / 累 / 不想……”\n应对者：“算了，先玩会儿手机吧。”\n\n第二步：引入一个健康的成年人角色，修正内在父母，疗愈内在小孩\n对内在父母，可以告诉他，你的做法是想个人过得更好，但也许你用这种非常苛责的口吻说话会适得其反。你可能会让脆弱的小朋友、焦虑的小朋友，他们都更不安，而他们的需求跟你的需求同样的重要，而当他们的需求没被满足时，可能个人也没能过得更好\n对于内在的脆弱小孩，可以告诉 ta\n说你的这种需求是完全正当和合理的，你想要休息，你现在的日程表对任何人来说都是难以承受的，他就是会很累，就是你感到累，你想要休息这个需求我完全的认可、尊重和理解\n但当我们表达对脆弱的小孩的理解和共情后，焦虑的小朋友可能在旁边就坐不住了，他可能会说 “你怎么能这样？你不会真的同意了吧？你不会真的让他休息一整天吧？那么考试就要通不过了？也学习不到新知识了”\n那这个时候健康的成年人可以对焦虑的小孩这么说，我能够感觉到你现在很自责，虽然你已经做了这么多，但你好像觉得你还不够努力，你好像觉得你还可以做得更多。好像说休息一会就会让你觉得非常的自责，会让你觉得我怎么能这样，好像拉了很多的进度。我能理解你现在感觉到的这些着急、紧张，甚至是有点自责，但我想告诉你，他们不是真的。不管发生什么，不管你是不是去学习，不管你是休息了，你这个考试通过还是没有通过，你将来成为一个什么样的人。我想告诉你，我永远爱你，我会永远支持你。我是一个成年人了，我可以保护好你了。哪怕有一天周围的人对你都很生气，你爸爸对你很生气，他会指责你，但是没有关系，因为我是一个健康的成年人，我有能力保护你，你要相信我有能力保护你\n第三步：设计 “微小行动” 替代应对策略\n将 “应该” 拆解为可操作的步骤：比如说，原目标：“每天健身 1 小时” →\n新目标：“每天做 5 个深蹲”；允许渐进改变\n自由\n被无意义的工作束缚和折磨的打工人们，孜孜不倦地追求的一个目标，便是 “自由”：财富自由，时间自由，生活自由...\n如果工作是罪魁祸首，引发了我们的种种情绪，消耗了我们的生命力，导致我们不得不通过 “低级趣味” 来获得正反馈，陷入这个死循环后，进而伴随着被 “体制化” 了\n那是否不工作，就能够规避以上种种情绪及其衍生出来的问题；是否不工作，便能够自由？\n不上班就好了？\n说实话，作为一个前二十多年努力读书，毕业后就勤勤恳恳的在大厂上班，也不敢\ngap 的做题家，并没体会过不上班的感觉\n但是在一些时间比较长的节假日，在疯玩了几天后，会有一种很强烈的空虚感，一种如同前面提到的刷短视频一般的空虚感；不上班所给予的巨大的自由，在此刻似乎化身成摄魂怪，吸走一切，只剩下空虚和无聊\n在《欲望可以有界，意义必然无穷》，就讨论到了类似问题，大致就是每天睁眼起来不用上班很自由，但同时你要面临的是海量的选择，这些选择需要消耗的是你的大量的精力；并且人还是需要意义感，如果所做的事情没有太多意义感，你很快也会觉得无聊和枯竭，即使你一天什么也不做，或者在花了一天在玩游戏刷视频\n后来看到了这个回答，感觉很好地解释了这个现象，关键点总结起来就是：要有产出 (作品)，不能只看不做\n不上班，宅在家中，玩游戏、刷视频、想吃吃想睡睡，似乎是很多被上班折磨了很久的人的想象中的快乐；但吊诡之处在于，人类没有任何一种娱乐经得起长时间纯输入而不让人感到厌倦，比如说看小说、看动漫，只看完全不创作；或者玩游戏只玩，没有什么白金成就之类的游戏目标，没有写游评、写攻略的想法，纯玩，你很快就会腻（笔者对这一点可以说深有体会）\n产出很重要，这里的产出，可以是你看完一部电影、一部动漫后，写的一小段的感悟和感受，可以是通关游戏后、录的视频发到\nB\n站（即使只有个位数的播放量），可以是去一段旅游后在小红书写下的一个体验或攻略。因为产出会不断强化你 “在做一件有意义的事” 的认知，同时产出也是一种即时反馈，它极大的帮助人保持积极的心理状态和乐观的预期\n；如果你隐居在你的小公寓里坚持写小说，可能到最后都没多少人看，但你一定会在写小说这件事上不断获得满足感。因为产出是一种价值锚定，是区别于 “爱好” 和 “随便玩玩” 的重要依据；产出某种程度上替代了工作给人带来价值感的部分，如果你干的足够好，也可以替代工资这部分\n但产出不一定要变现，也不一定要别人的认可，甚至可能是对你以外的其他人都没啥意义（笔者写的很多文章就是如此）；最近比较喜欢听的一个播客栏目《远行者与破冰匠》，猫草老师说的一句话令笔者印象深刻，那就是 “创作给人第二次的生命”，无论是写作、录播客、做视频，都是创作；甚至养育一个小孩，也是创作。而其中的关键点，还是要有产出，有一个你能看得见摸得着的、经过你的构思和规划、未来能够被你重新看到并想起当时创作的过程的产出\n或许其实这也是短视频与长视频的很大区别之一，短视频提炼不出有效的观点，但是长视频对人有启发的点在于，看完后能够提炼出一些观点（产出）\n在《心理学博士怎么发疯？和李松蔚聊情绪：职场、东亚小孩、内耗、嫉妒和愤怒》这一期播客里提到，快乐是紧张感的解除，需要有一个过程和转变，如同前面提到的可乐是第一口才是快乐的，一直不用上班，没有压力的生活，也许只有前几个月，甚至是前几周才是快乐的；因为人的适应性还是非常强的，因为快乐和自由的日子也跟 “与善人居，如入兰芷之室，久而不闻其香，则与之化矣” 一样\n回到标题，不上班就好了吗？看起来并不是，也许可以不上班，但是在不上班的日子里，还是得找到自己的主线，来替换原来被工作驱使着去走的那条路径；而在新的主线上，你需要有产出的、需要有一个 “实现自我价值” 的机会\n我们与自由的距离\n在孟岩的这一期播客里《E33\n与自由的距离》，提到一个笔者非常喜欢的观点：附在身上的东西越少，就越自由\n这些附着，也许是他人和社交媒体给自己的标签、也许是公司和社会给自己的角色、也许是当时咬咬牙买房而背上的贷款、也许是奄奄一息的病人插在身上的五颜六色的管子\n除了身体疾病产生的病痛，其他的痛苦的确都是我们内心给予自己的，所以在保持身体健康，不轻易负债的前提下，其实个人是可以很自由的；keep\nyour identity\nsmall，不要给自己加太多标签，不要往自己身上挂太多东西，没有什么是必须要做的\n那为什么社会生产力在大力发展，但好像现代人越来越不自由了？在韩炳哲的《倦怠社会》里提到 “世界成了一座巨大的百货商店”\n\n在现代世界，一切神性和节日已不存在。世界成了一座百货商店。所谓的共享型经济 (Sharing-Ökonomie) 把我们每个人变成了售货员，期待着顾客的到来。\n我们用越来越劣质的快消品填满世界，世界在商品中窒息。这座百货商店和疯人院并无本质区别。看上去我们似乎拥有了一切，我们却失去了最根本之物，即世界。世界丧失了语言和声音。在交流的喧哗声中，宁静消失了。商品的堆积和大众化填满了一切空白。\n商品占据了天空和地面。商品化的世界不再适于居住，它失去了和上帝、神圣、奥秘、无限、崇高的联结。我们亦失去了惊奇的能力，生活在一座透明的百货商店里，成为透明的顾客，时刻受到监视和操控。\n逃离这座百货商店成为当务之急。我们应当把商店改造成一个庆典场所，在其中生命才能获得应有的意义。\n\n当世界沦为百货商店时，商品逻辑已渗透到存在的每个毛孔。共享经济的 \"分享\" 表象下，实则是将人性转化为可交易的 \"服务人格\"—— 民宿房东必须时刻保持微笑待客，网约车司机需要五星好评来维系生存。这印证了福柯的 \"自我规训\" 理论：我们主动将自我包装成商品，在数字评分体系下进行永无止境的自我优化\n百货商店的 \"透明性\" 实为全景敞视监狱的升级版 —— 大数据算法比边沁的圆形监狱更高效地完成规训:\n抖音的推荐算法精准操控注意力，外卖平台的送餐时间量化压榨配送员。当一切皆可数据化度量，荷尔德林 \"诗意栖居\" 的世界彻底沦陷，连星空都成为文旅地产的营销噱头\n在这座全球化的百货商店里，我们既是囚徒又是看守；而这也许就是为什么社会生产力在大力发展，但我们却越来越不自由了\n人群与羊群\n那难道逃离这座钢筋城市，逃离被消费主义构筑起来的现代社会，逃到 “瓦尔登湖” 去过梭罗一样的生活才算真正自由么\n年轻时听歌的时候只爱听曲，但是随着年龄渐长，开始逐渐关注那些藏在词里的故事，最近在听陈奕迅的《任我行》，也许能够某种程度上回答上面这个问题\n如同前面提到的笔者很喜欢的一个概念 “等价交换”，那自由的代价是什么？这个代价可能是孤独、恐惧甚至是死亡\n少年时天真地认为 “神仙鱼困于鱼缸不自由”，于是将其放归大海，却导致其死亡，才发现 “原来神仙鱼横渡大海会断魂”—— 过度的自由可能带来毁灭性后果；而 “冒险半夜上山” 却因无人分享自由而感到 “没趣”，亦是说明了孤独是自由的副产品\n\n天真得只有你 令神仙鱼归天要怪谁\n以为留在原地不够遨游 就让牠沙滩里戏水\n那次得你冒险半夜上山 争拗中队友不想撑下去\n那时其实尝尽真正自由 但又感到没趣\n\n年少时也许渴望 “缤纷乐园任我行”，但成年后逐渐意识到 “没有同伴不行”；因为到了某个年纪，会发现自己没有了当年冒险的魄力和勇气，会发现自由的代价自己似乎承担不起，会发现曾经迷途过，那滋味并不好受，所以才会 “怕追不上满街赶路人”，然后开始 “忌讳空山无人”、开始 “怕遥望星尘”\n\n从何时你也学会不要离群 从何时发觉没有同伴不行\n从何时惋惜蝴蝶困于那桃源 飞多远有谁会对牠操心\n曾迷途才怕追不上满街赶路人 无人理睬如何求生\n从何时开始忌讳空山无人 从何时开始怕遥望星尘\n\n而自由，其实是可以存在于日常生活的微小选择中，可以存在于 “抱住两厅双套天空海阔任你行” 中，也可以存在于 “马路戏院商店天空海阔任你行” 中，而非必须与世俗彻底割，与社会告别才能得到\n歌词最后的结尾，“顽童大了别再追问” 既是对天真的告别，也暗含一种释然：真正的自由或许不是脱离群体的 “赤地独行”，而是在群体中保持内心的独立\n\n顽童大了别再追问\n可以任我走怎么到头来又随著大队走\n人群是那么像羊群\n\n做一只清醒的羊\n当我们尝试剖开工作的茧房、爬出短视频的泥沼，试图寻找自由的蛛丝马迹，而推导出来的所有解法，似乎都指向 “内心” 这一话题；《任我行》里的那句 “人群是那么像羊群” 轻轻叩击真相 —— 或许真正的自由，恰在于清醒地成为羊群中的一只羊\n那些在写字楼里疲于奔命的我们，在短视频中机械刷屏的我们，在深夜焦虑内耗的我们，何尝不是被生存驱赶的羊群？但正如林夕在歌词中埋藏的辩证 —— 少年时执意离群索居的 “顽童”，最终发现 “马路戏院商店天空海阔任你行” 才是更深刻的自由。这并非对现实的妥协，而是在群体性生存中，学会与自己的孤独和解\n被规训的日常里，我们依然可以保留 “神仙鱼困于鱼缸” 般的天真：在会议纪要的夹缝写下诗行，在通勤地铁戴上耳机筑起精神堡垒，在\nOKR\n的间隙喂养流浪猫时触摸生命的柔软。这些微小选择，恰似歌词中 “赤地踏过雪花”，是羊群中独属于你的足迹\n不必为 “随大队走” 感到羞耻。西西弗斯推石上山的宿命里，加缪也能看见抗争的尊严；我们日复一日的 “体制化” 生活中，同样能开出自由的花。当你学会在晨会汇报时藏进一句聂鲁达的诗，在周报 PPT 里偷偷画一只跃出表格的鲸鱼，在团建聚餐后独自走向深夜亮着灯的便利店 —— 那一刻，你既是羊群中的一员，也是自己精神王国的君主\n人群如羊群，但总有些羊在低头吃草时，眼睛始终望向远方的雪山\n\n本文的一些参考材料\n\n《每个人都有至暗时刻，其实你并非孤独》\n\n《职场心累真相：90% 的疲惫和焦虑都来自这 3 种隐形内耗》\n\n《心理学博士怎么发疯？和李松蔚聊情绪：职场、东亚小孩、内耗、嫉妒和愤怒》\n\n《2018 年 4 月 1 日》\n\n人是怎么废掉的？\n\n“抽 卡？整 这\n个 没 用”\n\n每天大部分时间都用于刷剧短视频，人容易满足于这种 “奶头热”，长此以往是否会影响人的智商，反应能力呢？\n\n连续几个小时时间用来刷短视频会让人感觉空虚，用来专注做一件事情会让人感觉充实。这种差别的感觉从何而来？\n\n为什么都说碎片化信息无用，我却感觉刷手机也可以学到很多东西？\n\n《我知道我应该，但我真的不想》\n\n《远行者与破冰匠》\n\n《与自由的距离》\n\n《倦怠社会》\n\n《欲望可以有界，意义必然无穷》\n\n没有经济压力、不婚不育不工作、每天睡到自然醒、爱干啥干啥，这样的人会烂掉吗？\n\n任我行为什么能唱哭陈奕迅？\n\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"张潇雨的个人投资课 (1)- 市场规律","url":"/2021/11/14/%E5%BC%A0%E6%BD%87%E9%9B%A8%E7%9A%84%E4%B8%AA%E4%BA%BA%E6%8A%95%E8%B5%84%E8%AF%BE(1)-%E5%B8%82%E5%9C%BA%E8%A7%84%E5%BE%8B/","content":"因《得意忘形》播客认识张潇雨老师，从中听到了不少令人耳目一新的观点，后来发现张老师还是一名对冲基金经理，在得到上有一门课:\n个人投资课，总体听下来，将笔者断断续续学习过的一些投资方面的知识串了起来，值得写一篇文章记录一下。\n内容总体可分为两大部分：基本投资原则和具体的方法论，课程在讲解过程中将其划分为如下四个部分：市场规律、投资工具、自我局限和投资组合构建，前三个部分主要讲一些投资过程中最容易犯的错误，最后一部分则是讲一些具体的投资方法。本文主要是市场规律部分相关内容，同时简单介绍了个人投资者现对于专业投资者的一些优势与劣势。\n\n个人投资者的优势与劣势\n课程里的内容主要针对的是普通投资者的投资，而普通的个人投资者的特点是\n（1）投资不是你的全职工作，你还有其他的事业和职业去做\n（2）除了自己或者最多照顾到亲密的家人朋友以外，不会管理其他外部人士的资金\n个人投资者相对于专业投资的优势是：不需要像专业投资人那样舍近求远，舍易求难；即专业投资人面对着同行和背后投资人的压力，总得干些什么事情来证明自己，往往需要做一些身不由己的事情；\n但是作为个人投资者则没有这种压力\n课程里举了几个例子来说明这个观点，第一个例子是《彼得・林奇的成功投资》里第二章的例子专业投资者的劣势；第二个例子则是\n2000 年互联网泡沫；\n\n在 2000\n年的互联网泡沫里，作为一个基金经理，如果不买互联网的股票基本就要被市场抛弃了，甚至不光是你的同行，连把钱交给你的人，他们都会督促你赶紧买互联网股票，否则他们就要把钱抽走去别的地方投资了，那这个时候你买还是不买？在那个疯狂的年代，能耐得住寂寞，抵抗住压力，不参与这种全民狂欢的专业投资人是非常少的。有没有人确实做到了？\n有，那就是我们熟知的股神巴菲特，但是代价就是 1999 年它大幅跑输大盘，受到了无数人的嘲笑与质疑，当时间转到 2000 年初，整个市场开始崩盘，网络概念股价格直线跳水，完全跌到退市的公司也屡见不鲜，无数的基金价值被毁灭，投资人的血本无归，基金经理们也一夜之间没有了工作。\n所以经常有人问，美国几次市场泡沫都那么明显，普通人都能看出来，那些专业人士就看不出来吗？其实不是看不出来，而是这个时候你要不杀进去，别人就会认为你是个笨蛋，你的职业生涯可能也要提前终止了。\n\n而个人投资者的劣势则是接触到的资源非常有限，比如说一些收益较为可观的基金 (如桥水基金) 对投资的金额都有一定的门槛，因此，课程里认为对于个人投资者而言，花时间研究那些奇技淫巧的东西，其实是得不偿失的；反倒是掌握一些最朴素的投资方法，避免那些最明显的投资错误，才是走向投资成功的正途。\n房子还会不会是最好的资产\n对于中国人甚至整个亚洲人民来说，最喜欢的投资方式之一是买房，而在过去的好几年，房价的走势也的确很给力，很多人都将购买房子作为一种投资方式，课程里针对这种现象提出了一个有趣的观点：一个人的投资偏好往往会受到年少记忆的影响，就是在十几岁二十几岁相对年轻的时候，对整个投资市场和各种投资产品的印象和体验，很大部分就构成了一个人的投资偏好。\n课程里举了两个例子，第一个是美国的股票\n\n美国的 80 年代是股票市场历史上投资回报率非常好的一个黄金 10 年，但在这 10 年里，美国家庭财产投资在股票上的比例居然是下降的，而且之后的 90 年代市场继续走强，走出了一个很不错的大牛市，大家投资在股票市场的资金继续减少，比例从 60 年代的 40% 到 80 年代的 25%，再到 90 年代只剩 17% 了。而且不只是股票投资美国家庭的投资比例也在持续减少，曾经这个比例超过 70%，但到了 90 年代就只剩 40% 了。在这个阶段，美国整个股票市场翻了三四倍，但一大批投资者却把钱从股市上抽走了，完全没有享受到国家财富增长的成果，这到底是为什么呢？\n答案就是美国整个 70 年代的经济实在太惨了，在上世纪 70 年代，由于越战升级，中东石油危机，国家不合理的福利政策导致财政赤字急剧扩大等等一系列原因。美国国内大幅通货膨胀，同时经济停滞，失业率也非常高，这段时间美国股市的表现非常惨淡。从 1969 年末到 1979 年末这整整 10 年时间，美国的道琼斯指数和标普 500 指数几乎纹丝不动，10 年呢没有任何增长，算上通货膨胀甚至是亏损的。而且中间还有几次莫名其妙的大跌，让整个市场都处于一种担惊受怕的状态。\n你想想如果你出生在 60 年代，20 岁左右的时候呢发现谁投资股票谁倒霉，那么等你到了三四十岁的壮年期，家庭情况好了，收入也上来了，你会放心大胆的买股票吗？绝大部分人都不会这么做，因为小时候的记忆实在是太深了，但这样显然带来了一个很惨痛的后果，很多的普通投资者和勤勤恳恳的家庭错过了美国历史上最好的一段牛市，而钱都被敢于在熊市逆向投资的专业机构转走了\n\n另一个例子则是日本的楼市，这部分更多的参考资料见日本楼市的兴衰史，是怎样的一个过程？\n\n二战之后日本大力发展制造业，从六七十年代开始，日本经济开始崛起，以索尼和丰田为代表的优秀企业，把自己的产品卖到了全世界。到了 80 年代日本经济达到顶峰，全国从企业到个人都陷入了疯狂的买买买的状态。\n最著名的就是地产开发商三菱地所去美国买下了洛克菲勒大厦，还在大楼的最高处插上日本国旗。由于广场协议让日元大幅贬值，日元开始回流到国内市场，加上日本政府大幅降息扩大贷款，造成国内一时间产生了大量的钱，都流向了股票市场和房地产市场。结果就是当时东京市中心银座广场的地价达到了 25 万美元一平米，而且这可是在 1989 年，那个时候只要在东京或者大阪有房子的日本人都可以说是千万富翁。\n那么当时拼命在买房的人都是谁呢？正是经历了六七十年代日本经济起飞对未来无限乐观的那批人，他们在十几岁赶上好时候，到了三四十岁有了购买力自然就要买房，让财富增值。但后来的故事呢我们都知道了，日本经济泡沫破裂，很多家庭的倾家荡产，那些使用高额贷款买房的人瞬间负债累累，欠的钱可能一辈子都还不清。日本整个国家和民众迎来了一个失落的 20 年\n\n曾经的美国人由于股市 10 年惨淡，再也不敢买股票，错过了之后的大牛市；\n当年的日本人由于国家经济迅速发展，房价节节高升，结果无数人买到了最高点上之后一路崩盘。这个例子告诉我们一个被忽视，但非常简单的道理，那就是要一个被很多人忽视，但非常简单的道理，那就是要尽量忘掉你的投资偏好，多元配置你的投资组合。\n因为各类资产（股票、房子、债券、石油、黄金等）都有一个轮换周期，可能一下就是 10 年、15 年、甚至 20 年，这对于整个市场只是一个很小的时间段，但对于我们每个人来说，可能就是人生中最黄金的一段时光。同时市场很难预测，没人能准确说出来接下来 10 年 20 年投资什么最好。因此，如果我们做到足够的多元分散，你就可以提高自己压中宝的概率，不错过人生中重要的财富增值的机会。而想做到这样，最重要的就是不要根据过去几年甚至十几年的经验，简单的认为接下来同样的事情还会重复发生，因为市场总在变化，也许在下个周期里真正大涨的东西你想也想不到\n因此，这部分有以下 3 个结论值得重点关注\n\n我们很容易因为自己过去的经历就对某一种资产有特殊偏好，这样很容易让你错过增长最快的那个资产品类\n\n不管是短期还是长期，你都很难预测哪类资产在未来表现会最好\n\n因此作为投资者不要对资产有明显的偏好，多元分散投资不只是规避风险，更是抓住赚钱的机会\n\n择时陷阱\n择时就是选择时机，英文叫做 market\ntiming，本质就是我们对市场对接下来各种资产的表现一个短期走势的预测，然后决定是否要买入或卖出；课程里的建议是不要择时。\n以股票为例，单个交易日的大涨大跌会比较难，那有没有可能成功预测比较长时间段的趋势呢？比如有没有可能提前预测到 2008 年的金融危机？答案是可以的，但这件事最难的地方在于即使你真能预测金融危机会到来，但你怎么知道它恰好会发生在 2008 年，而不是 2007 年或者 2009 年？\n在以 08 年次贷危机为原型的电影大空头里，主角之一的原型迈克尔巴黎，其实从 2005 年就开始做空刺激贷款了，结果是他硬扛了两年多，直到 2007 年才赚到钱，中间也遭受了无数的质疑。在知乎上的这个回答也提到了这一点，如何评价电影《大空头》（The\nBig Short）？ - Z Abigail 的回答 - 知乎,\n这部分其实也侧面佐证了上面提到的个人投资者相对于专业投资者的优势\n\n如凯恩斯所言，“market can stay irrational longer than you can remain\nsolvent” 假设存在一个平行的宇宙，美国在 05/06 年改变移民策略 —— 买一套价值 xx 元以上的房产就可以自动获得永久居留权，由此推迟了次贷危机，使之在 2012 年而不是 2008 年爆发。请问电影中的那些 fund\nmangers 真的有足够的钱满足 margin call 和 AUM\n撑到 2012 年吗？在投资行业越来越机构化的今天，短时间的 underperformance\n（一年）都可能导致大量投资者撤资，更不要说连续几年了。举个例子，很多在 2014 年底大量买 E&amp;P 公司股票 / 债券的 funds 在 2015 年年底因为投资人大量撤资，被迫关门\n\n课程里另一个例子就是老虎基金，这个例子其实也是上面这个知乎回答的一个佐证\n\n从 98、99 年开始，金融市场突然刮起了科技旋风，那个时候最热门的就是互联网股票，但那时候的互联网公司没有一个有盈利的，大多数呢都是噱头，光靠炒概念股价又能翻好几倍。\n在这种环境下，作为价值投资者的罗伯逊显然认为市场的泡沫太大了，于是他开始做空科技股，但在当时那个市场环境，\n再烂的公司股价也是非常整个市场呢毫无理性可言。结果由于做空这些股票，老虎基金损失惨重。\n不仅如此，当时由于所有人都在疯狂的追涨科技公司，很多资金也从传统经济流向了互联网领域，所以传统的价值股也受到了很大的冲击。如老虎重仓持有的美国航空 12 个月里面跌了接近一半，等到了 2000 年 2 月底，老虎基金已经跌的不成样子了，而且基金越跌，投资者也越拼命的撤资，\n于是仅仅一年多时间，老虎基金管理的资产就从巅峰的 230 亿美元狂跌到 65 亿美元，等到了 3 月罗伯逊彻底扛不住了，干脆决定关闭基金。\n但最令人唏嘘的是什么呢？老虎基金是在 2000 年 3 月 30 日正式关闭的，20 天前啊纳斯达克指数刚刚创下了历史新高，结果仅仅到 4 月，整个市场就开始暴跌，一个月就下跌了 15%，接下来两年跌了快 80%。想一想，如果罗伯逊是在这个时候做空的，他的回报是难以想象的，所以虽然他的判断是对的，但就是倒在了黎明之前，择时也正是这么一件非常难的事情。\n\n既然择时那么难，那具体的方法论是什么呢？这部分课程会在第四部分提，一句话来说就是根据\nPE/PB\n分位数定投宽基指数。其实哪怕定投了，还有一个问题需要考虑，就是何时止盈或止损的问题，这个问题比较\nopen，知乎上有个回答可以参考下如果做基金定投的话，什么时候应该卖出呢？\n- TopView 的回答 - 知乎，课程则是从另一个角度来阐述了这个问题，\n部分观点可能有争议，但是笔者觉得其中的 “个人投资是一个无限游戏” 说的很对\n\n我们如果想回答什么时候应该卖出的时候，不如来想一想什么时候不应该卖出，因为我们把主要的错误的卖出理由排除掉，剩下的答案可能就很接近正确了，我觉得有两个最常见的错误的卖出的理由，\n第一个是仅仅因为价格跌了，所以我就得卖出。这句话的背后其实有这样一个假设，就是你有能力每次都能买到这个东西价格的最低点，因为这样你买完之后它才不会跌，但是你觉得这件事有可能吗？\n第二个错误的卖出理由和第一条是对称的，那就是仅仅因为一个资产价格涨了多少，也不应该是你卖出他的理由。我知道这条可能有点争议。比如伟大的对冲基金保罗杜德琼斯就说过，当你买入的股票达到了目标价的时候，你就应该卖出，因为后面不论这个股票是涨是跌，都和你的认知范围和认知能力无关了。\n所以我在这里说的并不是一个绝对真理，更多的是对你的一种提醒。那就是很多投资者有一个习惯就是买了一个股票或者什么基金，结果接下来一两个月正好涨了 10% 或 15%，那么干脆卖出把挣的钱落袋为安好了。这是有问题的，原因有如下\n2 点\n第一点，你要意识到个人投资是一个无限游戏，就是除非这个钱你真的要拿出来消费掉，否则你的投资总是要继续回到市场，不断循环滚动生生不息的，就像一个没有终点的游戏，比如你卖出基金挣了 15%，然后你就把钱存到银行了吗？其实呢大部分时间也不会，你会拿着这笔钱再去投资到新的地方，那这个新的投资标的就比之前那个一定好吗？如果真的这么好，那么当初你为什么没有投他？这些问题都是需要去诚实的思考的。一个比较合理的正当卖出理由是你发现了这笔钱更好的去处，发现了更好的投资标的，比如说你一直看好的一个股票，突然受短期情绪影响跌了很多，进入了你的射程范围，那么你去更换一下持仓，这是可以理解的。当然这一切的前提都是你真的能判断这些资产的价值，而不是看上了那一点蝇头小利才选择卖出的。\n第二点，如果你因为账面盈利选择卖出，实际上相当于你下了一个判断，就是这个资产接下来会下跌，而且不会再涨回来了。你的这个判断最后会有两个结果，第一个是它下跌之后又涨回来了，那么这个时候你要不要买回来？(实际上这种想着我先卖出之后跌了再买回来的人，往往就是那些错过了腾讯茅台甚至比特币的那些人)。第二种则是你卖出之后价格下跌了，而且再也没涨回来，那么恭喜你，你可能做出了一个正确的决定。但这里还隐藏着一个问题，就是如果是这样的话，是不是意味着你当初买入的决定有一些问题呢？那么下一次你怎么防范类似的错误，而且最关键的是当你决定卖出的时候，你实际上在做一个判断和决策。而如果你的认知能力或者情绪管理能力不足的话，这种决策出错的概率是很大的。\n说了这么多错误的卖出理由，一个好的卖出理由是什么呢？我觉得就是你的买入理由不再成立了，有时候这是因为公司的基本面变化了，有的时候呢是因为你觉得本来一个资产价格低估，但是呢现在高估了，有的时候呢就是因为一开始就买错了，总之从本质上来说，你买入一个东西的成本，不应该成为你是否卖出的理由\n\n因此，这部分有以下 3 个结论值得重点关注\n（1）从短期来看，我们很难通过择时获得超额回报。对于股市来说，一年中最好的那些交易日分布非常不均匀，且和牛市还是熊市无关，所以我们很难挑中这些好的交易日，而一旦错过，对于我们的投资回报率就是致命的打击。\n（2）不仅短期来看，择时很难，就连准确判断牛市熊市的来临时间也非常难。即使是像老虎基金这样最著名的基金，也很难掌握择时这件事情，所以避免轻易的择时，避免压住短期的市场走势，我们就又能避免一种亏损\n（3）个人投资是一个无限游戏，一个好的卖出理由是是你的买入理由不再成立了\n宏观迷信\n这部分内容是为了说明个人投资者不需要过于依赖宏观经济来指导投资。\n课程里举了凯恩斯的例子来说明这个观点\n\n凯恩斯可以说是历史上最伟大的经济学家之一，也是整个宏观经济学的奠基人，但很少有人知道的是，他其实也是一个很成功的实战派投资人，不过可能和你想的不一样，凯恩斯不是一个靠预测宏观经济周期赚钱的人，而是和巴菲特一样是个股票的价值投资者。\n当时的故事这样的，在 20 世纪 20 年代，凯恩斯 40\n岁上下正在盛年，那时候他的宏观经济学理论也比较成熟了，所以在实战中经常依靠自己的理论进行投资操作。当时凯恩斯最喜欢的投资品类是外汇和期货，因为这两种资产和宏观经济的挂钩最为紧密，他觉得只要能弄懂宏观数据，把握好经济周期，赚钱简直是必然的。\n但结果是那个阶段的凯恩斯大部分的时候都在亏钱。比如在 1920 年，凯恩斯根据自己的理论分析，压住，英镑升值，做空法郎、德国马克和意大利里拉，但市场走势和它预测的正好相反，几种货币全部对英镑升值，他直接被平仓出局，大亏了一笔。凯恩斯这种预测失败的情况还发生过很多次。根据统计，从 1922 年到 1929 年的\n7 年里，凯恩斯有 5 年的投资收益都低于市场指数，成绩相当的惨淡。\n但是到了 30 年代，凯恩斯改变了自己的投资哲学，那个时候他已经是自己母校剑桥大学国王学院的财务主管了，会帮助学校打理一部分资金，再想之前那么不靠谱可不行了。所以他开始不再预测经济形势，而是寻找被低估的股票，并且大量买入并长期持有，成为了一个彻头彻尾的价值投资信徒，结果这种投资方式给他带来了丰厚的回报。他之后的 15 年的投资记录上升到了年回报\n9% 左右，而同期英国股市跌了\n15%。凯恩斯为什么会做出这种转变？他后来在一封给朋友的信里是这么解释的，他说宏观经济当然很重要，但是影响宏观经济的因素实在是太多太复杂了，常人难以掌握，相比之下寻找价值被低估的好公司要容易得多。\n\n此外，巴菲特、芒格也有类似的观点，认为宏观经济在投资中所起作用并非那么大。\n那如果我们不去研究这些宏观大问题，应该去研究什么呢？另一位著名的投资者乔尔蒂林哈斯特的一本书应该是解答这个问题的好选择。他之前写了一本书叫做\nbig money thinks small (中文叫《大钱细思》)，这个名字正好契合了这一讲的主题。蒂林哈斯特说如果你整天研究宏观经济形势，那你就是让自己陷入信息的汪洋大海了。作为极其专业的股票投资者，他说自己每天想的都是很具体的小问题，比如分析公司的时候，他问自己的都是消费者为什么会购买这家公司的产品或者服务？这家公司比起竞争对手来说，最大的不同是什么？什么东西会导致这家公司彻底失败等？\n而如果借用巴菲特的说法，投资的时候最重要的是弄清什么是重要的、可知的，如果一件事是不重要的或者不可知的，那我们就别管了。比如宏观经济问题是很重要，但对于普通投资者是不可知的，而什么东西是可知的？比如\n\n自己的投资期限有多长\n\n应该如何针对性的配置自己的资产\n\n购买国内外指数基金的时候，哪家的费用或者成本比较低\n\n如果自己在境外有一些存着的美元或者外币，如何更好的进行现金管理\n\n....\n\n把精力花在这些可知的事情上，才对我们的长期回报更有益处。\n因此，这部分有以下 2 个结论值得重点关注\n（1）宏观经济是很重要，但影响宏观经济的因素太多太复杂，即使凯恩斯席勒这样的大师也没法根据宏观经济指标来赚钱\n（2）与其把精力放在宏观经济这种重要但不可知的事情上，不如想的小一点，关注更可知的事情。比如我们怎样完成一个资产配置的方案，购买的产品呢有多大风险，成本怎么样等\n风险度量\n风险是难以量化的。对于收益这个东西我们可以非常清晰的量化，比如一年收益是 3%、10%、还是 50%，都可以算得清清楚楚。但你说风险是\n10%、30%，好像没有这个说法，甚至你再往前想一步，会觉得更加困惑。\n比如有人说这个投资产品风险有点高，估计 80%\n得亏钱。那这个 80% 是什么意思呢？是说这个产品如果投资 10 次会有 8\n次失败，还是说这个产品卖出 10 份得有 8\n个人亏损，还是历史上类似的项目 10 个里面有 8 个都倒闭了，好像都不是。另外这个亏钱是亏多少呢？是亏 20% 还是亏一半还是全都亏掉？\n课程里举了长期资本管理公司，简单来说就是这个公司曾通过复杂的数学模型在市场上获得可观的回报，但是因为一次黑天鹅事件 (即 1998 年俄罗斯金融危机) 而破产了；后来著名的投资者巴菲特和霍华德马克思都说过，用数学的方式计算和预测风险是很荒谬的，众多学者们这么做的主要原因是他们需要一个可计算的、客观的、能够查明来龙去脉的数字，否则研究就没法做了。\n如果风险如此不可捉摸，我们应该怎么做呢？答案也很简单，就是不要去想象风险的概念，而是要直接去衡量风险的结果，即在买入任何金融投资产品之前，都可以问自己这么一个问题，如果接下来它的价格跌了一半，我能理解到底发生了什么情况吗？我有应对的方法吗？如果回答不了这个问题那，这笔投资最好就不要做了。\n因此，这部分有以下三个结论值得重点关注\n（1）很多时候我们买了高风险的产品，但我们并不理解什么是风险，总以为亏损不会发生在自己身上，这是很危险的\n（2）风险很难被量化，不要衡量风险本身，而去衡量风险爆发之后的结果\n（3）具体应该怎么做？一个简单的方法是在买入任何投资产品之前都问自己这么一个问题，如果接下来它的价格跌了一半，我能理解到底发生了什么状况吗？我应该怎么去应对，如果回答不了，这笔投资呢就暂时不要做了，直到你弄清楚了这个问题。\n海外配置\n前面提到，由于我们很难预测在未来一段时间里到哪一个大类的资产会涨得最好，所以在投资的时候要尽量的多元分散，就是大家常听到的资产配置的概念。不过另一个值得关注的点是资产配置不只是在资产品类这个维度做就行了，在地理这个维度上也要进行。\n课程里针对普通人做海外配置时常有的两个问题做了解答\n(1) 我们真的有必要去海外投资吗？毕竟我们的生活主要都在国内，花钱也在国内，为什么要把钱放在我们不熟悉的地方？\n(2) 海外资产配置是不是富人的专利？\n针对第一个问题，课程里给了先锋领航基金呢之前发表过一篇学术研究，他们测算了从 1900 年到 2009 年长达 100 多年里 16 个主要国家股票市场的涨幅和人均真实 GDP 增速的关系，最终得出了一个结论就是两者没有关系，也就是说对于这些国家来说，股市回报和经济增长的速度并没有什么直接关联。课程里对这个结论归因为如下两个原因\n\n资产的价格。一个地方经济发展的速度当然可以很快，但如果人们为这种快速发展的预期付出了过高的价格的话，回报就不会太好；比如说房价\n\n全球化。由于全世界各个国家的经济联系越来越紧密，我们已经很难知道总体经济增长的时候利益会落到哪个具体的国家了。比如说耐克在福建、广东甚至越南生产出一双鞋，这双鞋的价值会实实在在的计入当地的 GDP，但鞋里面大部分的利润都被耐克一家美国公司赚走了；公司股价则是和公司的盈利能力是息息相关的，所以虽然新兴经济体增长快，但里面有很大一部分利润都被跨国公司拿走了\n\n所以我们想象中的那种新兴市场经济发展快，所以投资会赚到更多的印象是不准确的。这会导致一个什么问题呢？就是虽然我们国家未来仍然可能是世界上经济发展最强劲的国家之一，但这不意味着中国的股市、房市各种投资品市场会持续上涨。\n除此之外，课程里还提到了一个行为金融学的概念：本土偏好，即人们偏向于投自己国家的资产，比如日本股市的整个市值占全球总体市场的 7~8% 左右，但是日本本地的投资者会把 60% 左右的资产配置在本土市场上，同样美国股市的市值可以占到全球的一半，但美国投资者在本土股票上配比更高，几乎有 80%，那中国的投资者就更不用说了，毕竟大家都熟悉本国的投资品种，投资起来更让人安心。但是本土偏好的一个风险是我们没法用简单的方法判断出一个国家的资产是不是会升值，只投资任何一个国家其实都是有点冒险的，\n针对第二个问题，个人投资者在投资的时候，可以选择一些涵盖世界上主要股票市场的基金进行投资，如\nacwi 指数 ETF,\n还有一些其他的海外配置的方法课程里没有提及，只是提了 “稍微做一点功课都能找到”\n因此，这部分有以下三个结论值得重点关注\n（1）海外资产配置很有必要，因为一个资产能不能升值，跟一个国家的经济增长没有必然联系，所以我们不能因为中国经济发展迅速就忽略了海外投资。\n（2）做海外资产配置的时候，不能只挑我们熟悉的国家，因为全球股市表现最好的地方，很多时候我们都想不到。\n（3）想要投资全球市场，有不少产品可以实现，很多主流的大基金都能让我们坐拥全世界主要的股票市场\n小结\n这部分介绍的内容可总结如下\n1. 个人投资者的优势与劣势\n\n优势是不需像专业投资者那样舍近求远、舍易求难；劣势是可接触的资源有限\n\n掌握一些最朴素的投资方法，避免最明显的投资错误，才是走向投资成功的正途\n\n2. 避免投资偏好\n\n无论短期还是长期，都很难预测哪类资产在未来表现会最好\n\n个人容易因为过去的经历就对某一种资产有特殊偏好，但投资者不要对资产有明显的偏好\n\n3. 择时陷阱\n\n短期来看，我们很难通过择时获得超额回报；长期来看，准确判断牛市熊市的来临时间也非常难\n\n个人投资是一个无限游戏，一个好的卖出理由是是你的买入理由不再成立了\n\n4. 宏观迷信\n\n宏观经济是很重要，但影响宏观经济的因素太多太复杂\n\n与其把精力放在宏观经济这种重要但不可知的事情上，不如想的小一点，关注更可知的事情\n\n5. 风险度量\n\n买了高风险的产品，但并不理解什么是风险，总以为亏损不会发生在自己身上，这是很危险的\n\n风险难以被量化，因此不要去想象风险的概念，而是要直接去衡量风险的结果\n\n6. 海外配置\n\n一个资产能否升值，跟一个国家的经济增长没有必然联系\n\n做海外资产配置的时候，不能只挑我们熟悉的国家\n\n很多主流的大基金都能让我们坐拥全世界主要的股票市场\n\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"张潇雨的个人投资课 (2)- 投资工具与自我局限","url":"/2021/11/20/%E5%BC%A0%E6%BD%87%E9%9B%A8%E7%9A%84%E4%B8%AA%E4%BA%BA%E6%8A%95%E8%B5%84%E8%AF%BE(2)-%E6%8A%95%E8%B5%84%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%87%AA%E6%88%91%E5%B1%80%E9%99%90/","content":"本文是听了 个人投资课\n后的一些笔记，\n课程主要内容可划分为如下四个部分：市场规律、投资工具、自我局限和投资组合构建，前三个部分主要讲一些投资过程中最容易犯的错误，最后一部分则是讲一些具体的投资方法。\n本文是第二部分和第三部分的内容，主要介绍了在投资过程中对一些投资产品的误解，以及投资过程中的一些自我局限。课程第一部分的内容介绍可参考\n张潇雨的个人投资课 (1)- 市场规律\n\n指数基金\n投资最常见的方式之一是买股票，但课程的观点是作为一名普通投资者，想要靠购买个股赚钱是非常困难的，主要原因是选中好股票很难了\n课程里列举了标普 500\n指标的例子，这个指数基本涵盖了美国最大、最好、最有代表性的 500 家公司，但是根据学者的统计，从 1957 年到 1998 年之间，超过\n40 年里标普 500\n指数的变化，结果是在 40 年之后只有 74 家还存在于指数中，其他要么被收购，要么倒闭，要么衰退极大。而在这个\n74 家里，只有 12\n家跑赢了整体的指数，即你能压中超出市场平均回报的公司的概率只有区区的 2.4%。\n笔者附：虽然课程举的这个例子是为了推荐投资指数基金，而包括巴菲特等人也的确推荐普通人投资指数而不是个股，关于指数基金定投的教程网上也很多，但是笔者认为这里有\n2 点需要注意\n1. 这里统计的跨度是 40\n年，但是个人投资者往往会不会这么长期持有投资，而资产的涨跌都是有周期的，因此\n40 年跨度的数据并不意味着在这个过程中投资个股收益超过指数的比例只有\n2.4%\n2. 国内的市场跟美国不太一样，巴菲特建议的是在美国市场定投指数基金，但是在国内不一定适用，这部分内容更详细可参考\n千万不要再瞎买指数型基金了\n- 米多多的文章 - 知乎 和 基金定投这么好，为什么能坚持下来的人那么少？\n- TopView 的回答 -\n知乎，这两个回答，里面也介绍了指数基金一些其他知识\n\nA 股长期来看稳定上涨，但几十年间波动极大。主要特点是牛短熊长，没有美股的走势给力。这就导致，巴菲特口中散户用指数，战胜专业投资机构的 “神话” 并不会发生\n股神巴菲特已经总结过了，指数型基金最靠谱，无脑定投指数就行。但是但是！！！这是指美股，你看看纳斯达克，比很多主动性基金都厉害。A 股的指数基金一般在牛市才能够比主动型基金强，但是 A 股牛短熊长。A 股指数基金真的不适合无脑定投，尤其是上证 50、沪深 300 这样的，要适当高位止盈做波段。\n\n其实看到这里，你也能发现，投资这个东西并没有一个一劳永逸的方法，因为这个事情受到的影响因素太多了，所有的投资资料或课程都只是提供了一个视角给你，包括这门课也是，对个人而言，需要的是不断地学习和思考，搜集更多的资料和数据，辩证地看待单一一门课程或文章提供的观点。\n安全边际\n这部分主要强调了这个观点：好公司不等于好股票，买入价格的高低会严重影响投资收益\n大部分人在做股票投资的时，很容易陷入一个误区，认为一家好公司一定是一个值得投资的对象，比如说你用到了一个好产品，对这个公司产生了好感；发现周围很多人都对他家的东西评价很好；然后又看到这家公司的创始人的采访和报道，觉得人也非常靠谱；最后你在媒体上读到了一些对这家公司的分析，觉得特别有前途，顺应了时代的大趋势等。所有这些点点滴滴的信息最终都汇聚成了一个结论，这家公司不错，那么显然我们应该买点他的股票。\n但实际上这是不对的，先不说上面这些分析和结论是不是严谨，光是对一个问题的忽略，就能导致我们这种投资方式失败了，那就是价格，一家公司再好，如果你购买它的价格过高，也是很难赚到钱的。课程里举了如下例子\n\n银行业在任何国家都是一个支柱型和垄断型的产业，在我们国家更是。过去二十几年里，银行都是我们国家最赚钱的行业之一，而其中的一个佼佼者就是招商银行，从 2007 年到 2017 年，招行的年利润从 150 亿增长到了 700 亿，翻了 4 倍，多年化的增长率有 15% 之多。衡量银行的另外一个指标，每股净资产招商银行同样增长接近 4 倍。简而言之，\n在 10 年左右的时间里，\n招商银行通过不懈的努力和发展变成了一家更大更好的公司。那你说你要是买到这样一家优质公司的股票，然后长期持有应该能赚大钱了.\n可惜未必，在 2007 年末市场大牛市尾声的时候，招行的股价冲到了 23 块 5 左右，但之后股价一路走低，最低达到过 5 块多钱。直到接近 10 年之后，2017 年 7 月招行的股价才又回到了 24 块钱左右的水平。换句话说，如果你在 2007 年末买到了这样一家优质公司的股票，恐怕要忍受长达 10 年的账面亏损，才可能迎来曙光。所以你很有可能在黎明之前就倒在了半路上。\n这里的原因是什么？其实最大的问题就是你买入的价格太高了，07 年末的时候，整个市场其实已经相当狂热，所有股票的股价都被炒得很高，招行也不例外，所以即使那个时候我们能预测到招行这家公司未来会发展迅猛，利润会越来越好，但它那个时候的价格已经过高了，高到了需要花 10 年时间来消化的程度。\n\n所以就像我们开头说的，再好的公司，如果你不管价格就随意买入，很可能也会让你亏钱。那有人可能想个股的大起大落实在太正常了？如果我们买指数基金呢？实际上价格这个因素不只对个股有作用，对于整个市场也是成立的。\n\n2000 年 3 月的时候，纳斯达克创下了 5048\n点的历史最高点，等到整个市场再回到这个价格水平的时候，已经是 2016 年的 7 月，16\n年的时间已经过去\n同样对于标普 500 指数，2007 年的高峰等到 2013 年才再次初级\n至于 A 股就更明显了，2007 年沪深 300 的历史高点，直到 2019\n年初还没有回去，曾经 2015\n年的大牛市仿佛就要触到这个顶点了，但随之而来的又是一轮深深的下跌。而且你会发现不只是中美这两个大国，其他国家的股市都有类似的规律，只要你忽略价格问题，不假思索的追高，那么亏钱就是非常有可能的。\n\n那一个很自然的问题就是，怎么判断公司 / 指数当前的股价是否过高呢？课程里给出的答案是无法精确地判断出当前的价格是多少，但是可以做个大概的区间评估，用概率论的角度来说就是区间估计比点估计要更简单；用芒格的话来说就是，“预测一件事会不会发生，永远比预测一件事什么时候发生要简单的多”\n更学术的来说就是安全边际这个概念了，安全边际通俗说就是，如果你觉得一个公司值 10 块钱，结果市场有一天突然给出了一个五六块钱的价格，那么这个打折的部分就是你的安全边际，具体的方法论就是不管你给这家公司估值多少，最好给你心目中那个价格打个五六折，再去考虑买这只股票，说白了就是便宜就买，如下图所示\n\n更详细和通俗的解释可参考这个回答，上面的图其实也是来自这个回答，如何用生动的例子来解释「安全边际」?\n- 陈达的回答 -\n知乎，里面还介绍了严格的安全边际可能存在的一些局限性，值得一看。\n因此，这部分有以下 2 个结论值得重点关注\n(1) 好公司是不等于好股票的，买入时你的价格一定是一个必须要考虑的因素。\n(2) 怎么判断一个公司的股价是不是过高呢？其实除非极端情况出现，否则我们就是很难判断的，最好的处理方式就是在买入的时候给留出足够的安全边际，才有更大的赚钱的概率。\n抄底哲学与专业光环\n这里其实是两讲的内容，只是因为笔者觉得内容都比较单薄，就放在一块了～\n第一部分是强调了抄底需要谨慎，套用网上一张图就能很好表达这部分的内容了，\n结合前面的安全边际部分，笔者觉得对于个人投资者的方法论是：尽量等价格跌到安全边际下才买入，同时做好继续下跌的准备～\n\n此外，课程的另一个观点是即使是抄底，也尽量要去抄市场的底，而不是个股的底，因为市场极端的下跌是不太可能，但是课程里没给出相应的方法论，这部分可以参考\n基金大跌 20%，现在适合抄底吗？\n- 网叔的回答 - 知乎\n第二部分强调了不要迷信专业光环，根据课程提供了数据表明，无论是从 3 年 5 年还是更长的时间维度来看，大多数主动型、基金和专业人士都跑不赢大盘\n交易成本\n这部分主要为了说明在交易过程中，需要注意交易成本的高低，因为这对最终收益的高低影响较大\n以基金为例，一般涉及到的费用有管理费、托管费、申购赎回费和销售服务费，，这几种费用的详细解释可参考这篇文章\n购买基金，不可不知的费用,\n一些值得关注的点列在了下面\n\n管理费\n国内指数基金的平均管理费率在 0.69%\n左右。部分规模大，运行时间长的基金，管理费率会降低到 0.5%\n以下；主动型基金的管理费率一般是 1.5%\n托管费\n国内指数基金的托管费率平均在 0.14%\n左右，低的可以做到 0.1%。\n所有的基金，不管是场内还是场外，都会有管理费和托管费\n申购赎回费\n申购赎回费是场外基金的概念，对应场内基金的是买卖佣金，也就是在每次交易需要给的手续费\n申购赎回费是不同的，一般在 1%-1.5%;\n同的渠道申购，费用在不相同。比如在银行申购，费用一般不打折，在基金公司官网，部分网络平台申购，一般会有一定程度的折扣，有的时候甚至会打一折。\n销售服务费\n免申购赎回的基金，通常会在基金后面加个字母 C，被称为 C 类基金。这类基金不收取申购费用，但会收取固定比例的销售费用，一般是每年收取 0.4%-0.6%。投资指数基金，一般是长期持有，在申购费一折下，交易成本可以控制在\n0.1% 左右，所以我们一般不选择 C 类基金\n\n按照上面的计算，每年基本上会付出去 3% 左右的费用，那么这 3%\n的费用到底高不高，听着好像也没多少，我们可以做个计算\n\n假设你投资的这个基金平均每年回报是\n10%，这个成绩已经相当不错了，但是由于你支付了各种五花八门的费用，最终你真正拿到的收益应该是 7% 左右。那么我们假设一开始投资了 100 块，按照 10% 的每年的回报，10 年之后你的 100 块钱会变成 259 块钱，20 年之后你的 100 块钱会变成 673 块钱，分别翻了接近 3 倍和 7 倍，\n但是经过基金公司的雁过拔毛，按照 7%\n计算你的 100 块会变成多少？10 年之后是 197，20 年之后是 387，这是什么概念？即如果按照投资 10 年计算，由于费用的存在，你的利润被别人拿走了 40%，如果按照 20 年计算，你的利润被别人拿走了 50%，高达一半之多。\n换句话说仅仅因为这些看起来好像没有多少的费用，你辛辛苦苦攒钱投资了 20 年，最后一半的收益都奉献给基金公司和基金经理了，这听起来是不是实在有点荒谬，原因也很简单，就是我们最熟悉的复利的作用\n\n那对于投资者来说，具体要怎么做呢？课程里并没提及这一点，雪球上的这篇文章\n购买基金时不可忽视的手续费，如何降低手续费，提高基金收益给出了一些具体的方法论\n\n通过基金公司官网或第三方平台购买可以有 1 折的优惠，会比银行渠道购买更便宜，购买时可以多对比几个渠道\n\n股票型基金都会有交易费用，如果没有申购费，要留意销售服务费用\n\n指数基金费用相对更低，跟踪同一指数的基金收益差异不大，所以购买时可以多对比下同类型指数基金的运作费用和交易费用，特别是长期定投下，费用低的可以省下不少钱\n\n主动型基金手续费最高，但优秀的基金经理长期是可以大幅跑赢指数的，所以购买主动基金时，主要避免因频繁交易导致交易费用的上升\n\n最重要的一点一定要记住，不要频繁买卖基金，基金的交易成本高，短期交易成本更高\n\n自我局限\n这里主要分析了个人的自身的局限性是怎样阻碍我们投资成功的，主要分讲了以下几个因素：过度自信、反向复利、利益错位、场外因素，且总结得出最终的投资五大原则。\n过度自信\n这部分主要想说明在投资过程中，要区分清楚赚到的前依赖的是运气还是实力，避免过度自信。因为如果是市场原因或者偶发的事件赚到了钱，却把原因归结成了自己的能力，那么在未来等待着你的往往就是灾难。\n比如说在市场大牛市的时候，人人买股票都赚钱，人人都觉得自己是股神，但等到熊市以来，所有之前自认为正确的投资逻辑都失效了，很多人都会把之前赚的钱成倍的亏回去。\n那该怎么避免这种问题呢？课程里提供了两个参考方法：一个事前使用，一个事后使用\n（1）事前使用的方法是：在进行投资之前，把自己的投资逻辑记录下来，列出你的诉条理由以及背后的逻辑。等投资有了结果，可以相对容易的看出是不是因为当时的判断正确赚到钱的，这会让你更好的认知自己的投资能力。\n而且不管赚钱还是亏钱，这个习惯都能让你不断完善自己的认知体系和投资框架。长期来看你赚钱的概率就会慢慢提高了。我们为什么说巴菲特是股神？因为他投资已经六七十年、穿越了无数个周期，做对了无数多正确的决策，我们才认可了他的地位。对待他人是这样，对待自己也要一样，不要因为一两次的成功就沾沾自喜。\n（2）事后采用的方法是及时止损，这里的止损不是说只要账面亏钱了就要立刻卖出，那就变成了追涨杀跌的赌博游戏。\n这里说的止损是交易大师爱德华索普的方法，他说：“在我写下我的交易逻辑之后，我会放入一点钱，看看市场会不会验证我的逻辑。一旦我发现市场的表现和我预想的不一样，我就会先止损离场，给自己时间冷静下来，把问题想清楚再重新回去。这样虽然有时候会让我错过一些好的投资，但也能保证我不会犯下致命的错误，无法收场。”\n因此，这部分有以下 2 个结论值得重点关注\n（1）在赚到钱的时候一定要思考一个问题：我为什么能赚到这笔钱呢？是因为自己的能力，还是只是因为运气？如果是运气的话，这一次投资决策的逻辑就不能作为下一次的参考。\n（2）如何避免过度自信？课程里给了两个方法：一个事前方法是把自己的投资逻辑写下来，以便根据结果对比分析；还有一个事后方法是要及时止损，想清楚了再回到市场里，\n反向复利\n这部分主要强调了一个常识，就是产品的收益与其风险是成正比的，而亏损会带来的一个很可怕的问题是负复利，也是这一部分的小标题 “反向复利”，课程里举了如下例子\n\n假设你连续 10 年的投资收益是这样的，第一年赚 20%，第二年亏 15%，第三年赚 20%，\n第 4 年亏 15%，第 5 年又赚 20%，第 6 年再亏 15%，以此类推下去。\n那么这 10 年下来你总共的投资回报有多少？稍微计算一下就可以得出答案，\n就是 10.4%，\n10 年总的回报 10.4%，每年连 1% 都不到。算上通货膨胀的影响，实际上你不但没有赚到钱，还白白损失了 10 年的宝贵时间\n\n那普通人还有必要参与这种高风险高收益的投资么？课程里的建议是普通人要有慢慢变富的心理，避免高风险带来上面说的反向复利的问题\n而如果要投资这种高风险高收益的产品，课程里给了以下 2 点建议\n(1)\n如果你要做风险很高的投资，那么它带来的回报一定要足够高，潜在收益最好是几十倍几百倍甚至几千倍，否则是得不偿失的。\n(2)\n当我们去做高风险投资的时候，一定要做好亏损全部本金的准备。就是说如果这个钱完全损失掉，你也能接受的话，这就是一个相对比较好的心态。\n因此，这部分有以下 3 个结论值得重点关注\n\n我们应该接受一个事实，通过投资致富其实是一个相对漫长的过程，接受慢慢变富\n\n负复利对我们的长期投资回报率会有很大的影响\n\n如果你真的想去参与高风险高回报的投资项目，那么要记住两点，第一它带来的回报一定要足够高，第二你要做好亏损 100% 的准备。\n\n利益错位\n这部分主要强调了一些他人给出的投资建议未必值得参考，或者说不要简单根据对方的建议就直接采取行动。首先你要判断对方跟他的建议之间到底有什么利益关系，另外你还要考虑对方有没有出于感情等等其他因素有讨好你的可能。\n课程以华尔街的分析师不想得罪上市公司而给出卖出评级非常保守为例，说明了这种利益错位的现象，即给出的分析结论不一定是站在投资者角度去考虑的，因为这里面更实在的利益关系是分析师\n与上市公司；当然，这里也不是说华尔街的分析师都是没有独立思考能力或者毫无职业品德的人，好的分析师还是很多的，但是由于这种非常独特的利益分配机制，我们在学习他们的观点，获取他们的建议的时候，就要格外的小心，\n除了上面这些与钱挂钩的利益错位的问题，很多时候甚至别人只是出于关心，也会给你带来麻烦。比如说作为投资你买了个房子，但是由于市场前景不明，你在考虑是不是要卖出，你和一个懂一点买房投资的朋友聊起了天，你问他说我这个房子买的对吗？现在是不是该卖掉了呢？这个时候你的朋友就成了给你建议的人，而他很有可能出于对你的关心，想安抚你焦虑的情绪。所以分析了一通之后说，我觉得房价之后肯定能涨回去，还是不要卖了。\n这些例子不是为了说明不要听取对方的意见，而是为了说明开头的观点：听取建议时需要从利益层面去考虑建议的适用性，同时要保持独立思考的能力，因为最了解自己的人只有自己。\n场外因素\n这部分主要强调了决定我们投资结果成败的，有时候是专业知识和技能之外的一些东西，\n即标题说的场外因素。\n课程以大空头电影里的迈克尔巴里为例说明这个观点，大意就是虽然他的投资方向是对的，但是受到了投资者的不解和给的压力，最终虽然在\n08\n年的次贷危机一战封神，但最后的结果也是遭到撤资，最终关掉所管理的基金。\n对普通投资者而言，在做投资的时候也经常要面对别人的质疑和挑战，甚至很多时候他人其实是善意的关心，比如你的老公、老婆、父母、朋友，总会念叨一下你的投资到底是赔是赚，总会拿你的成绩和别人比一比，这个时候能够管理他们的预期，能够让他们很好的理解你长期的投资计划就非常重要了。\n在这一点上，同样在次贷危机中狂赚一笔的约翰保尔森就比迈克尔巴里做的好得多。当时在和投资人沟通的时候，他没有渲染自己是在赌博，美国房地产崩盘，而是说自己的基金针对潜在的远期灾难进行了一种对冲，\n因此，课程里强调的场外因素有两个\n(1) 第一个场外因素是：投资本来就是一件需要独立思考，很多时候非常孤独的事情，所以能够维护好你能独立决策的环境，对我们的长期成功的确是非常重要的。上面强调的就是这一点\n(2) 第二个因素是：要保持你场外赚钱的能力，也就是投资之外的收入，因为它能反过来帮助你的投资成绩。想象一下，如果你的职业相当成功，工资水平不错，那么即使市场下跌，你的压力也不会特别大，甚至你还可以拿钱来补仓。反过来说，如果你只靠投资赚钱，遇到熊市你可能会非常煎熬，甚至会被迫卖出你的投资，把亏损做实。课程里将这点总结成了这么一句话：越不需要靠投资赚钱的人，越有可能靠投资赚到钱。\n五大原则\n这部分更多是对前面内容的一个总结，即投资的五大原则：多元分散，被动为主，降低成本，保持恒心，不懂不做\n\n多元分散:\n我们很难预测未来一段时间哪些国家、哪类资产会涨得最好，可以做的就是让自己的投资组合尽量分散，覆盖各个投资市场和各个投资品类，保证足够的多元分散。\n被动为主:\n任何人哪怕是专业人士想要长期的打败市场都是很难的，尤其是当你发现一个好的基金经理的时候，很可能他的业绩已经开始走下坡路了，所以与其把钱交给他们，或者听信所谓专业人士的推荐，不如通过投资市场指数来获得平均的回报，长期看是更好的选择。\n降低成本:\n在投资中对收益率影响巨大，而且也最容易把握的就是投资的费用和成本，哪怕我们每年只省一点点，累积起来对我们投资的回报都有很大的加成。所以在执行和操作的时候，我们要尽量降低成本，\n保持恒心:\n任何人都难以预测短期的市场走向和运动规律，想靠预测市场迅速赚到大钱是不可能的，我们应该长线思考，保持恒心。\n不懂不做:\n投资市场非常复杂，里面绝大部分的东西我们都没有能力弄懂，所以最终我们做投资的底线应该是不投资自己不懂的东西。\n\n小结\n这部分内容小结如下\n1. 指数基金\n\n个股涨跌难以预测，投资基金会是更好的选择\n\n一直被推崇的指数基金比较适合美国市场，但是不一定适合牛短熊长的 a\n股\n\n2. 安全边际\n\n好公司不等于好股票，买入价格的高低会严重影响投资收益\n\n要给交易留出足够的安全边际，比如当一个东西的价格跌到了估值的 60%\n的时候才买入\n\n这里的估值做点估计比较难，更合理的是做区间估计\n\n安全边际在实操中会有一定局限性\n\n3. 抄底哲学与专业光环\n\n不要盲目抄底，即使是抄底，也尽量要去抄市场的底，而不是个股的底\n\n不盲信专业人士的分析\n\n4. 交易成本\n\n交易成本是影响最终收益的一个重要因素；常见的基金会收取管理费、托管费，交易费等\n\n购买时选择费用低的渠道\n\n对于追踪相同指数被动型基金，收益差别不大，优先选择费用低\n\n不收交易费用时注意是否有销售管理费\n\n降低交易频率\n\n5. 自我局限\n\n过度自信：明确赚钱靠的是运气还是实力，如果是运气需要注意；避免过度自信的方法论有两个，可分为事前方法和事后方法\n\n反向复利：避免高风险带来亏损而导致的反向复利，如果确实要做高风险投资要明确回报要可观\n\n利益错位：听取别人意见时需要考虑他人所站的立场，以及与各方的利益关系\n\n场外因素：投资是一个独立的过程，尽量减少受到外界影响；保持你场外赚钱的能力，越不需要靠投资赚钱的人，越有可能靠投资赚到钱\n\n五大原则：多元分散，被动为主，降低成本，保持恒心，不懂不做\n\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"张潇雨的个人投资课 (3)- 投资组合构建","url":"/2021/11/28/%E5%BC%A0%E6%BD%87%E9%9B%A8%E7%9A%84%E4%B8%AA%E4%BA%BA%E6%8A%95%E8%B5%84%E8%AF%BE(3)-%E6%8A%95%E8%B5%84%E7%BB%84%E5%90%88%E6%9E%84%E5%BB%BA/","content":"本文是听了 个人投资课\n后的一些笔记，课程主要内容可划分为如下四个部分：市场规律、投资工具、自我局限和投资组合构建，前三个部分主要讲一些投资过程中最容易犯的错误，最后一部分则是讲一些具体的投资方法。\n本文是最后一部分的内容，介绍了资产配置的必要性、大类资产基本分类即特点、以及三种经典的资产配置方法。课程前面内容介绍可参考\n\n张潇雨的个人投资课 (1)- 市场规律\n\n张潇雨的个人投资课 (2)- 投资工具与自我局限\n\n\n资产配置与大类资产\n这部分主要强调了资产配置的重要性，同时介绍了进行资产配置时有哪些候选，即大类资产的分类\n资产配置的重要性体现在其能较好回报的波动性，或者说最大回撤；课程里给出了如下的例子\n\n著名的投资大师耶鲁捐赠基金的掌门大卫斯文森说过，我们在市场上不管做什么，本质上都是在做三件事情，资产配置，市场择时和产品选择，\n在 1986 年，三位著名学者发表了一篇叫做投资组合业绩表现的决定因素 (Determinants\nof Portfolio\nPerformance）的论文，里面得出一个结论，投资组合回报的波动性有 93.6% 都是由资产配置决定的，而市场择时、产品选择的贡献可以忽略不计 ;\n等到了 1991\n年，另外三位学者对这项研究进行了后续的补充，在更换了一些研究对象和研究方法之后，他们得出的结论是投资组合回报的波动性有 91.5% 是被资产配置决定的，其他两项因素可以忽略不计，\n说白了就是再次肯定了之前的研究结论\n\n这个结论听起来很反直觉，比如说现在上证指数还没回到 07\n年的高峰，难道不能说明择时能力很重要吗？买到涨停股和跌停股，一天的差别就是 20%，难道不能说明选择产品的能力很重要吗？笔者觉得\n这篇文章\n里给出了这个结论适用的几个前提比较合理\n\n1、时间分散。为了防止配置资产时建仓成本过高，建议逐步增加至目标仓位，在市场高峰时，尤其要谨慎。\n2、不同产品间分散投资。投资组合的目的不是为了寻找涨停股，而是为了获取持续稳健的收益。分散投资可以降低单支股票选择所带来的影响。\n3、长期策略。短期投资和长期投资的业绩决定因素不一样。当投资时间覆盖一个经济周期以上时，选择具体产品的能力和选时能力的作用就会很小。\n\n因此，这里提到的资产配置，核心目标是在管理最大回撤这个指标的前提下，获得长期收益\n那么，在构建资产配置时，有哪些候选可以选择，或者说有哪些大类资产可供选择呢？因为大类资产配置对最大回撤指标和组合的波动性具有决定性的影响。\n大类资产的似乎没有严格的分类，具体可参考这个问题 大类资产有哪些类？以那些标准分类？；而从普通人的视角来看，常见的可投资的大类资产有\n\n权益类：主要就是股票市场，如 A 股、美股、港股、其他国家股等\n\n固定收益类：债券、银行理财投资等\n\n大宗商品：基本可归为能源、金属和农产品三大类 (如原油、黄金、小麦等)\n\n房地产：房地产信托投资基金\n\n现金：货币基金或短期债券\n\n股票型基金前面都讲得比较多了，这里主要介绍债券和大宗商品 (后面投资组合也会有部分相应介绍)\n关于债券，以下几点值得关注\n(1) 债券主要分为利率债和信用债；前者有国家背书 (如国债、地方政府债、中央银行票据等)，后者则没有 (如金融债、企业信用债)，发行人信用情况是影响债券收益率的重要因素。信用债的风险大于利率债\n(2) 债券的价格于利率是呈反比的，也就是说，当利率上升的时候，债券的价格是下降的；相反，利率下降的时候，债券的价格是上升的，关于利率如何影响债券、股票等价格可以参考\n利率是如何影响大类资产价格的？\n(3) 债券同样有基金，关于债券基金的一些细节可以参考 2021，终于有人把债券基金说清楚了，里面给出的一些收益率的参考数字如下\n\n短债基金长期平均年化在 3%-4%；\n长债基金长期平均年化在 5%-6%；\n二级债基比较优秀的，长期平均年化在可以到 7%-8% 左右。\n\n关于大宗商品，以下几点值得关注\n(1) 比较严重的金融危机或者战争危机来临的时候，黄金可以起到一定的避险作用，但这不意味着它在危机时价格一定会涨 (如 2008 年金融危机，金价同样跌到了 30% 之多)\n(2) 从 1973 年统计到 2017 年，黄金总体回报率在 6.9% 左右；能源和产品类的总体回报率在 5~6% 之间，比国债高，比股票低；但这两种资产的波动和回撤比股票和债券都要高一些，黄金的最大回撤曾经超过 60%，而大宗商品整体曾下跌超过 80%。\n有了相应地原材料，接下来就是如何组合这些原材料了，下面阐述的三种资产配置的方法是按照预期收益从低到高，预期波动程度从低到高，涉及的资产种类复杂程度也从低到高来排列的。虽然世界上没有绝对正确的资产配置方案，这三个组合都是大师推荐并经过长期的时间验证过的，所以从他们开始学习是个不错的选择。\n永久组合\n这个组合是哈利・布朗的永久投资组合，是简单来说，就是 25% 的股票，25% 的国债，25% 的现金和 25% 的黄金\n整体思想就是不追求高回报，但追求长期的稳定以及过程的顺利，25% 的股票负责整体回报，25% 的国债会带来稳定收益，而现金和黄金的部分会在极端的市场环境下提供保护；日常情况下永久组合无法跑赢偏重股票的组合，但如果遇上经济危机和金融海啸，在别人的投资都跌掉 30%、40% 的时候，如果你使用的是永久组合，那么你的损失会小很多，很容易保持很良好的心态，更容易把投资长期的坚持下去，这也是永久这个名字的意义所在。\n那么怎么衡量这个永久组合的表现？课程里将其与一个通用的\n60、40 组合做了对比\n\n有个标准叫做 60、40 组合，其实就是 60% 股票，40% 国债这样一个极其简单的组合，这个组合被公认为是最简单，历史上的收益和风险也最被广泛的接受的一个组合。可以说任何的资产配置方案，都是从 60\n40 这个极简的配置方法出发的，所以它就变成了业界公认的比较基准之一。\n那接下来我们就比较一下，我们用 25% 的美国标普 500 指数基金，25% 的美国 10 年国债指数基金，25% 的黄金指数和 25% 的短期货币市场基金 (即现金) 来实现永久组合的配置。从 1973 年计算到 2017 年，这 45 年的历史回报那么得出的结果是怎么样的呢？\n永久组合的年化回报是 8.15%，最大回撤 12.42%，夏普比率是 0.5\n60、40 组合的指标是年化回报 9.76%，最大回撤 29.69%，夏普比率 0.52\n而对于标普 500 指数为例，这组相关指标分别是年化回报 10.52%，最大回撤 50.21%，夏普比率是 0.43\n根据这组比较，可以很明显的看出几个规律\n第一，永久组合的长期回报 8.15%，的确跑输了 60、40 组合的 9.76%，当然也跑输了整个股市 10% 以上的回报，落后的幅度基本在每年 1~2% 左右。\n第二，\n长期收益率的落后换来的是过程的极大平稳。如果单纯投资股市，你要面临自己资产曾经腰斩，跌到一半的可能性，哪怕是 60、40 组合，最大回撤也有 30%，但是永久组合有有现金和黄金的保护，最大回撤只有 12% 左右\n第三，不管是永久组合还是 60、\n40 组合，都明显提高了投资的夏普比率，也就是投资者每承担一份风险，获得了更高的超额回报。\n也就是说这个资产配置的确提高了我们投资的效率\n\n那具体要怎么配置呢？课程给了如下建议\n1. 股票\n\n原则：不要轻易选股，要尽量降低成本\n\n用市面上能找到的正规可靠低成本的股票指数基金来代表股票的部分\n\n4\n家美国目前最大最可靠的基金公司：安硕、先锋、到富和富达，他们提供的指数基金产品都非常全面，而且价格低廉，\n\n2. 债券\n\n一般用美国或者中国的国债作为底层资产，由于国家信用背书，这两种产品都是非常安全的\n\n债券的种类也非常多，在具有专业知识的情况下，可以用其他的一些债券品种来充当这个部分的底层资产，但是选择要非常谨慎；详细可参考前面列出的关于债券的链接\n\n3. 黄金\n\n可以买入实体的黄金，更方便的方法是买入和黄金挂钩的基金\n\n4. 现金\n\n在国内可以放入活期存款，或者买入期限比较短，比如 3~6 个月的货币基金而\n\n如果是外币，可以买入各大基金公司提供的美国三个月短期债券基金或者国债货币市场基金\n\n值得注意的是，这里的组合更多是一个框架，可以灵活地进行调整，如\n(1) 具体成分的选择。比如在选择中国的股票指数基金的时候，可以加入香港恒生指数，甚至中概股指数，这些都是可以的，没有绝对的谁优谁劣；更多的是自己的判断和偏好，甚至有些专业人士比较喜欢加入行业型的指数基金，比如说医疗行业，消费行业等等，但是前提是你得有专业级的判断\n(2) 地理位置的选择，之前也介绍过，很难判断一段时间内哪些国家和地区的股市涨得最好，所以在投资的地理维度上保持多元分散也是有意义的。在课程中常用中国和美国的产品来举例，这是我们相对最了解的两个国家，但如果对其他国家有投资偏好，也可以加进自己的选择。比如说很多基金经理喜欢投资所谓的新兴市场，因为这些地区的经济增长更快，或者有的人觉得在漫长的增长停滞之后，日本经济会迎来复苏，所以他们愿意配置一些日经 225 指数基金\n(3) 每类资产比例的大小。比如永久组合是股票、债券、黄金、现金 4 等分的一个中庸的组合，但如果你希望长期回报更高，同时愿意接受多一点的波动和回撤，那么自然就可以少投资一点国债和现金，多加一些在股票上\n全天候组合\n这个组合的提出者是雷・达利奥，是桥水基金创始人之一，这个全天候策略产品当前也是桥水基金的主打产品之一，管理有几百亿美元之多，而且表现不俗，尤其在 2008 年和 2018 年，整体环境不好的时候，都取得了可观的正回报。\n但这个策略的所有细节，是超过 1500 名桥水员工经过多年打磨和研究得出来的，而且是非常动态的。作为普通投资者，我们不可能也没有必要全盘复制他们的方法，而且也确实做不到。但我们可以按照他们的思路构建自己的组合，达里奥也在很多场合给普通投资者推荐过他全天候策略的简化版，普通人通过这个版本也能达到和桥水基金相近的风险和回报，并且能在不同的经济环境下取得收益。\n具体的配置是：美国大盘股 18%，美国小盘股 3%，其他发达国家股票 6%，新兴市场国家股票 3%，10\n年期美国债券 15%, 30\n年期美国债券 40%，大宗商品和黄金各占 7.5% 这个配置有一下几个特点\n\n全天候组合对 30% 的股票部分进行了细化，\n把它们分配给了美国、其他发达国家和发展中国家 (所谓新兴市场）三个部分，这个也是之前一直说的多元分散自己的投资组合。\n\n比起永久组合，简化版的全天候组合增加了一个大宗商品类的资产。这类资产往往被认为和通货膨胀挂钩比较多，可以在通胀比较严重的时候给投资组合提供回报。换种角度也可以理解成，达利奥觉得单独投资黄金还不保险，还把一部分的额度分给了大宗商品来一起控制最大回撤\n\n从资产大类来看，全天候组合其实就是 30% 股票，55% 政府债券，7.5% 的黄金和 7.5% 的大宗商品，也就是说这个配置方案比之前的永久组合更激进一些，因为股票占比是更高的\n\n那这个组合的具体表现如何\n\n从 1973~2013 年的历史来看，\n全天候组合：年化回报 9.5%，最大回撤 14.59%，波动率 8% 左右，夏普比率是 0.51。\n60、40 组合：年化回报 9.76%，最大回撤 29.69%，波动率 10% 左右，夏普比率是 0.52。\n永久组合：年化回报 8.15%，最大回撤 12.42%，波动率 6.88%，夏普比率是 0.5\n在这组数据里我们很容易看到，全天候组合和 60、40 组合的回报基本相同，但不管是波动率、还是最大回撤，都要小得多，毕竟不到 15% 的回撤，比起 30% 来还是舒服很多的。当然了永久组合因为有 25% 现金的存在，所以波动和回撤都是最小的，但这也是用降低\n1~1.5% 左右的长期收益率换来的。\n\n至于具体的产品选择，基本跟前面提到的一样，只是课程里重申了选择产品的主要原则：来自大基金公司，成本费用低廉，没有基金经理主动的干预，只是被动地跟踪市场。\n另外这部分还提到了一个组合管理与资产配置中另一个重要的概念：再平衡 (rebalance)；即当你组合里各种资产的比例，随着市场涨跌已经偏离了最初设定的时候，要通过操作把它恢复成一开始的比例，有点像把手机或者电脑一键恢复成出厂设置。\n那么这里有两个问题需要回答，第一，我们为什么要再平衡？第二，我们什么时候应该做再平衡？这两个问题，投资学术界和实战界都有很多经验总结\n针对第一个问题，在平衡背后有两个理论依据\n(1) 均值回归理论，即一个资产的价格最终都会回归到它的平均水平。根据这个理论，如果一个资产涨得比较快，在组合里占比比较大，我们对它的预期就是接下来的回报会下降；而如果一个资产之前跌了很多，接下来很可能会上升。所以再平衡的过程就是卖出上涨的资产，买入下跌资产的过程，这样会带来一些超额收益\n(2) 行为金融学上的原理：大部分人都更喜欢追涨杀跌，所以再平衡实际上是在高点卖出，在低点买入，实际上是一个相反的举动，这样长期坚持下来也会有一些超额收益\n针对第二个问题，学术界也做过很多研究，最后发现不论是 1 个月、3 个月、6 个月还是 12 个月再平衡一次，差距都不是很大。所以作为普通投资者，我们每 12 个月再平衡一次就基本足以，这样也能降低交易成本。\n另外，课程里还提到可以给自己的再平衡设置一个幅度，比如说永久组合 4 个资产开始各占 25%，可以设定一旦某个资产涨到 35% 的时候，就把它卖出，然后买入占比最低的资产，对于全天候组合也是一样的\n最后，关于全天候组合的更多阅读资料可参考 Ray\nDalio 的「全天候交易策略」是什么？如何理解？ - 师纪瑞的回答 -\n知乎\n斯文森组合\n这个组合是大卫・斯文森管理耶鲁捐赠基金时提出的一个组合，当时的斯文森发现了\n2 个秘密\n\n要想保证长期的回报高，整个投资组合一定要偏重于股权 (也就是股票）。现在也是整个业界的共识，但是在 80 年代斯文森接管耶鲁基金的时候，绝大部分的大学捐赠基金都更喜欢债券、房地产、黄金这样的投资产品，不太敢下重注在股票这个品类上。\n(2) 多投股权会带来一个的问题，就是波动很大，那么有没有一些股权品类是相对比较稳定，不太随着市场上上下下波动剧烈的呢？斯文森就找到了这么一个品类，说出来你也很熟悉，\n就是风险投资和私募股权投资，也就是我们常说的 VC\n和对冲基金。\n\n但是这里一个显著的问题是，普通投资者很难复制这个方法，因为 VCP\n、对冲基金这样的机构一般是不会对普通投资者开放的；因此，斯文森提供了一个简化版的耶鲁投资法，对于个人投资者，有三个核心的原则和建议\n\n一个投资组合里应该有 6 种不完全相同的资产，但总体来说应该以股票为主，因为从长期来看，股票永远是回报最高的选择。\n\n投资者应该按照自己设定的时间频率来给组合进行再平衡，一般来说一年一次就可以。\n\n市场很难战胜，所以相对个股投资，\n那些低成本的指数基金是很好的选择\n\n其实这三点基本原则跟前面 2\n种组合大同小异，斯文森根据这个思路推荐了一个这样的资产配置方法：美国大盘股 20%、其他发达国家股票 20%、新兴市场国家股票 10%、30 年期的美国债券 15%、通胀保护债券 15%、房地产投资信托 20%。\n可以看出啊这个组合比我们之前介绍的组合要更偏重股票一点，整体分配给股权的部分达到了 50%。另外斯文森也比较喜欢房地产信托基金，房地产信托长期收益和股票类似，所以这个组合虽然成分比较多，操作起来呢也复杂一些，但总体收益要比之前介绍的两个组合要高一些，当然波动也会更大\n那这个组合的收益情况如何？还是以 1973 年到 2013 年这个区间为例，斯文森组合的年化回报 10.16%，最大回撤 41.6%，波动率是 10.68%，夏普比率是 0.46，与前面的一些组合相比，回报是最高的，但是最大回撤也是最大的，夏普比率也是最低的\n因子投资\n因子投资 (Factor\ninvesting) 简单来说，就是尝试找到股票涨跌和表现的原因 (即因子)，然后基于这些因子来找到更好的投资产品的方法\n以大盘股 / 小盘股的概念为例，简单来说公司市值规模比较大的就叫大盘股，规模小的就叫小盘股。历史经验告诉我们，小盘股的长期收益通常要比大盘股更高，即规模是影响股票表现的一个因素，也就是这里说的因子。\n学术界在二十几年前就已经开始研究因子了，这个领域的泰斗级人物尤金・法马，与同事提出了三因子模型，股票的回报至少可以从三个角度来解释：市场因子、规模因子和价值因子。\n具体一点就是股票的回报至少可以分解为三个部分\n(1) 市场的整体回报，即每只股票随着大势上上下下来回波动带来的回报，\n(2) 规模，比如说小公司的长期回报比大公司更好\n(3) 价值因子，简单来说就是比较便宜的股票，长期收益会比贵的股票更好\n值得注意的是，这里说的这三个因子只能解释股市的一部分表现，而不代表所有，因为市场的复杂性往往通过这几个因子无法完全解释得通；那这些因子有什么实际的指导意义么？课程给出了如下建议\n\n通过这些因子，可以大概解释很多明星投资人和基金经理带来的超额回报到底是从哪来的了\n比如说我们之前课程中介绍过的股神彼得林奇，他就以偏爱高增长的小盘股而著称，他买的很多股票都带有这种性质，而我们都知道巴菲特非常在意股票的价格，所以他的超额回报就有一部分来自价值因子\n虽然因子肯定没法解释他们的全部天才，但是也给我们提供了一个重要的思路，这时候一个想法就很自然的诞生了。假设我们按照股神彼得林奇或者巴菲特的方法，系统性的投资小盘股或者估值比较便宜的股票，不就相当于复制了他们一部分的能力了吗？\n换个角度说，被动投资指数是我们最倡导的投资策略，而靠主动选股持续的打败市场非常难，但是因子投资给了我们这么一个机会，就是用一种介于主动和被动之间的投资方法，把我们的长期收益提高那么一点点，而且还不用有特别高深的专业知识。\n\n那现在业界公认最有效的因子都有哪些？总结下来主要有主要有这么 6 个\n\n规模因子，认为小盘股长期比大盘股回报要高一些\n\n价值因子，认为价格便宜的股票长期要比贵的股票收益高一些\n\n低波动因子，认为总体波动程度比较小的股票，长期比波动大的股票回报要高一些\n\n红利因子，认为高分红的股票长期表现会更好\n\n质量因子，认为质地比较好的公司的股票长期回报会更高\n\n动量因子，认为过去一段时间股价表现比较好的股票，接下来表现会更好\n\n这六大因子就是目前投资界已经被学者和实战派们反复证明了长期有效的因子，不仅经受住了统计学上的考验，也是很符合商业逻辑的；在具体的操作上，因子投资最终也可以落到买基金上，比如之前提到过的各种指数基金公司，像安硕、先锋等等，旗下都有费率很低的因子投资指数基金；国内各大基金公司都推出了像各种红利基金、价值基金、低波动基金等等相关的产品，\n但是有一点值得关注，和大类资产的表现一样，因子的表现也有周期性和轮动性。换句话说，如果随便看 5 年甚至 10 年，有可能某一类因子表现很好，但另一类就很差，如果你正好选到了比较差的因子，那么还不如直接投资最普通的指数基金；比如从 2001 年到 2007 年，动量因子、价值因子、低波动因子和规模因子表现都不错，但是质量因子反而跑输大盘，而从 2007 年之后质量因子开始表现不错，但是动量因子和价值因子就没有那么好了，应对手段还是之前提到的大原则：多元分散，之前我们说的是要在资产种类上分散，那么在因子上也是可以分散的\n定投指数基金\n这部分主要介绍了指数基金的一些基础知识，包括指数基金的一些分类，市盈率和市净率的含义。同时建议定投宽基指数基金，但是在国内这个建议是否适用还需要进一步考究，这部分在\n张潇雨的个人投资课 (2)- 投资工具与自我局限，这里不再赘述\n根据指数投资标的涵盖的范围，可以大概将指数分为：宽基指数、行业指数、策略指数；\n\n宽基指数：就是我们最常见的没有任何主观因素的，直接按照规则被动的复制市场表现的股票指数。比如沪深 300 指数、中证\n500 指数、创业板指数等\n\n行业指数：在各个行业里按照一定的标准来选取公司，方便那些对某个行业有偏好的投资者来投资，如所谓的消费行业指数，医药指数，军工指数或者养老指数，\n\n策略指数：这个策略性基金和我们之前说到的因子投资基本是一回事，如大成中证红利指数基金、景顺长城、中证 500 低波动指数基金，申万零信沪深 300 价值指数基金、嘉实基本面 50 指数基金。这些基金本质上就是在把因子投资的理念应用在中国股市上，上面提到的几个基金使用了红利因子，低波动因子，价值因子和质量因子的理念\n\n关于行业指数，课程里着重强调了如下观点\n\n以我个人的意见，我觉得你要想投资某个行业的行业指数，不妨先问自己一个问题，那就是你对这个行业有没有超出平均水准的判断力？比如说很多人都会说看好中国的医疗行业，看好中国的养老行业，所以想投资相关的指数基金\n这个时候我总会反问一个这样的问题，你的这种看好有什么超出常人水平的依据吗？比如说我们都知道中国人口老龄化的问题，都知道随着生活水平的提高，未来医疗养老服务会不断升级，那么你知道的这些信息是不是已经被反映在指数基金的价格里了？\n换个角度说，你有多确定医疗行业在未来你的投资期限之内，比如说 2 年 3 年 5 年之内能跑赢大盘，如果你只是在媒体上看了一些消息，在公众号里看了几篇文章，就对一个行业觉得看好的话，我是不建议你投资行业指数基金的，宽基指数可能更适合你，而如果你是某个行业的从业人员或者专家，又或者对某个行业的观察和积累很深，那么投资行业指数就会是不错的选择。\n对于我自己来讲，我觉得我对绝大多数行业都没有判断能力，所以投资宽级指数获得市场平均回报就很知足了\n\n所以课程里的建议是投资最常见也最经典的宽基指数，至于行业指数和策略指数，可以作为你核心持仓的一种补充，在自己能理解和接受的范围之内做一些配置就可以了\n基金通常还会被分为场内基金与场外基金，两者的区别可参考这个回答，场内基金与场外基金各有什么优劣势？\n- 简七的回答 - 知乎，简单对比如下图 (图片也摘自这个回答)\n\n关于基金的一些基础知识，还可参考这个专栏 从 0 开始学基金投资，写得比较通俗，同时也比较全面了\n另外，课程里还提到经典的 PE、PB 及其百分位的概念，这部分内容可以参考\n一文带你搞懂市盈率、市净率、市销率，笔者觉得以下几点可以参考\n\n在公司每年都能稳定盈利的条件下，市盈率 (PE) 越低，散户投资回本的速度越快\n\n低市净率 (PB) 意味着投资风险小，万一上市公司倒闭，清偿的时候可以收回更多成本\n\n根据市盈率百分位加仓或止盈，30% 以下进入加仓区间，70% 以上进入止盈区间，可采取分批止盈方法\n\n市盈率的局限\n\n市盈率只反映过去和当下的市场\n\n金融相关行业不适合用，例如银行、保险、证券业\n\n周期行业不适用，例如钢铁、有色金属、化工等\n\n战略性亏损的企业不适用，例如京东、亚马逊、特斯拉等\n\n\n市净率弥补市盈率，即使因为行业周期出现盈利能力的波动，对于市净率也不会有过大的影响\n\n牛市看市盈率，熊市看市净率\n\n课程结束语\n结束语主要强调两点\n1.\n从头到尾学完了之后，应该觉得投资真的很难，真的挺有风险，市场很强大，我们能掌控的事情其实很少的；当你对市场、对投资这件事更有敬畏心之后，你赚钱的几率反而会提高很多。\n2.\n语言是个功能有很大局限性的沟通工具，它是有很大的模糊性和欺骗性的，而这种模糊性和欺骗性很容易就把我们的投资旅程引入歧途\n第一点比较好理解，关于第二点，课程列举了如下例子\n\n很多散户投资者，甚至很多专业投资人在买股票的时候都喜欢大盘蓝筹股，那么你知道蓝筹股这个词是怎么来的吗？\n实际上蓝筹的英文是 blue\nchips，来自于西方赌场，因为过去的赌场里通常有三种筹码，白色的、红色的和蓝色的，其中蓝色的是最值钱的，久而久之大家就使用蓝筹股来指代那些规模大的、发展稳定的、市场形象良好的、盈利高的大公司。比如美国著名的道琼斯工业指数，一度也被称为蓝筹股指数，它在 1928 年创立出来的时候，就是选取的 30 家最大型的工业企业来代表美国股市的走势。\n正因为蓝筹股的这种特性，很多投资者尤其是散户型的投资者都很爱买蓝筹股的股票，因为他们觉得所谓的蓝筹股肯定是比较稳定，买起来很踏实，回报比较高的股票，更好玩的是这个概念，漂洋过海来到中国之后，大家又延伸出来了一个白马股的概念，指的基本也是那些长时间的，业绩不错，业务管理水平比较高，商业模式也比较靠得住的大公司。而由于大家都知道这些公司不错所以比起仍然在等待被挖掘的黑马，这种公司更像是人人都看得见的白马，于是白马股这个词也就传开了\n那投资蓝筹股或者白马股的结果怎么样呢？\n在 2012 年，研究机构 research affiliates\n的创始人罗布阿诺特做了一项研究，他们发现有这么一个现象，如果选取每个行业内市值最大的公司的第一名，这应该够蓝筹够白马了吧，\n然后跟踪他们接下来 1 年、3 年、5 年和 10 年期的股价表现，这些公司有比较大的概率是要跑输整个行业的其他公司的，准确说一年内跑出的概率是 57%、三年是 58%、五年 60%、10 年就是 66%，跑输的收益率幅度平均在 4% 以上，而且这个现象横跨了世界上 9 个主要国家的 12 个行业。\n另外一个统计是从 1980 年到 2019 年，每一个 10 年全世界市值排名前 10 的公司，如果你看 2019 年上面都是什么苹果、微软、亚马逊、谷歌、腾讯、阿里这样的公司，但你仅仅往前翻 10 年，这个榜单就变成了中石油、埃克森、美孚、沃尔玛、汇丰银行、碧和碧拓这样的公司。再看 2000 年，英特尔、朗讯、ge、德意志电信榜上有名，而 1990 年则是东京三菱银行、丰田、富士银行这些日本公司的天下\n因此，这些所谓的大盘股蓝筹股，在每个时代不可一世，觉得怎么也不可能消失的庞大的公司，他们的位置真的有那么稳固吗？买这些公司的股票真的是稳赚不赔吗？大公司的表现就一定能超出市场平均水平，超出指数吗？\n这里需要记住的一点是做投资，千万不要执着于概念，更不要靠感觉行事。白马股不代表公司股价永远不跌，绩优股不代表业绩永远优秀，\n\n所以我们能看到，语言和文字是很有欺骗性的，几句话能传达的信息量非常有限，如果你执着于这些说法的字面意思，很可能就会被带的越来越偏。而很多时候，语言的这种欺骗性对我们的伤害还不是最大的，因为错的东西，我们吃亏个几次可能也就长记性了，可怕的是那种说法，其实挺对，也没想骗你，但表达出来非常模糊，很容易让人误解的东西。关于这一点，课程举了如下例子\n\n巴菲特那句 “要在别人恐惧的时候我贪婪，别人贪婪的时候我恐惧”，这句话有错吗？当然没错，可以说是一句金玉良言，但问题是这句话实在是太模糊了，模糊到每个人的理解都可能非常不同\n比如，别人恐惧我贪婪，这里的别人是谁？是你的家人朋友，还是你认识的散户投资者？是市场的主力资金，还是各国的央行和政府？你有数据统计有百分之多少的别人正处在恐惧情绪里吗？这些东西我们很难说的清楚；同样，怎么就叫恐惧了，是市场跌了 20% 叫恐惧，还是跌了 50% 叫恐惧？是一个月连续下跌叫恐惧还是连跌三年叫恐惧？继续再说贪婪，怎么就叫贪婪，是把全部身家拿去买股票叫贪婪，还是你敢买所有人都觉得要完蛋了的公司叫贪婪，这些还是很模糊。所以巴菲特老人家也经常吐槽大家过于追求咬文嚼字的习惯，比如他都说过，我不知道什么叫价值投资，因为所有的投资都应该叫价值投资，一个投资没价值，你还要投资吗？\n更有意思的是，很多时候你会发现，哪怕有些说法实际上毫无意义，人们也会选择去听取和相信，比如陈大老师在他写的那篇很有意思的，如何做一个打脸无忧的股评里，就提到过，股评家最爱用一些模棱两可的话来预测市场\n比如他们来说，市场正在进行估值修复，什么叫估值修复？是高变成低叫修复还是低变成高叫修复，好像怎么说都行。还有比如市场情绪正在升温，怎么就叫升温了？是资金多了，还是价格涨了，还是恐惧情绪升温，大家都要逃跑了，还是怎么说都行。还有什么市场风险释放？陈老师说我就想问一问市场风险有憋着不释放的时候吗？市场风险不释放会怎么样？是原地爆炸吗？\n所以在最后一节课里，我为什么要说这些？因为我知道一门我的个人投资课，不可能把世间投资的道理都覆盖到。在课程结束之后，你肯定还要去其他地方继续学习研究，不断提高自己的水平和认知，这是非常正确的。但是你不管学习什么投资知识，它的载体肯定都是语言和文字，这门课也不例外，而只要是语言就带有它的局限，就有覆盖不到的情况，具有模糊性和欺骗性。\n所以投资这门学科里没有金科玉律，也没有绝对真理，尽信书、\n不如无书，只有时时刻刻保持独立思考，求真求实，相信理性，不根据感觉形式，不去相信和追求一劳永逸的必胜之法，而和常识概率以及时间站在一起才是最最重要的。\n\n关于语言的局限性，得意忘形有 2 期节目有过类似的观点，可以去听一下\n\n#03：《降临》与语言哲学、线性思维缺陷与「时间」可能不存在吗？\n\n#34：语言的陷阱、「财务自由」的\nlegitimacy 以及「你到底喜不喜欢我啊？」\n\n小结\n这部分内容可总结如下\n1. 资产配置的必要性\n\n投资组合回报的波动性有基本由资产配置决定的，而市场择时、产品选择的贡献可以忽略不计\n\n资产配置的目的不是为了寻找涨停股，而是为了管理最大回撤这个指标的前提下，获得长期收益\n\n2. 大类资产\n\n权益类：主要就是股票市场，如 A 股、美股、港股、其他国家股等\n\n固定收益类：债券、银行理财投资等\n\n大宗商品：基本可归为能源、金属和农产品三大类 (如原油、黄金、小麦等)\n\n房地产：房地产信托投资基金\n\n现金：货币基金或短期债券\n\n3. 永久组合\n\n25% 的股票，25% 的国债，25% 的现金和 25% 的黄金\n\n股票负责整体回报，国债会带来稳定收益，而现金和黄金的部分会在极端的市场环境下提供保护\n\n可调整维度：具体成分、地理位置、每类资产比例\n\n4. 全天候组合\n\n美国大盘股 18%，美国小盘股 3%，其他发达国家股票 6%，新兴市场国家股票 3%，10\n年期美国债券 15%, 30 年期美国债券 40%，大宗商品和黄金各占 7.5%\n\n产品选择大原则：来自大基金公司，成本费用低廉，没有基金经理主动的干预，只是被动地跟踪市场\n\n再平衡 (rebalance)：均值回归与追涨杀跌，12 月调整一次即可\n\n5. 斯文森组合\n\n美国大盘股 20%、其他发达国家股票 20%、新兴市场国家股票 10%、30 年期的美国债券 15%、通胀保护债券 15%、房地产投资信托 20%。\n\n基本原则同上，需要 rebalance\n\n6. 因子投资\n\n规模因子，认为小盘股长期比大盘股回报要高一些\n\n价值因子，认为价格便宜的股票长期要比贵的股票收益高一些\n\n低波动因子，认为总体波动程度比较小的股票，长期比波动大的股票回报要高一些\n\n红利因子，认为高分红的股票长期表现会更好\n\n质量因子，认为质地比较好的公司的股票长期回报会更高\n\n动量因子，认为过去一段时间股价表现比较好的股票，接下来表现会更好\n\n和大类资产的表现一样，因子的表现也有周期性和轮动性\n\n7. 基金的基本概念与常识\n\n投资行业指数时需要问一下自己，看好这个行业有什么超出常人水平的依据\n\nPE/PB 及其分位数定义\n\n根据市盈率百分位加仓或止盈，30% 以下进入加仓区间，70% 以上进入止盈区间\n\n8. 语言的局限性与陷阱\n\n语言是个功能有很大局限性的沟通工具\n\n做投资，千万不要执着于概念，更不要靠感觉行事。白马股不代表公司股价永远不跌，绩优股不代表业绩永远优秀\n\n投资这门学科里没有金科玉律，也没有绝对真理，尽信书、 不如无书\n\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"强化学习笔记 (1)- 概述","url":"/2018/05/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)-%E6%A6%82%E8%BF%B0/","content":"本文主要介绍强化学习的一些基本概念：包括 MDP、Bellman 方程等，\n并且讲述了如何从 MDP 过渡到 Reinforcement Learning。\n\n强化学习的任务\n这里还是放上 David\nSilver 的课程\n的图，可以很清楚的看到整个交互过程。这就是人与环境交互的一种模型化表示，在每个时间点，大脑\nagent 会从可以选择的动作集合 A 中选择一个动作 \\(a_t\\) 执行。环境则根据 agent 的动作给 agent\n反馈一个 reward \\(r_t\\)，同时 agent\n进入一个新的状态。\n\n\nRL\n\n知道了整个过程，任务的目标就出来了，那就是要能获取尽可能多的 Reward。Reward 越多，就表示执行得越好。每个时间片，agent\n根据当前的状态来确定下一步的动作。也就是说我们需要一个 state 找出一个 action，使得\nreward 最大，从 state 到 action 的过程就称之为一个策略 Policy，一般用\n\\(\\pi\\) 表示:\n强化学习的任务就是找到一个最优的策略 Policy 从而使 Reward 最多。\n我们一开始并不知道最优的策略是什么，因此往往从随机的策略开始，使用随机的策略进行试验，就可以得到一系列的状态，动作和反馈：\n\\((s_1,a_1,r_1,s_2,a_2,r_2,...s_t,a_t,r_t)\\)\n这就是一系列的样本 Sample。强化学习的算法就是需要根据这些样本来改进 Policy，从而使得得到的样本中的 Reward 更好。由于这种让 Reward 越来越好的特性，所以这种算法就叫做强化学习 Reinforcement\nLearning。\nMDP（Markov Decision Process）\n强化学习的问题都可以模型化为 MDP(马尔可夫决策过程) 的问题，MDP\n实际上是对环境的建模；MDP 与常见的 Markov chains 的区别是加入了 action 和\nrewards 的概念。\n因此，一个基本的 MDP 可以用一个五元组 \\((S,A,P,R, \\gamma)\\) 表示，其中\n\n\\(S\\) 是一个有限状态集\n \\(A\\) 是一个有限动作集\n \\(P\\) 是一个状态转移概率矩阵，\\(P_a(s, s′)=P(s_{t+1}=s′|s_t=s, a_t=a)\\)\n表示在状态 \\(s\\) 下执行动作 \\(a\\) 后转移到状态 \\(s′\\) 的概率\n \\(R\\) 是一个奖励函数，\\(R_a(s, s′)\\) 表示在状态 \\(s\\) 下执行动作 \\(a\\) 后转移到状态 \\(s′\\) 所得到的即时回报 (reward)\n\\(\\gamma\\)\n是一个折扣因子，一般取值在 [0,1];\n用来区分当前回报和未来回报的重要性，一般会加在未来的回报前，减小未来回报的权重。\n\n因此，MDP 的核心问题就是找到一个策略 \\(\\pi(s)\\) 来决定在状态 \\(s\\)\n下选择哪个动作，这种情况下 MDP 就变成了一个 Markov\nchain，且此时的目标跟我们前面提到的强化学习的目标是一致的。\n回报与价值函数\n状态的好坏等价于对未来回报的期望。因此，引入回报 (Return)\n来表示某个时刻 t 的状态将具备的回报：\n\\(G_t = R_{t+1} + \\gamma R_{t+2} + ... =\n\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\)\n上面 \\(R\\) 是 Reward 反馈，\\(\\gamma\\) 是 discount\nfactor（折扣因子）, 跟前面 MDP 中的符号的含义一致。\n从上面的式子可以， 除非整个过程结束，否则我们无法获取所有的\nreward 来计算出每个状态的\nReturn，因此，再引入一个概念: 价值函数 (value\nfunction), 记为 \\(V(s)\\)，通过\n\\(V(s)\\)\n来表示一个状态未来的潜在价值。从定义上看，value function\n就是回报的期望：\n\\(V(s) = \\mathbb E[G_t|S_t =\ns]\\)\n引出价值函数，对于获取最优的策略 Policy 这个目标，我们就会有两种方法：\n\n直接优化策略  \\(\\pi(a|s)\\) 或者 \\(a = \\pi(s)\\) 使得回报更高\n通过估计 value function\n来间接获得优化的策略。道理很简单，通过价值函数可以知道每一种状态的好坏，这样我们就知道该怎么选择了（如选择动作使得下一状态的潜在价值最大），而这种选择就是我们想要的策略。\n\nBellman 方程\n采用上面获取最优策略的第 2 种方法时，我们需要估算 Value\nFunction，只要能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算 Value\nFunction？\n根据前面 \\(G_t\\) 和 \\(V(s)\\) 的定义，有\n\\[\\begin{align}\nV(s) &amp; = \\mathbb E[G_t|S_t = s]\\\\\\\n    &amp; = \\mathbb E[R_{t+1}+\\gamma R_{t+2} + \\gamma ^2R_{t+3} +\n...|S_t = s]\\\\\\\n    &amp; = \\mathbb E[R_{t+1}+\\gamma (R_{t+2} + \\gamma R_{t+3} +\n...)|S_t = s]\\\\\\\n    &amp; = \\mathbb E[R_{t+1}+\\gamma G_{t+1}|S_t = s]\\\\\\\n    &amp; = \\mathbb E[R_{t+1}+\\gamma v(S_{t+1})|S_t = s]\n\\end{align}\\]\n则有\n\\[\\begin{align} V(s) = \\mathbb E[R_{t+1} +\n\\gamma V(S_{t+1})|S_t = s] \\end{align}\\]\n上面这个公式就是 Bellman 方程的基本形态。从公式上看，当前状态的价值和下一步的价值以及当前的反馈 Reward 有关，\n其中透出的含义就是价值函数的计算可以通过迭代的方式来实现。\n从 MDP 到 Reinforcement\nLearning\n回到 MDP 问题，如果我们知道了转移概率 \\(P\\) 和奖励函数 \\(R\\)，那么便可通过下面的方法求出最优策略\n\\(\\pi(s)\\),\n首先，结合上面提到的价值函数和 Bellman 方程有\n公式 1：\n\\[\\begin{align} \\pi(s):=\\arg \\max_a\\\n\\{\\sum_{s'}P_{a}(s,s')(R_{a}(s,s')+\\gamma V(s'))\\}\n\\end{align}\\]\n公式 2:\n\\[\\begin{align}  V(s) :=\n\\sum_{s'}P_{\\pi(s)}(s,s')(R_{\\pi(s)}(s,s') + \\gamma\nV(s'))  \\end{align}\\]\n公式 1 表示在状态 \\(s\\)\n下的采取的最优动作，公式 2 表示在状态 \\(s\\) 下的价值，可以看到两者有依存关系\n而在 转移概率 \\(P\\) 和奖励函数 \\(R\\) 已知的情况下，求解 MDP 问题常见做法有\nValue iteration 或 Policy\niteration\nValue iteration\n在 Value iteration 中，策略函数 \\(\\pi\\) 没有被使用，迭代公式如下：\n\\[\\begin{align} V_{i+1}(s) := \\max_a\n\\sum_{s'} P_a(s,s')(R_a(s,s') + \\gamma V_i(s'))\n\\end{align}\\]\n下标 \\(i\\) 表示第 \\(i\\)\n次迭代，在每轮迭代中需要计算每个状态的价值，并且直到两次迭代结果的差值小于给定的阈值才能认为是收敛。\n计算的出收敛的价值函数后，通过公式 1 就能够得出策略函数 \\(\\pi\\) 了，其迭代过程如下图所示\n\n\nValue Iteration\n\nPolicy iteration\nPolicy iteration 同时更新价值 \\(V\\)\n和策略 \\(\\pi\\), 且一般可分成两步：\n\nPolicy\nEvaluation，策略评估，就是上面公式 2 的过程。目的是在策略固定的情况下更新 Value\nFunction 直到 value\n收敛，从另一个角度来讲就是为了更好地估计基于当前策略的价值\n Policy Improvement，策略改进，就是上面公式 1 的过程。就是根据更新后的\nValue Function 来更新每个状态下的策略直到策略稳定\n\n这个方法本质上就是使用当前策略 (\\(\\pi\\)) 产生新的样本，然后使用新的样本更好的估计策略的价值 (\\(V(s)\\))，然后利用策略的价值更新策略，然后不断反复。理论可以证明最终策略将收敛到最优\n具体的算法流程如下所示\n\n\nPolicy Iteration\n\n区别与局限\n问题来了，上面的 Policy Iteration 和 Value Iteration 有什么区别，\n为什么一个叫 policy iteration，一个叫 value iteration？\n原因其实很好理解，policy iteration 最后收敛的 value \\(V\\) 是当前 policy 下的 value\n值（也做对 policy 进行评估），目的是为了后面的 policy\nimprovement 得到新的 policy；所以是在显式地不停迭代\npolicy。\n而 value iteration 最后收敛得到的 value\n是当前 state 状态下的最优的 value 值。当 value\n最后收敛，那么最优的 policy 也就得到的。虽然这个过程中 policy\n在也在隐式地更新，但是一直在显式更新的是 value\n的，所以叫 value iteration。\n从上面的分析看，value iteration 较 之 policy\niteration 更直接。不过问题也都是一样，都需要知道转移概率 \\(P\\) 和奖励函数 \\(R\\)。\n但是对于 Reinforcement Learning 这一类问题，转移概率 \\(P\\) 往往是不知道，知道转移概率\n\\(P\\) 也就称为获得了模型\nModel，这种通过模型来获取最优动作的方法也就称为\nModel-based\n的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有\nModel-free 的方法来寻找最优的动作，像\nQ-learning，Policy Gradient，Actor Critic 这一类方法都是 model-free\n的。\n前面的方法问题是需要已知转移概率 \\(P\\),\n目的是为了遍历当前状态后的所有可能的状态，因此如果采用贪婪的思想，那么就不需要不遍历后面所有的状态，而是直接采取价值最大的状态动作来执行。\nQ-learning 实际上就是采用这种思想的，Q-Learning 的基本思想是根据 value\niteration 得到的，但要明确一点是 value iteration\n每次都对所有的 Q 值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，因此，我们只能得到有限的系列样本。具体的算法流程会再下一篇文章具体介绍。\n综上，本文主要介绍了强化学习的任务和一些概念，以及从 MDP 如何过渡到\nReinforcement，在后续的文章中会陆续介绍 Q-learning 类方法，Policy\ngradient 类方法以及结合两者的 Actor Critic 方法。\n\n参考资料\n\nMarkov\ndecision process\nDQN\n从入门到放弃 4 动态规划与 Q-Learning\n强化学习方法汇总\n\n","categories":["机器学习"],"tags":["机器学习","强化学习"]},{"title":"怎样用数据洞察你的用户","url":"/2017/06/10/%E6%80%8E%E6%A0%B7%E7%94%A8%E6%95%B0%E6%8D%AE%E6%B4%9E%E5%AF%9F%E4%BD%A0%E7%9A%84%E7%94%A8%E6%88%B7/","content":"本文内容主要来源于该知乎\nlive，主要介绍了受众定向（用户画像）的分类和方法、具体介绍标签体系建立以及如何进行行为定向。\n\n受众定向与用户画像\n原始行为数据无序杂乱，不能直接应用于业务，因此要将用户的原始行为转化为对用户的描述，也就是受众定向（用户画像），但是这里需要注意的是不能想当然地描述用户，而是要根据需求方（如广告主）的需求来为用户打上相应标签\n受众定向与常听到的用户画像的差别不大，均是研究如何描述用户，为用户打上标签，两者只是在着重点上有细微区别\n受众定向重点是可优化，也就是跟侧重于效果，而用户画像则更侧重于可解释性，然而在实际中对用户的描述往往是两者混合的，如对广告主需要可解释性强的标签，而对于模型则更侧重那些有效的标签（可解释性不一定好）\n\n\n受众定向与用户画像\n\n受众定向的分类\n下面以计算广告为例阐述可受众定向需要在哪些维度上做打标签\n1）用户维度 \\(t(u)\\),\n描述用户的固有属性 2）上下文维度 \\(t(c)\\)，描述用户浏览的内容 3）用户 - 广告维度\n\\(t(a,u)\\)，描述用户在某个广告主下的特有属性\n实际中需要为广告打上标签 \\(t(a)\\)，用于描述广告的固有属性，从而与用户匹配\n\n\n受众定向方法分类\n\n如上图所示，受众定向建立的标签一般有两种作用，而第一种作用的标签需要可解释性，第二种作用的标签则更强调其效果\n受众定向的方法\n同样以计算广告为例，下图展示了受众定向常用的方法，左边表示广告的生命周期，右边表示各个阶段的受众定向的方法和效果，其中越往左效果越好，具体方式的含义可参考这里\n\n\n常见受众定向的方式\n\n重定向只需要用到第一方数据 look-alike 需要用到第一方数据和第二方数据 s\nhyper-local 表示根据更精细的位置做定向 (移动端)\n标签体系的建立\n上面讲述了该在那些方面建立标签体系以及建立标签体系的方法，下面要讲一个重点的内容，就是该建立怎样的标签体系\n如下图所示，标签体系可以分为两大类，其中一类是结构化标签，可以认为这一类的标签是一个大的树状结构；另外一类则是非结构化的，也就是根据效果和需求驱动的标签体系，关键词就是一个典型的非结构化标签体系。\n\n\n受众定向标签体系\n\n这里需要注意的是，由于结构化标签结构上的完备性，往往会被大部分人采用，但是这种标签体系的效果未必就好，原因是这些结构化标签将重点放在了标签体系的完备性而往往忽略了广告主的具体需求。\n结构化的标签体系的一个典型例子是雅虎的 GD\n系统的标签体系，这个标签体系根据主观的判断来分类标签，并没有结合广告主的具体需求，在实际的效果并不好\n\n\n结构化标签\n\n下面的标签体系中虽然形式上类似于结构化标签，但是是根据各个广告主的需求来定制各个标签的，因此是非结构化的，也更实用\n\n\n非结构化标签\n\n非结构化标签体系的建立过程如下 1）确定行业\n2）了解行业里面用户的决策流程 3）根据决策流程定制各个流程中的标签\n如对于汽车行业，一般用户购买时会先考虑预算 (价格区间标签)，然后考虑车的用途（车型、大小等标签），最后考虑品牌（品牌标签）。\n实际中，具体的标签可通过不同的方法获取，下文要讨论的行为定向中也提供了一种获取具体标签的方法。\n行为定向\n下面讲述受众定向中的一个重点：行为定向，就是根据用户的历史行为给用户打标签。\n\n\n行为定向的定义\n\n行为定向首先要将用户的各种行为转化为标签，同时对各种行为进行加权，如下图所示就是通过三种行为（广告点击，搜索，浏览）为用户打标签，其过程都是根据用户操作对象的具体内容提取出标签（关键词，主题等）并进行叠加。\n\n\n行为定向的特征选择过程\n\n上面采用的方法并不复杂，其中一个很重要的原因就是行为定向的实时性要求，因为用户的行为的有效期一般不长，即从关注到最终的购买所持续的时间往往并不长，其次是因为要处理的用户的量级非常大。\n此外，由于各种行为所反映的用户的意图不一，因此需要对不同行为进行加权，而且对于不同的标签，加权的方式还不完全一样，如下是对不同标签的不同行为加权的一种方法\n\n\n加权方式\n\n上面建模采用泊松分布来处理，因为泊松分布就是描述某段时间内，具体数量的事件发生的概率，如上式表示在时间\n\\(t\\) 内，用户点击广告次数为 \\(h\\) 的概率。其中 \\(\\lambda\\)\n表示用户点击广告的频繁程度，也叫频繁性参数，该值越大，表示点击越频繁。\n\\(\\lambda\\)\n可描述为各种原始行为的加权和 , 其中 \\(w_{tn}\\) 为权重系数，\\(x_{tn}\\)\n为原始行为（N 种，如浏览、搜索等）的统计量，求解时通过 \\(h\\)\n的历史数量进行极大似然估计即可求出权重系数 \\(w_{tn}\\) ，然后直接在线上使用权重系数。\n此外，上面式子中 \\(t\\)\n表示对不同的标签建立不同的权重体系。\n上图中将各类行为变为具体的标签方法有以下几种，主要思路是找到用户的行为对应的内容（一般是具体文本），然后借助\nNLP 技术将内容转为标签\n\n\n行为变为标签的方法\n\n主要方法可以分为两种\n1）针对浏览行为、点击行为，相应的都会有具体内容，如浏览的页面的内容，点击的广告的具体内容，通过\nNLP 技术对内容提取关键词或主题分布，作为标签\n2）针对搜索行为，可分为通用搜索和垂直搜索，要解决的问题都是如何将搜索关键词变为标签。对于通用搜索，可以利用已有搜索引擎（百度，谷歌等）模拟用户搜索行为，从得到的搜索结果作为内容，从中提取标签方法可以采用与第一种相同的\nNLP\n技术；对于垂直搜索，可以通过淘宝、携程、汽车之家等垂直搜索引擎，直接从关键词返回的结果中提取标签，因为像这种垂直搜索引擎一般都会自定义好一套标签了，因此这里是采用了垂直网站已经分好的类。\n实际工程中，往往会从不同渠道获取用户不同的行为数据，将各种方式获取的日志整理在一起，称为\nSession log。Session log 中一行表示一个用户的数据，这样通过 MapReduce\n进行计算时，通过 map 过程即可完成某个用户的 targeting\n过程，也就是下图中的局部计算。\n除此之外，行为定向中往往要用到用户过去一段时间的数据（7 的整数倍，避免周六日带来的偏差），处理的数据是一个时间序列数据，处理的方法有以下两种：滑动窗口和时间衰减。\n\n\n处理时间序列的行为数据\n\n实际中一般采用时间衰减方式，因为时间衰减的方式计算的效率较高\n场景定向\n场景定向指的是判断出用户当前所处的场合和状态（地铁上、开会中、健身房等），针对的是移动设备。利用移动设备的传感器等搜集的信息，可以判断用户当前所处场景。\n以早餐推送为例，根据用户当前的速度可以判断出用户是否在坐地铁，根据时间判断用户是要去上班，根据用户的位置可以为用户推荐附近的早餐店，实际上这是一个真实的例子，在东京的一次肯德基推广活动中，通过移动设备行为数据的分析，活动方准确找到了那些从地铁出来准备吃早餐的人群，实时给他们送出了早餐优惠券。\n这里需要注意的是场景定向不是上下文定向，上下文定向针对的是媒体内容，而场景针对的是用户。\n人口属性定向\n人口属性定向中主要是要预测性别和年龄阶段，实际上就是分类问题\n\n\n人口属性定向\n\n受众定向的评判标准\n上述的受众定向的过程其实就是依据某个用户的各种历史行为，为该用户在所有的标签上打出一个分数，然后线上设定分数阈值，大于这个阈值则判断用户是这个标签的人群。因此一般来说阈值越大，标签人群越少，但是结果也会越可信。\n下图展示了该如何评测受众定向的有效性，横轴表示被打上标签的人群的比例，纵轴表示被打上标签的人群对广告的点击率，一般来说，某个标签圈定的人数越少，效果越好，也就是点击率越高，违反了这一变化规律则说明受众定向不起作用。\n\n\n受众定向评测\n\n综上，本文主要讲了受众定向中的行为定向\n\n确定具体行业\n研究行业用户决策过程，制定用户标签体系\n制定标签体系不要刻意追求规整、结构化的标签体系\n把用户的原始行为映射到标签体系中，并求出各种行为类型的权重\n根据 4\n可为用户在各个标签上打出一个分数，为标签设定阈值，则通过比较分数和阈值可以判断用户是否属于该标签人群\n通过 Reach/CTR 曲线评测定向是否有效\n\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"强化学习笔记 (3)- 从 Policy Gradient 到 A3C","url":"/2018/05/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(3)-%20%E4%BB%8E%20Policy%20Gradient%20%E5%88%B0%20A3C/","content":"在之前的文章 强化学习笔记 (2)- 从\nQ-Learning 到 DQN 中，我们已经知道 Q-Learning 系列方法是基于 value\n的方法，\n也就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法，那有没有更直接的做法呢？有！那就是直接更新策略。本文要介绍的\nPolicy Gradient 就是这类 policy-based 的方法， 除此之外，还会介绍结合了\npolicy-based 和 value-based 的 Actor-Critic 方法，以及在 Actor-Critic\n基础上的 DDPG、A3C 方法。\n\nPolicy Gradient\n基本思想\nPolicy Gradient 就是通过更新 Policy Network\n来直接更新策略的。那什么是 Policy\nNetwork？实际上就是一个神经网络，输入是状态，输出直接就是动作（不是 Q 值），且一般输出有两种方式：一种是概率的方式，即输出某一个动作的概率；另一种是确定性的方式，即输出具体的某一个动作。\n如果要更新 Policy Network\n策略网络，或者说要使用梯度下降的方法来更新网络，需要有一个目标函数，对于所有强化学习的任务来说，其实目标都是使所有带衰减\nreward 的累加期望最大。即如下式所示\n\\(L(\\theta) = \\mathbb E(r_1+\\gamma r_2 +\n\\gamma^2 r_3 + ...|\\pi(,\\theta))\\)\n这个损失函数和 Policy Network\n策略网络简直没有什么直接联系，reward 是环境给出的，跟参数 \\(\\theta\\)\n没有直接运算上的关系。那么该如何能够计算出损失函数关于参数的梯度 \\(\\nabla_{\\theta} L(\\theta)\\)?\n上面的问题没法给出更新策略，我们不妨换一个思路来考虑问题。\n假如我们现在有一个 Policy Network\n策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到 reward，或者 result。那么这个时候，我们有个非常简单的想法：如果某一个动作得到 reward 多，那么我们就使其出现的概率增大，如果某一个动作得到的 reward 少，那么我们就使其出现的概率减小。\n当然，用 reward 来评判动作的好坏是不准确的，甚至用 result\n来评判也是不准确的（因为任何一个 reward，result\n都依赖于大量的动作才导致的，不能只将功劳或过错归于当前的动作上。但是这样给了我们一个新的思路：如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！\n假设这个评价指标是 \\(f(s,a)\\),\n我们的 Policy Network 输出的 \\(\\pi(a|s,\\theta)\\) 是概率，\n那么可以通过极大似然估计的方法来优化这个目标。比如说我们可以构造如下目标函数\n\\(L(\\theta) = \\sum\nlog\\pi(a|s,\\theta)f(s,a)\\)\n比如说，对于某局游戏，假如最终赢了，那么认为这局游戏中每一步都是好的，如果输了，那么认为都是不好的。好的\n\\(f(s,a)\\)\n就是 1，不好的就是 - 1，然后极大化上面的目标函数即可。\n实际上，除了极大化上面的目标函数，还可以直接对 \\(f(s,a)\\) 进行极大化，如这篇博文 Deep\nReinforcement Learning: Pong from Pixels 中直接最大化 \\(f(x)\\) 也就是 \\(f(s, a)\\)\n的期望，可以看到，最后的结果跟上面的目标函数是一致的。\n\n\nmax f(s,a)\n\n评判指标的选择\n从上面的推导也可知道，在 Policy Gradient 中，如何确定评价指标\n\\(f(s,a)\\) 是关键。\n上面提到了一种简单地根据回合的输赢来判断这个回合中的每一步到底是好是坏。但是其实我们更加希望的是每走一步就能够获取到这一步的具体评价，因此出现了很多其他的直接给出某个时刻的评估的评价方式。如这篇论文\nHigh-dimensional continuous\ncontrol using generalized advantage estimation\n里就对比了若干种评价指标。\n\n\nselect judge function\n\n上面公式（1）中的 \\(\\Psi_t\\) 就是 t\n时刻的评价指标。从上图可以看到我们可以使用 reward，使用 Q、A 或者 TD\n来作为动作的评价指标。那这些方法的区别在哪里？\n根据这篇文章 DRL 之 Policy Gradient,\nDeterministic Policy Gradient 与 Actor-Critic, 本质的区别在于 variance\n和 bias 的问题\n\n用 reward 来作为动作的评价是最直接的，采用上图第 3 种做法 reward-baseline 是很常见的一种做法。这样做 bias 比较低，但是 variance 很大，也就是 reward 值太不稳定，会导致训练不会。\n那么采用 Q 值会怎样呢？Q 值是对 reward 的期望值，使用 Q 值 variance 比较小，bias 比较大。一般我们会选择使用 A，Advantage。A=Q-V，是一个动作相对当前状态的价值。本质上 V 可以看做是 baseline。对于上图第 3 种做法，也可以直接用 V 来作为 baseline。但是还是一样的问题，A 的 variance 比较大。为了平衡 variance 和 bias 的问题，使用 TD 会是比较好的做法，既兼顾了实际值 reward，又使用了估计值 V。在 TD 中，TD (lambda) 平衡不同长度的 TD 值，会是比较好的做法。\n在实际使用中，需要根据具体的问题选择不同的方法。有的问题 reward 很容易得到，有的问题 reward 非常稀疏。reward 越稀疏，也就越需要采用估计值。\n\n以上就是 Policy Gradient 的核心思想，通过 policy network\n输出的 softmax 概率 和获取的 reward (通过评估指标获取) 构造目标函数，然后对\npolicy network 进行更新。从而避免了原来的 reward 和 policy network\n之间是不可微的问题。也因为 Policy\nGradient 的这个特点，目前的很多传统监督学习的问题因为输出都是 softmax 的离散形式，都可以改造成 Policy\nGradient 的方法来实现，调节得当效果会在监督学习的基础上进一步提升。\nActor-Critic\n上面提到的多种评估指标其实已经涵盖了 Actor-Critic 的思想，原始的\nPolicy Gradient\n往往采用的回合更新，也就是要到一轮结束后才能进行更新。如某盘游戏，假如最后的结果是胜利了，那么可以认为其中的每一步都是好的，反之则认为其中的每一步都是不好的。其更新过程如下，图片摘自\nDavid\nSilver 的 Policy Gradient 课件 ，这种方法也叫 Monte-Carlo Policy\nGradient\n\n\nPolicy Gradient Update\n\n图中的 \\(\\log \\pi_{\\theta}(s_t,\na_t)\\) 是 policy network 输出的概率，\\(v_t\\) 是当前这一局的结果。这是 policy\ngradient\n最基本的更新形式。但是这个方法显然是有问题的，最后的结果好久并不能说明其中每一步都好。因此一个很直观的想法就是能不能抛弃回合更新的做法，而采用单步更新？Actor-Critic\n干的就是这个事情。\n要采用单步更新，意味着我们需要为每一步都即时做出评估。Actor-Critic\n中的 Critic 负责的就是评估这部分工作，而 Actor\n则是负责选择出要执行的动作。这就是 Actor-Critic\n的思想。从上面论文中提出的各种评价指标可知，看到 Critic\n的输出有多种形式，可以采用 Q 值、V 值 或 TD 等。\n因此 Actor-Critic 的思想就是从 Critic\n评判模块 (采用深度神经网络居多) 得到对动作的好坏评价，然后反馈给\nActor (采用深度神经网络居多) 让 Actor\n更新自己的策略。从具体的训练细节来说，Actor 和 Critic\n分别采用不同的目标函数进行更新， 如可参考这里的代码 Actor-Critic\n(Tensorflow)，下面要说的 DDPG 也是这么做的。\nDeep Deterministic\nPolicy Gradient(DDPG)\n上面提到的的 Policy Gradient\n处理问题其实还是局限在动作个数是离散和有限的情况，但是对于某些输出的值是连续的问题，上面的方法就不管用了，比如说自动驾驶控制的速度，机器人控制移动的幅度等。\n最开始这篇论文 Deterministic\nPolicy Gradient Algorithms 提出了输出连续动作值的 DPG (Deterministic\nPolicy Gradient) ; 然后 论文 Continuous control with deep\nreinforcement learning 基于 DPG 做了改进，提出了 DDPG (Deep\nDeterministic Policy Gradient)。\n这里 DPG 不详细展开说了，简而言之，主要就是证明了 deterministic\npolicy gradient 不仅存在，而且是 model-free 形式且是 action-value\nfunction 的梯度。因此 policy 不仅仅可以通过\n概率分布表示，也就将动作空间推到了无限大的。具体的理论课参考这篇文章 深度增强学习（DRL）漫谈\n- 从 AC（Actor-Critic）到 A3C（Asynchronous Advantage\nActor-Critic）\nDDPG 相对于 DPG 的核心改进是引入了 Deep\nLearning，采用深度神经网络作为 DPG 中的策略函数 \\(μ\\) 和 \\(Q\\) 函数的模拟，即 Actor 网络和 Critic\n网络；然后使用深度学习的方法来训练上述神经网络。两者的关系类似于 DQN 和\nQ-learning 的关系。\nDDPG 的网络结构为 Actor 网络 + Critic 网络，对于状态 \\(s\\), 先通过 Actor 网络获取 action \\(a\\), 这里的 \\(a\\) 是一个向量；然后将 \\(a\\) 输入 Critic 网络，输出的是 Q\n值，目标函数就是极大化 Q 值，但是更新的方法两者又有一些区别。论文中显示\nDDPG 算法流程如下\n\n\nDDPG\n\n从算法的流程可知，Actor 网络和 Critic\n网络是分开训练的，但是两者的输入输出存在联系，Actor 网络输出的 action 是\nCritic 网络的输入，同时 Critic 网络的输出会被用到 Actor\n网路进行反向传播。\n原始论文没有给出两个网路的具体示意图，这里给出一张这篇文章画的示意图，可以看到，Critic\n跟之前提到的 DQN 有点类似，但是这里的输入是 state + action，输出是一个 Q\n值而不是各个动作的 Q 值。\n\n\nnetwork visulization\n\n由于在 DDPG\n中，我们不再用单一的概率值表示某个动作，而是用向量表示某个动作，由于向量空间可以被认为是无限的，因此也能够跟无限的动作空间对应起来。\nAsynchronous Advantage\nActor-Critic(A3C)\n在提出 DDPG 后，DeepMind 在这个基础上提出了效果更好的 Asynchronous\nAdvantage Actor-Critic（A3C），详见论文 Asynchronous Methods for\nDeep Reinforcement Learning\nA3C 算法和 DDPG 类似，通过 DNN 拟合 policy function 和 value\nfunction 的估计。但是不同点在于 1. A3C 中有多个 agent\n对网络进行 asynchronous update，这样带来了样本间的相关性较低的好处，因此\nA3C 中也没有采用 Experience Replay 的机制；这样 A3C 便支持 online\n的训练模式了 2. A3C 有两个输出，其中一个 softmax output 作为 policy\n\\(\\pi(a_t|s_t;\\theta)\\)，而另一个 linear\noutput 为 value function \\(V(s_t;\\theta_v)\\) 3. A3C 中的 Policy network\n的评估指标采用的是上面比较了多种评估指标的论文中提到的 Advantage\nFunction (即 A 值) 而不是 DDPG 中单纯的 Q 值。\n整体的结构如下所示，图片摘自这篇文章。\n\n\nA3C\n\n从上面的如可知，输出包含两部分，value network\n的部分可以用来作为连续动作值的输出，而 policy network\n可以作为离散动作值的概率输出，因此能够同时解决前面提到的两类问题。\n两个网络的更新公式如下\n\n\nA3C update\n\nA3C\n通过创建多个 agent，在多个环境实例中并行且异步的执行和学习，有个潜在的好处是不那么依赖于 GPU 或大型分布式系统，实际上 A3C 可以跑在一个多核 CPU 上，而工程上的设计和优化也是这篇文章的一个重点。\n综上，本文主要介绍了 Policy Gradient 这一类的方法，最基础的 Policy\nGradient 是回合更新的，通过引入 Critic 后变成了单步更新，而这种结合了\npolicy 和 value 的方法也叫 Actor-Critic，Critic\n有多种可选的方法。对于输出动作为连续值的情形，前面那些输出动作概率分布的方法无能为力，因此提出了\nDPG 和 DDPG，DDPG 对 DPG 的改进在于引入深度神经网络去拟合 policy\nfunction 和 value function。在 DDPG 基础上又提出了效果更好的\nA3C，这个方法在 DDPG 上引入了多个 agent 对网络进行 asynchronous\nupdate，不仅取得了更好的效果，而且降低了训练的代价。\n\n参考\n\n深度增强学习之 Policy\nGradient 方法 1\nDeep\nReinforcement Learning: Pong from Pixels\n深度增强学习（DRL）漫谈\n- 从 AC（Actor-Critic）到 A3C（Asynchronous Advantage\nActor-Critic）\n深度强化学习（Deep\nReinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C\nintroduction\n\n","categories":["机器学习"],"tags":["机器学习","强化学习"]},{"title":"手机监控服务器登陆情况","url":"/2016/01/03/%E6%89%8B%E6%9C%BA%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%99%BB%E9%99%86%E6%83%85%E5%86%B5/","content":"通过往手机发短信提醒用户登录的方式也许有很多种，下面讲一种最容易实现的，实现起来也比较简单的。\n\n原理很简单: 中国移动提供 139.com 这样的邮箱，如果有邮件到达的会同时发送邮件标题到管理员对应手机，邮箱名是\n你的手机号@139.com。例如：当 13036110648@139.com 邮箱接收到邮件时，会同时给 13036110648 这个手机发送邮件到达信息，邮箱注册地址 http://mail.139.com/。\n其次，用户登录的时候会自动加载其用户主目录下的.bashrc 文件，那么我们可以在这个脚本里面加入执行发送邮件的命令，发送的内容为当前登录的用户及来源。\n发送邮件的命令为 mail, 如果提示找不到这个命令需要安装 mailx 这个软件包，发送邮件的命令如下所示：\nmail -s \"邮件主题\" XXX@139.com &lt; 文本形式的邮件\n文本形式的邮件里面的内容可以为空，这里的内容是记录该用户所有的登录记录。\n只需要在当前用户 (这里以 test 用户为例) 主目录下的.bashrc 文件添加下面这些内容即可\necho \"$(who am i)\" &gt;&gt; /home/test/login_history.log  mail -s \"$(who am i)\" 手机号@139.com &lt;/home/test/login_history.log  \n这样每一次 test 用户登录都会发邮件到 139 邮箱，邮件主题是这次登陆的一些信息，正文内容则是这个用户的所有登录记录。\n","categories":["Linux"],"tags":["Linux"]},{"title":"强化学习笔记 (2)- 从 Q-Learning 到 DQN","url":"/2018/05/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)-%E4%BB%8E%20Q-Learning%20%E5%88%B0%20DQN/","content":"在上一篇文章强化学习笔记 (1)- 概述中，介绍了通过\nMDP 对强化学习的问题进行建模，但是由于强化学习往往不能获取 MDP\n中的转移概率，解决 MDP 的 value iteration 和 policy iteration\n不能直接应用到解决强化学习的问题上，因此出现了一些近似的算法来解决这个问题，本文要介绍的就是基于\nvalue iteration 而发展出来的 Q-Learning 系列方法，包括 Q-Learning、Sarsa\n和 DQN。\n\nQ-Learning\nQ-Learning\n是一个强化学习中一个很经典的算法，其出发点很简单，就是用一张表存储在各个状态下执行各种动作能够带来的\nreward，如下表表示了有两个状态 \\(s_1,\ns_2\\)，每个状态下有两个动作 \\(a_1,\na_2\\), 表格里面的值表示 reward\n\n\n\n-\na1\na2\n\n\n\n\ns1\n-1\n2\n\n\ns2\n-5\n2\n\n\n\n这个表示实际上就叫做 Q-Table，里面的每个值定义为 \\(Q(s, a)\\), 表示在状态 \\(s\\) 下执行动作 \\(a\\)\n所获取的 reward，那么选择的时候可以采用一个贪婪的做法，即选择价值最大的那个动作去执行。\n这样问题就来了，就是 Q-Table\n要如何获取？答案是随机初始化，然后通过不断执行动作获取环境的反馈并通过算法更新\nQ-Table。下面重点讲如何通过算法更新 Q-Table。\n当我们处于某个状态 \\(s\\)\n时，根据 Q-Table 的值选择的动作 \\(a\\),\n那么从表格获取的 reward 为 \\(Q(s,a)\\)，此时的 reward\n并不是我们真正的获取的 reward，而是预期获取的 reward，那么真正的 reward\n在哪？我们知道执行了动作 \\(a\\)\n并转移到了下一个状态 \\(s'\\)\n时，能够获取一个即时的 reward（记为 \\(r\\)）, 但是除了即时的\nreward，还要考虑所转移到的状态 \\(s'\\) 对未来期望的 reward，因此真实的\nreward (记为 \\(Q'(s,a)\\)) 由两部分组成：即时的 reward\n和未来期望的 reward，且未来的 reward\n往往是不确定的，因此需要加个折扣因子 \\(\\gamma\\), 则真实的 reward\n表示如下\n\\[\\begin{align} Q'(s,a) = r +\n\\gamma\\max_{a'}Q(s',a') \\end{align}\\]\n\\(\\gamma\\) 的值一般设置为 0 到 1\n之间，设为 0 时表示只关心即时回报，设为 1\n时表示未来的期望回报跟即时回报一样重要。\n有了真实的 reward 和预期获取的 reward，可以很自然地想到用 supervised\nlearning 那一套，求两者的误差然后进行更新，在 Q-learning\n中也是这么干的，更新的值则是原来的 Q (s, a)，更新规则如下\n\\[\\begin{align} Q(s, a) = Q(s, a) +\n\\alpha(Q'(s, a) - Q(s,a)) \\end{align}\\]\n更新规则跟梯度下降非常相似，这里的 \\(\\alpha\\) 可理解为学习率。\n更新规则也很简单，可是这一类采用了贪心思想的算法往往都会有这么一个问题：算法是否能够收敛，是收敛到局部最优还是全局最优？\n关于收敛性，可以参考 Convergence\nof Q-learning: a simple proof，这个文档\n证明了这个算法能够收敛，且根据知乎上这个问题 RL 两大类算法的本质区别？（Policy\nGradient 和 Q-Learning)，原始的 Q-Learning\n理论上能够收敛到最优解，但是通过 Q 函数近似 Q-Table\n的方法则未必能够收敛到最优解（如 DQN）。\n除此之外， Q-Learning 中还存在着探索与利用 (Exploration and\nExploition) 的问题，\n大致的意思就是不要每次都遵循着当前看起来是最好的方案，而是会选择一些当前看起来不是最优的策略，这样也许会更快探索出更优的策略。\nExploration and Exploition 的做法很多，Q-Learning 采用了最简单的\n\\(\\epsilon\\)-greedy, 就是每次有 \\(\\epsilon\\) 的概率是选择当前 Q-Table\n里面值最大的 action 的，1 - \\(\\epsilon\\)\n的概率是随机选择策略的。\nQ-Learning 算法的流程如下，图片摘自这里\n\n\nQ-Learning\n\n上面的流程中的 Q 现实 就是上面说的 \\(Q'(s,a)\\), Q 估计就是上面说的 \\(Q(s,a)\\)。\n下面的 python 代码演示了更新通过 Q-Table 的算法，参考了这个 repo\n上的代码，初始化主要是设定一些参数，并建立 Q-Table,\nchoose_action 是根据当前的状态\nobservation，并以 \\(\\epsilon\\)-greedy 的策略选择当前的动作；\nlearn 则是更新当前的\nQ-Table，check_state_exist 则是检查当前的状态是否已经存在\nQ-Table 中，若不存在要在 Q-Table 中创建相应的行。\nimport numpy as npimport pandas as pdclass QTable:    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):        self.actions = actions  # a list        self.lr = learning_rate        self.gamma = reward_decay        self.epsilon = e_greedy        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)    def choose_action(self, observation):        self.check_state_exist(observation)        # action selection        if np.random.uniform() &lt; self.epsilon:            # choose best action            state_action = self.q_table.ix[observation, :]            state_action = state_action.reindex(np.random.permutation(state_action.index))     # some actions have same value            action = state_action.argmax()        else:            # choose random action            action = np.random.choice(self.actions)        return action    def learn(self, s, a, r, s_):        self.check_state_exist(s_)        q_predict = self.q_table.ix[s, a]        if s_ != 'terminal':            q_target = r + self.gamma * self.q_table.ix[s_, :].max()  # next state is not terminal        else:            q_target = r  # next state is terminal        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)  # update    def check_state_exist(self, state):        if state not in self.q_table.index:            # append new state to q table            self.q_table = self.q_table.append(                pd.Series(                    [0]*len(self.actions),                    index=self.q_table.columns,                    name=state,                )            )\nSarsa\nSarsa 跟 Q-Learning 非常相似，也是基于 Q-Table\n进行决策的。不同点在于决定下一状态所执行的动作的策略，Q-Learning\n在当前状态更新 Q-Table\n时会用到下一状态 Q 值最大的那个动作，但是下一状态未必就会选择那个动作；但是\nSarsa\n会在当前状态先决定下一状态要执行的动作，并且用下一状态要执行的动作的 Q\n值来更新当前状态的 Q\n值；说的好像很绕，但是看一下下面的流程便可知道这两者的具体差异了，图片摘自这里\n\n\nQ-Learning vs Sarsa\n\n那么，这两者的区别在哪里呢？这篇文章里面是这样讲的\n\nThis means that SARSA takes into account the control policy by which\nthe agent is moving, and incorporates that into its update of action\nvalues, where Q-learning simply assumes that an optimal policy is being\nfollowed.\n\n简单来说就是 Sarsa 在执行 action 时会考虑到全局（如更新当前的 Q\n值时会先确定下一步要走的动作）， 而 Q-Learning 则显得更加的贪婪和 \"短视\",\n每次都会选择当前利益最大的动作 (不考虑 \\(\\epsilon\\)-greedy)，而不考虑其他状态。\n那么该如何选择，根据这个问题：When\nto choose SARSA vs. Q Learning，有如下结论\n\nIf your goal is to train an optimal agent in simulation, or in a\nlow-cost and fast-iterating environment, then Q-learning is a good\nchoice, due to the first point (learning optimal policy directly). If\nyour agent learns online, and you care about rewards gained whilst\nlearning, then SARSA may be a better choice.\n\n简单来说就是如果要在线学习，同时兼顾 reward\n和总体的策略 (如不能太激进，agent 不能很快挂掉)，那么选择\nSarsa；而如果没有在线的需求的话，可以通过 Q-Learning 线下模拟找到最好的\nagent。所以也称 Sarsa 为 on-policy，Q-Leanring 为 off-policy。\nDQN\n我们前面提到的两种方法都以依赖于\nQ-Table，但是其中存在的一个问题就是当 Q-Table\n中的状态比较多，可能会导致整个 Q-Table 无法装下内存。因此，DQN\n被提了出来，DQN 全称是 Deep Q Network，Deep\n指的是通的是深度学习，其实就是通过神经网络来拟合整张 Q-Table。\nDQN\n能够解决状态无限，动作有限的问题；具体来说就是将当前状态作为输入，输出的是各个动作的\nQ 值。以 Flappy Bird 这个游戏为例，输入的状态近乎是无限的（当前 bird\n的位置和周围的水管的分布位置等），但是输出的动作只有两个 (飞或者不飞)。实际上，已经有人通过\nDQN 来玩这个游戏了，具体可参考这个 DeepLearningFlappyBird\n所以在 DQN 中的核心问题在于如何训练整个神经网络，其实训练算法跟\nQ-Learning 的训练算法非常相似，需要利用 Q 估计和 Q\n现实的差值，然后进行反向传播。\n这里放上提出 DQN 的原始论文 Playing atari with\ndeep reinforcement learning 中的算法流程图\n\n\nDQN\n\n上面的算法跟 Q-Learning 最大的不同就是多了 Experience\nReplay\n这个部分，实际上这个机制做的事情就是先进行反复的实验，并将这些实验步骤获取的\nsample 存储在 memory 中，每一步就是一个\nsample，每个 sample 是一个四元组，包括：当前的状态，当前状态的各种 action 的\nQ\n值，当前采取的 action 获得的即时回报，下一个状态的各种 action 的 Q 值。拿到这样一个\nsample 后，就可以根据上面提到的 Q-Learning\n更新算法来更新网络，只是这时候需要进行的是反向传播。\nExperience Replay\n机制的出发点是按照时间顺序所构造的样本之间是有关的 (如上面的 \\(\\phi(s_{t+1})\\) 会受到 \\(\\phi(s_{t})\\) 的影响)、非静态的（highly\ncorrelated and\nnon-stationary），这样会很容易导致训练的结果难以收敛。通过 Experience\nReplay\n机制对存储下来的样本进行随机采样，在一定程度上能够去除这种相关性，进而更容易收敛。当然，这种方法也有弊端，就是训练的时候是\noffline 的形式，无法做到 online 的形式。\n除此之外，上面算法流程图中的 aciton-value function\n就是一个深度神经网络，因为神经网络是被证明有万有逼近的能力的，也就是能够拟合任意一个函数；一个\nepisode 相当于 一个 epoch；同时也采用了 \\(\\epsilon\\)-greedy 策略。代码实现可参考上面\nFlappyBird 的 DQN 实现。\n上面提到的 DQN 是最原始的的网络，后面 Deepmind\n对其进行了多种改进，比如说 Nature DQN 增加了一种新机制 separate\nTarget Network，就是计算上图的 \\(y_j\\) 的时候不采用网络 \\(Q\\), 而是采用另外一个网络 (也就是 Target\nNetwork) \\(Q'\\), 原因是上面计算\n\\(y_j\\) 和 Q 估计都采用相同的网络 \\(Q\\)，这样使得 Q 大的样本，y 也会大，这样模型震荡和发散可能性变大，其原因其实还是两者的关联性较大。而采用另外一个独立的网络使得训练震荡发散可能性降低，更加稳定。一般\n\\(Q'\\) 会直接采用旧的 \\(Q\\), 比如说 10 个 epoch 前的 \\(Q\\).\n除此之外，大幅度提升 DQN 玩 Atari 性能的主要就是 Double\nDQN，Prioritised Replay 还有 Dueling Network\n三大方法；这里不详细展开，有兴趣可参考这两篇文章：DQN 从入门到放弃 6\nDQN 的各种改进 和 深度强化学习（Deep\nReinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C\nintroduction。\n综上，本文介绍了强化学习中基于 value 的方法：包括 Q-Learning 以及跟\nQ-Learning 非常相似的 Sarsa，同时介绍了通过 DQN 解决状态无限导致\nQ-Table 过大的问题。需要注意的是 DQN\n只能解决动作有限的问题，对于动作无限或者说动作取值为连续值的情况，需要依赖于\npolicy gradient\n这一类算法，而这一类算法也是目前更为推崇的算法，在下一章将介绍 Policy\nGradient 以及结合 Policy Gradient 和 Q-Learning 的 Actor-Critic\n方法。\n\n参考\n\nQ\nLearning\nSarsa\nWhen\nto choose SARSA vs. Q Learning\nDQN 从入门到放弃 5\n深度解读 DQN 算法\n深度强化学习（Deep\nReinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C\nintroduction\n\n","categories":["机器学习"],"tags":["机器学习","强化学习"]},{"title":"我们这一代人的困惑","url":"/2015/11/20/%E6%88%91%E4%BB%AC%E8%BF%99%E4%B8%80%E4%BB%A3%E4%BA%BA%E7%9A%84%E5%9B%B0%E6%83%91/","content":"本文是于宙在 TEDx 大会上的演讲。这篇文章有点长，不过非常值得你花 20 分钟把它看完。\n\n以下是演讲全文：\n大家下午好，\n很荣幸能够参加本次 TEDx 大会。\n自我介绍：我是大连人，高中就读于大连市二十四中。因为当时学习十分不努力，所以高中毕业之后选择了出国留学。这其实是很多本科出国留学的人不能说的秘密，辗转了几个学校，最终毕业于美国印第安纳大学凯利商学院，主修投资和金融衍生品。\n上学的时候迷恋炒股，学习依旧散漫，没能成为一个 “放弃了华尔街的高薪工作毅然回国” 的海归精英，真的颇为遗憾，因为实在没有什么华尔街的公司愿意要我。碰巧的是，毕业前两年股市和外汇的行情比较好，赚到了一点点资本，于是我决定回国做点生意。现在在大连从事餐饮行业，目前拥有 4 家芝士蛋糕店和 3 家火烧店。\n引言\n大学毕业之后第一次面对这么多人做演讲，坦率地说，非常的紧张。虽然年轻的时候我曾经畅想过很多次，功成名就之后能像我曾经的那些偶像一样和年轻的朋友们分享一下我是如何从一无所有走上人生巅峰的经验，然后语重心长地告诉大家，人活着不能像一根草而是要像一棵树，能走到金字塔顶端的只有雄鹰和蜗牛两种动物，我的成功你也可以复制等等。\n可是过了 26 岁之后我忽然意识到一个严肃的问题，就是自己的一生未必会取得很大的成就啊，所以当 TEDxDUFE 团队找到我说没关系即便你只是一个开小吃店的，我们也愿意为你提供这样一个和很多人交流思想的机会时，我的心情是多么地激动。因为公司还没上市，所以小草大树、雄鹰蜗牛、睡地板捡易拉罐这样的故事还不到说的时候。今天，只想和大家分享几个困扰了我和我身边的一些朋友十几年的问题，和在经历了一些变故和挫折后，我对这些问题的看法。\n努力奋斗真的能实现梦想吗？\n大家现在可以想象一下汪峰老师坐在转椅上，深情地望着你，对你说，“你的梦想是什么？” 周星驰老师的那句 “做人如果没有梦想，和咸鱼有什么区别？” 据说也激励了几代人。梦想这个东西是如此的重要，简直就是人生的一盏明灯。成功的人们成功的原因各不相同，但他们都不会忘记告诉你，无论到什么时候，都不曾忘记梦想，是他们成功的首要原因。以至于我们这一代人对于人生意义的最通常的理解，就在于坚持梦想并最终实现它。可很少有人愿意面对的一件事情是，大部分人的梦想永远，没错，永远都实现不了。\n你没听错，大部分人的梦想永远都实现不了。\n先和大家分享一个我之前的梦想。上大学的时候，我热衷于各式各样的赌博游戏，是学校旁边赌场的常客。我赌徒生涯的起点源于赌场里最基本游戏轮盘赌，轮盘上 1 到 36 个数字和两个 0，赔率是 1 赔 36。1 到 36 分为红黑两色，押注红黑的赔率是 1 赔 1。\n作为一个合格的接受过九年义务教育的人都知道，每一次轮盘开始转动的那一刻，都是一次纯粹的独立随机事件。但是赌博这件事情的魅力就在于，当你真正身处赌场，看到已经连续 4 次开出红色的时候，几乎所有人都会想把筹码压在黑色的那一面。而我当时的梦想，就是破译这其中的奥秘。\n我最初的策略非常简单，当连续三次开出奇数，就押注偶数，连续三次红色，就押注黑色。难以置信的事情发生了，在我严格地执行这个策略的情况下，前几次去赌场不但全身而退，每次都还赚了不少，以至于我产生了一种幻觉，也许游戏是有规律可循的，我已经看到了人生巅峰就在不远处向我招手。当然最终的结尾你们一定想到了，在经历过连续 18 个偶数，连续开出 21 次黑色后，我把之前赚到的钱都乖乖地还给了赌场。\n后来我知道，我那个愚蠢的梦想叫做赌徒谬论，就不具体展开讲了。但它对我意义深刻，我终于明白了在纯粹的随机事件面前，一切规律都是无谓的。\n生活中的事情有极个别和轮盘赌一样，属于纯粹的随机事件，比如双色球。可是几乎每一个中了双色球的人都会告诉你啊，他们花了多少精力去钻研往期号码，研究历史规律，付出了多少辛勤的努力，最终获得了成功。实际上，即使是纯粹由随机性主导的事情，只要参与的人的基数足够大，小概率事件总会发生。有趣的是，几乎所有在随机事件中的受益者，都会把这完全由运气决定的结果归功于自己的努力上。不仅仅是参与者本身，旁观者也会这么认为。再比如，中国好声音的冠军嘛。\n我们生活中遇到的所有事情基本可以分为三类，第一类纯粹由随机性决定，比如布朗运动和轮盘赌博；第二类纯粹由能力决定，比如英语六级考试、110 米栏之类；第三类，也是我们最常遇到的，由能力和随机性共同决定，比如创业、投资、恋爱或是梦想。\n我对励志大师们总告诉年轻人要不惜一切代价追逐梦想感到深深厌倦的原因就在于，大多数人的梦想虽然不是纯粹的双色球，但也绝对是由随机性主导的。在强大的随机性面前，付出再多辛勤的汗水，就好比夜以继日蹲在轮盘赌旁边渴望参透其中规律。前面说到中国好声音的冠军，张碧晨的那一句\nyou are my\ndestiny，听得我也是醉了。但毕竟那一刻，中国又有多少唱歌唱得和她一样好甚至更好的姑娘，如果真把成为好声音冠军作为一生的梦想，一生中都得在痛苦中度过。\n我个人很喜欢黄渤，但绝对不会用黄渤作为例子去激励一个我这种长相差的年轻人不惜一切代价去追逐演员梦，注意是不惜一切代价。因为无论是唱歌还是演戏，再多的努力也只能让你变得很优秀，它们并不存在可以量化的评判标准，想成为万众瞩目的明星，随机性的重要程度都远远大于实力。\n我想，一个人在年轻的时候，做的每一件事情，能清楚地区分其中随机性所占的比例并能心平气和地接受它，在我看来就是最宝贵的财富。\n那么在你的梦想中，运气又扮演了多重要的角色呢？当你深深地感知到这件事情的随机性也许不会青睐于你，是否还愿意坚持下去呢？对我而言，梦想永远是值得执着追求的，但我可以无比心平气和地接受，它就是永远无法实现。\n既然连梦都实现不了，还有什么事情值得努力呢\n去年这个时候，我发过一条微博:\n&gt; 这些年我一直提醒自己一件事情，千万不要自己感动自己。大部分人看似的努力，不过是愚蠢导致的。什么熬夜看书到天亮，连续几天只睡几小时，多久没放假了，如果这些东西也值得夸耀，那么富士康流水线上任何一个人都比你努力多了。人难免天生有自怜的情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。\n这段话在网上的疯传，是我始料不及的。更出乎我意料之外的是，我在评论中看到了相当一部分的骂声，还有人认真地给我写下了相当深刻的话，“你在拥有自己的光亮时不要吹熄别人的蜡烛，你不能因为你自己的不喜欢就否定别人。”\n很莫名其妙是吧，即使你刚刚听完我上一段关于随机性的看法，你也会知道，我从来都不觉得努力是一件无所谓的事情。恰恰相反，我一直相信，在能力没达到一定程度之前，你连面对随机性的资格都没有啊。张碧晨能拿好声音冠军自然离不开运气，但换成杨幂，评委不但不会转身，可能直接撒腿就跑了。\n可现在问题来了，那究竟什么才算是有价值的努力？这可以从我那条微博说起。去年这个时候，我和朋友在琢磨去大庆做点服装生意，决定去考察几个商场。我当时住在北京，因为之前晚上和朋友在外面玩得比较尽兴，回到家里已经比较晚了，担心睡觉睡过头会错过航班，那晚上就直接在沙发上靠了一晚。那是我第一次去哈尔滨，十一月份已经很冷了，衣服拿得不足，下了飞机冻得头疼。又因为没有提前订票，到了哈尔滨之后才买的火车票，发现就只剩站票了。于是，当我一晚上没睡，冻得头晕眼花，又在绿皮火车上站了两个多小时之后，抵达大庆的那一瞬间我觉得自己实在是太不容易了，将来必须要写进回忆录里面。可是，回头仔细一想，这些所谓的 “努力” 对我最终把那个服装生意做好，没有半毛钱关系。更何况，如果我前一天晚上能早点上床睡觉，多准备点衣服，提前在网上把火车票订好，完全可以舒舒服服地达到同样的目的。?\n我的那次经历像是自己二十多年生活中很多事情的缩影，沉溺在对结果没有直接帮助只是因为自己遭受了一些痛苦的行为中，误以为那就是努力。\n当我终于意识到我并不是唯一曾经把无意义的消耗当作努力的时候，忽然发现，原来生活中我觉得很努力的人，也许没那么勤奋，如果在正确的方向上坚持行动，超过他们也并不困难。\n因为我们这一代人对于勤奋和努力的理解，几乎清一色地来自于学校，更精确地说，在前二十多年的生活中，我们眼中最努力的人，就是那些最能拼命看书和做题的人。实际上，这种理解是极其片面而幼稚的，因为看书和做题本身，都是为了一个极其鲜明的目的而存在的，就是通过考试。这种勤奋的付出极其纯粹，更多的复习时间，更高的复习强度，一般而言，都可以直接地提高考试的分数，它们之间的联系鲜明而直接，每个人都看得懂。\n但生活的美妙之处却在于，很多事情在我们没做到一定程度之前，是完全没法理解的。\n这就好比学英语，十几年漫长的岁月里我都在幻想，要通过多么复杂的流程，多么精密的设计，多么全面的涉及和多么不可思议的努力，终于有那么一天，或许我就能因为前期的那些无懈可击的学习，说一口比较流利的英语，像说中文一样，可以边说边想，而不是说每一句话之前设计它的句式时态词汇然后在心里复述几遍再看上去流利地背诵出来。谁不是这么设想的呢？可惜，它不仅从来没有实现，并且让我看不到有任何实现的趋势，对于每一个设立目标的人来说，没有比这更痛苦的感受。\n但是在去了美国两年左右的时间之后，我忽然发现自己已经可以毫无障碍地说一口流利的英语了。这并非我采用了什么新的学习方法，而是因为去了印第安纳之后身边中国人很少，在没有选择的情况下，只能被迫用英语去交流和表达，在这个过程中，我并没有认真想过自己每天进步了多少，也没有阶段性的检验学习效果，只是不停地去听和说，因为没有选择嘛。直到两年多后的忽然有一天我才意识到，咦，自己好像真的已经可以了。但是我确实无法总结出来是如何一步一步做到的，只是那两年的时间，我一直都在很不情愿地用英语去生活嘛。\n一个人能获得的最可贵的能力，都和掌握一门语言一样，你所付出的努力不是能够获得即时回馈的，甚至在很长的一段时间内没有任何收获，直到积累到了一定的阶段后，忽然爆发出惊人的力量，连你自己都不清楚这一切是如何发生的。比如锻炼身体，读书写作，或者是做生意。当你经历了足够的量变终于引起质变时拥有的技能，大部分人是终身难以企及的，不是因为他们太笨，恰恰相反，因为他们都太聪明了。\n触发人类行动的最基本原理被称为反射，我们是需要即时回馈的物种。所以绝大多数人对于世界的理解度是线性的，但更多情况下，事物却是以漫长的潜伏震荡后爆发突破的形式发展的。我现在时常觉得，人在少年时期更容易掌握语言、乐器、美术这些成年后很难学的技艺，并非那小时候就是天资聪颖，而是小孩子很少会一个星期质疑一次自己收获了多少，都是闷头一练就是好几年，直到学会了才知道哦自己已经会了。只有聪明的成年人，才相信一本书读懂易经，10 句话揭秘马云的成功之道，30 天成为吉他高手的故事。\n简而言之，现实生活中，付出和结果之间往往没有那么立竿见影。在离开学校之后，当我们遇到的很多事情不再像做题和考试之间联系得那么紧密的时候，很多人的付出都是浅尝辄止的。而最可贵的努力，是选择一个正确的方向，那些无法立即获得回报的事情，依然能付出十年如一日的专注和热情，最终的结果也许不足以让你独孤求败，但足以出类拔萃。\n人这一生中是否有一个节点，过了之后一切都会好起来\n前面说了这么多，谈论的都与目标和实现目标有关。仔细想想，我们的一生好像都是在实现目标中挣扎着度过的。上初中的时候，老师告诉你，中考的淘汰率是最高的，只要闯过去，上了高中一切就好了。但上了高中的时候发现不是那么回事嘛，高中老师又说了啊，考上大学就进了天堂。于是你考上了大学，依然空虚迷茫各种草样年华，父母老师又告诉你，找到工作就好了。工作之后发现烦恼和忧虑依然都在，女朋友给你看马云的故事，告诉你等你事业有成就好了……\n你发现了吗，其实人这一辈子的每一个阶段都有新的痛苦和顾虑，周而复始，生生不息。绝对不会因为你考上大学，事业有成，迎娶了女神就从此\nhappily ever\nafter。但每一个阶段也有每一个阶段的快乐，无法替代。生活不是安徒生童话也不是好莱坞电影，从出生的那一刻起直到生命的尽头，都不存在什么节点，过去了之后一切幸福美满无忧无虑。\n每一段岁月都有它存在的价值，没有高低贵贱之分，都不应该被辜负。而我能想到的人这一生能做的最愚蠢的事情，就是把全部人生的希望都孤注一掷到未来的某个节点上，而忽略了生活本身应有的乐趣。哪怕你以后真正实现了那个执念中的目标，才会发现它远远没你想的那么美好。\n年轻的时候和哥们在操场上打篮球喝可乐的快乐，是以后高尔夫球会所里品红酒替代不了的。尤其男生，千万不要总想着等将来有钱了如何如何，且不说你以后很可能不会太有钱，而且相信我，就是有钱了也真的不能怎么样。生命就在每天的生活里，一切执念都是虚妄。和身边的人愉快相处，认真安排好每一天的活动，用心去感受每一天的心境，就是生活的意义本身。这其实是我今天最想分享给你们的事情。\n谢谢大家。\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"把手弄脏，去填补认知的缝隙","url":"/2024/07/14/%E6%8A%8A%E6%89%8B%E5%BC%84%E8%84%8F%EF%BC%8C%E5%A1%AB%E8%A1%A5%E7%9F%A5%E8%AF%86%E7%9A%84%E7%BC%9D%E9%9A%99/","content":"过去较长一段时间，一直有这种感觉：对于某个事情或想法，“听到了” 跟 “理解了” 中间有一段距离，“理解了” 跟 “实现了” 中间又有一段距离，“实现了” 跟 “做成了” 中间则会有一段更大的距离。简单来说，就是 “知” 与 “行” 之间存在着巨大的鸿沟，且仅站在 “知” 这一端的人往往是看不到这个差距所在的\n\n最近听到的这期播客 《E35.\n知识的缝隙》，更为系统和深入地探讨了这个现象及其底层原因，也让笔者对上面的 “距离” 有了更深的理解，用播客的话来说，这些距离就是那些认知的缝隙。播客从费曼学习法开始说起，逐步揭开了那些看似光滑的认知弧线下存在的缝隙，而躬身入局、把手弄脏，去填补这些认知的缝隙，也许就会有想不到的收获\n写这篇的文章的过程，亦是笔者在填补自己认知的缝隙的过程，文章可能有点发散，祝开卷有益～\n\n从费曼学习法说起\n费曼学习法,\n可以算是一个耳熟能详的方法了，这是一个以教为学，让输出倒逼输入的方法\n费曼学习法的基本流程是，就是给一个完全不懂某个理论或方法的人（有请小孩和老奶奶出场）去介绍这个理论或方法，然后在这个过程中，你会发现自己也不是非常理解其中的细节，不能用自己的话去把这部分说明白，然后需要回头去调研学习这部分，直至能够清晰地描述清楚；这个过程不断循环往复，也就是你学习的一个过程了\n而其核心的点，就是在给其他人讲述过程中，你就会发现大量此前自己可能只听过但不求甚解的细节，即下图的\ngaps in your understanding，而这，就是本文描述的认知的缝隙\n\n费曼学习法，最大的受益人，其实是讲述的那个人，因为在这个过程中，他填补了过去自己一直忽略的那些认知的缝隙\n对于这一点，笔者在写 blog\n过程中也是深有体会，每当笔者希望写一个感兴趣的话题时，脑子里的会冒出一个个的想法，这些想法就像一个个孤立的\ndots；当我们尝试通过文字和逻辑来 connect the dots\n时，你会发现需要你不断地理清楚需要用哪些线，来把哪些 dots\n给连接起来；那些你 “听过了”，甚至以为 “理解了” 的\ndots，此刻就像一个个犹抱琵琶半遮面的羞涩少女，让你见不到其庐山真面目\n而当你去反复去查漏补缺，去把那层若影若现的面纱揭开时，你找到的那些用来连接的线，恰恰就是在不断地填补你认知的缝隙的，当你完成这个拼接的过程后，就能得到一张更加全面的知识的图谱\n无独有偶，查理芒格的 “猩猩效应”，从另一个角度来阐述这些认知的缝隙是如何被填补的，在《2022 年度伯克希尔哈撒韦致股东的信》中是这么说的\n\n教学和写作一样，帮助我发展和理清了自己的思路。查理称这种现象为猩猩效应：如果你和一只猩猩坐在一起，仔细地向它解释你的一个宝贵想法，你可能会留下一只迷惑不解的灵长类动物，但你自己的思维会更清晰\n\n编程中的 小黄鸭调试法，也是在阐述这个道理，只是在这个过程中，费曼学习法中的小孩和老奶奶、查理芒格的那只猩猩，变成了你桌上的那只一语不发，但是你一旦跟它交流，便能发现自己的认知的缝隙的小黄鸭\n\n许多程序员都有向别人提问及解释编程问题的经历，而对象甚至可能是完全不懂编程的人。而就在解释的过程中，程序员可能就发觉了问题的解决方案。一边阐述代码的意图，一边观察它实际上的行为并做调试，两者间的任何不协调都会变得更明显，使人更容易发现错误所在。\n\n说到这里，又想起了在《中》的那一期，孟岩提到的一个观点：任何道理、方法，都是描述一个人的体验，都是观察世界的一个角度。如果这个角度或者方法足够的深刻、底层或者通用，你一定还能找到其它人的描述（ps，这期播客很棒，推荐一听）\n因此，无论是费曼学习法、猩猩效应，亦或是小黄鸭调试法，我想都是从多个角度观测到了这一点：认知的缝隙无处不在，当我们尝试把那些看似 “理解了” 的知识再说一遍时，也许就能发现这些缝隙所在\n看到那些认知的缝隙\n工作中，老生常谈的一个问题是，深度与广度，是持续精进一个方向，还是扩大自己的知识面，当然这二者并不矛盾，但是在时间有限情况下，深度还是广度，的确是需要做出抉择，否则就真的要成了 “吾生也有涯，而知也无涯。以有涯随无涯，殆已”\n有一种好奇心爆棚的人（包括笔者），不断学习新知识，来喂养那个嗷嗷待哺的装着你的好奇心的婴儿，是一个乐此不疲的过程，因为一直学的都是新知识，且学习曲线\n在学习的初期并不陡峭，只了解个所以然，犹如柯立芝效应那般的新鲜感，是能够支持你废寝忘食去做这个事情的\n但如果只有广度，持续这样停留在对浅层知识仅仅是了解的层面，很容易出现一种情况，即看起来好像什么都能做，但又什么都做不精，或者说上手去做的时候，才发现自己此前选择性地忽略了很多细节。而这，恰恰是那填补些认知的缝隙所需要的，就比如我们都知道把一头大象放进冰箱只需要\n3 步：打开冰箱 -&gt; 把大象放进去 -&gt;\n关上冰箱，但仅仅依靠这些信息，我们真的能够把大象放进冰箱么\n只有广度没有深度，我们很容易有一种 “知识的错觉”，误以为 “听到了”，就是 “理解了”，而 “理解了” 又差不多算 “实现了”；但真的想真的擅长一件事儿，做成一件事，绝不是听到了就可以了，而是需要填补很多很多的空隙\n邓宁克鲁格效应\n提到：你的能力越差，就越意识不到自己的能力有多差；同样的，我们填补的空隙越多，就会知道有更多的空隙我们看不到，引用在格雷厄姆的《How to Do Great\nWork》中的一段话，是这么说的\n\nOnce you've found something you're excessively interested in, the\nnext step is to learn enough about it to get you to one of the frontiers\nof knowledge. Knowledge expands fractally, and from a distance\nits edges look smooth, but once you learn enough to get close to one,\nthey turn out to be full of gaps.\nThe next step is to notice them. This takes some skill, because\nyour brain wants to ignore such gaps in order to make a simpler\nmodel of the world. Many discoveries have come from asking\nquestions about things that everyone else took for granted\n\n随着我们知道的越多，我们不知道的必然也会越多，在 The\nIllustrated Guide to a Ph.D. 有一个形象的描述\n想象一个包含所有人类知识的圆圈。你读完小学的时候，你知道的是其中的一个点（蓝色）。当你高中毕业时，你知道的更多了，是图中绿色的环。有了大学学士学位，你就获得了一个专业，图中红色区域属于你的状态（基础知识和突出的专业）。硕士学位深化了这个专业，深红色冒出的区域。在博士期间，阅读研究论文会把你带到人类知识的边缘（圆周）。一旦你到了边缘，你就会聚焦于边缘上的某点\n\n你在边缘上努力工作了几年，直到有一天，边缘被你突破了，你创造了新的知识）。那个你凸起的鼓包，叫博士学位\n\n当我们懂得越多，代表我们知识的那个圆的面积越变越大时，我们会发现我们对这个世界不了解的地方也会越来越多（圆的周长越来越大）\n在这个过程中，人很容易从慎重的初学者到无意识的无能者，可能圆就不会再变大的；用播客的话来说就是 “因为大脑在我们的脑海中绘制了一条看似光滑的弧线，这条光滑的弧线会欺骗我们已经懂了这件事，但下面其实有很多缝隙”，如果没法突破这一点，我们很可能一辈子就在那个小圆里坐井观天，固步自封了\n在我很喜欢的一期播客《我们都是不明真相的群众》里，方丈提到的几个观点，无论是 “在投资上，发现自己傻 X 比证明自己牛 X 重要一万倍”，还是 “在更长的时间维度面前，再聪明的人也是一个傻瓜”，都在时刻提醒我们去承认自己对绝大多数事情知道得并不多，去注意到那些认知的缝隙（ps，这期播客也很棒，推荐一听）\n因此，我们对待知识，要足够的诚实和耐心，引用《文明、现代化、价值投资与中国》里面一段话是这么说的\n\n知识是慢慢积累起来的，但是你必须一直抱有对知识诚实的态度，这个概念非常重要，因为人很难做到真正的客观理性。人都是感情的动物，我们相信的东西，对我们有利的东西通常就会成为我们的预测。我们总是去预测这个世界对我们很好，而其实客观上我们都明白这个世界不是为你存在和安排的。所以做到对知识的诚实很难，但是非常重要。\n知识是一点一滴积累起来的，当你用正确的方法去做正确的事，你会发现知识的积累和经济的增长是一样的，都是复利的增长。过去所有学到的经验都能够互相印证、互相积累，慢慢的你就开始对某些事情有把握了 (能力圈）\n\n把手弄脏，填补缝隙\n纸上得来终觉浅，绝知此事要躬行，把手弄脏，或许是我们去填补认知的缝隙里绕不过去的一环。把手弄脏，这里的一层含义是要亲自去体会与实践，另一层含义，则是需要我们在这个过程中有足够的耐心，去发现那些细微的差异，那些常常被忽略的认知的缝隙\n播客里举了一个史蒂夫・乔布斯对咨询行业的看法的例子，乔布斯很不喜欢咨询行业，他认为\nown something（承担责任）和 do something（把手弄脏）是完全不同的事情；do\nsomething\n非常重要，但咨询行业不是这样，他们没办法在从建议到实施的所有行动阶段都能清楚看到自己的建议，也没法看到那些根本想象不到的细节。他们无法在这个过程中面对失败，积累经验、学习新东西，更无法自己振作起来，重新出发，继续尝试\n就好像我们去理解香蕉这个水果，如果你只是隔岸观火的话，你得到的仿佛是一副挂在墙上的、二维的香蕉的照片；你可能拥有一整面墙的香蕉、苹果、草莓的照片，向别人去炫耀你懂多少，但那些照片只是二维的，如果你没有亲手把弄，它永远不可能变成三维的；你永远不知道那些你根本不知道的细节，也不知道这些水果的滋味\n我们经常在学习，也经常给出建议，但如果不承担风险和责任，也不参与具体的实施过程，这里有大量的缝隙是我们根本不知道的；比如说我们都知道把一头大象放进冰箱只需要\n3 步：打开冰箱 -&gt; 把大象放进去 -&gt;\n关上冰箱，但是以何种角度放进去能最节省冰箱的空间、怎么能够在下次打开的时候更好把大象拿出来，如果我们不躬身入局、把手弄脏，是无法切身体会到的\n《Rich Dad,\nPoor Dad》有一句话 “intelligence is the ability to make\nfiner distinctions”, 翻译过来就是 “智慧，是区分细微差别（finer\ndistinction）的能力”\nFiner\ndistinction，就是在我们在填补了认知的缝隙后，所获得的一种觉察能力，是我们对一个东西或者一件事情足够专注，观察得足够多面、挖得足够深，\n是我们对一件事情不停地放大、对这条弧线背后的缝隙都看得更清楚之后，所拥有的那种更精细、更微妙、更具体的理解；而当我们对一件事情的了解和体验越多的时候，当我们越能拥有\nfiner distinction\n的时候，我们的大脑就可以将这些东西和其他事物联系在一起。同时我们也可以越来越无意识地去做这些事情，然后有带宽腾出来处理其他事情；达到那种 “无他唯手熟尔” 的境界\n当你对一件事情专注足够长的时间，挖得足够深的时候，你就会在这件事情上发展出极其细微的差别和专业技能，这在《10x is easier than\n2x》 中被称为 unique\nability。简单来说，就是做同样的事情，你得到的结果可能比别人要强很多很多倍，同时你付出的成本，比如说时间、心情等，这些要低很多，也就是我们常说的能力圈了\n无论是 finer distinction，还是 unique\nability，都是需要我们把手弄脏，去填补那些认知的缝隙后，才能获取到的；那具体到方法论，我们又该怎么做？\n首先就是要对知识诚实，需要承认自己可能对绝大多数事情知道的并不多，我们对这个世界的认知的过程，就仿佛在一个巨大的房间里摸索，如盲人摸象般只能看到其中一隅，而承认自己的无知，拥有开放的心态，才会有把手弄脏的意愿；在躬身入局后，不要默认所有东西都是合理的，反之需要不停的追问和思考，就像拿着一个放大镜在看一样，不停发现那些缝隙；当我们循环这个过程足够长的时间后，finer\ndistinction 也许就会自动浮现出来\n《异类》这本书提出了著名的一万小时定律，但我们需要的不是 1 万个小时，而是 1 万次迭代。因为如果 1 万小时都是不停的重复的话，你就不会去看到那么多的空隙，也不会去填满那么多的空隙\n无论是一万小时定律还是一万次迭代，都需要很长的时间，以及很深的专注能力。如果你把这些时间看作是成本，恐怕很难坚持下去，但如果这本身就是你很喜欢的事情，热爱、兴趣和好奇心，就会驱动着你去做足够长的时间，挖得足够深，观察得足够多面，放大得足够多，填满足够多的缝隙；别人把这些时间和付出当作成本，需要 “忍耐”、需要 “熬”、需要 “坚持”，而对你来说，这个过程像是在玩儿，而这也是\nunique ability 产生的必经路径\n在如何培养自己的 unique ability ，格雷厄姆在《How to Do Great\nWork》也给出了一个建议\n\nThe factors in doing great work are factors in the literal,\nmathematical sense, and they are: ability, interest, effort, and\nluck. Luck by definition you can't do anything about, so we can\nignore that. And we can assume effort, if you do in fact want to do\ngreat work. So the problem boils down to ability and interest. Can you\nfind a kind of work where your ability and interest will combine to\nyield an explosion of new ideas?\n\n播客里提到一个概念叫 ikigai，如下图所示\n\n简单来说，它分了四个维度，分别为：\n\nWhat you LOVE：你的热爱\n What you are GOOD AT：你的擅长\n What the world NEEDS：世界需要什么\n What you can be PAID FOR：你的工作\n\n然后这几个圈的交集，分别代表了激情（热爱和擅长的交集）、使命（热爱和世界需要的交集）、专业（擅长和工作的交集）、职业（工作和世界需要的交集）；而如果如果把这四样都集齐了，那就是\nIkigai 的状态\n小结\n零零碎碎说了这么多，最后还是用播客的一段话来总结，因为写的真的是太好了\n\n当我们认识这个世界的时候，那些看似完美的认知弧线的背后有无数的知识的缝隙，对知识缝隙的足够了解变成了\nfiner distinction。那对一件事的 finer distinction 会变成我们的 unique\nability，就是独特的能力，而这个独特的能力将在自由市场上换回相匹配的超额收益。但所有这些都需要漫长的时间和足够的专注，也就是需要足够的长度，足够的深度。\n我们如何才能支付这种成本呢？那就是兴趣、热爱和好奇心（笔者觉得还要加上一个时间），这些将把别人眼中的努力变成我们自己内心的游戏。当然没几个人一开始就知道我们自己喜欢干什么，这基本是件不可能的事儿。所以也许最重要的是开始是我们把手弄脏去和真实的世界接触，同时在这个过程中保持开放，不断给自己的生活增加一些方差，去尝试一些新的东西。那在这个过程中勇敢的追随自己的审美、兴趣和好奇心，慢慢排除选项，同时让自己不断成长\n最终如果你足够幸运的话，就有可能碰到自己的一生所爱。希望那个时候你已经设计好了能够让自己自由选择的环境，我想这整个过程也就是\nikigai\n\n还记得很早之前看到的这个回答，到底经历了什么，才会让一个二十出头的年轻人对生活失去希望？，当时看到高赞的回答似懂非懂，但是在写完这篇文章后，似乎更能理解了\n\n是他的灵魂走在了年龄前面\n如果你没有去亲身实践你所懂得的道理，当这样的道理越多，你就越割裂，就越无法走在人生的道路上，在短暂的年纪里，懂得太多，知道的太多，接触的太多，想要的太多，你害怕的就越多，失望的就越多，感觉无能为力的就越多，继而生活的灰色地带也就越多\n如果感觉思想停滞不前，那就说明一定是思想走得太快了，你需要去等等你的身体，需要你去花费时间在实践上，然后，你的思想才能更进一步。这二者永远是相辅相成的\n\n所以去躬身入局、把手弄脏吧，去填补那些认知的缝隙，去实践那些 “没能让你过好这一生的道理”，去找到属于我们的\nikigai～\n","categories":["闲话几句"],"tags":["闲话几句"]},{"title":"投资这件事 (1)- 认知与心态","url":"/2022/05/04/%E6%8A%95%E8%B5%84%E8%BF%99%E4%BB%B6%E4%BA%8B(1)-%E8%AE%A4%E7%9F%A5%E4%B8%8E%E5%BF%83%E6%80%81/","content":"从年初了解 有知有行\n开始，断断续续看了不少上面的内容：听完了里面投资第一课，E\n大干货合集、投资知识体系里的文章也基本是已读状态，一直处于输入的状态；\n感觉是时候该 connect the dots，形成一个更系统的框架融入自己的知识体系中，\n于是便有了这个系列的文章。\n这个系列的文章绝大部分内容来自于有知有行，也会有一些笔者深究后调研的内容，且按照笔者的理解划分为：认知与心态、概念与常识、买与卖三大模块。妄图将投资这个大话题以及有知有行的编辑们整理的上百篇文章浓缩到这篇小小的笔记中，自然无法面面俱到，所以这篇文章还是会挑选笔者关注的一些内容，更详细的内容可参考有知有行以及本文里的相关引用。\n本文是认知与心态的部分，主要是投资前的心理建设部分，包括对待财富、投资的认知，投资时的心态管理 (收益与风险的预期、投资的时间周期) 等。\n\n关于财富\n财富的意义不用赘述，绝大部分人的绝大部分时间都花在了这件事上，这里主要讲\n2\n部分内容，一是聊聊财富自由这个老生常谈的话题，二是为了达到自由往往需要经历的财富积累环节。\n多少钱才算自由\n财富自由，是一个老生常谈的问题，每个人对这个问题都有自己的答案。对于当前笔者而言，财富自由不意味着能得到什么东西，而是意味着拒绝什么，比如说拒绝一份强度高但是成长性弱的工作、拒绝在生病时因囊中羞涩而得不到治疗的窘境等，不被某些东西约束，才是笔者认为的自由。\n而看着各种把杠杆拉满去买房，因为失去现金流而断供的例子，比如知乎上的年底被车企裁员，房贷还不起怎么办？回答，以及笔者在写这篇博客时，\n各大互联网公司因为裁员而导致断供的各种新闻，笔者就有个疑问：把未来三十年未知的现金流作为赌注去拉满杠杠，来买个落地安身之处，因为房贷而活得小心翼翼，算是真的自由吗？\n另外，从个人角度来看，当前的财富是否能够让其自由，本质上跟这个人的欲望有关，E\n大的微博 《 一个人缺不缺钱与欲望相关》也里讲了类似的观点，“欲望如果太多，无论你拥有多少，你都是缺乏的、焦虑的”\n然而，这个问题的吊诡之处在于，人是欲望驱动的动物，缺少了多巴胺带来的那种冲动，人很容易裹足不前，那该怎么破？这篇文章《要多有钱我们才会感到满足？—— 满足的多世界理论》给出了一个思路，笔者的理解就是通过自我察觉，问清楚自己到底是什么在驱使你的欲望：是你的真我，还是被规训的思想？\n关于这部分，在《金钱的价值究竟是什么》中也给出了类似的观点，同时给了一个衡量标准\n\n很多人觉得财务自由遥不可及，得几千万，还有说几个亿的，其实根本不用那么多。不说什么大富大贵，单单是让自己的\n“睡后收入”\n超过目前的工作收入，让我们有能力选择更适合自己的事业、可以根据意愿选择和谁在一起度过时光，对于生活就已经是巨大的助力。\n财务自由为的不是想要什么就有什么，而是当有东西不想要的时候，你能有底气拒绝。人的欲望永无止境，但是拒绝糟心事，真的没多难。\n\n财富的积累\n获取财富的途径从本质来说就是提供他人需要的服务，或者通俗点说就是卖东西，知乎上的这个回答概括了常见的四种手段：卖信息、卖钱、卖他人的注意力、卖自己的时间。当然，这里说的只是个人的财富积累，如果放到更宏观的范围，财富来源于生产效率的提升，这部分可参考\n《钱是从哪儿来的》\n常说的打工人就是在卖自己的时间，除了这个途径外，对于绝大部分人，比较可行的方法，只能是稍微节约一点，然后拿着一点本金去投资，也就是上面说的卖钱。而卖钱其实也依赖着你的积累下来的本金，所以财富的积累是一个绕不过去的问题\n如何做好财富的积累？《关于财富和消费》和\n《尽早投资，慢慢变富》都给出了一些建议，笔者整理如下\n\n积少成多 ;\n无论你的收入是多少，除非真的特别特别少，每次拿到任何收入，都强制性存起来至少\n20%,\n然后用适当的精力稍微学点理财的知识，选择一些相对安全的方式，把你的钱投出去，股票、基金、债券、余额宝，甚至房地产…… 然后好好工作，继续提升自己，多挣点，然后多存点，多投点……\n\n尽早开始 ;\n一个是复利 (复利其实并没有那么神奇，后面会详细说)，一个是给自己尽早试错的机会，《为什么说投资要趁早》\n\n开源：要么在本行业努力，争取做到前列，拿到更高的回报。要么利用业余时间做一点自己喜欢的副业，增加收入。主业\n+ 副业 +\n金融投资，一个人变成三个人，财富积累自然快。收入越多，结余越多，距离自己的目标就会更近。\n\n节流；不要被消费主义洗脑，消费主义的形式太多了无法一一列举，具体可以参考这个问题下的回答，\n有哪些专门为中产阶级量身定制的消费陷阱？\n\n理性投资。辛苦积累的财富，迅速消失只有四个方式：黄、赌、毒，以及乱投资。\n\n虽然上面的两篇文章强调了财富积累的重要性，但是也强调了存钱、节流这个环节过犹不及\n\n不要存太多。除非你收入特别高，真的花不完。一般的工薪阶层，不要存超过\n40%。我这人最相信「平衡」两个字。生活是用来享受的，金钱是我们的奴隶，不是我们的主人。在先把那部分投资的钱存好后，剩下的就用来好好生活，享受人生。为了投资，牺牲生活，得不偿失。终有一天，你会发现，你的工资与你的投资收益相比，已经微不足道了。\n不要很痛苦地做一件事。比如说很痛苦地坚持攒钱，很痛苦地面对亏损，很痛苦地学习和投资，很痛苦地工作等等。因为持续痛苦地做一件事，大概率不会有太好的结果。也许在你的心中，坚持这件事的未来是非常光明的。但是，也许你的意志并没有你自己想的那么坚强。终有一天，稻草压倒了骆驼，爆发出来的负面情绪可能会让你觉得过去的坚持都是无用、可笑的\n\n另外，\n关于财富的积累，知乎上的这个回答也很好，基本也包含了上面说的开源节流的思想，同时也外延了婚姻、生娃等抉择，强烈推荐阅读，23 岁到\n35 岁该如何实现资产增值？如何不陷入结婚生孩子买房的恶性循环中？ -\n李艾维的回答 - 知乎\n关于投资\n为什么要投资，为了赚钱啊；前面也提到了，搞钱的手段里，卖钱是普通人最可行的途径之一，而《尽早投资，\n慢慢变富》里面给了如下这个说法，其实就是强调了常说的被动收入的重要性\n\n穷这个字，底下是个「力」字。富呢？底下是个「田」字。什么意思呢，意思就是，你朝九晚五，辛苦劳作，每天起得比鸡早，睡得比狗晚，最后大概率还是穷。因为你是在出卖体力，手停口停。\n然而有资产的人呢，有房的收房租；有股票的拿股息，碰上牛市翻几倍；有公司的雇一帮人为自己挣钱…… 当你的资产大到一定程度，你就变成了「富」人，因为你有了「田」，于是你可以恬不知耻不劳而获地成为社会的寄生虫了。\n\n神奇的复利？\n说到投资，往往少不了的一个概念就是复利，几乎所有的投资入门的文章都在强调着复利的重要性，但是复利真的有那么神奇吗？\n在这之前，我们先看下由复利这个概念衍生出来的一个励志公式，即每天进步一点点，一年后的变化是巨大的，1.01^365 = 37.7834，反之则是\n0.99^365 = 0.02551\n这个公式初看时也许能给人打满鸡血，但是仔细思考便会发现其中的 2\n个逻辑漏洞\n\n指数级增长要求每天都在昨天基础上提升 1%,\n这显然是不现实的，加法增长更加符合个人的成长\n\n增长是有饱和区，或者说瓶颈期的，增长到一定阶段大概率会出现疲软阶段，\n增长曲线会是 s 型而不是指数型的\n\n《你看到的神奇复利都是骗人的》里，就针对这个问题做了拆解，我们可以把复利公式拆成如下形式\n复利 = 本金 × (1 + 收益率)^时间\n可以看到，关键因素在本金、收益率和时间，对这三个因素进一步分析，便能得到如下结论\n\n\n足够多的本金很重要。本金的增加并不是理财能带来的，是要你不断努力去从外部获取。现在市面上各种沙雕的理财培训，坏就坏在他们只给你讲复利的重要性，却从来不告诉你，你那点本金，复利上天也没多少。\n时间周期要拉得足够长。“一年里，每天都比前一天进步\n1%“” 这件事情是极不合理的。你可能会说，我每天能比昨天多背 5\n个单词啊？不难啊？抱歉，这是线性叠加，不是复利。而且，不光不是复利，你还会背的越多，忘的越多。365\n次方的确是非常美好的想像，可惜现实生活中并不存在。比较合理的算法，应该是用\n「年」 为单位。这样你会发现，要达到 365\n次方，你大概需要十辈子\n\n(3) 收益率往往被高估。做任何复利增长的曲线的时候，都会假设一个\n10% 或者 15% 这样的增长率。但是，在真实世界中，你不但找不到 20%\n的利率，你也找不到 15% 的利率，你甚至找不到 10% 的利率。\n\n这里并不是要否定复利这个概念，而是需要强调如果你真的相信复利增长曲线，那你就应该接受一个现实：变富是需要慢慢实现的（这部分后面会详细说）。这个慢慢，可能是三年五年、更可能是十年二十年的持之以恒丝毫不敢懈怠。\n管理好预期收益率\n预期很重要，预期过高，带来的是失落，预期较低，带来的也许是意外的小惊喜，这是个公理，不止适用于投资\n前面提到计算复利时收益率往往被高估，那在投资中的预期收益率是多少才合理呢呢？《股票的预期收益率应该是多少？》里从宏观经济出发分析了这个值，先说结论：8%～10%\n的长期年化收益率是可以预期的\n首先是 “长期年化” 这个词，这个把时间拉到以年为维度，身边经常能听到有人赚\n20%、50%，动辄翻倍的也不在少数，这里很有可能是 1 个月或 1\n笔买卖的收益，但均摊到一年，未必就是这么多，甚至可能是负的。\n而从宏观经济角度来说，整个经济体的增长，是每个企业、每个劳动者共同努力不断创造财富的结果，量化的指标就是\nGDP（国内生产总值）。过去 20 年，中国 GDP 年均增速在 9% 左右。如果把 GDP\n增速理解成「所有大大小小的企业增长速度的均值」，那么投资到运营水平更高的企业也会获得一些超额收益。文章里给的经验，8%～10%\n的长期年化收益率是可以预期的。\n8%～10% 的估算逻辑就是这样来的，而如果进一步深挖，文章里使用 ROE (Return\non Equity) 这个指标来做了估算，ROE\n通俗来说就是股东放在公司的每一块钱，能产生多少的回报。所以查理・芒格说过，一只股票的长期回报率，基本会和企业长期实现的\nROE 靠拢，文章里举了一个咖啡店的例子，值得一看。\n文章还提到了长期坚持 “低买高卖”，并且系统性地选到表现比较好的公司，我们就有可能把预期收益率拉高到\n12%～15% 的水平。那 15%\n的长期收益率高不高？其实非常高了。股神巴菲特这么多年的长期收益率，也差不多就是\n12%～15% 的水平\n另外，E 大的文章《请把预期收益率降下来》提到了类似的观点，这里不赘述了，只摘录下面这段对笔者比较有启发的话\n\n别跟别人比赛。不要在熊市中安慰自己，大盘跌了\n60%，我只赔了 55.5%。我跑赢了大盘。那没有任何意义。不要在牛市里懊恼，大盘涨了\n100%，我只赚了\n80%，我真是个 loser。看看新闻，那么多人发了大财，为什么不是我？记住，你要的是绝对收益。要比，你只需要跟自己比 —— 我的资产，有没有比上个月、上个季度、去年增加了？\n\n这段话其实跟张潇雨的播客 《场上没有别人》里的观点很相似，那就是投资可能是每个人一生中最能掌控的事情之一，这个过程不需要排名，不需要跟外部比较，只需要对自己诚实。\n接受慢慢变富\n看了上面的对复利的拆解和预期收益率，我们不得不接受一个事实：那就是变富的过程会很漫长。上面的《你看到的神奇复利都是骗人的》里也提到了：巴菲特的巨额资产，绝大部分都是在他\n50 岁以后赚到的\n\n《接受了慢慢变富，才能越来越有钱》里提到了这个观点\n\n投资和做其他生意相比，优势绝不在于来钱快，投资的优势在于可以一直做下去，而且可以慢慢越做越大。而大部分做生意的人，来钱都比投资快，但很难长久，即使可以做的久，也基本保持在一个规模，没法越做越大。\n所以我们做投资要发扬投资的优点，就是要有耐心慢慢来，把财富越积越多，而速度恰恰是投资的缺点。而很多人完全搞反了，只想快速赚一把就走人，这完全是避开了投资的优点，而发扬了投资的缺点，逆势而行。\n\n想要接收慢慢变富，行动上有几点笔者觉得较为重要：闲钱投资、别加杠杆和停止内耗\n\n闲钱投资\n\n想要有好的心态，一定要用闲钱投资，什么是闲钱？就是短期内亏了也不会影响生活的钱，这一点非常重要，否则心态上很容易崩，也很容易造成主动亏损。这一点在半佛\n2018 的 live 《2018，如何通过投资理财让自己过得更好？》里听到过，时至今日仍然印象\n\n别加杠杆\n\n另一点就是不要加杠杆了，还是上面的那个\nlive，印象深刻的一句话是 “投资里，从 1 块到 100，1 万，1000\n万都是有可能的，但是从 0 到 1 是不可能的”，E 大的文章 《只要本金还在，就有无数机会》也是相同的观点\n\n停止内耗\n\n在播客《孟岩对话张潇雨：在投资这件事儿上就别折磨自己了》里，\n提到了如下观点，也是笔者认为心态上比较重要的一点\n\n我们在生活中不要自己找新的事情折磨自己，给自己找麻烦。什么意思呢？就是我总觉得好多人学投资，是苦大仇深的，是来这翻身来了。周围的人都不理解自己，投资变成了一个出口。努力学习没有问题，但没必要卷，没必要非要在这件事情上证明什么。因为你证明不了什么。而是要在这个过程中去了解自己、舒展自己。在一个没有那么多内耗和磨损的情况下去做这些事情，这个事情才会越做越顺，越做越好。\n同时，在这个过程中学会不去苛责自己，不去过度地评价自己，想那么多有的没的。这个是非常非常重要的。\n\n价值投资？长期投资？\n在投资里，经常能听到的一个词就是价值投资，同时也常伴随着的另一个词是长期投资；那价值投资里的价值指的是什么？多长时间的投资才算长期投资？价值投资是否意味着长期投资？\n如果用 Wikipedia 上对价值投资定义，主要特点就是对于买入价格与公司资产估值之间的差距必须有足够安全边际\n在 E 大比较早之前写的《我所理解的价值投资》，定义也是类似的\n\n价值投资，不必然是长期投资，不能用时间长短来衡量；\n价值投资，不必然是低市盈率投资，不能用市盈率来衡量；\n价值投资，必然是以远低于股票内在价值的价格买入，要学会如何衡量企业真正的内在价值；\n价值投资，必然是以保住本金为前提，以足够的安全边际为度量衡，取得合理的投资回报。\n\n更通俗来说就是低买高卖，但是这个事情很难，难在哪里？文章里给出的答案如下 (但是笔者觉得更难的是比较难判断是当前是贵了还是便宜了，这部分在买与卖会介绍一些参考指标)\n\n就难在一个字：忍。\n贵的时候，忍着不买。不管别人如何冷嘲热讽，坚持内心的标准。只要不够便宜，我就是不买。\n不贵的时候，忍着不卖。抗拒自己所有的人性，拿着自己看好的标的。死死拿着，哪管你大浪滔天？\n\n值得注意的是，价值投资并不是唯一的投资方式，也不是最好的投资方式，或者说没有最好的投资方式。常常被自诩为价值投资者们嗤之以鼻的趋势投资，在\nE 大上面的文章和 《我所理解的趋势投资》中，是这么说的\n\n趋势投资不去管价值，主要任务是判断趋势，趋势延续就继续持有 / 空仓，趋势改变就卖出 / 买入。在趋势投资者的眼中，价格有时候倒是高了更值得买入。比如雪球的 28 策略，你会发现历史收益也会很不错。\n价值和趋势我不能说谁一定好，适合你自己就好。但是，有一点请注意。千万不要某个品种一路上涨你总是狠不下心去追，结果它涨了很多后跌了一点趋势已变的时候你就抱着价值投资的信心接了进去。然后它一路暴跌已经很有价值的时候你又变成了趋势投资者，觉得趋势很差不如卖掉以后走势好了再买\n价值投资者的势，应该是对于一个国家，一个地区，一个公司真正大势的把握。\n\n而说到长期投资，各种投资课程、投资大师，都会强调长期投资、长期持有、长期规划的重要性，多久才算长呢？\n《投资多久才算长期？》针对美股和\nA 股的数据做了回测，给出的大致结论无论是美股还是 A 股，持有 10\n年以上，收益都是正的。\n如果说长期投资能拿到收益，那为什么长期投资还那么难坚持呢？文章给出了\n2 个原因\n(1) 投资回报在时间上的分布不均匀，即主要收益是由 1%\n的交易日贡献的 (所以需要保证 “当闪电劈下来的时候，你最好保证自己在场”)。这导致了不管对于市场，还是对于个股，很可能我们大部分时间都在进行无聊的等待，而真正的上涨（或者下跌），只是在很短的时间内发生的。换句话说，等待相当煎熬，所以很多人就会选择卖出，然后去追新的热点\n(2) 市场总是轮动的，这里的轮动，指的可能是资产表现的轮动，可能是投资风格的轮动，也可能是行业板块的轮动等等，其实也就是周期的一种体现（关于周期在后面的概念与常识中会讲到），这导致了当前市场最受到追捧的那个主题，和你喜欢或者擅长的主题不一致，那么再坚定的人，也会对自己的持仓产生怀疑。\n了解了这些后，文章给了如下建议\n\n长期投资需要的时间。这些因素都是客观的，并不会因为我们希望快速变富而产生变化。唯一能控制的，是投资的本金。\n因此，对我们来说，可能更重要的事情是，放下焦虑、专注在工作和生活上，在我们自己不断变得更好的同时，去收获更多的工资、股权等其它收入。与此同时，做好资产配置，根据自己的兴趣慢慢地拓展能力圈，在合理的范围内提高我们能获得的收益率\n\n而如果针对基金这个投资标的，《基金投资应该持有多长时间？》给了更详细的数据的回测和分析，给出的结论如下\n\n回到「基金应该持有多长时间」这个问题，答案如下：\n（1）我们需要刷新一下「长期持有」的概念，8\n年以上才是真正意义上有稳定绩效表现的「长期投资」，即至少持有 2\n个以上完整的周期。\n（2）如果你还接受不了 8 年以上的周期，那么你可以考虑持有 4～5\n年（回报效率最高），而且，务必做个「战略性择时」，千万别在估值高点买入基金！\n（3）在合适的点位买入，并持有足够多的周期，这是基金投资成功的关键。\n\n了解上面的价值投资和长期投资后，我们知道两者没有必然的联系，很多价值投资者在下跌时自称的长期投资，很多时候都是在自欺欺人而已\n重新认识你自己\n很多时候，我们都认为自己对自己都是非常了解的，但事实上并非如此；2022\n年上半年，股市有了几次大跌，孟岩在 03/17 的写的《恐惧》里面有段话是这样的\n\n在牛市的时候填写调查问卷，大家都觉得下跌 30%\n完全能够承受，但真的到了熊市，即使只跌了 15%，可能你已经非常恐慌了。\n另外，在风险测评问卷上填写下跌 30% 很简单，但你很容易低估真的下跌 30%\n会对你的心理造成什么样的影响。我们可以在脑海中做压力测试，但无法测试当我们投资失败回到家，看到家人失望表情时的心情。\n\n出现这个问题可能原因是多重的，比如说可能仓位控制不合理，杠杆太高等；而其中的一个不可忽略的原因就是我们其实并没有真正认识自己，或者说不清楚极端环境下自己会作出何种表现\n文章的结尾写到：“恐惧是真实的，也是宝贵的。它是一种微妙的身体信号，也是检验我们是否真的相信的试金石。记住那些恐惧的时刻，让恐惧穿过自己”。其实就是建议我们在极端情绪下进行自我察觉，学会体会自己的情绪，与自己的情绪和平共处。E\n大的《聊聊投资中的那些恐惧》里则提到了面对这些恐惧一些更详细的方法\n\n从小仓位开始，训练自己能享受盈利。不要怕回吐利润。坚定一点：只要你买的不贵，最终一定会一路向上。\n面对资本市场的危机，我的建议：\n首先，不要慌。\n第二，冷静地看自己手里的东西。有没有可能死的，有没有特别贵的。如果有，找机会处理掉。\n第三，保命。\n第四，找机会进场捡漏。如果没有能力识别，就不捡，保命。\n第五，坚持不慌。一开始不慌，最惨的时候更不慌。再次审视手里的东西。跌得很惨，但是会不会死。不会就不用慌。尝试捡漏。\n\n另外，关于察觉并自己的情绪，推荐读一下这个回答，怎样进行良好的情绪管理？，里面提到的三步法值得实践一下\n\n回到投资上，《认识你自己，才是投资这件事的终极乐趣》里也提到类似的问题\n\n无论是什么样的投资，要赚钱的路径只有一个：在价格低的时候买入，价格高的时候卖出。有趣的是，大多数散户回过头观看自己的行为，结果常常是反过来：他们在价格高涨时杀入市场，迅速下跌时期匆匆离场。也就是说，他们在高买低卖。\n是恐惧驱动了我们的大部分行为。经历过才知道，真正能做到低买高卖绝非容易。它需要在还没有媒体宣传和大众舆论的时候就有独立思考的精神，需要耐得住寂寞，忍受住波动，需要不执着，拿得起放得下，需要随时准备承认自己错了，需要远见、信心、谦逊、坚持……\n\n另外，文章还提到了一个观点：无论是有限游戏还是无限游戏，属于你的机会，一辈子就那么几次。我所说的机会，是长时间，相对意义上改变结局的大机会。大的机会是时代造就的，不是你自己。那这些机会长啥样呢？文章给了几个例子：\n\n信息不对称，特指找到和自己相关的，熟悉的市场\n\n市场失效，一个新兴市场在刚开始常常是不够有效的\n\n技术、算法和投资理论，但文章指出了 “投资圣杯” 根本不存在，任何投资理论都是暂时的，且没有高低贵贱之分\n\n至此，文章提出说真正的圣杯是对于自我的了解\n\n向内看。在大部分人向外求索的时候，你能明白那不可能成为真正的钥匙，最多只能让大家站在同一起跑线上，而真正的「圣杯」是对自我的了解，那本身就是一种优势\n\n此外，文章里提到的创业的例子，笔者比较有共鸣，因为笔者前一段时间也常常在问自己，为什么懂得很多道理，\n但是仍过不好这一生？原因就是知易行难，懂得的成本太低了，但是真正执行的成本太高了，这个成本意味着长期坚持，意味着要反馈频次低的情况下重复做着同一件事，这需要一种愚。\n\n在创业的那几年，我读过很多书，见过很多人，也曾试图找规律，到底什么样的创业者才能成功？聪明的？学历高的？创业经验丰富的？有人格魅力的？能忽悠的？……\n终于有天我体会到：太聪明的人，并不适合创业。因为他们能在很短的时间内分析出利弊，甚至计算出成与不成的概率。他们能设计出漂亮的商业模式，能说服一流的投资人，能招到很好的团队。可是，他们不够愚痴，不够执拗。\n世界上的聪明人那么多，如果机会是能分析出来的，为什么轮到你？最顶尖的创业者，一定在某种程度，某些方面有一种傻劲儿。他们并非料事如神，占尽先机。他们一定在一段或长或短的时间里，做了所有聪明人都不愿意做的事。他们是所有理性人中的非理性者，把脑子无法算计的东西，甩手交给了心。\n这种傻来源于什么呢？\n某种热爱。某种感性。某种直觉。\n\n回到主题，投资这个过程，也是一个不断自我觉察的过程；金融市场，也是一个认识自我的绝佳修炼场，场上只有你自己，在那些大跌和大涨的日子里，体会自己的恐惧和狂喜，重新认识你自己。\n小结\n本文主要讲了投资前的一些心理建设部分，主要包括对财富、投资的理解，对投资时长、收益率的预期，如何通过投资去重新认识自己等。\n如果说投资是一场游戏，收益是我们的得分，那么在游戏前，做好心理建设和必要的认知，放下焦虑，才能享受这场游戏的美妙，也能从这场游戏中重新认识自我。\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"投资这件事 (2)- 概念与常识","url":"/2022/06/18/%E6%8A%95%E8%B5%84%E8%BF%99%E4%BB%B6%E4%BA%8B(2)-%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%B8%B8%E8%AF%86/","content":"从年初了解 有知有行\n开始，断断续续看了不少上面的内容：听完了里面投资第一课，E\n大干货合集、投资知识体系里的文章也基本是已读状态，一直处于输入的状态；\n感觉是时候该 connect the dots，形成一个更系统的框架融入自己的知识体系中，\n于是便有了这个系列的文章。\n这个系列的文章绝大部分内容来自于有知有行，也会有一些笔者深究后调研的内容，且按照笔者的理解划分为：认知与心态、概念与常识、买与卖三大模块。妄图将投资这个大话题以及有知有行的编辑们整理的上百篇文章浓缩到这篇小小的笔记中，自然无法面面俱到，所以这篇文章还是会挑选笔者关注的一些内容，更详细的内容可参考有知有行以及本文里的相关引用。\n本文是概念与常识的部分，主要是笔者在学习过程中接触到的一些概念性的知识，对于熟悉的人来说，也许是 “常识”，笔者则是在尝试将这些不熟悉的内容变为自己的常识，也希望对你有用。\n第一部分的内容见 《投资这件事 (1)- 认知与心态》\n\n周期\n春夏秋冬，阴晴圆缺，万物皆有周期，市场也存在着周期，这部分主要讲市场周期的表现形式，产生原因以及应对方法。\n牛熊交替与轮动\n市场是有周期，主要变现为股市每天的起起伏伏，或者是各个行业的此起彼伏；最常见的就是牛熊交替，行业轮动与风格轮动\n以牛熊交替为例，《市场皆有周期：牛市特征、熊市特征》里就讲了牛市和熊市的几个阶段\n\n「牛市三阶段」：\n第一阶段，只有少数特别有洞察力的人相信，基本面情况将会好转；\n第二阶段，大多数人都认识到，基本面情况确实好转了；\n第三阶段，每个人都得出结论，基本面情况将会变得更好，而且永远只会更好\n「熊市三阶段」:\n第一阶段，只有少数深谋远虑的投资人才能意识到，尽管形势一片大好，基本面普遍被乐观看涨，但是基本面肯定不会一直顺风顺水；\n第二阶段，大多数人都认识到基本面正在越变越糟；\n第三阶段，每个人都相信基本面只会变得越来越糟。\n\nE 大的文章《什么是熊市，什么是牛市？》里也提到类似的观点，这里就不赘述了，而牛熊交替，意味着市场总是在极度乐观和极度悲观中来回摆动，在《什么是钟摆意识？》中，把这种现象描述如下\n\n证券市场中的情绪波动，就像一个钟摆的运动一样。这个钟摆来回摆动，形成一道弧线，弧线的中心点完美地描述了这个钟摆的「平均」位置。但是事实上，钟摆待在这个弧线的中心点位置的时间极短，一晃而过。相反，钟摆几乎大部分时间都在走极端，弧线两端各有一个极端点，钟摆不是在摆向极端点，就是在摆脱极端点\n\n除了牛熊交替形成的周期，行业轮动也会形成周期，《聊聊 A 股历史上的机构抱团事件》里就提到了由于行业轮动以及机构投资者的考核机制、排名压力等因素，形成了各种「抱团」行为，文章里是这么说的\n\n按照我们的定义，持续加仓并持有一个板块接近至超过\n30%，视为「抱团」。类似的情况发生过四次，分别是：\n(1) 2007Q1-2010Q1 抱团金融，持续 13 个季度\n(2) 2009Q3-2012Q3 第一次消费抱团，持续 13 个季度；\n(3) 2013Q1-2016Q1 抱团信息科技，持续 13 个季度\n(4) 2016Q1 - 当前第二次消费抱团，已经持续了 13 个季度；但如果从\n2017Q1 算正式开始抱团，目前只持续了 10 个季度\n每个板块的业绩推动自有其原因和逻辑，货币超发和信贷放量时金融板块业绩改善并大幅回升的动力；通胀升温是消费板块业绩大幅改善的动力；新技术和并购趋势是信息科技板块业绩大幅改善的动力。\n\n除了行业轮动，风格轮动也是常见的周期表现，《证券市场是轮回，是周期》里提到从规模上可以将风格归为大、小两种\n\n证券市场上，是讲究「风格」的。比如从规模上讲，大、小就是两种风格。\n我们持仓中的\n50、红利（策略指数，可以归入大指数）、300、金融（行业指数，也算大指数）都算是「大股票指数」。\n500 当然不算大指数，但在目前这个市场上，也不能算小指数。毕竟已经有\n3500 只 A 股，第 301～800\n怎么能算「小」呢。中证 1000 勉强算是小指数。我们买的创业板指数其实严格来说也不能算小指数，姑且把它列入。\n\n基于这个维度划分的风格是存在轮动的，表现如下\n\n「风格」是会不断轮换的。虽然这个轮换周期甚至可能长达十年，但一定会轮换。很多人在\n2012 年到 2014\n年，小股票动辄数倍涨幅而大股票纹丝不动的时候，戏称大蓝筹是「大烂臭」，甚至写出万字雄文论证大股票将永远退出 A 股历史舞台。当然，后面发生的事大家都知道了。\n2017\n年以来，蓝筹股走出了一波行情。颇有一些近视的同学，又开始坚定否认小股票的投资价值。「大的就是好的，漂亮\n50 最好，小股票永远没戏了。」\n\n同样地，如果去分析美国的股市，也会发现相同的现象，观察美股历史（因为美股历史较长，数据极其丰富翔实，所以用它举例），你会发现非常明显的大小轮动记录。也就是说，大股票和小股票通常会各领风骚数年，最终两者超长期涨幅不会差太多，甚至小股票会稍微略高一点\n什么导致了周期\n周期是如何产生的？ 《为什么市场会有周期？》里提到，市场周期主要由三个周期影响决定 —— 经济周期、企业盈利周期和情绪周期\n\n经济周期\n\n经济周期是某个国家、或者整个世界在一定时间内的经济产出情况，增长或衰退。一个经济体的长期经济发展情况，通常是由人口出生率、劳动生产率、科技发达程度等等这样的因素决定\n\n企业盈利周期\n\n即使 GDP\n的变化不大，企业盈利水平的波动也可能会很大，造成这种现象的原因有多个，文章里提到了两个原因：经营杠杆和财富杠杆，这里就不张开讲了\n\n情绪周期\n\n市场是参与者行为的总和，而人心的变化，往往会导致市场出现非常极端的变化，而这就是情绪周期。文章认为前面两个周期变化，都比不上情绪周期对市场波动的影响大文章里举了「漂亮 50」的例子，旨在说明人们对这些大蓝筹股的情绪的转变\n至此，文章指出了一次完整的周期如下\n\n市场的气氛和叙事开始变化 &gt;&gt;\n导致投资者开始追捧某一类股票 &gt;&gt;\n随着追捧股价节节走高印证了牛市的故事 &gt;&gt;\n更多的人蜂拥而至并让市场产生这种景气不会消失的错觉 &gt;&gt;\n市场气氛开始掉转 &gt;&gt; 投资者接连出逃股价大幅下降 &gt;&gt;\n公司股价长时间的一蹶不振……\n\n经济形势好的时候，企业家过度盲目地投资再生产，银行等金融机构在经济上升周期的信用宽松（容易贷款）也助长了这种行为，这些在经济好的时候进一步放大了企业的盈利。\n反应到市场周期上 —— 媒体都是好消息，企业盈利屡超预期，一片欣欣向荣。这时候投资者的心理和情绪就接管了现场。「这样的景象一定能持续下去，黄金十年不是梦」，于是股票价格大涨。\n某个时候、某个因素导致企业盈利不及预期，而这时最后的接盘手也已经进场，于是股市开始下跌。经济形势良好时候的经营和财务杠杆，此时变成了企业的包袱，企业的盈利迅速减少甚至亏损。股市进一步下跌，媒体上都是坏消息，投资人认为这样的景象一定会持续下去，经济要完，于是抛售股票。企业裁员、投资者身家缩水，大家纷纷勒紧裤腰带，减少不必要的消费，企业的收入和盈利进一步降低…… 直到开始下一个周期。\n因此，经济周期、企业盈利周期、市场情绪周期，波动逐级放大，但终会收敛回归到长期趋势本身，这个长期趋势，也就是一个国家经济和企业盈利的增长。\n但是这三个周期，以及各种其他周期（比如信贷周期）与因素的叠加，造成了整个市场的起伏波动，而人心在这里的作用尤其之大。\n经济机器是怎么运行的\n上面虽然在探讨周期是如何产生时最后提到了债务周期，但是没有详细说，而\nRay Dalio 的《经济机器是怎样运行的》里面则详细分析了债务周期是如何影响经济周期的，并且从宏观经济的角度，用通俗的语言解释了经济运作原理，值得一看\n关于 Ray Dalio，张潇雨的播客里也介绍过这位大佬，#12：（奇葩的）华尔街大佬与人生导师\nRay Dalio，主要讲了 principle 对其的指导意义\n文章指出世界经济运行主要由三个因素驱动：1. 生产率的提高 2. 短期债务周期\n3. 期债务周期，这三个因素共同组成了一个经济发展的模型\n\n\n下面则是这个经济模型里的一些要素和发展规律\n\n信贷\n\n\n信贷是经济中最重要的组成部分。如果借款人保证偿还债务，而贷款人相信这一承诺，信贷就产生了。任何两个人都可以通过协定，凭空创造出信贷。\n现实生活中大部分所谓的钱，其实都是信贷。美国国内的信贷总额大约为 50\n万亿美元，而货币总额只有大约 3\n万亿美元。信贷可以使收入增长在短期内超过生产率的增长，但在长期内并非如此。信贷如果造成超过偿还能力的过度消费，就是不良信贷。\n\n\n短期债务危机\n\n\n信贷会导致支出的增加，且信贷可以立刻凭空产生；而如果支出和收入的增长速度超过所出售的商品的生产速度，价格就会上涨，我们把价格的上涨称为通货膨胀。\n央行不希望通货膨胀过高，因为这会导致许多问题。央行在看到价格上涨时就会提高利率，随着利率的上升，有能力借钱的人就会减少，同时，现有的债务成本也会上升，每个月信用卡的还款额会增加。\n由于人们减少借债，并且还款额度增长，所以剩下来用于支出的资金将会减少，因此支出速度放慢，而由于一个人的支出是另一个人的收入，环环相扣，人们的收入将下降。由于支出减少，价格就会下跌，我们称之为通货紧缩，经济活动减少，经济便陷入衰退\n如果衰退过于严重，而通货膨胀不再成为问题，央行将降低利率，使经济活动重新加速。随着利率降低，偿债成本下降，借债和支出增加，出现另一次经济扩张\n短期债务周期通常持续 5～8\n年，在几十年里不断重复。但是请注意在每个周期的低谷和高峰后，经济增长和债务都超过前一个周期。因此在长期内，债务增加的速度超过收入，从而形成长期债务周期\n\n\n长期债务危机\n\n\n尽管人们的债务增加，但贷款人会提供更宽松的借贷条件，这是因为大家都以为形势一片大好，因为仅注意最近出现的情况。最近出现的情况是什么呢？收入一直在增加，资产价值不断上升，股票市场欣欣向荣，现在是繁荣时期，用借来的钱购买各类资产，很划算。当人们过度借贷消费时，泡沫便产生了。\n债务与收入比例称为债务负担，只要收入继续上升，债务负担就可以承受。于此同时资产价格迅猛上升，人们大量借钱来购买资产，因为投资促使资产价格日益升高，人们感觉自己很富有，因此即使积累了大量债务，收入和资产价值的上升帮助借款人在长期内保持良好的信用。但是这种情况显然无法永久持续下去。\n债务负担不断增加使偿贷成本越来越高，到了一定的时候，偿贷成本的增加速度超过收入，迫使人们削减支出，由于一个人的支出是另一个人的收入，收入开始减少，而偿贷成本继续增加，导致支出继续减少，周期开始逆转。\n这时便到达了长期债务的顶峰，债务负担变得过重。美国和欧洲在 2008\n年就发生了这一情况。日本在 1989 年和美国在 1929\n年因同样原因发生了这一情况\n\n最后讲的例子，其实就是次贷危机 (2008),\n日本房地产泡沫 (1989) 和美国的大萧条 (1929)；这个时候经济会进入去杠杆化时代\n\n在去杠杆化过程中，人们削减支出，收入下降，信贷消失，资产价格下跌，银行发生挤兑，股票市场暴跌，社会紧张加剧，整个过程开始下滑并形成恶性循环\n在衰退中，可以通过降低利率来刺激借贷。但是在去杠杆化过程中，由于利率已经很低，低至零，从而丧失刺激功能，因此降低利率不起作用。美国国内的利率在\n1930 年代的去杠杆化期间下降到零，在 2008 年也是如此。\n\n为了对去杠杆化，通常采用以下四种办法\n\n削减支出（紧缩）\n\n减少债务（债务违约和重组）\n\n财务再分配\n\n发行货币\n\n这四种办法的的基本原理和因果关系如下\n\n削减支出会导致收入下降，当收入下降速度超过还债的速度，因此债务负担实际上更为沉重。削减支出的办法引起通货紧缩。企业不得不削减成本，这意味着工作机会减少，失业率上升。这导致下一个步骤，即必须减少债务。\n借款人不还钱，存款人会担心银行没钱，于是纷纷取出存款，银行受到挤兑，个人、企业、银行出现债务违约，这种严重的经济收缩，就是萧条。萧条的一个主要特征是人们发现原来属于自己的财富中很大一部分其实并不存在。\n很多贷款人不希望自己的资产消失，同意债务重组。债务重组意味着贷款人得到的还款减少，或偿还期延长，或利率低于当初商定的水平，无论如何，合约被破坏，结果是债务减少，贷款人希望多少收回一些贷款，这强过血本无归。\n在去杠杆化过程中，政府的预算赤字飙升，原因是政府的支出超过税收。政府必须加税或者举债以填补赤字。但是要从哪里拿钱？从富人手中，通过征税把财富从富人那里转到穷人手中。\n因为支出的很大一部分是信贷，但是萧条时期信贷消失，所以人们钱不够花，那么怎么办？央行发行更多货币，\n央行通过用这些货币购买金融资产，帮助推升了资产价格，从而提高了人们的信用，但是这仅仅有助于那些拥有金融资产的人。因此，为了刺激经济，央行和政府必须合作。央行通过购买政府债券，其实是把钱借给政府，使其可以通过刺激计划和失业救济金，来增加购买商品和服务的支出，这增加了人们的收入，也增加了政府的债务，但是这个办法将降低经济中的总债务负担。\n\n文章指出\n去杠杆化是一个把高债务水平变化到低债务水平的过程。为了使经济再次恢复正常，这个通货再膨胀的阶段大约要持续\n7～10 年，因此有失去的 10 年这个说法\n最后，文章给了三条建议，\n其本质都是要不要过渡依赖信贷，而是要依赖生产率的提升 (而不是信贷) 来提升收入\n\n第一，不要让债务的增长速度超过收入。因为债务负担最终会将你压垮；\n第二，不要让收入的增长速度超过生产率。因为这会使你最终失去竞争力；\n第三，尽一切可能提高生产率。因为生产力在长期内起到最关键作用。\n\n个人如何面对周期\n如果说上面是宏观经济的角度去分析周期产生原因以及一些应对措施，那 E\n大下面的几篇文章里讲的就是从个人投资者的角度去看这个周期以及一些应对的手段\n《周期如同春夏秋冬》就从个人投资者的角度，谈了对周期的看法，以及对当时 (2019 年) 所处周期的判断\n\n就像花开有期，每个品种都有它的起伏周期。从周期的角度讲，你应该做的是尽量识别每个大类的春夏秋冬，然后根据大致的天气状态，增减衣服，舒舒服服地活下去。\n这几年，消费、公用事业走得非常好，去年以及今年债券走得不错，今年黄金异军突起。这所有的现象都说明我们处在典型的 —— 宏观经济下行周期。(笔者附，指的是经济不好时股票也不好，债券黄金等资产更受青睐？)\n在这个周期中，受宏观经济影响小的消费、医药（本来医药这两年应该表现非常好，不过行业遭遇新政冲击表现不及预期，但其实已经非常不错）、现金流强的公用事业等板块就成了抱团取暖的避风港。\n接下来呢？\n简单地说，下一个周期将是逐渐走出宏观经济下行的周期，那个周期股票一定表现很好。其中表现最好的一定不会再是消费或者公用事业，而是金融与科技类这些高弹性、有想象力的板块。债券表现不会太差，因为流动性还在。在目前这个周期表现非常好的贵金属会表现相对来说不会依然这么亮眼。\n当你发现上述情况发生，你就应该知道宏观经济已经走入了下一个阶段。不必看太多宏观经济数据，因为只有资本市场才是最敏感最领先的指标。股票先于宏观数据，而债券又会先于股票，这是因为做股票的人非常敏感，而做债券的人更敏感。\n至于下个阶段什么时候到来，我不知道，但我认为，在合适的时候应该慢慢的为下一个阶段做准备\n\n面对牛熊交替，《什么是熊市，什么是牛市？》中指出无法判断熊市或牛市何时回来，你能把握的就是自己账户里的资金、持仓品种，以及自己的心态情绪，\n要买入便宜的东西，分配好资产，踏踏实实等着更便宜或者价值回归。除此，还提到需要耐心\n\n金融投资中的权益类投资，尤其我们这种自诩为「价值投资」的方式，它绝不是这个月\n1%，下个月 1%，今年 12%，明年\n12%…… 权益类投资不是这样的，这样的叫做固定收益，或者\nP2P，或者麦道夫。\n权益类投资的特点就像春夏秋冬。有的时候发大财，有的时候平平无奇，有的时候黯然神伤。\n\n而面对风格的轮动，《证券市场是轮回，是周期》给出如下建议\n\n我给各位的建议，是投资万万不可短视。这个短视的意思，不仅是看不到长久的未来，更是被短期的历史所迷惑。熊市的短视中看一切都是利空，一切都是灰暗再无希望；牛市的短视中一切都是利好，股市永无天花板；大股票表现好就是漂亮\n50 永远涨涨涨；小股票表现好就是成长股一生牛牛牛。\n证券市场是轮回，是周期。大周期套小周期，大轮回里有无数小轮回。千万千万别被最近发生的事所迷惑。\n\n另外，资产配置的重要性，也能通过周期的角度说明，即除了权益类资产，还要配置债券、石油、黄金等资产，因为这些品种的周期与股票不同，而多个相关性很低的品种相叠加，会带给我们被熨平的周期，让我们四季如春，资产不断新高。最常见的就是股债轮动，\n而对于熊市或者说下跌，《我们热切盼望的是下跌》中是这样说的\n\n没有危机是不会出现让你暴富的大底的。这也是为什么会出现大底，以及为什么大底没几个人敢买的原因 —— 因为出大事了。\n行情最差的时候，很多人预期会更差，因为形势看起来真的很差，而且看起来会更差。这个时候，很多之前看多的人会卖出，很多看空的人会继续看空。之前看多坚持不卖的人已经算好，然而他们已经没钱买了，除了股息。\n\n投资标的\n这里谈到的投资标的主要是指金融类资产，常见的基本上可以分为三大类\n(1) 权益类资产：股票，基金等\n(2) 固收类资产：定期，国债，企业债，央行票据，债券基金等\n(3) 现金类资产：现金，银行存款，货币基金等\n权益类资产指的是你拥有这个资产全部或者部分所有权，也享有这部分资产产生的收益；固收类资产一般是债券 + 定期；现金类的则是常见的各种货币基金，这三类资产风险和收益都是从高到低。\n除了上面说的三大类资产，黄金，原油和房地产也是常见的投资标的，但是这里着重讲权益类资产。\n指数与个股\n无论是有知有行团队还是 E 大，都更推崇指数而不是个股，在《指数到底哪里好？》、《为什么我很少谈论个股？》、《指数与股票的区别》、《指数基金的五大优势》几篇文章里，总结了指数的一些优势，笔者将其归纳为估值、风险、收益、成本这四个方面。\n\n估值\n\n指数投资与个股投资最大的不同，是有一个大致可估算的区间\n\n比如之前说的下跌 80%\n原则，就是价格区间。比如钻石坑或者哈迪斯顶，就是价值原则。知道了顶和底，你就可以开心的设计你的策略了。如果你知道它极大概率不会跌破某个价格，你就可以以那个价位为底，设计一整套或者几套长期或者短期交易的策略。\n\n但是股票则是很难预估出高点和低点\n\n股票如何估值？PE？PB？ROIC？现金流折现？内含价值？马化腾差点 100\n万卖掉腾讯，结果因为没人买而作罢这种事我会乱说？小马哥都没法对自己的公司正确估值，一般人……\n我认为股票的低点不是最低点。没有人能预测股票的最低点。买股票最好的时机，我认为是突破布林线中轨的时候。最好是一根大阳线突破，伴随成交量。当然，成交量突破中轨前就要有…… 这很重要。这时候还不要买，等一等。也许股价会回抽，回抽确认中轨有效的时候，全力买进！\n\n\n风险\n\n指数可以认为是 “不会死” 的品种，这是由指数的自动更新迭代的机制决定的，《安全的指数投资》是这么说的：只要交易所不关门，只要基金公司不倒闭，哪怕这只基金清盘了，都没事 —— 基金清盘，扣除一定费用后，你的钱会按照净值还给你。股票退市了你可什么都没有了\n\n指数会有一个自动更新迭代的机制，比如每年，或者每半年，甚至更加频繁地重新编制。大部分指数重新编制成份股时不会大动干戈，通常只改变其中的\n20%，比如对于 50 只成份股的指数来说，每次大概变动就 10 只，先剔除 10\n只，再纳入新的 10\n只。走人的股票是不符合指数编制要求的，而请进的是那些符合的。所以一旦成份股中有公司亏损，或者甚至业绩不佳就会有可能被轰走。在一个经济正常发展的国家里，永续的指数长期就是上涨的。\n\n因此，指数具备优胜劣汰的天然属性，这一点在《沪深 300 十大权重股变化的两点感想》中也提到了，即 2021\n年的沪深 300 已经与十年前完全不同，不是当年的「金融 300」指数了\n\n收益\n\n这部分其实在上面的风险中提到的指数自动更新迭代机制中也提到了，\n\n个人始终认为基金是老百姓最好的投资工具。不管股票基金债券基金还是货币基金。本人的基金金额是股票的\n8 倍。我研究了半天，基金的表现始终还是比我好。\n\n在《为什么我很少谈论个股》中，讨论了为什么牛市中大多数人干不过指数\n\n第一，指数本身就是股票组成的，简单来说，一定有一半股票跑不赢全市场指数。\n如果是权重股行情，那就不用说了，80% 股票跑不过大指数。\n第二，大多数人都是不停的交易。\n你自己看成交量。牛市换手率在 100%-200%\n之间。半年所有股票就能换手一次，你就知道多少人在不断的买进卖出了。一般都是买进强势股，卖出弱势。按照均值回归，可想而知大概率是买了就跌，卖了就涨了。你不换？你不换你手里的就是不涨。服不服？你换了才涨。\n第三，低位是没人买的。\n大多数人是越涨越兴奋。场外观望的也是越高越买。买当然要买强势的，结果就是\ngg。\n\n\n成本\n\n这里主要指个人投入精力的人力成本，包括选股、盯盘等花费的精力\n指数的优点是，对于选股能力不强的朋友（90%\n的散户）来说，可以不用去学习枯燥的会计知识、跟踪瞬息万变的公司变化，只要大致看看历史估值，找个低位买入即可。\n但是股票需要学习更多的概念，还需要做到 “能选出好的股票、能买在低位、能卖在高位、有坚定的信念在赔钱的时候相信它会涨、回本后坚信还能涨”；这里的每一步都涉及到比较多的细节，这里就不赘述了，更详细可参考文档里的内容。\n另外，这篇长文《股票交易背后的残忍真相》里也花了很长篇幅说明个股投资存在的风险，里面还涵盖了很多股票交易的现象，推荐读一下\nA 股指数\n在《常聊的 A\n股指数》，E 大提到了其主要关注的 A\n股的指数 (2021/01)：沪深 300、中证 500、创业、科技、消费、医药；其中前两者属于大盘宽基指数，后三者都可归类为行业指数，这些指数都比较好理解，；而有知有行则专门为红利指数这个关注度没那么高的指数写了三篇文章\n《红利指数系列（一）：深证红利为什么风景独好？》\n《红利指数系列（二）：标普红利的高开低走》\n《红利指数系列（三）：普通却实用的中证红利》\n红利指数是一个根据股票的股息和分红来进行编制的指数，市场上红利指数有很多，即便是主流的红利指数，也分为上证红利、深证红利、中证红利、标普红利这\n4\n个，并且这四个指数虽然在名称中都有「红利」二字，都将自己定位于寻找股息率高、分红稳定的股票，但其实他们四个差别很大，这与\n4\n个指数的编制方式关系较大。文章提到这点笔者比较认可: 很多人有误区，觉得选主动型基金才需要关注基金经理的投资理念和选股逻辑，指数不需要。事实上，指数基金也是同样需要分析，因为指数的编制方法决定了它的选股策略。\n\n深圳红利\n\n红利指数的一般排名规则就是按照股息率从高到低进行排序，但深证红利却是考虑了股票过去三年累计分红金额（绝对值）占整个深市分红总金额的比例，这个比例越高，排名越靠前，跟通俗来说就是它考虑是分红金额这个绝对值，而不是股息率（股息率\n= 每股分红 /\n每股价格），这样的编制方式会对那些每股价格高但是股息率低的股票有优势，即深证红利偏向于大盘股\n选完成分股后，该考虑每只成分股的权重了，也就是指数的加权方式。深圳红利采用的加权方式不是股息率加权，而是自由流通市值加权（其他三个红利指数采用的都是股息率加权）。即哪个股票的自由流通市值大，权重就高。作为一个红利指数，加权的指标却不是红利而是市值\n因此，从上面的编制方法可知，深证红利的红利属性较弱，跟我们常规认为的红利指数有很大的不同。红利只是它筛选的指标之一，除此之外，它还要看日均成交额、经营状况、现金流等数据。文章认为 “深证红利不太像一个红利指数，更偏向于一个基本面指数”\n\n标普红利\n\n标普红利全称是「标普中国 A 股红利机会指数」，其编制方式是建立一个成分股备选池，这个备选池需要满足过去\n3 年盈利增长必须为正、过去 12\n个月的净利润必须为正的要求。然后，所有入选的股票按照年度（过去 12\n个月）股息率排名选出最高的 100\n只股票，按照股息率加权构成指数。在构成指数的过程中，为了分散，规定了每只股票权重不超过\n3%，单个行业不超过 33%。\n但是标普指数公司只给了这些介绍，并不公布历史持仓，导致大家没办法分析，根本不知道这个指数具体怎么选股，而这种编制方式存在几个隐患\n(1) 成分股的股息稳定性不好；标普红利想寻找 A 股市场上具有稳定高分红能力的公司，但事实上它只看过去一年的股息率排名。过去一年的周期太短了，这么短的周期，会选中一些出于特殊原因提高自身股息率的公司\n(2) 周期股占比太高 ;\n标普红利选股逻辑会使得它持有很高比例的周期股，\n容易在经营业绩很好、分红多的时候纳入周期股，但周期过去后，这些个股的股价会大幅下跌。而股价下跌会进一步导致这类产品股息率一直很高，很难从指数中剔除出去，指数就会被这类成分股的股价下跌所拖累。\n(3) 对垃圾股的容忍度高；并没有像其他红利指数那样剔除 ST\n等垃圾股，而是在所有 A 股中进行选择\n(4) 指数样本调整频率高；\n标普红利每半年调整一次样本，调整频率比其他红利指数高，\n本来就是按照过去一年的股息率排名选股，成分股变动已经挺大了，再加上半年调整一次，这就导致成分股变动更大\n\n中证红利\n\n中证红利的成分股是从沪深 A 股中进行选择，不对某一市场和赛道过于依赖。作为对比上证红利是从沪市中选择，深证红利是深市中进行选择。\n虽然标普指数和中证红利都是从整个 A\n股中选择，但是相比于标普指数的编制方式导致成本股变动过大，中证红利在这方面好了很跟标普红利比，中证红利是按照过去两年的股息率进行加权的，考察周期更长。另外，为了避免公司的特别行为，它要求公司过去两年连续分红且每年的税后现金股息率都要大于\n0, 具体的编制方式为\n\n过去两年连续现金分红且每年的税后现金股息率均大于 0,\n按照过去两年的平均税后现金股息率由高到低进行排名，选取排名在前 100\n名的股票\n\n文章在这部分给的建议是中证红利跟其他红利指数比，很普通，但具有市场代表性，如果你想投资红利指数，请选择中证红利。\n海外指数\n投资过程中容易有本地偏好，即只投资自己所处区域对应的金融市场，但是从资产配置的角度来说，把资产合理配置到相关系数很小的投资品种中，才能有效平滑风险，关于资产配置会在后续的买与卖中详细展开\nE 大早在《聊聊海外市场》中就提到，在全球配置的过程中，成熟市场\n+\n新兴市场比较合理，A 股无疑是个新兴市场，那么将 A 股与美国、欧洲甚至香港的资产统一配置。\n在里面提到了投资海外市场的一个工具， QDII (Qualified Domestic\nInstitutional Investor) 基金，简单来说就是海外市场的基金，基本分类跟 A\n股其实也差不多，《关于 QDII 基金的一切\n看这一篇就够了》里讲了 QDII 基金发展的一些历史以及 QDII\n基金的一些常识，推荐读一下\nQDII 基金的种类跟 A\n股的基金差不多，有主动股票型、指数型、混合型、债券型、另类投资型 (包括商品型、房地产信托型、绝对收益型、资产配置型 FOF) 等。而 QDII - 指数型基金，是 QDII 基金中最重要的工具性品种，覆盖了多个重要的海外市场股指，比如恒生指数、标普 500、纳斯达克 100、英国富时 100、德国 DAX、日经 225 等，此外，还覆盖了部分海外医药行业指数，如标普 500 医疗保健等权重、标普全球 1200 医疗保健、纳斯达克生物科技等\n值得注意的是，除了正常的股市波动的风险，QDII 基金还存在 2\n个额外的风险\n(1) 汇率风险 ;\nQDII 基金都是出海投资的，到海外市场投资就得换外币，持有外币资产就必然要面对汇率波动的风险。而汇率有涨有跌，汇率波动带来的影响也有正面、有负面。人民币贬值，对基金净值产生积极影响，人民币升值，对基金净值产生负面影响。\n(2) 外汇限额带来的流动性风险 ; 国家采取外汇配额制，基金公司所获配的外汇额度在一定时期内都是固定的。如果 QDII 基金申购踊跃，导致外汇额度被耗尽，那要想再新增申购，就只能等别人赎回腾出额度才行。比如近来各路资金老想抄海外股市、原油大跌后的底 (2020)，大量资金涌入导致大半 QDII 基金因为没有额度而暂停申购\n对于第二个问题，如果基金本身可上市交易，那在折溢价率合理的前提下，通过二级市场来达成策略计划倒是一种应对方案。\n指数基金与主动型基金\n选择股票基金的时候通常有 2\n种类型：指数基金和主动基金；选择哪一种是个值得思考的问题\n这里的「主动」指的是其选择投资标的，部分或完全由基金管理团队决定，由于基金的信息披露制度是周期性（基金季报）、不完全性（部分股票），非及时性的（季度过去 20 多天公布季报），所以你不可能知道一只主动管理基金过去做什么（部分知道），现在在做什么，未来会做什么。\n因此，孟岩在《如何选择主动型基金？》中提到了选择主动型基金的难度：如果只依靠基金经理过去的业绩去选择基金时，会存在下面\n2 个问题\n(1) 在 07\n年回测时发现一个有趣的现象，选择每年排行靠后的基金，第二年通常可以跑赢大部分同行，这岂不是意味着可以通过买某年排名后几名的基金来战胜市场了？后来发现，造成这个「结果」的「原因」是：由于市场风格切换的原因，在某一年成功「赌」到某个风格的基金经理通常会排名市场前列，大仓位「赌错」的基金经理则会排名靠后。当第二年市场风格切换的时候，排名靠后的基金经理的业绩则又排名前列。\n(2) 由于基金经理的变动比较频繁，并且存在多人管一只基金、离职造成业绩空档等现象，因此通常会采用市场拟合等各种方式去修补一个基金经理的业绩曲线。这样进一步带来了研究的不准确\n而在《指数基金的五大优势》中，就提出了指数基金相对于的\n5 个优点，认为指数基金更适合普通投资者。\n\n\n可预测的投资对象。指数基金过去和现在投资了什么我们一清二楚；而且指数基金将会如何投资，我们也一清二楚；可预测的投资对象会带来一个巨大的优势：可估值。所以大家才会看到指数市盈率，股息率等信息\n\n永续 +\n上涨。指数会有一个自动更新迭代的机制，比如每年，或者每半年，甚至更加频繁地重新编制。一旦成份股中有公司亏损，或者甚至业绩不佳就会有可能被轰走。在一个经济正常发展的国家里，永续的指数长期就是上涨的。\n\n注意力区别。相比选择被动管理基金，那些选择主动管理基金的投资者更容易被短期业绩影响，在买入基金之前的选择标准就喜欢考虑短期业绩\n\n制度风险低。主动基金有可能会出现老鼠仓或利益输送的现象，而指数基金不会\n\n管理费、托管费低。指数基金的管理费率相比主动管理类型的基金就低很多，托管费也低一些。有的指数基金，特别是 ETF 基金会多一个标的指数许可使用费，费率很低。\n\n\n而 E 大在《指数与股票的区别》和\n《指数到底哪里好？》里都提到了更推崇指数基金的原因，原因跟上面的类似\n\n主动基金的问题就在于决定它成绩好坏的环节过多。有时候过去的成绩好你并不知道它的基金经理是运气好还是实力强，你也不知道这个实力强的基金经理明天还会不会继续管理这只基金。总之，一切都是未知。\n我投资指数基金的两个最重要原因：\n第一，指数基金是真正的几乎 100% 「权益类资产」。\n主动基金赋予基金经理的权力太大，可以不断增减仓位。而配置资产这件事我自己完全可以完成，我购买指数基金就是为了配置我资产中「权益类」资产。我买\n10% 的指基，就希望有 10% 的权益类资产；如果主动基金把仓位变成\n80%，我的权益类资产变成 8% 了，我还怎么配置资产？\n第二，指数基金可以估值。\n指数基金发展到今天，类别已经很多。低估、成长、价值、行业…… 我可以通过对每个类别进行估值，并计算业绩增速，根据模型投入不同的资金。而主动基金则完全无法估值，你根本不知道这只基金的基金经理下个月会买卖什么股票。你所有的宝，都压在基金经理和基金公司的身上。\n简单地说，指数基金持有的股票是有「规则」的，主动基金没有。没有规则的东西很难把握。\n长期看，一定有 20%～30%\n的主动基金表现会比指数强，即使你能在指数基金上低买高卖，也一定有\n10%～20% 的主动基金长期表现更好。如果你能够找到这些在未来表现出色的\n10%～20%\n主动基金，那么你就适合投资主动。否则，被动指数投资是很适合你的。\n\n另外，文章还着重提到了 “在股市赔钱，大概率不是因为你买了指数还是主动，而是因为你牛市入场，熊市出场”,\n而相比投资主动还是被动来说，建立你自己的投资体系才是真正重要的事。\n选择基金还需要考虑什么\n前面讲了指数与个股、指数基金与主动性基金的一些区别，且更推崇普通投资者投资指数基金，那在选择时还需要考虑哪些因素。\nE 大在《我选择指数的\n2 个思路》给了 2 个选择指数的参考指标\n\n第一个是绝对跌幅。\n我个人的定义是最高点到最低点绝对跌幅是跌到 20%～32%（跌到\n20%～32%。也就是说下跌\n68%～80%）。任何指数到了这个区域我都会大力布局。\n第二个是连跌时间。\n在 A 股，连跌三年被我看做黄金机会。一个是空间，一个是时间。\n\n而常说的指数往往也分为宽基指数和行业指数，顾名思义，前者投资的是整个大盘，后者则是某个行业，在《如何选择好资产中？》，给出的策略是：通过以沪深 300 和中证 500 为主，其它类似消费、医药、信息、红利等指数基金为辅的方式，形成自己的资产组合。其实就是宽基指数和行业指数相结合的方法，这跟\nE 大在在前面提到的 6\n种指数也是不谋而合的，至于具体的仓位配置，会放到买与卖中讨论。在《为何普通人应投资多行业均衡配置的基金？》里也提到了类似观点，这里就不展开了。\n虽然前面都是在推崇指数基金，但是如果要选择主动基金，孟岩在《如何选择主动型基金？》中还是给了一些参考；这个过程与其说选择主动基金，不如说是在选择主动基金经理\n\n在《聪明的投资者》中，格雷厄姆认为：要想在一生中获得投资的成功，并不需要顶级的智商、超凡的商业头脑或秘密的信息，而是需要一个稳妥的知识体系作为决策的基础，并且有能力控制自己的情绪，使其不会对这种体系造成侵蚀。\n我的公式：知识 x 情绪 x 意愿\n知识\n你需要了解一个基金经理的投资哲学和系统。而看人比看公司，要更难。我们可以根据基金经理的访谈、文章等信息来判断他是否具备完整、自洽、符合投资大道的投资哲学，以及拥有进化这个系统的开放心胸和方式。然后再根据他在历史上的业绩、季报中股票的选择和调整来判断他是否在按照自己的「知」来进行「实践」。\n情绪\n观察一个人和组织，最好的方式是当「极端事件」发生的时候。比如 2018 年 10\n月市场风声鹤唳的时候，基金经理是否能够平和、稳定、甚至发出一些乐观的声音；除了个人的情绪，我们还需要考察基金公司对情绪带来的影响。\n意愿\n「投资系统」也有了，「情绪控制」也能做到，那如何判断你委托、依靠的人能\n100%\n的站在你的角度为你着想和服务呢？我想可能有三种方式：名的绑定、利的绑定、时间\n\n主动基金最关键在于基金经理，但选择主动基金时对及基金本身还是有一些指标可供参考的，在《同一个基金经理，不同的基金，到底选哪个好？》里就讲了一些可参考的指标\n\n基金规模\n\n规模太小的话，会存在清算的风险，对于投资者来说，基金一旦清算，那就会从浮亏变成实亏，规模小于\n5000 万的基金，最好不要碰。\n而规模太大也会有问题，这里的大指的是超过百亿的基金；基金行业的规定，一只基金持有一家上市公司市值不得超过基金资产净值的\n10%；此外，基金持有这只股票的仓位不能超过这家公司股份的\n10%。所以规模太大的话，就要把每只股票的仓位降低，同时也会多配更多的股票，但基金经理可能没有那么多精力去调研那么多家公司，而且市场上好公司本来就不多。\n\n仓位集中程度\n\n仓位集中的股票起伏都比较大，谨慎选择规模大，仓位集中的基金，这类基金的风险会比较大。仓位分散的基金可能在市场震荡或者下跌的时候也分散了风险，业绩更具稳定性。而通过基金的重仓股票，也能初步的判断这只基金的投资风格以及投资的范围\n\n机构持有比例\n\n在基金资料页面我们可以看到持有人结构比例，一般有个人和机构两大类，机构持有者指的是企业、社会团体、事业法人等组织，个人的话就是散户。\n受机构青睐的产品说明是比较优\n质的，因为他们投资的不是自己的钱，而是关系到整个机构和整个团队的，投资前都会比较谨慎，会对基金公司以及基金经理做好充分的调研。所以在比较持有人结构上，文章建议选择机构资金占比多的，因为遇到股灾的时候，相对机构来说，个人投资者可能更容易受情绪影响，会出现频繁申赎的情况，资金频繁的进出会影响到基金的净值波动。\n\n新基金\n\n不建议买新基金，因为新基金的往往在牛市发行的频率较高，容易在高点建仓，同时手续费比较贵，很多都不打折\n非权益类资产\n这里要讲的非权益类资产主要是四大类：债券、原油、黄金、房地产\n在《E 大是如何看待各个大类品种的特性？（上）》和《E 大是如何看待各个大类品种的特性？（下）》中提到其资产配置的种类和对一些资产的看法\nE 大的资产配置主要分为了 A 股、境内债券、成熟市场股票、新兴市场股票、境外债券、原油、黄金、房地产，在且慢上可以看到其具体的配置标的和比例\n同时谈到了对这些资产的一些看法，这里主要摘录非权益类资产部分\n\n债券\n\n债券是可估值资产，比如说现在十年期国债收益率 3.2%，你就知道，它就比\n2% 的时候有投资价值。\n历史十年期国债收益率的波动范围，大致是 2.5%~5%\n之间，可以根据这个配置你的债券资产。配置时需要注意 2 点\n\n信用债与利率债\n\n利率债大多是国债、国开债等国家发行的债券。\n它们更多的与利率水平相关，不用考虑信用问题，因为几乎是刚性兑付的。而信用债的收益率会高，但偿付信用是依据发行债券企业的信用。\n这样你就应该理解，在经济危机中，只有利率债会一枝独秀。因为那几乎是唯一可以确定保本的资产。\n\n债券影响因素\n\n债券的走势几乎只与两点有关：收益率和流动性\n所谓流动性就是货币当局释放的货币多少。\n债券投资的最佳时期，就是当局释放大量货币，然而经济又没有起色的时候。\n这个时候钱多了，但经济不好人们不敢买股票，结果债券价格就会大涨。\n\n黄金\n\n黄金的波动特性是巨幅波动，类似于涨十年，跌十年，然后再涨十年这种\n通过回顾历史，发现每次美联储降息都预示着美国经济将出现一些问题，股市有可能下跌。\n在这种流动性被释放，而经济和股市又出现问题的时候，黄金自然成为很多投资者的选项。\n所以黄金的一个投资时间点： 流动性释放 +\n预期不好，避险功能被激活\n原油和房地产文章里涉及的不多，同时在资产配比里也不高，这里就不详细张开了\n几道计算题\n前面都是讲一些概念性的东西，这里则是从数学的角度去讲一下投资中的几道算术题，通过数字能够对股市中的涨跌更加敏感。在《E 大：投资中的六道数学题（上）》就提到了几个让人在初看时就搞混的题目\n涨 10% ≠ 跌 10%\n在投资中能常听到的一道数学题是\n\n如果你持有的股票下跌 33%，那么需要上涨 50%\n才能回本；如果你持仓的基金下跌 50%，那么上涨 100% 才能打平。\n\n从这道数学题上我们可以总结出一条投资经验：永远不要让你的资产大幅缩水，否则，回到高位，再创新高将是非常困难的\n1.8×0.6 &lt; 1.05×1.05\n以下两种场景中选择一种作为你未来的投资收益率，你会选哪个？\n(1) 第一年赚 80%，第二年赔 40%，第三年赚 80%，第四年赔 40%……\n(2) 每年赚 5%。\n也许很多人在看到第一个选项，直观感觉就是第一年赚 80%，第二年赔\n40%，那两年合计能赚 40%，每年 20% ，当然比 5% 强。\n但是在了解到前面提到的涨跌幅不能直接等价，我们会发现在偶数年，第一种情况的年化收益率是\n4% 左右，还真不如第二种的每年 5%。更极端一些，如果单数年赚 90%，双数年赔\n50%，那么长期收益率居然是负的\n另外，这个也适用于基金经理和散户朋友\n\n各类基金经理有多少吗？至少有上万人。这上万人绝大多数因为没有名气，所以管理的基金规模都不大。当他们在某年赚到很多钱的时候，其实赚的绝对数字很小。比如\n10 个亿规模，赚 100%，规模到了 20 个亿，赚了 10 个亿。\n这样他在牛市成为冠军基金经理，名气大增，申购量暴增 5 倍、10\n倍。于是，他管理了 100 亿资金。好的，熊市来了，这时候当他赔\n40%，那么两年合计，到底赚了多少钱呢？\n亏损 40 亿 - 盈利 10 亿 = 消失 30 亿。\n然而，他第一年赚 100%，第二年赔\n40%，其实两年合计收益率还是正的。但是，30 亿就这么消失了。\n实际上，当他的规模到了 100 亿以后，只要下跌 10%，之前牛市赚到的\n100% 利润就已经消失殆尽了。\n这个道理，也同样适合散户朋友。当你用一点点小钱在牛市中赚了幅度很大的钱，然后开始信心十足的转移储蓄加码买入的时候，很有可能是在消灭财富的前夜。\n\n这也是 E\n大一直在文章里强调的 “熊市不赔钱或者极少亏损，牛市大致跟上指数” 的原因。如果几轮市场周期你都做到了，你赚到的钱是那些大起大落的人永远也赚不到的。\n80 - 50 ≠ 30\nE 大有一个 7080 定律，即一个不会死的品种极限跌幅在\n80%。这就引出了一个问题，假如在腰斩，也就是下跌 50%\n的地方买入，到了跌幅 80% 的地方，你的亏损幅度是多少？\n答案并非你认为的 80% - 50% = 30%，而是\n60%；计算也很简单，假如原来是 10 块，跌到 5\n块的时候买入，则跌到 2 块时，对于你而言，亏损是 (5-2)/5 = 60%\n同理，你在同样品种已经下跌 60% 的地方买入，到了下跌 80%\n的时候并非浮亏 20%，而是 50%。在已经下跌了 70% 的地方买入，到了下跌 80%\n的时候并非浮亏 10%，而是 33.33%。\n这里说明了一个道理，那就是从 0 到 -10% 的距离，与 -70% 到\n-80%\n的距离完全不同。这也是为什么熊市越到后来越惨烈的原因，那个时候财富的缩水远超一般人的承受能力，即使账面只变动了几个百分点。在熊市的末期，大多数一路抄底的人都会彻底崩溃。因为他们的损失只比站在山顶上的人小一点而已\n另外，这道数学题也说明了一个道理，不要轻易抄底或者说接飞刀，如果基线跌幅在\n80%，那你抄在了 60% 上，可能的跌幅还有 50%，而 E 大的文章里是这么说的\n\n对那些非常狂热急于抄底的朋友说几句。\n没错，在股市上确实有所谓的「别人贪婪我恐惧，别人恐惧我贪婪」这句话。但实际上这是在很极端的时候才好用。在大多数情况下，市场是聪明的，市场大多数时候不是傻子。如果你认为市场大多数时间是傻子，那你才是傻子。\n而且，你永远逆潮流而动，最终的结果就是被蜂拥而至的群众们踩在脚下。所以，还是要对市场和群众保持敬畏\n\n概率与赔率\n在投资中，我们通常关注的是收益率，而在《概率之外，多数人不会的赔率思维》中，把这个收益率更细地拆成了概率和赔率，文章是这么说的\n\n投资是基于概率和赔率的游戏。\n所谓概率，指的是一个事件发生的可能性有多大；所谓赔率，指的是这件事发生后盈利或亏损的程度。好的投资是两个因素共同作用的。\n用一个简单的公式来表达：投资收益 = 概率 x 赔率。\n如果只注重概率，当遇到极端事件时可能会赔光；如果只注重赔率，那我们可能会浪费掉很多还不错的「积小胜为大胜」的投资机会。\n\n文章给了一个博彩业的如何利用赔率和概率，来达到坐庄永不亏损的目的\n\n世界杯决赛，德国对英格兰。赌球玩家们开始下注了：买德国队赢的资金合计有\n120 万，买英格兰队赢的合计有 80 万。\n如果你是庄家，你需要操控这场比赛来获利吗？你需要准确预测结果来获利吗？答案是：不需要。\n赌局里有个因素叫「赔率」。赔率为 1.2，表示玩家投入 100 元，赌对了会拿到\n120 元（包含 100 元下注本金）。\n假设德国队赢的赔率是 X，根据上面的资金分布，德国队赢时庄家要支付的资金是\n120 x X 万，赔给买德国队赢的玩家。\n只需要保证 120 x X &lt; 120 + 80 ，即 X &lt;\n1.67，你就一定不会亏钱。\n同样地，英格兰队赢的赔率是 Y，只需要保证 80 x Y &lt; 120 + 80 ，即 Y\n&lt; 2.5 ，你就不会亏钱。\n当然咯，你还可以设定一点「安全边际」。把德国队赢的赔率设为\n1.50，英国队赢的赔率设为 2.25：\n则有如下结果\n德国队赢，你有 120 + 80 - 120 x 1.50 = 20 万利润；\n英国队赢，你有 120 + 80 - 80 x 2.25 = 20 万利润；\n决赛没有平局，无论哪边赢，你都能锁定 20 万。\n\n文章还提到了长期资本 (LTCM) 忽视了概率小，但是赔率高的情况而导致了最终消失的例子。长期资本的投资策略是单次获利收益不高，但是获利概率很大，因此通过\nLTCM\n加杠杆的办法扩大收益。但最终因为俄罗斯债务危机，走向了破产清算的末路。\n宏观常识\n虽然在之前的《张潇雨的个人投资课 (1)- 市场规律》提到宏观迷信这个问题，即宏观经济在投资中所起作用并非那么大，不需要整天研究宏观经济。但是在学习了一段时间后，发现了 “股票的长期增长来源于经济的增长”，不禁对之前的结论产生了一些怀疑。\n后来在 E 大的《宏观经济跟股票走势有关系吗？》中看到了比较好的分析和比喻\n\n人们通常关心哪些被认为与股市有关的宏观数据？我想无非是这几个：GDP、CPI\n和 PMI（欧美很重要的就业数据在中国完全无用）。\n我仔细比较了这几个数据与 A 股走势的关系，非常惊讶地发现：宏观数据与股票走势几乎没有任何关系 —— 除了\nPMI ，这个数据与 A 股关联度不错\n为什么宏观经济数据与股市走势如此让人摸不着头脑？看起来毫无关联性？\n在我看来，宏观经济是一个人，而股市是一条狗。他们就像主人牵着狗出去遛弯，有时候狗会跑得很快，有时候狗又会停下来对自己感兴趣的东西闻来闻去。而此时，主人的速度虽然有快有慢，但他跟狗不会是同样的快慢，所以他们之间会时远时近。\n1996 年、2006 年、2007\n年，就是这条狗发现了自己感兴趣的异性，疯狂地试图挣脱主人手里的绳子往前跑；而到了\n2005 年、2008\n年，它又碰上了打狗队，害怕地躲在主人身后很远很远。这样，有趣的问题就出现了，如果你试图用主人的行走速度去判断狗应该在哪里，就是一件非常可笑的事。\n其实，整件事中你最应该关注的不是主人的速度，而是一样真正有意义的东西，那就是 —— 那根绳子。你知道主人在哪儿，你就知道狗即使落后或超出，它的极限会在哪儿。这根绳子才是真正有价值的东西。\n这根绳子是什么？估值。\n当然，还有很重要的东西决定狗的跑动，比如吸引狗的骨头（投资者的预期）、狗的精神状态（货币量）。\n\n在《是什么决定了股市的涨跌》提到的例子也能用来说明这一点\n\n当你用很短的时间，透支了很长时间成长的时候，你就需要用更长的时间来还债。2001\n年股市 PE 到了 68 倍，之后 5\n年虽然中国经济稳步增长，指数照样跌了一半就是这个道理\n\n所以，前面提到的结论并没错，但宏观经济与股市的关系绝非简单的线性关系。用宏观经济指导股市投资恐怕越错越远。世界上绝大多数正常发展的国家，股市都会一路向上。而我们要做的不要再去盯着上证指数是\n2200 点还是 3200\n点了 (宏观经济与实际投资回报关系不大是是一个原因，而上证指数本身的编制方式是另一个原因)。想办法找到那个万年长青的企业，或者找到一个组合完善的组合才是重要的。\n那为什么这里还要提宏观经济呢？因为了解宏光经济一些相关的知识，能然我们对钱的流动有更好的认知，会让我们更好判断出当前的 “人” 在哪，速度是快是慢等。\n银行体系\n这里主要介绍现代银行体系，了解这些会对新闻里常说的央行加息、货币紧缩等词汇背后的含义更加清楚\n在《我们真的了解钱吗？》详细讲了货币的演变历史，很有意思，这里摘录其中关于的关于商业银行的部分，也推荐看对应的视频\n银行分为两种，商业银行和中央银行，的区别如下\n\n商业银行就是咱们普通老百姓平时存钱，办理贷款业务的银行。一般都是股份制，有机构，或者私人参与，不完全由国家控股，所以它的利益诉求，是为了盈利，也就是说赚钱和发展是它们的首要目的。主要业务就提供储蓄和贷款 ; 主要的盈利方式，就是吸收居民的存款，然后给个人或者企业放贷，然后赚中间的息差\n中央银行，是一个国家公共机构，\n目的是，调控好国家的宏观经济，监管金融市场，保证国家金融制度良好的运行。如中国的中央银行全称叫做【中国人民银行】，常听到的【央行】或者【央妈】就是指它；美国的中央银行，全称叫做【美国联邦储备系统】，也就是常听到的【美联储】\n\n中国的商业银行，其中靠前的六家大型商业银行，工商、农业、中国、建设、交通、邮储，属于国家控股的国有商业银行，所以它们除了追求自身盈利之外，同时也承担了配合落实国家货币政策的职责。而往下一级的商业银行比如我们比较常见的\n中信、招商、华夏、平安\n等等并非完全国有，所以侧重点更偏向于自身盈利和发展。\n美国的商业银行大部分都是机构持股，政府持股的比例相对较少所以美国的商业银行基本都是以自身盈利赚钱发展为主要目的，所以立场方面非常的清晰。\n央行的主要职能包括\n\n印钞，发行货币；比较好理解不展开\n\n作为政府的银行；代理国库的收支，以及代理政府债券的发行，为政府提供贷款，保管管理国家的黄金以及外汇储备等\n\n监管其他商业银行、制定调整货币政策等\n\n与其他商业银行进行存、贷、拆借等业务\n\n这里着重讲最后一点，即央行和其他商业银行的业务往来，主要有以下三种\n(1) 为商业银行做最后贷款人\n即商业银行的钱如果都贷款出去了，这个时候如果有很多储户来取钱，商业银行里的钱不够的话，银行的信任就会出现问题，这个时候央行就会贷款给商业银行，帮助兜底。\n(2) 处理全国商业银行的票据清算\n大致的意思是每一家商业银行在央行都有自己的独立账户，我们平时日常的个人或者企业跨行转账的时候，收付的票据以及资金清算这些数据最终都会传导到央行统一处理\n(3) 集中收取存放所有商业银行的存款准备金\n商业银行每收到一笔存款，都要把这笔存款拿出一小部分放在央行，而放在央行的这部分钱就叫做存款准备金，这个比例就是存款准备金率，\n控制这个准备金率能控制商业银行创造出来的货币量，或者说市场上流通的货币量，文章举了下面的一个例子\n\n假如现在商业银行里一分钱就都没有，而这个时候你存了 100 元现金在银行里，你的银行的账户里就有了 100 元存款。假设存款准备金率是 10%\n那么按照这个比例，你现在存的这 100 元，商业银行就要放 10\n块钱在央行，那么剩下的 90 元商业银行就可以拿去放贷。\n假如这 90\n元贷给了你的朋友，你的朋友拿到钱以后，用这笔钱去服装店买了一件衣服，\n而服装店老板赚到这笔钱以后，又会把这 90\n元，存进商业银行。于是现在服装店老板的银行账户里，也有了 90 元\n那么现在商业银行里的存款总数，就变成了你的 100 元加上服装店老板的 90\n元，总共是 190 元\n现在把这个例子继续下去，商业银行收到了服装店老板的 90\n元存款以后，又会拿出 90 元的 10%，也就是 9 块钱放在央行，那么它又有了 81\n块钱可以放贷，而这 81\n块钱贷出去之后，又会被人存回商业银行，然后商业银行又会继续放贷。就这样循环到最后你会发现最初你存进去的 100 元，经过商业的银行不断的放贷操作，最终扩张 10 倍，变成了\n1000 元\n这个倍数就叫做货币乘数 ;\n而这 1000 元里面，只有你最开始存进去的那 100 元才是真正的纸币，所以这 100 元，我们称之为 “基础货币”,\n而剩下的那 900 元，就只是个数字而已，并没有对应的纸币，这种没有纸币对应的钱，我们称之为 “派生货币”\n而这最多就只能扩张 10 倍，是由于存款准备金率是 10% 决定的，这个率越低，能扩张的倍数就越大（笔者附，这里的计算基于无穷级数\n\\(\\sum_{n=0}^{\\infty} x^{n} =\n\\frac{1}{1-x}\\)）\n目前我国基础货币 + 派生货币总量一共是 233 万亿左右，\n而这其中基础货币的数量又占多少呢？答案是：只占了很小一部分，大概七分之一不到；就是说咱们社会上流通的钱大部分其实都是派生货币，只是个数字而已，并没有对应的纸币。这也是为什么银行最怕挤兑了\n\n利率\n这里的利率，直观理解就是常说的利息，或者获得资金的成本；如存款利率是银行向存款人获取资金的成本，贷款利率则是贷款人向放贷人（一般为银行及金融机构）获取资金的成本；在宏观经济中，利率常常会作为一个调控经济的手段，一般来说，利率上升时，各方获取资金的成本上升，会抑制积极性，而利率降低时，获取资金的积极性提高，投资扩大，刺激经济。\n常说的 降准与降息\n中的降息，指的就是降低利率，两者的更详细的区别和含义如下\n\n降准：降低存款准备金率，这个概念在上面有一个详细的例子说明；降准会让商业银行有更多可放贷的钱\n\n降息，就是降低存款基准利率或者贷款基准利率，投资者存款收益就变少了，于是会更愿意将钱取出来用于投资，例如购买股票、房子。相应的，企业和个人的贷款成本也会降低，从而使更多的人愿意贷款去做投资\n\n降准是放钱出来，增加了市场资金量，而降息并没有增加市场资金，只是改变人们的资金投向，鼓励人们更多的去消费和投资\n\n降息比降准影响更大，市场对利率也更加敏感；市场货币量的变化，影响主要是对金融机构；而价格的调节，则是直接影响整个市场\n\n在这个视频里《【干货】关于利率，你需要知道的那些事儿》，则更详细地把利率拆成了无风险利率，风险溢价和银行的油水三大部分；并指出央妈主要调控的是无风险利率（十年期国债收益率）这部分，并且指出了利率与通胀、债券、股市等关系，这里直接讲结论，\n1. 利率与通货膨胀：长期低利率容易导致通货膨胀，这时候中央银行往往会提高利率\n2. 利率与债券：长期利率是拿债券价格反算出来的，\n两者成反比关系，所以利率涨了，债券价格会掉 (具体公式可以参考视频)，这是比较明确的关系\n3. 利率与股票：加息初期对股市是利空的，但是长期没有非常明确正相关或负相关；这里举了一个地球与月球的例子很精彩\n4. 利率与房地产 / 黄金，关系都是不确定的\n而在《E 大是如何对多类品种、信息保持有效关注的》中，也提到了类似观点\n\n利率会传到到各个大类资产上。 首先会影响债券。\n利率上升，债券价格会下跌。 利率下降，债券价格会上涨。\n然后是股票，然后是商品。\n利率又与通货膨胀紧密联系。 通常情况下，恶性通胀会导致利率上涨。\n这样就尽量少一些持有债券和股票。多持有金银等贵金属。\n缓慢的通胀，利率水平较低，则可以多持有一些债券和股票。\n通货紧缩可以多持有债券、少持有商品和股票。\n\n通货膨胀\n通货膨胀是一个耳熟能详的概念，大概意思就是钱不值钱了，但是当我们要更深入的去理解通胀，你会发现它比我们所看见的其实要更加复杂。因为在实际的过程中，钱这个东西是流动的，只有当钱流到流到某一个市场的时候，这个市场的东西才会涨价\n比如流到了汽车市场，汽车就会涨价，流到的生活用品市场生活用品就会涨价。所以在真实的情况中，每一个市场的通胀都是不一样的\n常见的衡量通胀的指标是 CPI(Consumer\nPrice Index)，但是在 《你为什么总是那么穷》\n和 《我们真的了解钱么》指出用\nM2 - GDP 评估更为合理，M2 可以简单理解为全社会所有的钱，\n结合上面的银行体系概念，M2 = 全社会所有的基础货币 + 派生货币\nCPI 的数据是准确的，我们平时买的猪肉，大米，以及生活用品，确实只涨了这么多。但是，它只能反映出一部分，因为各国的\nCPI\n往往没有把金融资产这一项计算在内，而现实中最大的通胀往往就发生在这个领域\n文章举了如下例子\n\n08 年到现在十几年的时间里，标普 500 指数从一千多点，一直上涨到了现在的四千多点，翻了三倍多，而这背后很大的一部分原因就是\n\"通胀\"\n去年到今年仅仅一年的时间，\n比特币的价格从不到一万美元，暴涨到现在的五万多美元，这其中有部分原因也是因为通胀。\n因为去年疫情到现在美国疯狂印钱超发了大量的货币，而这些钱又大量流入金融资产导致了金融资产价格上涨。所以无论是股市，比特币，这些金融资产的上涨其实都是一种通胀现象。\n而这种金融资产的通胀，在我国的载体则是房子。从 08 年到现在我国的房地产价的大幅上涨，这背后很大一部分原因，其实也是因为通胀。超发的货币大量流入房地产行业循环，所以造成房地产价格大幅的上涨。\n\n那为什么我国的通胀会发生在房地产呢？\n上面的银行体系中提到了货币是如何增发的，在总体 M2\n中派生货币占了很大一部分，而能派生出来的原因银行的放贷，或者说信贷扩张是货币增发的源头\n那我们最大的信贷源头，是哪里呢？那就是房地产了，过去这么多年贷款最多就是房企，和居民购房贷款，比如我们去看 08 年以前，你会发现贷款买房的人是很少的，但 08 年信贷政策大力开放之后，大家都开始贷款买房，这种大规模的信贷扩张，是支撑各个地方这些年的房价上涨的重要因素之一。\n《我们真的了解钱么》是这么描述这个现象的\n\n房价不断上涨，房地产行业发展旺盛，房企又会不断贷款加大投入盖房子而由于房地产关联的上下游行业非常之多，所以房地产行业发展旺盛的会间接带动很多上下游相关企业向银行贷款的需求\n居民、房企、上下游无数行业，这些需求加在一起，就产生了大规模的信贷扩张，也就超发了大量的货币。\n所以房地产其实本质上就是一台巨大的印钞机。在正常情况下，这台巨大的印钞机超发出来的货币是可以传导到其他消费市场的。打个比方：\n房企发展的好，不断建房子，房地产上下游关联的这些行业从业者，以及持有多套房产的群体收入也会增加，收入增加，家庭消费支出也会随之增加，钱就会传导到其他消费市场，结果应该是整个市场一起发生通胀。应该表现如下图\n\n但真实的情况是，大部分人赚到钱之后，又继续拿这些钱去买房子了，这样就形成了一个循环，因为房价不断上涨，所以大部分人又都把钱拿去买房子，大量资金不断涌入楼市，显示中的货币变成了这个样子\n\n\n那为什么美国的资产通胀主要发生在股市呢？文章也提到了一个区别是在于货币的传导机制不同\n\n我国主要是通过信贷的扩张的方式增发货币，增发出来的钱大多数只是数字。而美国增发货币，那真的就是实打实的印钱\n美国的商业银行并非国有，所以它们的诉求是自身盈利为主。所以当市场出现风险时，商业银行为自己的安全不会大量发放贷款。所以美国在很多时候无法像我们一样通过信贷扩张去增发货币。\n但是钱总得往外放出去啊。那怎么办呢？既然商业银行不愿意帮忙，那就是央行亲自下场买东西把钱放出去。美国金融市场上有大量的美国国债，房产抵押债券，这样的债券资产，而美联储就会直接印钱，然后用这些印出来的钱，直接在金融市场上大量购买美国的国债，和其他债券资产。这样钱就进入了金融市场，造成了金融资产的不断上涨。\n08 年美国金融危机股市暴跌，美国政府就是用这样的方式把美股拉了回来；去年疫情美股暴跌，熔断了 4 次，\n美国用的也是同样的套路直接印钱，向金融市场直接注入大量基础货币。才把美股暂时拉回来了。不过美国这样疯狂的印钱迟早是会出问题的，今年美国的消费市场通胀就非常厉害，物价大幅上涨，这都是疯狂印钱的后果\n与我国的房地产一样的逻辑，本来钱是可以通过金融市场传导到其他市场的。但是因为大量货币输入，股市不断上涨，又会导致资本不断回流到金融市场，大量的资金在金融市场内循环，膨胀，于是就有了我们看到美国股市超过 10 年的大牛市\n\n看完中美不同的通货膨胀，我们能够得到这个结论：通胀一直存在，但是通胀的存在是不均衡的，并且这种不均衡在现代金融体系的催化下会愈演愈烈。在我国，这种现象表现为大家房价涨的太快，所以自己买不起，其实背后更底层的逻辑是：很多人在拿并没有怎么通胀的工资，去买已经大幅通胀的房子，当然会觉得很困难；在美国也是如此，持有金融资产的群体资产大幅的增长，而没有金融资产的人群收入则长期低迷；\n所以需要充分认识到：钱就像水，当钱流到哪里最多，在哪个领域循环，相对应哪里就会出现更多的投资机会，但是当下房地产的大趋势已经过去，当房子开始逐渐失去投资价值，金融属性开始慢慢减弱，那么分化的拐点就已经不远了。到了那个时候，大量的资金自然会去寻求回报更高的资产领域，文章指出未来长期投资大方向会看好 A 股的发展\n在《你是跑赢 CPI\n的穷人吗？如何正确理解通货膨胀》，则对持有何种金融资产来应对通胀提出如下观点：在短期内\n（持有 1\n年）股票、债券和短期国债都很难有效地对冲通货膨胀风险。但是从长期来看\n（持有 30\n年)），股票的实际收益率几乎不受通胀影响，而债券收益率被远远抛在后面\n小结\n本文主要讲了投资中的一些概念与常识，包括周期的表现、成因和应对方法，常见投资标的分析，几道容易犯错的计算题以及一些宏观常识，内容不是非常成体系 (因为都摘录了笔者不是非常熟悉的部分～)，也无法立刻教会你如何去投资，甚至看完后可能隔几天就忘了；但是为笔者理解这个世界提供了更丰富的角度，套用那句 “投资是认知的变现”，这部分就是在拓展认知，帮助我们从更多视角去了解整个社会的财富分配以及运作机制，即便无法立马变现，了解这些这些底层逻辑，也是挺有意思的，而随着个人财富的积累，相信这些知识也有能 “变现” 的那一天。\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"投资这件事 (3)- 买与卖","url":"/2022/07/02/%E6%8A%95%E8%B5%84%E8%BF%99%E4%BB%B6%E4%BA%8B(3)-%E4%B9%B0%E4%B8%8E%E5%8D%96/","content":"从年初了解 有知有行\n开始，断断续续看了不少上面的内容：听完了里面投资第一课，E\n大干货合集、投资知识体系里的文章也基本是已读状态，一直处于输入的状态；\n感觉是时候该 connect the dots，形成一个更系统的框架融入自己的知识体系中，\n于是便有了这个系列的文章\n这个系列的文章绝大部分内容来自于有知有行，也会有一些笔者深究后调研的内容，且按照笔者的理解划分为：认知与心态、概念与常识、买与卖三大模块。妄图将投资这个大话题以及有知有行的编辑们整理的上百篇文章浓缩到这篇小小的笔记中，自然无法面面俱到，所以这篇文章还是会挑选笔者关注的一些内容，更详细的内容可参考有知有行以及本文里的相关引用。\n本文是买与卖部分，也是实操部分；投资赚取的收益本质上就四个字：低买高卖，虽然只有四个字，但是如果要做好这个事情非常难，本文试图为这个没有标准答案的问题寻找一些可参考的解。\n这个系列前面两部分的内容可参考\n\n《投资这件事 (1)- 认知与心态》\n\n《投资这件事 (2)- 概念与常识》\n\n\n如果将 “低买高卖” 进一步拆解，我们能够进一步拆成以下几个直观问题\n\n买什么\n\n什么时候买，买多少\n\n什么时候卖，卖多少\n\n而这几个问题又涉及到实操中常说的资产配置、仓位控制、估值等实操部分，下面会详细介绍这几部分，并在最后尝试回答上面几个问题。\n资产配置\n资产配置本质上就是在做分配，即把我们需要投资的金额分配到各种投资标的上，具体的投资标的在上一篇文章《投资这件事 (2)- 概念与常识》中已经提到了，基本就是：股票 (A 股、美股、港股)、债券 (利率债、信用债)、黄金、大宗商品、现金等；每个大类里还可以细分，比如 A 股可以分成大盘小盘，再加上消费医药这样的长牛指数，美股可以分大盘小盘，美国医药美国消费，美国房地产。在\nE 大其主导的长赢计划中，把其投资的标的细分如下\n\n上图从内开始第二圈的类别，分别是：A 股、货币、债券、海外新兴市场股票、海外成熟市场股票、商品。\n第三圈更加细分，在以上大类中，再次细分为：A 股价值股、A 股大盘股、A 股中小盘股、A 股行业股、国内债券、海外债券、港股、海外互联、欧洲、美国、原油、黄金。\n这就是资产配置，关键在于区分大类，配置比例\n资产配置的一个重点是资产相关性，即需要将资产分配到相关性低的品种\n在《详解家庭资产配置》中，有一张资产相关性的参考图，表里的数字表示资产之间的相关性，相关系数的计算方法比较多，笔者猜测这里是使用了皮尔逊相关系数，取值在\n-1 和 1 之间，表示从负相关到正相关，绝对值越大，相关性越强\n\n文章的的观点是把资金分散到相关性很低的品种中，这样当某个品种表现不佳时，另一个品种可以将我们的收益率保持在不差的水平，其实就是风险分散\n文章里以上证 50 和中证 500 的相关性作为例子，说明了要同时持有大小盘这\n2 种类型的资产\n\n在某些时段，大、小盘股票会表现出不同力度的上涨，这种上涨是很难预期的。所以，最佳策略是同时持有大、小盘，动态平衡。谁涨得多就卖一些，买入涨得少的。在熊市初期与中期，尽量多持有大盘股指数。跌幅会比小盘股指数小很多。在估值回归到正常情况下，开始加大力度加仓小盘股。未来这部分投资的弹性会远大于大盘股。\n\n那 A 股的大、小盘具体有哪些选择？《相关性与资产配置（4）》里指出，大的选\n50 和 300，小的选 500 和创业。另外，还提到了行业指数是否需要考虑\n\n我个人认为，做为宽基指数的补充，行业指数非常有必要配置。尤其是一些长期必然走牛的行业。这些行业的持有，应该比宽基指数更加坚定。不到出奇高估的时候，绝不卖出，宁愿做过山车也要持有。当然，可以用我们的策略平滑收益。比如目标市值。\n当然，行业指数配置的比例要低于宽基。可以考虑的是医药、消费大类下的细分指数，或者干脆就是医药和消费指数。信息指数与创业板指数走势相关性\n0.99，确实没有必要重复配置。\n另外像养老、环保这样非基础性行业，偏向概念性的指数，也可以在特定时间段配置\n\n这样，未来路线图已经清晰：50、300、500、创业、医药、消费。当然，你如果就选其中的几个也没有任何问题，比如你觉得就买\n50、500，或者 300、创业，都没有问题；\n关键在于，一定要大小搭配\n上面是 16 年的结论，而在 21 年，《详解家庭资产配置》针对资产相关性的结论进化成了：相关性分析更多的应该是作用于大类资产配置，如股（国内外）、债、金、油等。具体到每一个小类品种配置，相关性分析的意义则小了很多。因为 A 股市场很多时候是齐涨共跌，只是每个品种涨跌幅度不同，这样就无法由相关性分析来体现差异性\n另外，上面讲的基本上都是长期投资中的资产配置，而在有知有行中，则把这个范围拓展得更大，即把一个人的所有资产分为了四笔钱：长期投资、活钱广利、稳健理财和保险保障；也是其投资体系中的四大模块，本系列文章讲的基本都是长期投资的内容，其他部分内容可参考\n投资知识体系\n仓位控制\n上面提到了要配置何种资产，紧接着的问题自然就是要配置多少，而这就涉及到尤为重要的仓位配置，这部分会介绍仓位配置的一些基本思路和原则，具体的仓位配置方法也会简单介绍，但是这里的方法也只是一个大的指导。\n涨跌都要舒服\n在《仓位控制的艺术》提到，仓位是个性化的，没有一个仓位配置能适合所有人，因为每个人对风险和收益的要求不同。\n而投资，首先心里要舒服。因为 “你不舒服，你慌乱，你兴奋或者痛苦，就很容易犯大错”，文章给出的实践经验如下\n\n首先，你要问自己几个问题：如果我手上的股票或者指数，跌了 40%，我会不会痛苦？如果我手上的股票或者指数，涨了\n40%，我会不会难过？\n你的股票跌了，你会痛苦，就说明你的仓位重了；反之，你的股票涨了，你痛苦，说明你仓位太轻。\n好了，现在遵照这两个问题答案，把你的仓位调整到最平衡的位置：也就是说，涨\n40% 和跌 40% 的时候，都不是那么难过。这时候，就合适了。\n\n在《你凭什么在股市赚钱》中提到的 “涨跌都舒服”,\n是同样的观点，\n同时也指出 “很多朋友以为自己舒服了，结果市场跌一跌才发现并不了解自己。我早就跟你说过了，你很可能没有你自己想象的这么坚强。即使现在不难受，随着市场进一步下跌，你才有机会认识真正的自己”，这部分在系列文章的第一篇 - 认知与心态。里面讲的重新认识你自己是同样观点\n满仓与空仓\n在《聊聊动态再平衡》和《贪字的二重定义》都提到，满仓或空仓都不是非常理智的行为\n\n对于一个心智相对成熟的、愿意长期投资的投资人来说，「满仓」、「空仓」都不是应有的状态。不论你的本钱是\n10 万，50 万，100 万还是 5000\n万。满仓意味着你极度看好后市，空仓意味着你认为下跌不远。基于人类对于预测的没有天分，这样\nshow hand 的赌博虽然可能赢，但无疑长期来看破产是不可避免。\n\n关于空仓，则提到有 2\n个误区，一是不要空仓等大底，因为没人能预测绝对的大底；另一个则是在极度高估的时候也要保留一些仓位；笔者对这里的第二点存疑，既然都知道是极度高估了，为什么还要留仓位，笔者没亲身经历过大牛市，但是看\nE 大的文章在 2015\n的狂热时候是都清仓了的，可能这里想表达的意思是总体狂热的市场里也有一些被低估的资产，可以持有这些资产？\n\n空仓等大底，也是一种贪。只要是贪，就会有风险。「踏空」在\n72 倍 PE 不可怕，如果有些板块十几倍 PE 还不敢买哪怕\n1%，那么真的就有点风险了\n估值过高的时候大量卖出，但至少留 15%\n仓位永不离场。朋友们，放心，长期看，指数会一路上扬。不信你看看深综指的走势。这\n15%\n不会让你赔钱。估值回到历史均值附近，开始少量买入。回到大底区域，重仓买入。其他时间，用各种方法保本，赚钱，等待大牛市。通常我的经验是，这样做一轮熊市到最低点总资产减少幅度不会超过\n15%。而牛市资产则会增加 N 倍。\n\n足够的低成本仓位\n前面虽然提到投资赚取收益的关键在于低买高卖，但是而影响收益除了低买高卖，另一个很关键的因素，那就是具体买多少，在《建立足够的低成本仓位》提到了这一点\n\n对于大多数普通人来说，进行金融投资活动的重点，我个人认为，应该是「建立足够的低成本仓位」。这里面有两个重点。\n第一个是「足够的」。仓位不够，说什么都白搭。想要实质性地在金融投资中得到足够的回报，仓位一定要够。我本人除房产外，大概\n90%～93% 的可投资资产都在除现金等价物外的金融投资中。大概 70%～75%\n的资产都在 ETF\n计划中。这十几年的投资经历告诉我，足够的仓位才能在上涨中赚到足够的钱。\n第二个是「低成本」。我对控制持仓成本有着异乎寻常的执着，哪怕有时候会错失一些机会，也在所不惜。低成本持仓带来的好处显而易见：除了会让你赚得更多，也会使你的心态非常平和。在上涨赚钱的时候更容易拿得住 —— 大多数人赚不到钱的原因，就是某个品种一套十年，好不容易回本后马上卖出，再买入另一个套十年的品种。如果你的成本低，你就会更容易安安静静地等着你的持仓利润不断增长，让利润飞奔。\n\n分配策略\n终于到了具体的分配策略，看了几篇文章总结下来，基本思路就是先分配各大类的比例，然后根据每个大类当前的估值，分配在这个类中应投入的仓位\n在《没人能替你做资产配置，除了你自己》和《细说资产配置》中，就提到了该为每个大类配置的仓位比例\n\n在你的终极配置中，单个品种配置不要低于\n5%。低于这个数字对你的资产组合没有意义。然而也尽量不要高于\n20%，最多不能高于\n30%（如果你特别钟爱 A 股，可以把大、小股票分为单个品种，也就是不要多于\n60% 配置）。\n在大类投资组合中，仓位在 5% ~ 10%\n以内，属于偏小，这样的仓位即使上涨较多，对总收益贡献也不会很大。而如果你的单一品种仓位超过\n30%，则一定是过高，万一有个风吹草动，会对你的组合造成非常大的影响。那么总体来说，建议持仓品种的仓位在\n15% ~ 20% 之间，大类品种不超过 6 ~ 7 个。\n\n上面的策略是给每个大类分配比例，在分配好大类的比例后，还需要考虑的是每个大类当前的仓位控制：即将分配给当前大类的\n15% ~ 20%\n的现金，具体该投入多少到大类对应的标的中？答案就是估值\n在《如何避免底部「吃不饱」？》，给了一个比较简易的仓位配置方法\n\n根据指数基金历史百分位，进行仓位控制。 如果目前在历史 50%\n的地方，则将仓位控制在 50%。 如果目前是历史 10%，那么给 90%\n仓位。 如果现在是历史 90%，那么就给 10%。 以此类推。\n这是中性投资者的仓位，激进与保守，可以分别 ±10%。\n\n而在《我们任何的买入，都要以估值为基础》则更加细化这个方法\n\n仓位的确定是看全市场估值。要综合全市场 PE、PB 和加权 PE\n五年、十年数据。对于计划来说，大的原则是：100% -\n现处百分位，得出应该配置多少仓位。这是适合一般投资者的。\n比如说，现在全市场估值是 75%，那么 100 - 75 = 25，则一般投资者应配置\n25% 仓位。对于计划来说，用得是更加保守的一种算法：在现处百分位还在 50%\n以上的时候，配置仓位减半。\n那么，上面的公式就变成：(100 - 75) / 2 = 13%。也就是在历史 75%\n的时候，配置 13% 仓位。当现处百分位回到 50% 附近，则迅速将仓位补足到接近\n50% 的地方。比如可以补到 40%～45%。随着估值继续下降，则完全按照 100% -\n现处百分位的仓位配置。到了 35%\n以下，则将这个数字艺术性地放大。比如在历史 30% 的时候，应该配置 70%\n仓位，但可以在这个时候加大投入，配置 75% 乃至 80%。也就是说，在\n100% -\n现处百分位这个公式的基础上，估值高的时候保守配置，估值低的时候激进配置。\n\n再平衡\n在平衡在之前的文章《张潇雨的个人投资课 (3)- 投资组合构建》中也提到了，在《设计适合你自己的资产配置组合》提出了相同的方法论，并且提供了更多细节\n\n具体方法：用动态平衡的方式，操作优质行业 +\n优质增强指基（标准指基亦可，大小平衡，如 300 + 500）+ 优质主动基金。\n具体操作步骤如下：\n首先，确定你的调仓周期。最低不要低于三个月，最高不要超过一年。原因我不解释，我建议经验少的朋友可以六个月调整一次。\n其次，确定你的最高、最低仓位。我建议除非特别激进的朋友，股票仓位最好不要超过 80%-85%。如果你只是个普通投资者，那么\n5:5 或者 6:4 都很适合。\n第三，确定你的品种。我的建议是增强或标准宽基指数基金（50、300、500、1000、创业、科创）与优质行业基金结合。有能力可以再去找几个优质主动基金加入配置。这里要注意，这只是我的建议，如果你学有余力，这部分的配置甚至可以延伸到个股上。买了好股票赚钱，同时可以有打新收益。只要是股票类资产，都可以算作仓位。只是我强烈建议\n70% 的普通投资者就不要买个股了。\n第四，考虑目前合适的仓位。这个可以考虑估值，也可以参考我经常会给出的仓位建议。\n第五，定期实施你的计划。\n\n在《假如没有下一轮，自己该如何参考长赢进行投资？》，提到类似观点，而且提到了更极致的动态比例的仓位配置方法，但同时风险也会更高\n\n固定比例配置\n根据自己的风险承受能力，确定各大类资产的基础配置比例。风险承受能力低，就多配置利率债。承受能力高，就多配置权益类资产，然后再平衡。\n再平衡从两个角度考虑：空间、时间。时间好理解，不要太频繁，也不要太久。我看\n3\n个月到半年比较合适。空间意味着你要给每个品种设置涨跌空间。比如，你给 A 股设置\n50% 配比，结果它几个月涨了 50%，占比已经超过\n60%。这时候你就要根据空间原则，将它的持仓占比降下来，卖出一些。\n\n\n动态比例配置\n等你有了经验，就可以尝试动态配比。根据每类资产的便宜程度，一定范围内给它不同的占比。比如如你所说，A 股估值极低的时候，你给\n70% 仓位，极高的时候给\n30%。如果你有能力，这样长期看效果会更好一点。但是要注意，这个对于投资人要求极高。没有经验没有能力，很可能弄巧成拙。所以要慎重。\n\n估值体系\n仓位控制中提到每一大类里的仓位控制依赖这个大类当前的估值，那如何估值是一个核心问题，这部分给出了一些参考方法\n识别顶部与底部\n在《如何避免底部「吃不饱」？》中，提到了怎么识别顶部和底部区域，给出了一些可参考的标准，指数估值，以及估值历史百分位就是其中一个方法。这个方法是基于历史数据以及 “合理” 的股市数据，综合考虑一个市场以及一个指数处于合理，又或者过冷还是过热的情况。\n这个方法极具指导意义，但不能只靠这个判断，因为还要考虑投资者情绪以及政策变化，包括货币政策、财政政策以及专门针对市场的政策。而\nE 大在判断 2015 的顶部和 2018 的底部时其实都依赖了这个估值指标，即 PE 和\nPB\n在《回顾 2015\n年的哈迪斯之顶》是这么说的\n\n中国股市自 1995 年以来，有过三次 PE 超过 60\n的尖峰时刻，分别是 2001 年、2007 年和 2015 年\n其中，尤以 2015 年风云变幻、诡异莫测。\n6 月 15 日，两市全市场 PE 超过 72\n倍，远远将历史最高抛在身后。随后，暴雨倾盆，泥沙俱下。短短的三周时间内，PE\n居然已经跌到了历史均值附近！\n要知道，2001 年，阴跌整整 2 年，才在 2003 年跌破 PE\n均值，进入投资区域；2007 年 10 月创出最高后，在 2008 年 5 月才跌破 PE\n均值，进入投资区域\nA 股整体 PE 60 倍，整体 PB 5.5\n倍以上，被我定义为「哈迪斯之顶」\n\n而在《2018\n年钻石底发车解说》里，是这么说的\n\n今天数据没出来，但如果尾盘半个小时没有猫腻，那么今天 A 股全市场\nPB 应该会降至 2.5\n以下了。这是一个标志性的事件，意味着进入真正意义上的低估区域。\n根据我的评价体系，A 股整体即将由「黄金区域」进入「钻石区域」。除非我的评价体系失效了，否则各位投资的 A 股即将进入一个极其难得的大底区域。\n这个区域从 2005 年至今，出现过四次。大致是 2005、2008、2012 和现在\n2018。十三年四次，十年三次，其实真的并不容易，各位且买且珍惜。\n但是请注意，「区域」并不意味着不会再跌。区域也是有空间的。\nPB 从 2 跌到 1.5，不考虑业绩增长，也还有 25%\n的空间，所以做好思想准备。\n有同学问，为什么不等 PB 到 1.5 再买？这么想的同学，2012 年将错过 2008\n年以后，近十年来唯一的超级大底。当时 PB 也只是到 2\n而已。三年后，创业板涨了 590%\n「钻石底」是一个底部区域，不存在预测某个点是大底的情况。预测某个点是大底这件事，不是傻子就是骗子才会干\n\n在《A\n股的历史估值》中，也提到了判断顶部和底部的一些参考指标\n\n一个指数的估值是否进入过去十年最高 10%\n区域，具有非常非常重要的意义。通常情况下，某个指数进入过去十年最高\n10% 区域，就是本人清仓区域。\n从全市场加权估值（非市值加权，外部综合经济数据加权）来看，A 股历史出现过\n4 次大顶，分别是\n1996、1997、2000、2007、2015。这三次大顶的高度几乎完全一致，超过\n45 的区域\n大底则不完全相同。2005 年之前大底只是 30。2005\n年之后的大底全部在 20 左右或以下。分别是 2005、2008、2012。2008\n比较特殊，杀的太厉害，居然杀到了 15 左右。\n\n这几篇没有给出全市 PE、PB 数据获取的参考地址，但是乐咕乐股上分别提供了沪市和深市的数据，东方财富网上也提供了近两年的市场估值走势情况，可以参考一下。\n上面的例子虽然提供了顶部和底部的参考数据，但不是说当前的\nPE、PB\n的绝对值一定要比历史大底低才可进场，因为绝对的底部是无法预测的；《贪字的二重定义》里是这么说的\n\n用我们跟踪超过 10 年的 A 股全市场 PE 来看，2008 年这个数字到过 17\n倍。如果在 2012 年你还希望 17 倍入场，那么到现在你都一分钱也赚不到 ——2012\n年大底，全市场 PE 值是 25 倍。\n同时，随着无风险利率的变化，我们不能用刻舟求剑的方法，去推导「大底」的估值应该是多少\n\n而在《如果现在买还可能再跌，为什么不等估值更低时买？》，也提到了类似的观点，笔者理解本质就是绝对的底的点难以预估，因为受到了太多因素的影响，只能大概率买在不贵的地方；另外，文章还提到一点笔者觉得比较值得借鉴：在进行一笔投资之前，我更愿意想到的是最多会赔多少，而不是赚多少。其实很多时候，困难之所以会成为困难，正是因为你没有做好准备。如果你做好了准备，那么困难来临的时候，通常会变成机遇。\n回到 “避免底部吃不饱的” 这个问题，通过数据、情绪、政策导向判断底部后，进行仓位布局是，还需要注意节奏问题，即 “到了底部区域，你的节奏一定不能再继续慢慢买。\n因为金融市场，极度高估和极度低估都非常罕见，一旦出现，必须大力买入”。而在《投资要讲求节奏》则更详细描述了控制节奏的一些方法，\n简单来说就是到了很贵或很便宜的时候要加速卖出 / 买入，否则投资节奏可以放缓\n\n交易节奏的速度。在上涨趋势良好的时候，你的卖出节奏要放慢。不着急，慢慢卖。留出空间 —— 也就是说，第二份卖出和第一份之间要有空间。\n买入当然也是同理。在趋势向下的时候，第二份买入也要和第一份拉开空间。\n那么，什么时候该加速买入？就是当你发现目标已经便宜到与你手上的仓位完全不匹配，要尽快建立仓位的时候，比如去年底我们做的。同理，当价格贵到手中仓位已经明显太大的时候，就不要再慢慢卖出。\n\nPE、PB 的缺陷\n那 PE/PB 有没有缺陷？在《判断估值的方法》提到了一个点，即\nPE、PB 不一定适用于个股，因为存在发展和周期的问题\n文章没有详细展开讲，但是知乎上这个回答针对这两点举了比较详细的例子\n\n企业发展其实有不同阶段，有创业期，成长期，繁荣期，衰退期。不同阶段，企业的估值可能会有比较大的变化。一般而言，创业期与成长期，企业快速发展，在这个时候也是容易给予高估值的时候。很简单，企业营业收入从无到有，基数低，增速相对就快，所以我们看到为什么创业板股给予高估就是这样的道理。有时候，PE 高有其逻辑。如果你不能接受一定的高 PE，事实上你也就排除了成长股的个股了。这类股确实是爆发力猛，十倍股往往从这其中出现，但大家也清楚，十倍股不是那么好抓的，高收益潜力的背后，其实是高风险，这对个人的能力要求极高。\n繁荣期，则对应的是企业的发展成熟阶段，一般慢慢发展也成了行业的龙头企业了。盈利稳定的情况下，市盈率可能会回落到 20 左右\n很多新手投资者容易拿当前的估值比较低，就觉得他是好票。事实上并不是，在这里要讲一个价值陷阱的问题。价值陷阱特别容易出现在周期股中。很多上游原材料个股，因为原材料的涨价，业绩连续一年甚至持续多年上涨，所以股价也飙的老高，市盈率看起来可能只有个位数，很多人就认为估值低，可以买。但事实上，行业的反应可能往往令人大失所望，在这里列一个逻辑：原材料持续上涨 —— 有利可图 —— 很多人会去扩产 —— 等到扩产产能出来之后 —— 产品供大于求 —— 价格开始出现下降 —— 恶性竞争持续 —— 企业利润出现下滑，利润下滑之后 —— 市盈率一算就跟之前不一样了 —— 可能变成市盈率大。对于周期股，我自己的心得是在低市盈率时卖出，在高市盈率买入，就是对于周期股来说，和我们平时的认知不同，他们不是估值越低越好，可能估值存在滞后性。\n\n但是，对于所有股票这个大群体来说，整体估值就非常重要，因为不可能所有的股票都高成长，也不是所有的股票都具备周期特征。如果一个市场上绝大多数股票都很贵，那么这个市场大多数人就很难赚钱。\nPE、PB 不适用于个股，但是适用于市场的这个观点，在其他文章中《别做盛宴之后买单的那个》也提到了，而个股的估值在《最直观的估值方法》中有提到，这里就不展开了\n一些工具\n这里介绍一些能看各类指数或者总体大盘估值情况的工具\n\n蛋卷基金上的指数排行\n\n有知有行的温度计\n\n且慢的每日估值\n\n乐咕乐股的全市指数估值\n\n....\n\n定投策略\n定投在各个理财课程里都被塑造成一个个美丽的神话：每月省下一笔小钱做定投，三、五、十年以后突然发现变成了一大笔财富。\n这个传说是如此美好：不需要学习高深的投资知识，不需要盯盘，只要每月省下一点钱，不知不觉中就能赚上大钱，以至于不少人对其趋之若鹜。\n但结果真的如此吗？结论是不一定，下面就从几方面说明定投可能存在的一些陷阱\n定投不用择时？\n经常听到的一个说法是定投可以随时开启，不用择时，但是《什么时候开启定投合适》告诉我们开启的定投的时间很重要，文章模拟了四种情况下定投的收益，这四种情况分别是先跌后涨、先涨后跌、震荡上涨，震荡下跌，\n这四种情况模拟的定投结果如下\n从收益的角度来说，定投最适合的行情是先下跌后上涨，其次是震荡下跌。因为这两种情况都能很好的利用定投平摊成本的特点，借助市场下跌摊低成本，等待行情回归时迅速回本并赚取收益；而最糟糕的情况则是先上涨后下跌，\n回到标题里的问题，定投也要择时，但这里的择时不是要精确踩准一个时间点，而是选择一个大概率是底的地方开启，结合市场牛熊轮回的周期特性，熊市时更适合开启定投。\n另外，《别纠结》指出日定投、周定投、月定投实际对收益影响不大，不需要在这部分上考虑过多，还提到了过度交易这个现象\n\n为什么很多平台最后还是推出了「周定投」，甚至「日定投」呢？这主要是为了满足很多人心理上的需求 —— 经常操作一下账户，似乎这样可以提高收益。\n行为金融学里，把人们这种喜欢频繁交易的现象称作：过度交易。而根据研究，多数时候，频繁的交易并不会带来更高的收益，反而增加了交易成本。\n\n降低定投成本\n定投最大的特点是平摊成本 —— 只要我们能把成本摊得越低，就越有助于我们提高定投的收益。这就是市场上常见的一些定投策略的基础思路：进一步摊低定投成本。\n在《定投成本，还能更低吗？》中，提到了降低定投成本的三种方法：均线偏离法、移动平均成本法和估值法。原理都是类似的：借助某种指标，判断目前市场的高低情况，在低位多投入，高位少投入；最大区别则在于所选取的市场高低判断指标不同。\n这几种方法的细节对比这里不展开，文章最终推荐的是估值定投法，即基于\nPE、PB\n等估值指标进行定投，低估值时投入多份，高估值时少投入或不投入。具体的估值可以看前面估值体系里提供的一些工具\n同时文章通过数据分析指出，合理卖出才是提高定投收益的更大杀器，那该在何时卖出？\n何时卖出？\n看到这个问题，基本上大家都会不假思索地回答在最高点卖出，但要识别牛市顶端肯定是很难的，我们也很难期望卖在牛市的顶端。\n在《买得好不如卖得好？》中，提了三种定投卖出的基础策略：目标收益率法、目标估值法、最大回撤法\n这几种方法的细节对比这里不展开，文章推荐的方法是最大回撤法，因为前两种方法都存在着无法绝对估准的问题，因为每一轮牛市的涨幅都不同且不可预测，难以设置绝对的止盈目标和估值\n最大回撤法会设置一个可容忍的最大回撤幅度，当目标指数下跌超过该幅度，就卖出所有资产，结束定投。文章举了天天基金的慧定投的例子\n\n它先设置了一个目标收益率，但牛市究竟能上涨多少，这是不能测算的，为了让投资者能享受牛市更多的收益，慧定投在达到\n20% 的目标收益后，如果市场继续上涨则继续持有，一旦出现回撤，回撤幅度达到\n10%，就迅速卖出全部资产。\n对比前两种方法，这种方法在该时间区间内，获得的累计收益率最高。原因是它充分利用了牛市过程中情绪性上涨不可预测的特点，转而判断牛市结束时点，避免了目标收益率设置不合理导致的过早卖出\n\n因此，文章的结论是：目标收益率 + 最大回撤法、目标估值法 +\n最大回撤法，实际中，天天基金的慧定投是这么做的，且会在低估的时候加大定投的金额。\n定投何种资产？\n定投是一种灵活控制成本的投资模式，而如果要获取收益，本质上还是要投入到好的资产，在《定投依旧要选好资产》中，提到了选择资产的\n2 个要素: 长期收益情况和波动情况。\n长期收益是定投收益的最大来源，\n而一个资产的价值高低从根本上来说还是其未来现金流决定的。以股票为例，未来的现金流来源首先是公司的盈利增长，其次是股息，最后是估值。再比如债券，未来现金流来源主要是利息。\n另一个则是波动情况，一些定投的文章就主张定投要选高波动资产，《定投依旧要选好资产》指出这个说法对也不对。说对，是因为高波动资产确实能增加我们在低位买入的概率增加，从而摊低成本；说不对，是因为从最终结果来看意义并不大。\n文中举了中证 500 和标普的例子，两者涨跌幅很接近，差不多都是 90%\n左右，但是，中证 500 的波动率要大得多，回测结果显示最终定投中证 500 的收益还被指数收益率稍低的标普 500 反超了，原因是中证 500 虽然波动率大，但我们没办法总是买在便宜的时候\n而回到具体的投资标的上，还是之前一直推崇的指数基金，因为主动型股票基金存在着基金经理不稳定的问题，但指数基金不受这个影响\n\n主动型基金基金经理变更的这个大问题，在指数基金身上是基本不存在的。因为指数基金的管理基本上是量化的，基金经理在里面起到的作用很小，不怕他离职。最差的情况是这只指数基金终止清盘，我们也可以很容易在市场上找到跟踪同个指数的产品替代。这也是为什么很多人说的指数基金「永远不会死」。\n不过，在指数基金定投里还有一点要注意，就是最好选择宽基指数而非单一行业或主题指数。\n\n网格策略\n这里的网格策略，是一种波段策略。在《波段随谈》中提到了波段的一些优势，\n包括心态上会更舒适，同时能增加安全垫等\n\n它会让你对下跌能够乐观接受甚至有些期盼。尤其是当实现一波收益后，你甚至盼着赶紧跌好让你再来一次，虽然你的长线持仓会有损失。这种心态的改变会让你的状态变得极度舒适。投资的时候，状态是否舒适非常非常重要。不舒适就有可能带来变态的心理以及变形的交易。\n你买的品种趋势不好，中期会下跌。但下跌并非一路跌到底，中间会有无数反弹。你把反弹利润吃到了，你的安全垫就加厚了。当品种持续下跌后，你买了很多筹码，但由于过程中不断吃反弹利润，成本持续降低，最终在底部区间你只是极少浮亏甚至没有浮亏。\n\n同时，文章也指出 “这不是赚大钱的路”,\n因为大钱是坚定持有中赚到的。这点你还不明白，以后就会懂。但是波段很必要，为什么？因为波段给你提供源源不断利润的同时，也能让你真正坚定持有未来赚大钱的仓位。\n那网格策略具体是如何的操作？E 大有 2 篇文章详细介绍了这个策略\n在《网格策略基础（1.0\n版）》中，指出了网格策略的一些基本步骤\n\n第一步：确定交易品种。\n第二步：列出网格表格。表格中包括交易价格、交易金额、交易日期。\n第三步：做压力测试。\n第四步：设置交易提醒。\n第五步：按照交易提醒进行交易。\n\n首先，做网格的品种，一定是有底的品种。也就是说，不会死的品种。什么会死？比如个股，这里不是说个股一定会退市，凉透了那种死。它如果基本面发生变化，这辈子也涨不回来了，在网格策略中，也算死了。而做网格最佳的品种，则是 “那些没完没了的上下折腾，几年后回头一看我去你怎么一点都没动啊；这种品种简直可以说是网格策略的最佳伴侣，如果你能找到一个，基本上就等同于你找到提款机了。”\n其次，一定要做好压力测试，因为 “网格策略最关键的地方不是能赚多少钱，而是你可以知道最坏情况发生后，自己的账户会怎样”；具体做法文章描述如下\n\n设计交易表格的时候，根据具体情况，模拟最大下跌幅度。比如说，你现在要开始一个中证 500 的网格，那你就应该知道，下跌\n60%，几乎一定是最坏情况了。甚至下跌 50%\n也非常困难。那么你如果相对来说激进一点，就可以以 40%\n设计压力测试。保守一点，就按照 50% 或者 60% 设计。\n如果你真的不知道到底应该给多少最大下跌空间，你可以参考 ETF\n计划里面每次关于指数最大跌幅的判断。在那个基础上，再加 10%\n基本上就是铁底。\n\n具体的交易策略，就是列出网格表格，\n包括交易价格、交易金额、交易日期，然后随着下跌逐步购买，且在上涨时卖出，\n文章里是这么说的\n\n如果从 1 开始，以 5%\n幅度为一网，则买入价格分别为：1、0.95、0.9、0.85、0.8…… 卖出价格除第一网\n1.0 买入的要在 1.05 卖出外，其它每一网都是上一网的买入价格。即， 0.95\n买入的一网，1 元卖出；0.9 买入的，0.95 卖出，依次类推。\n所以，你 1 元买入的部分，在 1.05 清仓，赚到 5% 利润。0.95 买到的，在\n1 元清仓，赚到 5.26%…… 最终，在 0.7 元买入的部分，0.75\n清仓，则这部分收益率就可以到 7.14%。\n\n可以看到，这里的策略有 2\n个关键点，一是开始的时机，文章指出是 “价格略低于价值的时候”，但是这个很难有一个统一的标准，文章建议是观察\nETF 计划，如果 ETF\n计划开始买入，说明这个东西不太贵；二是网格大小 ,\n文章给的参考值是普通的品种一般给 5%。波动大的品种 (比如券商指数) 给\n10%\n另外，还提到了走势相似的品种不要重复开，比如说都是大盘股指数，50 和\n300 就别开 2 个网格了。\n上面是 1.0 的策略，在《网格策略进阶（2.0\n版）》则提出了进阶的策略，相对于 1.0 有了三个点的改建\n\n留利润，即每次的卖出时只拿回本金，剩余的利润继续放在里面\n\n逐格加码，即约低估，买入的金额越多，这里跟定投时低估加倍定投原理一致\n\n多级网格，即除了 5% 的网格，还可以可以再设置一个 15% 的中网，一个\n30% 的大网\n\n小结\n本文主要讲了投资中的实操部分：买与卖；在了解了本文所介绍的知识体系后，再回到我们在最开始提的几个问题\n\n买什么\n\n什么时候买，买多少\n\n什么时候卖，卖多少\n\n相信答案也就就很明确了，做好资产配置，买入相关性低的资产；在低估值的时候买，高估值的时候卖，指数的估值一般参考\nPE 和\nPB，更着重看其区间 (历史百分位) 而不是绝对值；而买卖多少依赖仓位控制中的分配方法，低估时仓位更重，高估时仓位更轻。\n另外，本文还提到两种策略：定投策略和网格策略，前者偏向长期的投资，后者则更偏向短期的波段操作，里面都给出了一些实操建议。\n即便如此，投资还是很难；因为上面只是说了一个大原则，或者说 “正确的废话”，具体每个品类，PE、PB\n到了多少认为是低估值，也没有一个明确的答案，历史上每次的大涨和大跌时这两个值也不尽相同；又或者是即便是道理没错，面对真实的下跌\n80% 时，逆势加仓还需要有足够的勇气和子弹。\n在笔者整理的过程中，虽然尽量把文章的内容尽量整理得脉络比较清晰，但总体感觉还是会比较混沌，没有绝对的真理或圣杯，几乎任何觉得有道理的建议或理论，都能找到反例来推翻；回到最终，你会发现还是一个 “度” 的问题，但是这个 “度” 的把控则是非常非常奇妙的，而这也许就是投资被称作一项艺术的原因？\n投资里有个 “盈亏同源” 的概念，顾名思义，就是盈利与亏损都是出自同一个源头，或者说都是同一个原因造成的；其本质是也是在描述上面说的混沌的现象，市场是不确定的，无法预测的，要放弃对所谓确定性的追求，同时摸索出适合自己的选择判断体系；这几篇文章在做的事情就是尝试归纳\nE\n大、有知有行已经摸索出一套体系，不能称得上是完美，但是自洽，而且在真实的系统中验证过有效性的；当然有效的系统肯定不止一套，但是愿意这么系统地整理且公开的，笔者目前就找到这一套，也希望对你有用。\n","categories":["拾人牙慧"],"tags":["拾人牙慧","投资"]},{"title":"操作系统课程总结","url":"/2016/12/18/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/","content":"本文为操作系统的课程小结，主要讲述 Linux\n内核的一些知识，参考的主要教材为《Linux 内核设计与实现 (第 3 版)》，除此之外还参考了网络上的若干资料，因为 Linux 本来就是可以写若干本书的内容，所以这里只会涉及到博主上课时接触到的一些知识点，由于博主知识有限，个中定存在错漏地方，望不吝指出。\n\n第一章：内核简介\nLinux 内核特点\n\n动态加载内核模块\n支持对称多处理（SMP）\n&gt;\n在计算领域，对称多处理是一种多处理机硬件架构，有两个或更多的相同的处理机（处理器）共享同一主存，由一个操作系统控制\n内核可抢占 (Preemption): 要理解 Preemption 首先要了解操作系统的\nContext Switch &gt;Context Switch (上下文切换)\n指任何操作系统上下文 (上下文简单说来就是一个环境，如进程上下文就是 CPU 的所有寄存器中的值、进程的状态以及堆栈上的内容，中断上下文就是硬件的参数和内核需要保存的一些其他环境) 保存和恢复执行状态，以便于被安全地打断和稍后被正确地恢复执行。当发生进程调度时，进行进程切换就是上下文切换 , 一般操作系统中通常因以下三种方式引起上下文切换:\n&gt;1. Task Scheduling\n(任务调度)。任务调度一般是由调度器代码在内核空间完成的。\n通常需要将当前 CPU\n执行任务的代码，用户或内核栈，地址空间切换到下一个要运行任务的代码，用户或内核栈，地址空间。\n&gt;2. Interrupt (中断) 或 Exception\n(异常)。中断和异常是由硬件产生但由软件来响应和处理的。这个过程中，涉及到将用户态或内核态代码切换至中断处理代码。\n&gt;3. System Call\n(系统调用)。系统调用是由用户态代码主动调用，使用户进程陷入到内核态调用内核定义的各种系统调用服务。系统调用实质就是通过指令产生中断，也称为软中断。\n&gt;\n&gt; 实际上，进程从用户态进入内核态的方式只有两种：中断和异常。(系统调用实际上最终是中断机制实现的)\n\nPreemption (抢占)\n是指操作系统允许满足某些重要条件 (例如：优先级，公平性) 的任务打断当前正在\nCPU\n上运行的任务而得到调度执行。并且这种打断不需要当前正在运行的任务的配合，同时被打断的程序可以在后来可以再次被调度恢复执行。\n\n线程的实现：内核并不区分线程和其他的一般进程，线程是一个标准的进程，与进程的最大区别在于是否有独立的地址空间\n设备管理：所有设备都是文件\n\n内核版本号\n主版本号.从版本号.修订版本号，从版本号为偶数时为稳定版本，奇数时为开发版\n内核应用\n\n内核开发、移植\n驱动\n文件系统\n云计算与虚拟化\n\n第二章：内核的编译与安装\n简单教程：http://www.cnblogs.com/hdk1993/p/4910362.html\n补丁概念\n内核开发特点：\n\n不能访问 c 库，只能使用 c 的语法\n缺乏内存保护机制\n浮点数难以使用\n注意同步和并发（原因：竞争条件（可抢占多任务系统），解决方法：自旋锁和信号量）\n保持可移植性\n\n第三章：进程管理\n进程描述符\n内核通过一个任务队列（task list）组织所有的进程，任务队列是一个双向链表，结构如下图所示\n\n图中链表中每一项都是类型为 task_struct\n，称为进程描述符的结构。进程描述符包含了一个具体进程的所有信息，如：\n\n进程状态\n进程的地址空间\n PID\n 指向父、子进程的指针\n打开的文件\n......\n\ntask_struct 的存放位置\n\n2.6 之前存放在进程的内核栈底\n 2.6 之后改为了通过内核栈底的一个结构（thread_info），这个结构中有一个指针指向其 task_structure\n\n\n关于进程的内核栈和用户栈\n参考：http://blog.csdn.net/dlutbrucezhang/article/details/9326857\n每个进程都有自己的堆栈，内核在创建一个新的进程时，在创建进程控制块 task_struct 的同时，也为进程创建自己堆栈。一个进程有 2 个堆栈，用户堆栈和系统堆栈；用户堆栈的空间指向用户地址空间，内核堆栈的空间指向内核地址空间。当进程在用户态运行时，CPU\n堆栈指针寄存器指向的\n用户堆栈地址，使用用户堆栈，当进程运行在内核态时，CPU 堆栈指针寄存器指向的是内核栈空间地址，使用的是内核栈；\n当进程由于中断或系统调用从用户态转换到内核态时，进程所使用的栈也要从用户栈切换到内核栈。进程因为中断（软中断或硬件产生中断），使得 CPU 切换到特权工作模式，此时进程陷入内核态，进程进入内核态后，首先把用户态的堆栈地址保存在内核堆栈中，然后设置堆栈指针寄存器的地址为内核栈地址，这样就完成了用户栈向内核栈的切换。当进程从内核态切换到用户态时，最后把保存在内核栈中的用户栈地址恢复到 CPU 栈指针寄存器即可，这样就完成了内核栈向用户栈的切换。\n\ntask_struct 的组成部分\ntask_struct 中包含了进程的所有信息，如进程的状态，优先级，pid，父进程与子进程，运行的时间，与文件系统的交互情况，内存使用情况 (mm_struct) 等。\n这里详细介绍的几个重要组成部分：\n进程的状态\n广义来说，对所有操作系统而言，进程的状态一般可以分为 running,ready 和 block 状态，其中 running 表示进程正在 cpu 上跑，ready 表示进程正在等待 cpu 分配执行的时间片，一旦分配了时间片即可进入 running 状态，而 block 表示当前的进程正在等待某些资源 (如用户的输入)，只有得到了这些资源，才可进入 ready 状态。\n但是在 Linux\n中，为进程定义了五种状态，与上面所说的状态略有不同，每种状态定义如下：\n1. R (task_running) : 可执行状态\n只有在该状态的进程才可能在 CPU 上运行。而同一时刻可能有多个进程处于可执行状态，这些进程的 task_struct 结构（进程控制块）被放入对应 CPU 的可执行队列中（一个进程最多只能出现在一个 CPU 的可执行队列中）。进程调度器的任务就是从各个 CPU 的可执行队列中分别选择一个进程在该 CPU 上运行。这种状态包含了正在执行的进程和等待分配时间片的进程，即包含了上面的 running 和 ready 状态。\n2. S (task_interruptible): 可中断的睡眠状态\n处于这个状态的进程因为等待某某事件的发生（比如等待 socket 连接、等待信号量），而被挂起。这些进程的 task_struct 结构被放入对应事件的等待队列中。当这些事件发生时（由外部中断触发、或由其他进程触发），对应的等待队列中的一个或多个进程将被唤醒。\n通过 top 命令我们会看到，一般情况下，进程列表中的绝大多数进程都处于 task_interruptible 状态（除非机器的负载很高）。毕竟 CPU 就这么一两个，进程动辄几十上百个，如果不是绝大多数进程都在睡眠，CPU 又怎么响应得过来。\n3. D (task_uninterruptible): 不可中断的睡眠状态\n与 task_interruptible 状态类似，进程处于睡眠状态，但是此刻进程是不可中断的。不可中断，指的并不是 CPU 不响应外部硬件的中断，而是指进程不响应异步信号。\n绝大多数情况下，进程处在睡眠状态时，总是应该能够响应异步信号的。但是 task_uninterruptible\n状态的进程不接受外来的任何信号，因此无法用 kill\n杀掉这些处于 D 状态的进程，无论是 kill,\nkill -9 还是 kill -15，这种情况下，一个可选的方法就是 reboot。\n处于 task_uninterruptible 状态的进程通常是在等待 IO，比如磁盘 IO，网络 IO，其他外设 IO，如果进程正在等待的 IO 在较长的时间内都没有响应，那么就被 ps 看到了，同时也就意味着很有可能有 IO 出了问题，可能是外设本身出了故障，也可能是比如挂载的远程文件系统已经不可访问了.\n而task_uninterruptible 状态存在的意义就在于，内核的某些处理流程是不能被打断的。如果响应异步信号，程序的执行流程中就会被插入一段用于处理异步信号的流程（这个插入的流程可能只存在于内核态，也可能延伸到用户态），于是原有的流程就被中断了。\n在进程对某些硬件进行操作时，可能需要使用 task_uninterruptible 状态对进程进行保护，以避免进程与设备交互的过程被打断，造成设备陷入不可控的状态。这种情况下的 task_uninterruptible 状态总是非常短暂的，通过 ps 命令基本上不可能捕捉到。\n4. T (task_stopped or\ntask_traced)：暂停状态或跟踪状态\n向进程发送一个 sigstop 信号，它就会因响应该信号而进入 task_stopped 状态（除非该进程本身处于 task_uninterruptible 状态而不响应信号）。\n向进程发送一个 sigcont 信号，可以让其从 task_stopped 状态恢复到 task_running 状态。\n当进程正在被跟踪时，它处于 task_traced 这个特殊的状态。“正在被跟踪” 指的是进程暂停下来，等待跟踪它的进程对它进行操作。比如在调试的时候对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于 task_traced 状态。而在其他时候，被跟踪的进程还是处于前面提到的那些状态。\n对于进程本身来说，task_stopped 和 task_traced 状态很类似，都是表示进程暂停下来。而 task_traced 状态相当于在 task_stopped 之上多了一层保护，处于 task_traced 状态的进程不能响应 sigcont 信号而被唤醒。只能等到调试进程通过 ptrace 系统调用执行 ptrace_cont、ptrace_detach 等操作（通过 ptrace 系统调用的参数指定操作），或调试进程退出，被调试的进程才能恢复 task_running 状态。\n5. Z (task_dead -\nexit_zombie)：退出状态，进程成为僵尸进程\n在 Linux 进程的状态中，僵尸进程是非常特殊的一种，它是已经结束了的进程，但是没有从进程表中删除。太多了会导致进程表里面条目满了 (PID 数目有限)，进而导致系统崩溃，倒是不占用其他系统资源。\n僵尸进程已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态等信息供其他进程收集，除此之外，僵尸进程不再占有任何内存空间。\n进程在退出的过程中，处于 TASK_DEAD 状态。在这个退出过程中，进程占有的所有资源将被回收，除了 task_struct 结构（以及少数资源）以外。于是进程就只剩下 task_struct 这么个空壳，故称为僵尸。\n之所以保留 task_struct，是因为 task_struct 里面保存了进程的退出码、以及一些统计信息，而其父进程很可能会关心这些信息。比如在 shell 中，$? 变量就保存了最后一个退出的前台进程的退出码，而这个退出码往往被作为 if 语句的判断条件。\n当然，内核也可以将这些信息保存在别的地方，而将 task_struct 结构释放掉，以节省一些空间。但是使用 task_struct 结构更为方便，因为在内核中已经建立了从 pid 到 task_struct 查找关系，还有进程间的父子关系。释放掉 task_struct，则需要建立一些新的数据结构，以便让父进程找到它的子进程的退出信息。\n子进程在退出的过程中，内核会给其父进程发送一个信号，通知父进程来 “收尸”。\n父进程可以通过 wait\n系列的系统调用（如 wait4、waitid）来等待某个或某些子进程的退出，并获取它的退出信息。然后 wait 系列的系统调用会顺便将子进程的尸体（task_struct）也释放掉。\n但是如果他的父进程没调用 wait 或 waitpid() 等待子进程结束，那么它就一直保持僵尸状态，子进程的尸体（task_struct）也就无法释放掉。\n如果这时父进程结束了，那么 init 进程自动会接手这个子进程，为它收尸，它还是能被清除的。但是如果如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是为什么系统中有时会有很多的僵尸进程。\n当进程退出的时候，会将它的所有子进程都托管给别的进程（使之成为别的进程的子进程）。托管的进程可能是退出进程所在进程组的下一个进程（如果存在的话），或者是 1 号进程。\n1 号进程，就是 pid 为 1 的进程，又称 init 进程。linux 系统启动后，第一个被创建的用户态进程就是 init 进程。它有两项使命：\n1）执行系统初始化脚本，创建一系列的进程（它们都是 init 进程的子孙）；\n2）在一个死循环中等待其子进程的退出事件，并调用 waitid 系统调用来完成 “收尸” 工作\ninit 进程不会被暂停、也不会被杀死（这是由内核来保证的）。它在等待子进程退出的过程中处于 task_interruptible 状态，“收尸” 过程中则处于 task_running 状态。\n\n父进程与子进程\n\n进程只有一个父母 ，在进程的\ntask_struct 中的 parent 表示\n进程可以有 0 个以上的子女，在进程的\ntask_struct 中的 children 表示\n比较有趣的一点是在 windows 中并没有父子进程的概念。\n\n进程的若干 ID\n\npid：进程的 ID, 唯一标识一个进程，系统中可用的 PID\n是有限制的，因此系统中进程的总数也是有限制的\n pgrp：进程的组 id，进程\n uid：启动进程的用户 id\ngid：启动进程的用户所在组的 id\neuid，egid\n：euid 和 egid 又称为有效的 uid 和 gid。出于系统安全的权限的考虑，运行程序时要检查 euid 和 egid 的合法性。通常，uid 等于 euid，gid 等于 egid。有时候，系统会赋予一般用户暂时拥有 root 的 uid 和 gid (作为用户进程的 euid 和 egid)，以便于进行运作。（特殊权限：suid，sgid）\n\n上面关于 task_struct 的组成所涉及到的只是很小一部分，更详细的内容可参考\nlinux 进程描述符 task_struct 详解和 task_struct 结构体字段介绍 --Linux 中的 PCB。\n进程与线程\n线程基本概念\n按照教科书上的定义，进程是资源管理的最小单位，线程是程序执行的最小单位。在操作系统设计上，从进程演化出线程，最主要的目的就是更好的支持 SMP 以及减小（进程 / 线程）上下文切换开销。\n一个进程至少需要一个线程作为它的指令执行体，进程管理着资源（比如 cpu、内存、文件等等），而将线程分配到某个 cpu 上执行。一个进程可以拥有多个线程，此时，如果进程运行在 SMP 机器上，它就可以同时使用多个 cpu 来执行各个线程，达到最大程度的并行，以提高效率；同时，即使是在单 cpu 的机器上，采用多线程模型来设计程序，正如当年采用多进程模型代替单进程模型一样，使设计更简洁、功能更完备，程序的执行效率也更高，例如采用多个线程响应多个输入，而此时多线程模型所实现的功能实际上也可以用多进程模型来实现，而与后者相比，线程的上下文切换开销就比进程要小多了，从语义上来说，同时响应多个输入这样的功能，实际上就是共享了除 cpu 以外的所有资源的。\n线程与进程的比较\n1)\n调度。在传统的操作系统中，拥有资源和独立调度的基本单位都是进程。在引入线程的操作系统中，线程是独立调度的基本单位，进程是资源拥有的基本单位。在同一进程中，线程的切换不会引起进程切换。在不同进程中进行线程切换，如从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换。\n2)\n系统开销。由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、\nI/O 设备等，因此操作系统所付出的开销远大于创建或撤销线程时的开销。而线程切换时只需保存和设置少量寄存器内容，开销很小。此外，由于同一进程内的多个线程共享进程的地址空间，因此，这些线程之间的同步与通信非常容易实现，甚至无需操作系统的干预。\n3)\n地址空间和其他资源（如打开的文件）：进程的地址空间之间互相独立，同一进程的各线程间共享进程的资源（包括地址空间），某进程内的线程对于其他进程不可见。\n4) 通信方面： 进程间的通信方式有这样几种：\n\n共享内存\n消息队列\n有名管道\n无名管道\n信号\n文件\n socket\n\n线程间的通信方式上述进程间的方式都可沿用，且还有自己独特的几种\n\n互斥量\n自旋锁\n条件变量\n读写锁\n线程信号\n全局变量\n\n线程的实现方式\n线程的实现可以分为两类：用户级线程 (User-Level Thread, ULT) 和内核级线程 (Kemel-Level Thread,  KLT)。前者更利于并发使用多处理器的资源，而后者则更多考虑的是上下文切换开销。\n在用户级线程中，有关线程管理的所有工作都由应用程序完成，内核意识不到线程的存在。应用程序可以通过使用线程库设计成多线程程序。通常，应用程序从单线程起始，在该线程中开始运行，在其运行的任何时刻，可以通过调用线程库中的派生例程创建一个在相同进程中运行的新线程。下图 (a) 说明了用户级线程的实现方式。\n在内核级线程中，线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口。内核为进程及其内部的每个线程维护上下文信息，调度也是在内核基于线程架构的基础上完成。下图 (b) 说明了内核级线程的实现方式。\n在一些系统中，使用组合方式的多线程实现。线程创建完全在用户空间中完成，线程的调度和同步也在应用程序中进行。一个应用程序中的多个用户级线程被映射到一些（小于或等于用户级线程的数目）内核级线程上。下图 (c) 说明了用户级与内核级的组合实现方式。\n\n多线程模型\n有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式，如上面的图实际上就包含了三种经典的多线程模型。\n1) 多对一模型\n将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成。\n此模式中，用户级线程对操作系统不可见（即透明）。\n优点：线程管理是在用户空间进行的，因而效率比较高。\n缺点：当一个线程在使用内核服务时被阻塞，那么整个进程都会被阻塞；多个线程不能并行地运行在多处理机上。\n2) 一对一模型\n将每个用户级线程映射到一个内核级线程。\n优点：当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强。\n缺点：每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。\n3) 多对多模型\n将 n 个用户级线程映射到 m 个内核级线程上，要求 m &lt;= n。\n特点：在多对一模型和一对一模型中取了个折中，克服了多对一模型的并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程，开销太大的缺点。又拥有多对一模型和一对一模型各自的优点，可谓集两者之所长\n需要注意的是在 Linux\n中，从内核的角度来说，它并没有线程这个概念，内核把所有的线程都当成进程来实现。在内核中，线程看起来就像是一个与其他进程共享了一些资源的普通进程，每一个线程有其唯一的 task_struct；\n关于这个说法，可参考 Linux 线程的前世今生\n&gt; 在 Linux\n创建的初期，内核一直就没有实现 “线程” 这个东西。后来因为实际的需求，便逐步产生了 LinuxThreads\n这个项目，其主要的贡献者是 Xavier\nLeroy。LinuxThreads 项目使用了 clone()\n这个系统调用对线程进行了模拟，按照《Linux 内核设计与实现》的说法，调用\nclone() 函数参数是\nclone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)，即创建一个新的进程，同时让父子进程共享地址空间、文件系统资源、文件描述符、信号处理程序以及被阻断的信号等内容。也就是说，此时的所谓 “线程” 模型符合以上两本经典巨著的描述，即在内核看来，没有所谓的 “线程”，我们所谓的 “线程” 其实在内核看来不过是和其他进程共享了一些资源的进程罢了。\nLinux 的内核线程\n\n内核线程是标准的进程，只存在于内核空间\n内核线程没有地址空间\n内核线程只能由其他内核线程创建\n\n进程的创建和结束\n进程创建的两个步骤\n进程的创建可以分为两个步骤： 1)\nfork: 创建一个子进程即复制当前的任务，新进程与其父进程的区别仅在于 PID,\nPPID 以及特定的资源 (如某些资源的统计量，没有必要继承)。父子进程同时执行，因此调用一次返回两次。\n2)\nexec：将一个程序装入地址空间并执行，只有子进程执行，重建其地址空间，区别于父进程。\nfork\n操作直接把所有资源复制给新创建的子进程，这种实现大批量的复制无疑或导致执行效率低下，因为行为是非常耗时的，因为它需要：\n\n为子进程的页表分配页面\n为子进程的页分配页面\n初始化子进程的页表\n把父进程的页复制到子进程相应的页中\n\n创建一个地址空间的这种方法涉及许多内存访问，消耗许多 CPU 周期，并且完全破坏了高速缓存中的内容。在大多数情况下，这样做常常是毫无意义的，因为许多子进程通过装入一个新的程序开始它们的执行，这样就完全丢弃了所继承的地址空间。所以\nlinux\n采用了写时复制 (copy-on-write) 的策略。\n写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候。在页根本不会被写入的情况下 — 举例来说，fork () 后立即调用 exec ()— 它们就无需复制了。fork () 的实际开销就是复制父进程的页表以及给子进程创建惟一的进程描述符。\n实现的时候 fork\n通过 clone() 系统调用实现，clone() 通过一系列的参数标志指定父子进程需要共享的资源，clone() 调用 do_fork(), 而 do_fork() 调用 copy_process(), 而 copy_process() 做了以下事情：\n\n调用 dup_task_struct 复制内核栈、thread_info 和 task_struct\n检查用户进程限额\n改变子进程 task_struct 结构中的部分内容，子进程状态置为 TASK_UNINTERRUPTIBLE\n为子进程获取一个有效的 PID\n根据传递给 clone() 的参数复制资源\n父子进程平分剩余的时间片\n返回指向子进程的指针\n\n除了 fork 以外，linux 中还有一种创建进程的方式\nvfork，vfork 与 fork 功能相同，子进程共享父进程的地址空间 (内核连子进程的虚拟地址空间结构也不创建)。创建完成后父进程阻塞，直到子进程结束或执行 exec。vfork\n也是通过向 clone 系统调用传递特定的标志实现。\n进程的结束\n以下几种情况会出现进程的结束：\n1）正常结束 (显示或隐式地调用 exit() 系统调用）\n2）进程收到不能忽略也不能处理的信号或异常\n执行 exit() 函数的过程：\n\n释放进程的地址空间\n释放进程使用的资源\n给其父进程发送一个信号，并标示自己的状态为 TASK_ZOMBIE\n调用调度程序，执行其他进程\n\n当父进程收到子进程结束信号时，收回子进程的\ntask_structure 和 thread_info, 没有回收就称为了僵尸进程。\n信号\n基本概念\n信号机制是进程之间相互传递消息的一种方法，信号全称为软中断信号，也有人称作软中断。在 linux 中每个信号有一个名字 (以 SIG 开头)，且定义为一个整数，共有 64 个信号。\n软中断信号（signal，又简称为信号）用来通知进程发生了异步事件。进程之间可以互相通过系统调用 kill 发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。注意，信号只是用来通知某进程发生了什么事件，并不给该进程传递任何数据。\n信号来源分为硬件类和软件类：\n\n硬件方式\n\n用户输入：比如在终端上按下组合键 ctrl+C，产生 SIGINT 信号；\n硬件异常：CPU 检测到内存非法访问等异常，通知内核生成相应信号，并发送给发生事件的进程；\n\n软件方式\n通过系统调用，发送 signal 信号：kill ()，raise ()，sigqueue ()，alarm ()，setitimer ()，abort () 等\n\n收到信号的进程对各种信号有不同的处理方法。处理方法可以分为三类：\n1）类似中断的处理程序，对于需要处理的信号，进程可以指定处理函数，由该函数来处理。\n2）忽略某个信号，对该信号不做任何处理，就象未发生过一样。但是有些信号是不能忽略的，如 SIGKILL、SIGSTOP 和一些硬件异常信号\n3）对该信号的处理保留系统的默认值，这种缺省操作，对大部分的信号的缺省操作是使得进程终止。\n相关的系统调用\n上面的第一种处理方式是通过系统调用 signal 来指定进程对某个信号的处理行为。signal 函数的定义如下\n#include &lt;signal.h&gt;typedef void (*sighandler_t)(int);sighandler_t signal(int signum, sighandler_t handler);(返回值: 如果成功则返回先前的handler，否则返回SIG_ERR)“handler”可取下面的三个值中任意一个：用户定义的函数，或SIG_DEF(恢复参数signum所指信号的处理方法为默认值),或SIG_IGN(忽略参数signum所指的信号)\n通过 kill() 系统调用可以给进程发送一个信号，kill 函数的声明如下\n#include &lt;sys/types.h&gt;#include &lt;signal.h&gt;int kill(pid_t pid, int sig);(返回值: 成功为0, 否则为-1)\n而 raise() 系统调用可以说是 kill() 系统调用的一个特例，用于给当前进程发送一个信号，其定义如下：\n#include &lt;signal.h&gt;int raise(int sig);  (返回值: 成功为0, 否则为-1)\n除此之外，系统调用 alarm() 的功能是设置一个定时器，当定时器计时到达时，将发出一个信号 SIGALRM 给进程。该调用的声明格式如下\n#include &lt;unistd.h&gt;unsigned int alarm(unsigned int seconds);(Returned value: 0, or the number of seconds remaining of previous alarm)\n而系统调用 pause 的作用是等待一个信号。该调用使得发出调用的进程进入睡眠，直到接收到一个信号为止。该调用的声明格式如下：\nint pause(void); \n利用 alarm 函数和 pause 函数实现\nsleep, 同时可参考这里 unsigned int sleep1(unsigned int nsecs) {    if ( signal(SIGALRM, sig_alrm) == SIG_ERR)        return(nsecs);    alarm(nsecs);     /* 开始计时 */      pause();      /*定时信号来时被唤醒*/    return(alarm(0) ); /*关闭定时器 */}int sig_alrm() {   signal(SIGALRM, sig_alrm) ;}\n可靠的信号机制\nLinux 系统共定义了 64 种信号，分为两大类：可靠信号与不可靠信号\n\n不可靠信号：\n也称为非实时信号，不支持排队，信号可能会丢失，\n比如发送多次相同的信号，进程只能收到一次。信号值取值区间为 1~31；\n可靠信号： 也称为实时信号，支持排队，信号不会丢失 ,\n发多少次，就可以收到多少次。信号值取值区间为 32~64\n\n信号的注册与注销\n\n注册\n\n在进程 task_struct 结构体中有一个未决信号的成员变量\nstruct sigpending\npending。每个信号在进程中注册都会把信号值加入到进程的未决信号集。\n非实时信号发送给进程时，如果该信息已经在进程中注册过，不会再次注册，故信号会丢失；\n实时信号发送给进程时，不管该信号是否在进程中注册过，都会再次注册。故信号不会丢失；\n\n注销\n\n非实时信号：不可重复注册，最多只有一个 sigqueue 结构；当该结构被释放后，把该信号从进程未决信号集中删除，则信号注销完毕；\n实时信号：可重复注册，可能存在多个 sigqueue 结构；当该信号的所有 sigqueue 处理完毕后，把该信号从进程未决信号集中删除，则信号注销完毕；\n信号的处理\n内核处理进程收到的 signal 是在当前进程的上下文，故进程必须是 Running 状态。当进程唤醒或者调度后获取 CPU，则会从内核态转到用户态时检测是否有 signal 等待处理，处理完，进程会把相应的未决信号从链表中去掉。\n也就是说 signal 信号处理时机为：\n内核态 -&gt; signal信号处理 -&gt; 用户态：\n\n在内核态，signal 信号不起作用；\n在用户态，signal 所有未被屏蔽的信号都处理完毕；\n当屏蔽信号，取消屏蔽时，会在下一次内核转用户态的过程中执行；\n\n信号相关函数\n进程处理某个信号前，需要先在进程中安装此信号。安装过程主要是建立信号值和进程对相应信息值的动作。\n信号安装函数signal()：不支持信号传递信息，主要用于非实时信号安装；sigaction():支持信号传递信息，可用于所有信号安装；（通过sigaction实现signal函数）\n信号的发送系统调用 kill()：用于向进程或进程组发送信号；sigqueue()：只能向一个进程发送信号，不能像进程组发送信号；主要针对实时信号提出，与sigaction()组合使用，当然也支持非实时信号的发送；alarm()：用于调用进程指定时间后发出SIGALARM信号；setitimer()：设置定时器，计时达到后给进程发送SIGALRM信号，功能比alarm更强大abort()：向进程发送SIGABORT信号，默认进程会异常退出。raise()：用于向进程自身发送信号；\n信号阻塞函数： sigprocmask(int how, const sigset_t *set, sigset_t *oldset))：检测或更改(或两者)进程的信号掩码不同how参数，实现不同功能SIG_BLOCK：将set中的信号添加到进程阻塞信号集（并集）SIG_UNBLOCK：从进程阻塞信号集删除set中的信号(差集)SIG_SETMASK：将set指向信号集中的信号，设置成进程阻塞信号集sigpending(sigset_t *set))：获取已发送到进程，却被阻塞的所有信号，也就是当前未决的信号集sigsuspend(const sigset_t *mask))：用mask代替进程的原有掩码，并暂停进程执行，直到收到信号再恢复原有掩码并继续执行进程。（pause)\n第四章 进程调度\n调度器用于选择进程运行，分配 CPU 执行时间\n调度器运行的时机\n\n进程阻塞在一个 I/O 操作上.\n 硬件中断.\n 进程时间片到.\n 内核主动调用调度器\n\n调度目标\n\n有效性：完成尽可能多的工作。\n交互性：尽快响应用户\n公平性：不允许任何进程饥饿。\n\n哪一个目标最重要取决于取决于目标场景\n\n桌面系统：交互性，尽快相应用户\n服务器：有效性，保证每个用户的请求都能够被完成\n\n调度策略\n进程的类型\n\nI/O 密集型进程:\n较多的交互性，高优先级，大时间片。如文本编辑\n CPU 密集型进程:\n较少的交互性，低优先级，较小的时间片。如视频解码\n\n进程优先级表示\n在 linux 中用 top 或者 ps 命令会输出 PRI/PR、NI 这两个指标值，其含义如下\n\nPRI\n：进程优先权，代表这个进程可被执行的优先级，其值越小，优先级就越高，越早被执行\nNI\n：进程 Nice 值，可用于改变 PRI 的值，PRI(new)=PRI(old)+nice。\n\n在 Linux 系统中，Nice 值的范围从 - 20 到 + 19（不同系统的值范围是不一样的），每个进程都在其计划执行时被赋予一个 nice 值。在通常情况下，子进程会继承父进程的 nice 值，比如在系统启动的过程中，init 进程会被赋予 0，其他所有进程继承了这个 nice 值（因为其他进程都是 init 的子进程）。\n进程的 nice 值是可以被修改的，修改命令分别是 nice 和 renice,\n对非 root 用户，只能将其底下的进程的 nice\n值变大而不能变小 , 若想变小，得要有相应的权限。对 root 用户，可以给其子进程赋予更小的 nice 值。\n进程抢占的时机\n\n当一个进程的优先级高于当前正在运行的进程的优先级\n当一个进程的时间片为 0.\n\n调度器\n内核 2.4 是 O (n) 调度器，2.5 及后改成了 O (1) 调度器，采用的新的数据结构为运行队列和优先级数组，同时改善了 SMP 的可拓展性。\n两种数据结构的解释如下 运行队列 (struct runqueue\n)：给定处理器上可执行进程的链表，运行队列进行操作前要先锁住。\n优先级数组：Linux\n调度器维护两个优先级数组：活跃的和过期的数组。优先级数组是提供\nO(1) 调度的数据结构\n优先级数组是一个结构体，其定义如下所示： struct prio_array {   int nr_active; /* 任务数目*/   unsigned long bitmap[BITMAP_SIZE]; /* 优先级位图*/   struct list_head queue[MAX_PRIO]; /* 优先级队列*/ };\n调度器为每一个 CPU 维护了两个进程队列数组：active 数组和 expire 数组。数组中的元素着保存某一优先级的进程队列指针。系统一共有 140 个不同的优先级，因此这两个数组大小都是 140。同时该调度算法为每个优先级都设置一个可运行队列，\n即包含 140 个可运行状态的进程链表，每一条优先级链表上的进程都具有相同的优先级，而不同进程链表上的进程都拥有不同的优先级。\n除此之外，\n每个优先级数组还包括一个优先级位图 bitmap。该位图使用一个位 (bit) 来代表一个优先级，而 140 个优先级最少需要 5 个 32 位来表示，\n因此只需要一个 int[5] 就可以表示位图，该位图中的所有位都被置 0，当某个优先级的进程处于可运行状态时，该优先级所对应的位就被置 1。\n优先级数组中分为了活跃和过期两种，过期的优先级数组存放过期队列，活跃的优先级数组存放实际队列。\n过期队列是所有用完了时间片的进程。 实际队列是没有用完时间片的进程。\n当一个进程用完了时间片时，重新计算其时间片，并放入到过期队列中。\n当实际进程队列为空时，交换过期队列和实际队列。\n\n重新计算时间片过程\n\n动态优先级用于计算优先级\nnice+进程交互性的奖励或罚分 为了确定一个进程是否是交互性的，\nLinux 记录了一个进程用于休眠和用于执行的时间（0-MAX_SLEEP_AVG，默认 10ms）。一个进程从休眠恢复到执行时，优先级增加；运行一段时间后会减小。\n静态优先级用于计算时间片\n进程创建时，子进程与父进程均分父进程剩余的时间片.\n任务的时间片用完时，基于任务的静态优先级重新计算时间片\n\n负载平衡程序 找最繁忙的运行队列选择一个优先级数组（过期的优先）选择优先级最高的链表选择一个不是正在运行的，不在高速缓冲的，可移动的进程抽取重复上述步骤，直至平衡\n抢占可分为用户抢占和内核抢占： 用户抢占发生在\n\n从系统调用返回用户态\n从中断服务程序返回用户态\n\n内核抢占发生在\n\n中断服务程序正在执行，且返回内核空间之前\n内核代码再一次具有可抢占性时\n处于核心态的任务直接调用 schedule ()\n 内核中的任务阻塞\n\n实时调度策略\nSCHED_FIFO: 先入先出方式调度的实时进程，即该进程一旦执行便一直运行到结束。\nSCHED_RR:\n通过时间片轮转的方式调度的实时进程。在运行了指定的时间片后会被抢占并重新调度。但如果没有其他优先级高于或等于它的实时进程与其竞争，它还会得到继续运行的机会。\n第五章 系统调用\n基本概念\n系统调用为用户空间进程提供一个访问内核接口，在 Linux 中，系统调用是唯一合法访问内核的入口。\n系统调用的目的主要有两个： 1）为用户空间提供一个统一接口。\n2）保证系统的安全和稳定\nAPI、POSIX、C 库\nAPI/POSIX/C 库的区别与联系\n一般情况下应用程序通过应用编程接口 API，而不是直接通过系统调用来编程。在 Unix 世界，最流行的 API 是基于 POSIX 标准的，即 POSIX 说明 API 和系统调用之间关系，。\n\napi 是函数的定义，规定了这个函数的功能，跟内核无直接关系。它们可以实现成一个系统调用，也可以通过调用多个系统调用来实现，而完全不使用任何系统调用也不存在问题。实际上，API 可以在各种不同的操作系统上实现，给应用程序提供完全相同的接口，而它们本身在这些系统上的实现却可能迥异。\n\nLinux 的系统调用接口，像大多数 Unix 系统一样，以 C 库的形式提供，C 库实现了\nUnix 系统的主要 API，包括标准 C 库函数和系统调用。\n\n系统调用的实现\n在 Linux 中，每一个系统调用分配有一个 syscall\n号。这是一个唯一的整数，用于指定系统调用.\nsyscall 号分配后，不能够改变或回收.\n一般地，系统调用都是通过软中断实现：产生一个异常，系统切换到内核模式，执行异常处理程序，即系统调用处理程序，在 x86 上，定义的软中断是函数 system_call()。\nsystem_call() 函数检查系统调用号 syscall，如合法，调用指定的系统调用。\n\n增加一个系统调用的过程 1.\n首先在系统调用表的最后加入一个表项。 2.\n对于每一种支持的体系结构，系统调用号必须定义在\n&lt;asm/unistd.h&gt;. 3. 系统调用必须被编译进内核映像。\n实现一个新的系统调用的好处：\n1）系统调用容易使用容易实现。 2）系统调用的性能在 Linux 中非常快。\n缺点: 1）系统调用号需要官方授权给你。\n2）系统调用一旦进入稳定的内核，其接口就不能再改变，否则会影响用户空间的应用.\n3）需要将系统调用分别注册到每个需要支持的体系结构。\n4）系统调用在脚本中不宜使用，不能直接从文件系统访问。\n5）如果仅仅进行简单的信息交换，系统调用就大材小用\n从用户空间访问系统调用：Linux\n提供了一组宏，用于直接访问系统调用。它设置寄存器内容，并执行 trap 指令。这些宏是\n_syscalln(), 这里 n：0-6。\n第六章 内核数据结构\n\n链表\n队列\n映射\n红黑树\n\n映射是键到值的关联关系。Linux 中的映射是将一个唯一的标识符（UID）映射到一个指针，实现方式有：\n\n数组\n散列表\n自平衡二叉树\n Linux 采用的方式：radix 树（）\n\n数据结构的选择原则如下：\n\n链表：主要操作是遍历数据\n队列：生产者消费者模式\n映射：映射一个 UID 到一个对象\n红黑树：存储大量数据，并且迅速检索\n\n红黑树是一种自平衡二叉搜索树，具有以下性质：\n\n所有的节点或者红色，或者黑色\n所有叶节点都是黑色\n叶节点不包含数据\n所有非叶节点都有两个字节点\n如果一个节点是红色，则其子节点都是黑色\n在一个节点到其叶子节点的路径中，如果总是包含同样数目的黑色节点，则该路径相比其他路径是最短的\n\n第七、八章 中断和中断处理\n基本概念\n操作系统要管理硬件，就必须要能够与硬件通信。中断能够使硬件能够发出通知给处理器 (CPU)，例如按下键盘时，键盘控制器就会发出一个中断请求；处理器 (CPU) 接收到中断请求后，会马上通知内核进行处理，因此硬件设备生成中断时并不会考虑与处理器的时钟同步，也就是说中断可以随时产生。\n不同情况下对应的中断不同，每个中断都有一个唯一的数字标示，通常被称为中断号（IRQ），如键盘和硬盘的中断号就不同。中断号的不同，内核处理的程序也不同，因此有一张表格用于记录每个中断号及其对应的处理程序，称为中断向量表。\n说到中断，常常会提及到异常。异常与中断不同，异常产生的时候必须考虑与处理器的时钟同步。实际上异常也常被称为同步中断。常见的异常有处理器执行过程中由于代码的缺陷而执行了错误指令（如除上 0），或者是在执行期间出现了特殊情况（如缺页），必须依靠内核来处理的时候，处理器就会产生一个异常。前面说到的系统调用实际上就是通过异常来实现的，异常也可称为软中断。\n中断处理程序\n由前面的简介可知，响应一个特定的中断，内核会执行一个与之对应的特定函数，这个函数就叫做中断处理程序。\n在 Linux 中，一个设备的中断处理程序是它设备驱动程序（管理设备的内核代码）的一部分，因此，中断处理程序实际上就是普通的 C 函数，与其他内核函数的真正区别在于这些程序运行于被称为中断上下文的特殊上下文中，该上下文不可被阻塞，也就是说进入中断服务程序后，\n不会被其他响应中断。\n上半部和下半部\n中断要求尽快处理，但是往往中断又要完成较多的工作量。考虑两者间做一个权衡，中断处理被分成了两个部分：上半部和下半部，上半部完成那些重要、有严格时限的、与硬件相关的工作，下半部则完成那些允许被稍后完成的工作。接收到一个中断后，上半部（实际上就是中断处理程序）会立即执行，然后返回中断前原先运行的程序，而下半部会在合适的时机在执行（通常下半部分在中断处理程序一返回就会马上运行）。中断处理程序的下半部分（如果有的话）几乎做了中断处理程序所有的事情。它们最大的不同是上半部分不可中断，而下半部分可中断。下面以网卡为例做简单说明：\n当网卡接收到来自网络的数据包的时候，网卡会向内核发出中断，内核通过网卡已注册的中断处理程序来做出应答，中断开始的时候，内核会快速拷贝网络数据包到系统内存，因为网卡上接收的网络数据包的缓存是固定的，而且相比于内存来说很小，假如数据包占满了缓存，后续的数据包只能被丢弃，所以这个任务是最紧急的，当网络数据包全被拷到内存后，中断任务算是完成了，这个它会将控制权返回给系统中断前原先运行的程序。至于处理数据包的操作会在下半部进行。\n尽管上半部和下半部的结合能够改善系统的响应能力，但是，Linux 设备驱动中的中断处理并不一定要分成两个半部。如果中断要处理的工作本身就很少，则完全可以直接在上半部全部完成。\n上半部\n注册与释放中断程序\n对于设备的每一种中断，设备的驱动程序需要为其注册一个相关的中断处理程序，以便通知内核该如何处理该中断。\n驱动程序通过 request_irq() 函数注册一个中断处理程序。\n卸载驱动程序的时候，需要注销相应的中断处理程序，并释放中断线（设备的中断处理器与 CPU 的直连线），通过调用 free_irq()\n实现\n编写中断处理程序\n典型的定义 static irqreturn_t intr_handler (int irq, void *dev_id, struct pt_regs *regs)irq: 中断号dev_id: 区分共享中断线的多个设备regs: 保存中断前的处理器的寄存器和状态irqreturn_t：IRQ_NONE, IRQ_HANDLED\nLinux中的中断处理程序都是无须重入的，也就是说一个给定的中断处理程序正在执行的时候，相应的中断线在所有的处理器上都会被屏蔽，以防止在同一中断线上接收另外一个新的中断，但是其他的中断线上的中断能够被处理。\n中断上下文\n在讨论中断上下文之前先讨论一下进程上下文。\n用户空间的应用程序，通过系统调用，进入内核空间。这个时候用户空间的进程要传递\n很多变量、参数的值给内核，内核态运行的时候也要保存用户进程的一些寄存\n器值、变量等。所谓的 “进程上下文”，可以看作是用户进程传递给内核的这些参数以及内核要保存的那一整套的变量和寄存器值和当时的环境等。\n相对于进程而言，就是进程执行时的环境。具体来说就是各个变量和数据，包括所有的寄存器变量、进程打开的文件、内存信息等。一个进程的上下文可以分为三个部分：用户级上下文、寄存器上下文以及系统级上下文。\n（1）用户级上下文：正文、数据、用户堆栈以及共享存储区；\n（2）寄存器上下文:\n通用寄存器、程序寄存器 (IP)、处理器状态寄存器 (EFLAGS)、栈指针 (ESP)；\n（3）系统级上下文:\n进程控制块 task_struct、内存管理信息 (mm_struct、vm_area_struct、pgd、pte)、内核栈。\n当发生进程调度时，进行进程切换就是上下文切换 (context\nswitch). 操作系统必须对上面提到的全部信息进行切换，新调度的进程才能运行。\n而对于中断上下文而言，硬件通过触发信号，导致内核调用中断处理程序，进入内核空间。这个过程中，硬件的\n一些变量和参数也要传递给内核，内核通过这些参数进行中断处理。所谓的 “\n中断上下文”，其实也可以看作就是硬件传递过来的这些参数和内核需要保存的一些其他环境（主要是当前被打断执行的进程环境）。中断时，内核不代表任何进程运行，它一般只访问系统空间，而不会访问进程空间，内核在中断上下文中执行时一般不会阻塞。\nLinux\n内核工作在进程上下文或者中断上下文。提供系统调用服务的内核代码代表发起系统调用的应用程序运行在进程上下文；另一方面，中断处理程序则代表硬件运行在中断上下文。中断上下文和特定进程无关。\n运行在进程上下文的内核代码是可以被抢占的（Linux2.6）。但是一个中断上下文，通常都会始终占有 CPU，不可以被打断。正因为如此，运行在中断上下文的代码就要受一些限制，不能做下面的事情：\n1、睡眠或者放弃 CPU：这样做的后果是灾难性的，因为内核在进入中断之前会关闭进程调度，一旦睡眠或者放弃 CPU，这时内核无法调度别的进程来执行，系统就会死掉。除此之外，在中断处理函数中调用一个内核 API 之前，应该仔细分析它以确保其内部不会触发阻塞等待。\n2、尝试获得信号量：如果获得不到信号量，代码就会睡眠，会产生和上面相同的情况\n3、执行耗时的任务：中断处理应该尽可能快，因为内核要响应大量服务和请求，中断上下文占用 CPU 时间太长会严重影响系统功能。\n4、访问用户空间的虚拟地址：因为中断上下文是和特定进程无关的，它是内核代表硬件运行在内核空间，所以在终端上下文无法访问用户空间的虚拟地址\n中断处理的实现\n中断处理系统在 linux 中的实现是非常依赖于体系结构的，例如要依赖于处理器，所使用的的中断控制器的类型，体系结构的设计及机器本身。\n下图是中断从硬件到内核进行处理的一个流程\n\n下半部\n下半部的任务主要是执行与中断相关的工作，这些工作没有被中断服务程序本身完成.\n下半部分并不需要指明一个确切时间，只要把这些任务推迟一点，让它们在系统不太忙并且中断恢复后执行就可以了。通常下半部分在中断处理程序一返回就会马上运行。内核中实现下半部的手段不断演化，目前已经从最原始的 BH（bottom\nhalf）衍生出 BH（在 2.5 中去除）、软中断（softirqs 在 2.3 引入）、tasklet（在 2.3 引入）、工作队列（work queue 在 2.5 引入）。\nsoftirqs 机制\n软中断结构定义如下： struct softirq_action {               void (*sction) (struct softirq_action *);/*待执行的函数*/               void *data;  /*传给函数的指针*/        }\nkernel/softirq.c 定义了一个32个元素的结构数组,\n每个被注册的软中断都占据该数组的一项，因此最多可能有32个软中断，且下标小的软中断优先级高。\n中断处理程序在返回前触发它的软中断，使其在稍后执行。在执行过程中，一个软中断不会抢占另外一个软中断，唯一可以抢占软中断的是中断处理程序。执行就是遍历上面提到的结构数组，并处理那些有软中断的位置（值为 1）。\n软中断保留给对时间要求最严格和最重要的下半部使用，如网络和 SCSI 设备。使用软中断的过程大致为分配索引号-&gt;注册处理程序-&gt;触发软中断\ntasklet 机制\ntasklet\n是基于软中断实现的，与软中断相比，tasklet 更常用，软中断一般用于那些执行频率很高和连续型要求很高的情况。\n引入 tasklet，最主要的是考虑支持 SMP，提高 SMP 多个 cpu 的利用率；不同的 tasklet 可以在不同的 cpu 上运行。tasklet 可以理解为 softirq 的派生，所以它的调度时机和软中断一样。\n每个处理器都有一个用于辅助处理软中断的内核线程:ksoftirqd/n, 当内核中出现大量软中断的时候，这些内核线程就会辅助处理他们。这个内核线程是对于重新触发的软中断是否立即处理的问题的一个折中，最终是不会立即处理这些重新触发的软中断，而是添加这样一个线程使得在软中断数目过多时也能够被迅速处理。\nwork queue 机制\n工作队列可以把工作推后，然后交给一个内核线程去执行，这些内核线程被称为工作线程。工作队列一个很重要的特性就是允许工作重新调度和睡眠。\n选择何种机制\n从设计上讲，Softirq 提供最少的顺序保证，这需要 Softirq 处理函数采取一些额外的步骤保证数据安全，因为两个以上的同类型 softirqs 只能同时运行于不同的 CPU。Softirq 多用于时间要求严格和使用频度高的场合。\n如果代码不能很好地线程化，tasklet 意义较大 Tasklets\n有一个简单的接口，由于两个同类型的不能同时运行，他们非常易于实现。\n如果你的延期的工作需要运行于进程上下文（重新调度和睡眠）,\n唯一的选择是 work queue\n第九、十章 内核同步\n基本概念\n在现代操作系统里，同一时间可能有多个内核执行流在执行，因此内核也需要一些同步机制来同步各执行单元对共享数据的访问。尤其是在多处理器系统上，更需要一些同步机制来同步不同处理器上的执行单元对共享的数据的访问。\n临界区和竞争条件\n临界区：访问和操作共享数据的代码段。\n竞争条件：多个执行线程处于同一个临界区中。\n同步就是保证不安全的并发不发生，即竞争条件不发生。需要同步的情况有：\n\n中断\n Softirqs 和 tasklets\n 内核抢占\n用户空间的睡眠和同步\n SMP\n\n死锁的产生条件\n1. 互斥 (mutual exclusion)：系统存在着临界资源； 2. 占有并等待 (hold and\nwait)：已经得到某些资源的进程还可以申请其他新资源； 3. 不可剥夺 (no\npreemption)：已经分配的资源在其宿主没有释放之前不允许被剥夺；\n4. 循环等待 (circular\nwaiting)：系统中存在多个（大于 2 个）进程形成的封闭的进程链，链中的每个进程都在等待它的下一个进程所占有的资源；\n死锁预防与死锁避免\n死锁预防\n防止死锁的发生只需破坏死锁产生的四个必要条件之一即可 1)\n破坏互斥条件\n如果允许系统资源都能共享使用，则系统不会进入死锁状态。但有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。\n2) 破坏不剥夺条件\n当一个已保持了某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放，或者说是被剥夺了，或从而破坏了不可剥夺条件。\n该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销，降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源，如 CPU 的寄存器及内存资源，一般不能用于打印机之类的资源。\n3) 破坏请求和保持条件\n釆用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行后，这些资源就一直归它所有，也不再提出其他资源请求，这样就可以保证系统不会发生死锁。\n这种方式实现简单，但缺点也显而易见，系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致 “饥饿” 现象，当由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。\n4) 破坏循环等待条件\n为了破坏循环等待条件，可釆用顺序资源分配法。首先给系统中的资源编号，规定每个进程，必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源 Ri，则该进程在以后的资源申请中，只能申请编号大于 Ri 的资源。\n这种方法存在的问题是，编号必须相对稳定，这就限制了新类型设备的增加；尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使甩资源的顺序与系统规定顺序不同的情况，造成资源的浪费；此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。\n死锁避免\n死锁的预防是通过破坏产生条件来阻止死锁的产生，但这种方法破坏了系统的并行性和并发性。。\n死锁产生的前三个条件是死锁产生的必要条件，也就是说要产生死锁必须具备的条件，而不是存在这 3 个条件就一定产生死锁，那么只要在逻辑上回避了第四个条件就可以避免死锁。\n避免死锁采用的是允许前三个条件存在，但通过合理的资源分配算法来确保永远不会形成环形等待的封闭进程链，从而避免死锁。\n其主要思想如下：\n1. 如果一个进程的当前请求的资源会导致死锁，系统拒绝启动该进程；\n2. 如果一个资源的分配会导致下一步的死锁，系统就拒绝本次的分配；\n在这个思想下诞生出来的便是著名的银行家算法：把操作系统看做是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源，当进程首次申请资源时，要测试该进程对资源的最大需求量，如果系统现存的资源可以满足它的最大需求量则按当前的申请量分配资源，否则就推迟分配。当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过了该进程对资源的最大需求量。若超过则拒绝分配资源，若没有超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。\n同步的机制\n原子操作\n\n基本的操作\n不可中断\n不能分割的指令 \n\n原子操作的两组接口 1. 原子整数操作：使用一种特殊的类型\natomic_t 2. 原子位操作：在位级别上进行操作\n自旋锁\n原子位和原子整数仅能对简单的整形变量进行原子操作，对于复杂的数据复杂的操作并不适用。需要更复杂的同步方法实现保护机制 —— 锁。\n自旋锁：同一时刻只能被一个可执行线程持有，获得自旋锁时，如果已被别的线程持有则该线程进行循环等待锁重新可用然后继续向下执行。\n使用自旋锁时要防止死锁：\n\n自旋锁不可递归，自旋处于等待中造成死锁；\n中断处理程序中，获取自旋锁前要先禁止本地中断，中断会打断正持有自旋锁的任务，中断处理程序有可能争用已经被持有的自旋锁，造成死锁。\n\n读写自旋锁：将锁的用途直接分为读取和写入。\n\n多个读者能同时持有读锁\n没有读者时只能有一个写者能持有写锁\n\n信号量\n信号量是睡眠锁。如果有一个任务试图获取信号量时，有一下两种情况：\n1）信号量未被占用：该任务获得成功信号量；\n2）信号量已被占用：信号量将任任务推进等待队列，让其睡眠，处理器继续工作；当信号量被释放后，唤醒信号量队列中的任务，并获取该信号量。\n信号量适用于长时间的持有。持有信号量的进程可以睡眠，但是不能试图获自旋锁。\n读写信号量，与读写自旋锁类似\n自旋锁与信号量的对比\n自旋锁与信号量对比如下：\n\n\n\n需求\n建议使用锁\n\n\n\n\n低开销加锁\n优先使用自旋锁\n\n\n短期锁定\n优先使用自旋锁\n\n\n长期锁定\n优先使用信号量\n\n\n中断上下文加锁\n自旋锁\n\n\n持有锁需要睡眠\n使用信号量\n\n\n\n完成变量\n信号量的简易方法。\n一个任务在等待完成变量，另一个进程在进行某种工作。当一个进程完成工作后，使用完成变量去唤醒在等待的任务，使两个任务得以同步。\nBKL: 大内核锁\n大内核锁本质上是一个全局自旋锁，但是它又不同于自旋锁，自旋锁是不可以递归获得锁的，因为那样会导致死锁。但大内核锁可以递归获得锁。大内核锁用于保护整个内核，而自旋锁用于保护非常特定的某一共享资源。同时持有 BLK 是也可睡眠。\n整个内核只有一个大内核锁，其实不难理解，内核只有一个，而大内核锁是保护整个内核的，当然有且只有一个就足够了。\n大内核锁是历史遗留，内核中用的非常少，一般保持该锁的时间较长，因此不提倡使用它。\n顺序锁\n内核 2.6 引入，类似于读者自旋锁，但是为写者赋予了较高的优先级，写者优先，读者正在读时允许写者写。\n禁止抢占\n内核是可抢占的，被抢占的进程可能处于临界区。\n解决方法：使用自旋锁作为非抢占区的标志，因为一个自旋锁被持有，内核便不能进行抢占。\n顺序和屏障\n当处理器和硬件交互时，时常需要确保一个给定的读操作发生在其他读或写操作之前。在多处理器上，可能需要按写数据的顺序读数据。但是编译器和处理器为了提高效率，可能对读和写重新排序。但是，处理器提供了机器指令来确保顺序，指示编译器不要对给定点周围的指令序列进行重新排序。这些确保顺序的指令称为屏障 (barrier)。\nrmb() 方法提供了一个 “读” 内存屏障，也就是说，在 rmb () 之前的读操作不会被重新排在该调用之后，同理，在 rmb () 之后的读操作不会被重新排在该调用之前。\nwmb() 方法提供了一个 “写” 内存屏障，这个函数的功能和 rmb () 类似，区别仅仅是它是针对写而非读。\n总结\n\n保证同步的最简单的方法，原子操作\n自旋锁，最常用的方式，轻量级，只能有一个保持者，忙等\n信号量，睡眠锁，适用稍少\n\n第十二章 内存管理\n连续分配\n连续分配是指为一个用户程序分配连续的内存空间。连续分配有单一连续存储管理和分区式储管理两种方式。\n单一连续存储管理\n在这种管理方式中，内存被分为两个区域：系统区和用户区。应用程序装入到用户区，可使用用户区全部空间。其特点是，最简单，适用于单用户、单任务的操作系统。CP／M 和\nDOS\n2．0 以下就是采用此种方式。这种方式的最大优点就是易于管理。但也存在着一些问题和不足之处，例如对要求内存空间少的程序，造成内存浪费；程序全部装入，使得很少使用的程序部分也占用 — 定数量的内存。\n伙伴系统\n分区式存储管理\n为了支持多道程序系统和分时系统，支持多个程序并发执行，引入了分区式存储管理。分区式存储管理是把内存分为一些大小相等或不等的分区，操作系统占用其中一个分区，其余的分区由应用程序使用，每个应用程序占用一个或几个分区。分区式存储管理虽然可以支持并发，但难以进行内存分区的共享。\n分区式存储管理引人了两个新的问题：内碎片和外碎片。内碎片是占用分区内未被利用的空间，外碎片是占用分区之间难以利用的空闲分区 (通常是小空闲分区)。\n为实现分区式存储管理，操作系统应维护的数据结构为分区表或分区链表。表中各表项一般包括每个分区的起始地址、大小及状态 (是否已分配)。分配方式有固定分区和动态分区。\n固定分区 (nxedpartitioning)\n固定式分区的特点是把内存划分为若干个固定大小的连续分区。分区大小可以相等：这种作法只适合于多个相同程序的并发执行 (处理多个类型相同的对象)。分区大小也可以不等：有多个小分区、适量的中等分区以及少量的大分区。根据程序的大小，分配当前空闲的、适当大小的分区。\n动态分区 (dynamic partitioning)\n动态分区的特点是动态创建分区：在装入程序时按其初始要求分配，或在其执行过程中通过系统调用进行分配或改变分区大小。\n与固定分区相比较其优点是：没有内碎片。但它却引入了另一种碎片 —— 外碎片。动态分区的分区分配就是寻找某个空闲分区，其大小需大于或等于程序的要求。若是大于要求，则将该分区分割成两个分区，其中一个分区为要求的大小并标记为 “占用”，而另一个分区为余下部分并标记为 “空闲”。分区分配的先后次序通常是从内存低端到高端。动态分区的分区释放过程中有一个要注意的问题是，将相邻的空闲分区合并成一个大的空闲分区。\n伙伴系统\n固定分区和动态分区方式都有不足之处。固定分区方式限制了活动进程的数目，当进程大小与空闲分区大小不匹配时，内存空间利用率很低。动态分区方式算法复杂，回收空闲分区时需要进行分区合并等，系统开销较大。伙伴系统方式是对以上两种内存方式的一种折衷方案。\n伙伴系统规定，无论已分配分区或空闲分区，其大小均为 2 的 k\n次幂，k 为整数， l≤k≤m，其中：\n2^1 表示分配的最小分区的大小， 2^m 表示分配的最大分区的大小，\n假设系统的可利用空间容量为 2^m 个字， 则系统开始运行时，\n整个内存区是一个大小为 2^m 的空闲分区。在系统运行过中，\n由于不断的划分，可能会形成若干个不连续的空闲分区，将这些空闲分区根据分区的大小进行分类，对于每一类具有相同大小的所有空闲分区，单独设立一个空闲分区双向链表。这样，不同大小的空闲分区形成了 k (0≤k≤m) 个空闲分区链表。\n当需要为进程分配一个长度为 n 的存储空间时:\n首先计算一个 i 值，使\n2^(i－1) &lt; n ≤ 2^i，然后在空闲分区大小为 2i 的空闲分区链表中查找。若找到，即把该空闲分区分配给进程。否则，表明长度为 2i 的空闲分区已经耗尽，则在分区大小为 2(i＋1) 的空闲分区链表中寻找。若存在 2(i＋1) 的一个空闲分区，则把该空闲分区分为相等的两个分区，这两个分区称为一对伙伴，其中的一个分区用于配，而把另一个加入分区大小为 2i 的空闲分区链表中。若大小为 2(i＋1) 的空闲分区也不存在，则需要查找大小为 2^(i＋2) 的空闲分区，\n若找到则对其进行两次分割，以此类推。\n在最坏的情况下，可能需要对 2^k 的空闲分区进行 k\n次分割才能得到所需分区。合并空闲内存的过程与分割的过程类似。\n离散分配\n前面的几种存储管理方法中，为进程分配的空间是连续的，使用的地址都是物理地址。如果允许将一个进程分散到许多不连续的空间，就可以避免内存紧缩 (将各个占用分区向内存一端移动，然后将各个空闲分区合并成为一个空闲分区)，减少碎片。基于这一思想，通过引入进程的逻辑地址，把进程地址空间与实际存储空间分离，增加存储管理的灵活性。地址空间和存储空间两个基本概念的定义如下：\n地址空间：将源程序经过编译后得到的目标程序，存在于它所限定的地址范围内，这个范围称为地址空间。地址空间是逻辑地址的集合。\n存储空间：指主存中一系列存储信息的物理单元的集合，这些单元的编号称为物理地址存储空间是物理地址的集合。\n根据分配时所采用的基本单位不同，可将离散分配的管理方式分为以下三种：\n页式存储管理、段式存储管理和段页式存储管理。其中段页式存储管理是前两种结合的产物。\n页式管理\n将程序的逻辑地址空间划分为固定大小的页 (page)，而物理内存划分为同样大小的页框 (page\nframe)。程序加载时，可将任意一页放人内存中任意一个页框，这些页框不必连续，从而实现了离散分配。\n该方法需要 CPU 的硬件支持，来实现逻辑地址和物理地址之间的映射。在页式存储管理方式中地址结构由两部构成，前一部分是页号，后一部分为页内地址 w（位移量）.\n页式管理方式的优点是：\n1）没有外碎片，每个内碎片不超过页的大小 2）一个程序不必连续存放。\n3）便于改变程序占用空间的大小 (主要指随着程序运行，动态生成的数据增多，所要求的地址空间相应增长)。\n缺点是：要求程序全部装入内存，没有足够的内存，程序就不能执行。\n每个进程有一个页表，用于完成逻辑页号 (本进程的地址空间) 到物理页面号 (实际内存空间，也叫块号) 的映射，如下图所示\n\n段式管理\n在段式存储管理中，将程序的地址空间划分为若干个段 (segment)，为每个段分配一个连续的分区，而进程中的各个段可以不连续地存放在内存的不同分区中。程序加载时，操作系统为所有段分配其所需内存，这些段不必连续，物理内存的管理采用动态分区的管理方法。\n段式存储管理也需要硬件支持，实现逻辑地址到物理地址的映射。\n程序通过分段划分为多个模块，如代码段、数据段、共享段，这样做的优点是：可以分别编写和编译源程序的一个文件，并且可以针对不同类型的段采取不同的保护，也可以按段为单位来进行共享。\n段式存储管理的优点是：没有内碎片，外碎片可以通过内存紧缩来消除；便于实现内存共享。缺点与页式存储管理的缺点相同，进程必须全部装入内存。\n类似页式管理的进程页表，段式管理中每个进程也有一张进程段表。\n页式管理 vs 段式管理\n页式和段式系统有许多相似之处。比如，两者都采用离散分配方式，且都通过地址映射机构来实现地址变换。但概念上两者也有很多区别，主要表现在：\n1)、需求：是信息的物理单位，分页是为了实现离散分配方式，以减少内存的碎片，提高内存的利用率。或者说，分页仅仅是由于系统管理的需要，而不是用户的需要。段是信息的逻辑单位，它含有一组其意义相对完整的信息。分段的目的是为了更好地满足用户的需要。因为一条指令或一个操作数可能会跨越两个页的分界处，而不会跨越两个段的分界处。\n2)、大小：页大小固定且由系统决定，把逻辑地址划分为页号和页内地址两部分，是由机器硬件实现的。段的长度不固定，且决定于用户所编写的程序，通常由编译系统在对源程序进行编译时根据信息的性质来划分。\n3)、逻辑地址表示：页式系统地址空间是一维的，即单一的线性地址空间，程序员只需利用一个标识符，即可表示一个地址。分段的作业地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址。\n4)、段比页大，因而段表比页表短，可以缩短查找时间，提高访问速度。\nlinux 中的内存管理\n页和区\nlinux\n采用上面提到的页式管理方法。MMU 以页为单位管理内存。对于 32 位，每个页的大小为\n4K；而对于 64 位而言每个页的大小为 8K。内核用 struct page 结构体表示每个物理页，它们组织在 mem_map 数组中\n内核把页划分在不同的区 (zone), 总共 3 个区，具体如下：\n\n\n\n区\n描述\n物理内存（MB）\n\n\n\n\nZONE_DMA\nDMA 使用的页\n &lt;16\n\n\nZONE_NORMAL\n 可正常寻址的页\n 16 ~896\n\n\nZONE_HIGHMEM\n 动态映射的页\n &gt;896\n\n\n\n执行 DMA 操作的内存必须从 ZONE_DMA 区分配\n一般内存，既可从 ZONE_DMA，也可从 ZONE_NORMAL 分配，但不能同时从两个区分配\n内存分配和释放\n页的分配与释放：页的分配通过\nalloc_pages 函数实现，而释放则通过__free_pages\n实现\n字节的分配与释放：字节的分配可通过 kmalloc，vmalloc 实现\n1）kmalloc void * kmalloc(size_t size, gfp_t flags)\n该函数返回的是一个指向内存块的指针，其内存块大小至少为size,所分配的内存在物理内存中连续且保持原有的数据 (不清零)，释放时通过 kfree 释放\n其中部分 flags 取值说明：\nGFP_USER： 用于用户空间的分配内存，可能休眠；\nGFP_KERNEL：用于内核空间的内存分配，可能休眠；\nGFP_ATOMIC：用于原子性的内存分配，不会休眠；典型原子性场景有中断处理程序，软中断，tasklet 等\n2）vmalloc void * vmalloc(unsigned long size)\n该函数返回的是一个指向内存块的指针，其内存块大小至少为size,所分配的内存是逻辑上连续的，物理上可能连续也可能不连续。\n与 kmalloc 不同，该函数没有 flags, 默认是可以休眠的。\nSlab 分配器\nslab 分配器是一种策略，用于缓存内核对象，其主要工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而 slab 分配器是基于对象进行管理的，相同类型的对象归为一类 (如进程描述符就是一类)，每当要申请这样一个对象，slab 分配器就从存储某一类对象的高速缓存组中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统。slab 分配对象时，会使用最近释放的对象内存块，因此其驻留在 CPU 高速缓存的概率较高。\n其中高速缓存 (cache),slab 和 对象的关系如下： Cache: 存储某种类型的对象.Slab: 包含有缓存的对象（由cache划分出来）Object: 经常使用的数据结构，例如inode \nslab 有三种状态 Full：没有自由的对象\nPartial：部分自由，先从这里分配 (减少了页的分配和释放)，再找 empty 的，如果两者都找不到，创建一个新的 slab\nEmpty：含有未分配的对象\n选择哪种方法分配\n\n\n\n频繁分配和释放\n Slab 分配器.\n\n\n\n\n 需要以页为单位分配内存\nalloc_pages()\n\n\n从高端内存分配\nalloc_pages()\n\n\n默认\nkmalloc()\n\n\n不需要连续的页\nvmalloc()\n\n\n\n第十五章 进程地址空间\n基本概念\n进程地址空间指进程能够使用的地址范围，每一个进程看到的是一个不同的线性地址，一个进程使用的地址与另一个进程使用的地址无关。内核会通过增加或删除线性地址中的区域，动态修改进程地址空间。\n进程地址空间由进程可寻址的虚拟内存组成，linux 采取的虚拟内存技术使得所有进程以虚拟方式共享内存。对于某个进程，它好像可以访问所以物理内存，而且它的地址空间可以远远大于物理内存.\n进程只能访问有效区域中的内存地址。如果进程访问的内存地址不再有效内存区域，或者以非法的方式访问有效区域，内核会杀掉这个进程并输出一个 “Segmentation Fault”\n信息\n内存描述符\n内核用一个称之为内存描述符的数据结构（mm_struct）表示一个进程的地址空间。mm_struct 部分组元素如下：\n\nmmap 和 mm_rb 字段是不同的数据结构，但含有同样的东西，即地址空间中的所有内存区域\nmm_users 字段表示使用这个地址空间的进程数目\nmmlist 链表将所有 mm_struct 连接在一起\n\n内存描述符的分配与释放\ncopy_mm() 函数用于在\nfork() 时复制父进程的内存描述符（使用 vfork() 的时候进程与子进程共享地址空间），当与指定的地址空间相关联的进程结束时，会调用 exit_mm()\n函数\n进程的内存区域\n进程的地址空间可划分为不同的区域，应用在不同的场景，如下就是常见的几种内存区域：\n1）文本段（text section），存放可执行文件的操作指令，也就是可执行文件的代码的内存映像，包含一些字符串、常量和只读数据\n2）数据段（data section），数据段用来存放可执行文件中已初始化全局变量，也就是存放程序静态变量和全局变量\n3）bss section，未初始化全局变量的内存映像\n4）堆（heap），堆是用于存放进程运行中被动态分配的内存区域，它的大小并不固定，可动态扩张或缩减。当进程调用 malloc 等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用 free 等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）\n5）栈（stack），栈是用户存放程序临时创建的局部变量，除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中\n每个内存区域均由以下参数刻画 1）起始地址 2）长度 3）访问权利\n内存区域由内存区域对象表示，存于 vm_area_struct\n结构，也叫 VMA，描述给定的地址空间上的一个独立的内存区域。VMA\n结构能够表示上面提到的多种类型的内存区域。\ntask_struct，mm_struct,\nvm_area_struct 的关系如下所示\n\nVMA 中还有 VMA 标志，用于指定内存区域中页的行为或信息，各标志及其含义如下\nVM_READ：页可读 VM_WRITE：页可写\nVM_EXEC：页可执行 对于代码:\n可被映射为 VM_READ 和 VM_EXEC，但不能映射为 VM_WRITE\n对于数据：可被映射为 VM_READ 和 VM_WRITE，但不能映射为 VM_EXEC\nLinux 的分页\n下面先以 32 位的系统为例讲述原始的分页\n每个地址可以通过一个 32 位的整数描述，其中整数的 32 为分配如下\n\n\n页目录包含 1024（2^10）个页表\n页表描述 1024（2^10）个页\n每页大小 4 KB（2^12） (PAGE_SIZE)\n1024 * 1204 * 4KB = 4GB\nCR3 (在 task_struct 的 TSS) 含有页目录的物理基地址\n\n寻址时利用线性地址的低 2231 位从页目录找到对应的页表，然后利用线性地址的低 1221 为从页表找到对应的页，最后用低 12 位从页中找到最终地址。过程如下所示：\n\n上面是原始的两级分页，但是 Linux 使用 3 级分页，\n顶层页表是 page global directory (PGD)，二级页表是 page middle directory (PMD)，最后一级是\nPTE，整体如下所示\n\n与 task_struct 等关系如下图所示 \n第十三章 文件系统\next2 文件系统\next2 于 1993 年创建，是 ext 的改进版本，具有如下的特点：\n\n支持 UNIX 所有标准的文件特征\n可管理大硬盘，一个分区的容量最大可达 4TB\n 它使用变长的目录项\n，支持 255 个字符的长文件名，可扩展到 1012 个字符\n使用位图来管理数据块和节点的使用情况\n使用了块组的概念，从而使数据的读和写更快，更有效，也使系统变得更安全可靠\n\n文件系统结构\n整个 ext2 文件系统结构如下所示：\n\n文件系统中存储的最小单位是块（\nBlock），一个块究竟多大是在格式化时确定的，例如 mke2fs 的\n-b 选项可以设定块大小为 1024、 2048 或\n4096 字节。而上图中引导块（Boot Block）的大小是确定的，就是\n1KB，引导块是由\nPC 标准规定的，用来存储磁盘分区信息和启动信息，任何文件系统都不能使用启动块。\n启动块之后才是\next2 文件系统的开始，ext2 将磁盘分区看成是由块组组成，每个块组包含一个或多个块。每个块组大小相同，顺序存放，且每个块组都由以下部分组成。\n1）超级块 (Super\nBlock)：描述整个分区的文件系统信息，例如块大小、文件系统版本号、上次\nmount 的时间等等。超级块在每个块组的开头都有一份拷贝。\n2）组描述符 (Group Descriptor\nTable)：由很多块组描述符组成，整个分区分成多少个块组就对应有多少个块组描述符。每个块组描述符存储一个块组的描述信息，例如在这个块组中从哪里开始是\ninode 表，从哪里开始是数据块，空闲的 inode\n和数据块还有多少个等等。和超级块类似，块组描述符表在每个块组的开头也都有一份拷贝，这些信息是非常重要的，一旦超级块意外损坏就会丢失整个分区的数据，一旦块组描述符意外损坏就会丢失整个块组的数据，因此它们都有多份拷贝。\n3）块位图 (Block\nBitmap)。块位图就是用来描述整个块组中哪些块是空闲可用的，它本身占一个块，其中的每个\nbit 代表本块组中的一个块，这个 bit 为 1 表示该块已用，这个 bit 为\n0 表示该块空闲可用。\n与此相联系的另一个问题是：在格式化一个分区时究竟会划出多少个块组呢？主要的限制在于块位图本身必须只占一个块。用\nmke2fs 格式化时默认块大小是 1024 字节，可以用\n-b 参数指定块大小，现在设块大小指定为 b 字节，那么一个块可以有 8b 个\nbit，这样大小的一个块位图就可以表示\n8b 个块的占用情况，因此一个块组最多可以有 8b 个块，如果整个分区有\ns 个块，那么就可以有 s/(8b) 个块组。格式化时可以用\n-g 参数指定一个块组有多少个块，但是通常不需要手动指定，\nmke2fs 工具会计算出最优的数值。\n4）索引节点位图 (inode\nBitmap)。和块位图类似，本身占一个块，其中每个 bit 表示一个\ninode 是否空闲可用。\n5）索引接点表 (inode\ntable)。一个文件除了数据需要存储之外，一些描述信息也需要存储，例如文件类型（常规、目录、符号链接等），权限，文件大小，创建 /\n修改 / 访问时间等，也就是\nls -l 命令看到的那些信息，这些信息存在\ninode 中而不是数据块中。每个文件都有一个 inode，一个块组中的所有\ninode 组成了 inode 表。\ninode 表占多少个块在格式化时就要决定并写入块组描述符中，\nmke2fs 格式化工具的默认策略是一个块组有多少个 8KB 就分配多少个\ninode。由于数据块占了整个块组的绝大部分，也可以近似认为数据块有多少个\n8KB 就分配多少个 inode，换句话说，如果平均每个文件的大小是\n8KB，当分区存满的时候\ninode 表会得到比较充分的利用，数据块也不浪费。如果这个分区存的都是很大的文件（比如电影），则数据块用完的时候\ninode 会有一些浪费，如果这个分区存的都是很小的文件（比如源代码），则有可能数据块还没用完\ninode 就已经用完了，数据块可能有很大的浪费。如果用户在格式化时能够对这个分区以后要存储的文件大小做一个预测，也可以用\nmke2fs 的 -i 参数手动指定每多少个字节分配一个 inode。\n6）数据块 (Data\nBlock)。数据块用来存储文件的具体内容，在 linux 中文件类型有以下几种\n\n普通文件\n目录\n符号连接\n字符设备特殊文件\n块设备特殊文件\n命名管道\n Socket\n\n于普通文件，文件的数据存储在数据块中。\n对于目录，该目录下的所有文件名及其 inode 存储在数据块中，除文件名之外，\nls -l 命令看到的其它信息都保存在该文件的 inode 中。注意这个概念：目录也是一种文件，是一种特殊类型的文件。\n对于符号链接，如果目标路径名较短则直接保存在\ninode 中以便更快地查找，如果目标路径名较长则分配一个数据块来保存。\n设备文件、FIFO 和 socket\n等特殊文件没有数据块，即文件大小为 0，设备文件的主设备号和次设备号保存在\ninode 中。\n文件查找\n文件查找的流程：\n1）从当前进程的根目录项中（current→ fs → root）找到它的根目录的 inode 编号\n2）根据该编号和超级块的 s_inodes_per_group，计算出该 inode 所在的块组\n3）查找该块组中的 inode 表，找到描述根目录文件的 inode\n4）根据该 inode 的描述，读取其数据块（如果是文件）或得到目录项列表（如果是目录，然后返回步骤（2）直到读到最终的文件）\next2 的内存数据结构\n为提高效率，尽量减少访问磁盘次数，在安装 Ext2 分区时，内存中存放着部分磁盘数据结构，并使用缓存技术保持磁盘数据结构更新。\n缓存模式共有 4 种\n\nAlways cached：一直缓存\nFixed limit：固定数量的数据结构保存在缓存中\nDynamic：只要索引节点或块使用，相关数据就保存在缓存中\nNever：任何高速缓存中都不保存\n\next2 中的数据结构及其缓存情况如下所示\n\n文件系统的创建\n创建 Ext2 文件系统实际上就是建立 Ext2 文件系统的磁盘数据结构与内存数据结构，在 linux 中通过 mke2fs 程序实现，这个程序实际上执行了以下两个步骤：\n1）格式化 2）创建文件系统\next3 与 ext4\next2 文件系统的下一个版本是 ext3 文件系统，它和 ext2\n文件系统在硬盘布局上是一样的，其差别仅仅是 ext3\n文件系统在硬盘上多出了一个特殊的\ninode（可以理解为一个特殊文件），用来记录文件系统的日志，也即所谓的\njournal。Ext4 支持更大的文件和无限量的子目录。\n虚拟文件系统（VFS）\n基本概念\n为支持各种文件系统，Linux 内核在用户进程（或 C 标准库）和具体的文件系统之间引入了一个抽象层，使得文件、目录、读写访问等概念成为抽象层的概念，因此各种文件系统看起来用起来都一样，该抽象层称之为 “虚拟文件系统（VFS）”。类似于类和对象的关系，其中 VFS 是类，各种文件系统是具体的对象。\nVFS 一方面提供一种操作文件、目录及其他对象的统一方法，使用户进程不必知道文件系统的细节。另一方面，VFS 提供的各种方法必须和具体文件系统的实现达成一种妥协，毕竟对几十种文件系统类型进行统一管理并不是件容易的事。\n为此，VFS 中定义了一个通用文件模型，以支持文件系统中对象（或文件）的统一视图。\nLinux 对 Ext 文件系统族的支持是最好的，因为 VFS 抽象层的组织与 Ext 文件系统类似，这样在处理 Ext 文件系统时可以提高性能，因为在 Ext 和 VFS 之间转换几乎不会损失时间。\ntask_struct 结构中 VFS 相关的字段\n\nfs – 包括 root, pwd, 指向 dentrie 的指针\n files – 文件描述符矩阵 fd [\n], 每一个元素指向打开文件对象的指针\n\n主要的数据结构\n1）超级块 对于每个已经挂载的文件系统，VFS\n在内核中都生成一个超级块结构（struct super_block 实例），超级块代表一个已经安装的文件系统，用于存储文件系统的控制信息，例如文件系统类型、大小、所有 inode 对象、脏的 inode 链表等。\n超级块相关的文件系统操作为读、写、清除和删除 inode。\n2）inode\nVFS 处理文件的关键是 inode，每个文件或目录都有且只有一个对应的 inode（struct inode 实例），其中包含元数据 (权限，拥有者，时间信息，大小，连接计数\n) 和指向文件数据的指针，但 inode 并不包含文件名。系统中所有的 inode 都有一个特定的编号，用于唯一的标识各个 inode。文件名可以随时更改，但是索引节点对文件是唯一的，并且随文件的存在而存在。\ninode 和 super\nblock 在存储介质中都是有实际映射的，即存储介质中也存在超级块和 inode。但是由于不同类型的文件系统差异，超级块和 inode 的结构不尽相同。而 VFS 的作用就是通过具体的设备驱动获得某个文件系统中的超级块和 inode 节点，然后将其中的信息填充到内核中的 struct super_block 和 struct inode 中，以此来试图对不同文件系统进行统一管理。\ninode 的相关操作包括： create – 创建一个普通文件\nlink/unlink/rename – 增加、删除、修改目录内容\nsymlink, readlink, follow_link – 软连接操作\nmkdir/rmdir – 创建目录文件 mknod –\n创建设备文件 truncate – 修改文件大小\npermission – 检查访问权限\n3）dentry\nVFS 把目录当做文件对待，为了方便路径查找，VFS 引入了目录项的概念，每个目录项代表路径中的一个特定部分如 (/bin/vi 中包含了 /,bin 和 vi 三个目录项目对象)。目录项对象通过\ndentry 结构体表示。dentry 结构的主要用途就是建立文件名和相关的\ninode 之间的联系。\n目录项有三种有效状态\n\nused: 表示该目录项对应一个有效的索引节点，且该对象正在被使用\n unused:\n表示该目录项对应一个有效的索引节点，但是该对象没有被使用\n negative: 表示该目录项没有对应的有效索引节点\n\n由于块设备速度较慢（于内存而言），可能需要很长时间才能找到与一个文件名关联的 inode。Linux 使用目录项缓存来快速访问此前的查找操作结果。在 VFS 读取了一个目录或文件的数据之后，则创建一个\ndentry\n实例（struct dentry），以缓存找到的数据。Dentry 缓存通过哈希表访问，因此时间较快。\n4）打开的文件对象\n文件对象主要用于关联文件和进程，在打开文件的时候创建，主要描述一个打开文件的相关信息，包括当前的读写指针等，通过 struct file 结构体表示。\n在 task_struct 中有一个数组，数组中的每一个元素都是指向一个打开的文件对象，这个数组就称为文件描述符数组。\n上面提到的数据结构之间的关系如下所示 \n共享数据结构的情况：\n\n在不同的目录上挂载同一个文件系统 ：共享超级块\n通过不同的硬连接打开同一个文件：共享 inodes\n 打开同一个文件两次：共享 dentries\n 调用 dup ()：共享打开文件对象，例如:\n2&gt;&amp;1\n\n参考： task_struct 结构体字段介绍 --Linux 中的 PCB\nLinux\n进程状态说明 Linux\n线程实现机制分析 Linux 进程管理 ——fork () 和写时复制\nlinux 中断处理的上半部和下半部\n死锁的产生、预防和避免\nlinux 内存管理总结之进程地址空间\nExt2 文件系统布局，文件数据块寻址，VFS 虚拟文件系统\nLinux\n虚拟文件系统（VFS）介绍\n","categories":["操作系统"],"tags":["操作系统"]},{"title":"文本分类中的一些经验和 tricks","url":"/2019/02/06/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%E5%92%8C%20tricks/","content":"最近在总结之前做的文本分类实验的一些经验和\ntricks，同时也参考了网上的一些相关资料 (见文末)，其中有些 tricks\n没尝试过，先在这里记下，或者日后能用上。\n\n这里的经验和 tricks\n大概可分为两部分：预处理部分和模型训练部分，下面分别介绍\n预处理\n\n文本更正，主要是将文本标准化，包括繁体转简体，全角转半角，拼音纠错等\n文本泛化，如一个手机号码，因为有几千万的手机号码，不可能为每个手机号码设一个特征，所以最好将手机号码转化为同一个特征；另外表情符号、人名、地址、网址、命名实体等也要考虑这种泛化，泛化的程度这个视具体的任务，比如说地址可以以国家为粒度，也可以以省份为粒度\n规范文本为统一长度时，取所有长度的均值或者中位数，但是别取最大值；截断时根据具体任务考虑从前面阶段或从后面截断\n构建数据集的 vocabulary 时，需要考虑以下几个方面\n\n取前 N 个高频词或者过滤掉出现次数小于某个阈值的词\n根据具体任务确定是否需要去掉 stop words\n 假如采用了预训练的词向量，要尽可能让 vocabulary\n中的词语能找到对应的词向量 (这个问题也涉及到预训练的词向量和分词器的选择)\n\n 词向量的选择，当数据集较小时，直接使用预训练好的词向量（如 google、facebook 开源的），且不用微调；当训练集比较大的时候，可随机初始化进行训练，也可以对预训练的词向量进行微调（微调收敛得更快，但是结果差异不大）\n分词时考虑以下几个方面\n\n是否需要分词，使用 char-level 的方法时不需要分词，但是在很多场景下\nword-level 的效果都要比 char-level 的要好\n分词时可以只保留长度大于 1 的词 (会去除很多停止词)，对结果精度没什么影响，但是可以有效降低特征维度\n采用预训练的词向量时，要保证分词后的大部分词语能够出现在预训练的词向量表中，否则这个词语的\nembedding\n就相当于是随机初始化，预训练的词向量没有提供任何信息；具体方法可参考这里\n\n数据增强\n\n常见的方法有：drop (随机删掉文本)、shuffle (随机改变文本顺序)、replace (用近义词进行替换)\n 数据增强对 word-level 的方法有一定的提升，但是对于 char-level\n的方法效果不一定好，甚至会起到反作用\n\n\n模型训练\n\n规则有时能解决大部分的问题，不一定要用到模型，使用时要权衡模型带来的收益和复杂性\n传统的机器学习方法根据其特征工程的不同可分为三大类\n\n词袋模型：将出现的词记为 1，否则记为 0，问题是维度高且稀疏性严重\n向量空间模型：根据文档频率、互信息、信息增益、χ² 统计量等进行了特征 (词语) 的选择，同时通过\ntfidf\n值为每个词赋权重；一定程度上缓解了上面提到的词袋模型维度高且稀疏性严重的问题\n主题模型：pLSA/LDA/HDP\n等主题模型将文本表示低维实数向量，类似于深度学习中的 embedding，但是比\nembedding 有更好的解释性\n\n fasttext 简单、速度快，是一个非常不错的\nbaseline；随着问题复杂性增加可依次尝试 CNN -&gt; RNN -&gt; BERT\n 对于深度学习模型，把模型变得更深更宽更复杂往往能够提升效果；但是当模型复杂到一定程度的时候，提升的效果微乎其微甚至没提升\n rnn 类模型用双向一般会比单向要好\n使用 dropout (经验值为 0.5)\n基本都能提升效果，使用的地方包括：embedding 层后、FC 层后\n训练震荡问题：增加随机采样因素尽可能使得数据分布 iid，默认\nshuffle\n机制能使得训练结果更稳定。如果训练模型仍然很震荡，可以考虑调整学习率 或\nmini_batch_size\n采用预训练的 embedding 并进行 finetune 时，在最开始 embedding\n的学习率设为 0，训练到效果较好时才开始 finetune embedding 层\n学习率的设置考虑以下几个方面\n\n经验值一般为 1、0.1、0.01、0.001,\n一般从 1 开始尝试。很少见学习率大于 10 的\n学习率一般要随着训练进行衰减，衰减系数一般是 0.5；衰减时机可以是验证集准确率不再上升，或固定训练\nN 个 epoch 后\n比起自定义的衰减方法，更便捷的方法是使用自适应梯度的办法，例如\nadam,adadelta,rmsprop\n等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率\n对 RNN 来说，如果要处理的序列比较长，或者 RNN 层数比较多，那么 learning\nrate 一般小一些比较好，否则有可能出现结果不收敛，甚至 Nan 等问题。\n\n超参数的设置经验可参考 A Sensitivity Analysis of (and\nPractitioners' Guide to) Convolutional Neural Networks for Sentence\nClassification\n模型融合时，差异性越大，融合效果越好，具体可参这里\n\n\n参考\n\n在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的 tricks？\n知乎看山杯夺冠记\n用深度学习（CNN RNN\nAttention）解决大规模文本分类问题 - 综述和实践\n深度学习网络调参技巧\n\n","categories":["NLP"],"tags":["深度学习","NLP"]},{"title":"文本文件和二进制文件","url":"/2015/11/26/%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6/","content":"文本文件和二进制文件的定义\n　　首先，计算机的存储在物理上是二进制的，也就是在物理存储方面没有区别都是 01 码。所以文本文件与二进制文件的区别并不是物理上的，而是逻辑上的，也就是编码上。简单来说，文本文件是基于字符编码的文件，常见的编码有 ASCII 编码，UNICODE 编码等等。二进制文件是基于值编码的文件，你可以根据具体应用，指定某个值是什么意思（这样一个过程，可以看作是自定义编码。\n\n文本文件与二进制文件的存取\n　　文本工具打开一个文件的过程是怎样的呢？拿记事本来说，它首先读取文件物理上所对应的二进制比特流，然后按照你所选择的解码方式来解释这个流，然后将解释结果显示出来。一般来说，你选取的解码方式会是 ASCII 码形式（ASCII 码的一个字符是８个比特），接下来，它 8 个比特 8 个比特地来解释这个文件流。例如对于这么一个文件流 \"01000000_01000001_01000010_01000011\"(下划线 ''_''，为了增强可读性手动添加的)，第一个 8 比特 ''01000000'' 按 ASCII 码来解码的话，所对应的字符是字符 ''A''，同理其它 3 个 8 比特可分别解码为 ''BCD''，即这个文件流可解释成 “ABCD”，然后记事本就将这个 “ABCD” 显示在屏幕上。\n　　文本文件格式存储时是将值作为字符然后存入其字符编码的二进制，文本文件用‘字符’作为单位来表示和存储数据，比如对于 1 这个值，文本文件会将其看做字符‘1’然后保存其 ASCII 编码值（这里假定是 ASCII 编码），这样在物理上就是 0x31 这个二进制值，而若是二进制保存 1，则直接保存其二进制值，比如如果程序中是处理 1 为整数则保存的二进制值就是\n0x00000001 (4 字节）。\n　　假如文件存储的编码与读取的编码不同，那么就无法呈现文章原来的信息，例如用记事本打开文本文件会乱码，用音乐播放器无法打开视频文件。\n总结\n　　综上，可以知道文本文件与二进制文件就是编码方式不一样而已，而这个是用户行为，把一个数据以什么样的编码（字符还是值本身）存入文件是由用户主动选择的，也就是写入的接口选择，如果以二进制接口方式写入文件那么就是一个二进制文件，如果以字符方式写入文件就是一个文本文件了。既然有写入时候的编码也就会有读出的编码，只有两个编码对应才能读出正确的结果，如用记事本打开一个二进制文件会呈现乱码的，这里稍微提一下后缀名，后缀名并不能确定其是否就是文本文件，二进制文件也可以是 txt 后缀名，后缀名只是用来关联打开程序，给用户做备注用的，与文件的具体编码没有关系。\n　　最后文本文件和二进制文件主要是 windows 下的概念，UNIX/Linux 并没有区分这两种文件，他们对所有文件一视同仁，将所有文件都看成二进制文件。\n","categories":["操作系统"],"tags":["操作系统"]},{"title":"有价值的数据应该如何交易","url":"/2017/06/16/%E6%9C%89%E4%BB%B7%E5%80%BC%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E4%BA%A4%E6%98%93/","content":"本文的内容主要来源于该知乎\nlive，主要介绍了哪些行为数据是有价值的，以及广告领域中数据是如何交易的，最后还讨论了数据隐私的问题。\n\n有价值的数据分类\n数据有价值密度之分，并不是说数据量越多就一定越有效\n这里的数据主要指的是用户的行为数据和用户的标识数据，而有价值的用户行为数据主要有:\n决策行为、主动行为、半主动行为、被动行为，这些行为的价值递减，但是数据的量递增，因为有价值的数据一般量都不大\n决策行为\n决策行为对应着转化 (conversion) 或预转化 (pre-conversion),\n也就是购买了商品或将商品加入了购物车。这些行为对应着非常明确的用户兴趣，价值也非常高\n主动行为\n主动行为对应着搜索（search）、广告点击（Ad\nClick)、搜索点击（Search\nClick），这种行为表明了用户已经有了明确的意图，但是最终决定还不清楚，价值也很高。\n需要注意的是，这些行为里面往往会有作弊的流量在里面，需要去除掉。\n半主动行为\n半主动行为对应着分享（share），网页浏览（page\nview), 这种数据的量最大，用户意图较弱，因为用户可能只是随意在浏览，这些数据也有一定价值。\n被动行为\n被动行为是强加给用户的行为，如广告的浏览（注意不是点击，而是强推给用户浏览），这种行为甚至会有负面作用，价值基本可以忽略\n社交关系\n社交关系指的是不直接利用用户的行为数据（有可能是用户的行为数据过于稀疏），而是利用与其在社交网络（微博、Facebook\n等）上有关联的的用户的信息进行定向。\n这种方法在某个人的行为不足而无法进行精准的行为定向时有效。\n用户 ID\n用户 ID\n，也就是用户标示，是最重要的数据，因为所有的行为数据有效的前提是需要先确认这些行为数据是属于哪个用户的，标识一个用户的\nID 在不同的环境下有不同的方法，下面是常见的场景和方法\n\nweb/wap 环境：使用\ncookie，生命周期短（1~2 周），存续性差，但是跨域名的时候需要映射\n ios 应用：使用 IDFA（ID For Advertiser)，存续性好于 cookie，但 ios10\n有更严格的政策\n安卓应用：使用 Android ID，存续好于 IDFA；有些也使用\nIMEI（手机标识），但是 Google Play 上是不给用的\n无以上 ID 场景：使用 FingerPrint（IP+UserAgent -&gt; hash），存在\nhttp 头中，可作缺省标识；\n但是在移动端使用效果不是很好，因为几乎每个应用都有一个内置的浏览器\n\n三方数据划分\n下面以广告中用到的用户数据为例讲述三方数据的划分，在广告中根据数据来源的不同可以将数据划分为第一方数据，第二方数据和第三方数据。\n如下图所示，第一方和第二方分别是指广告主和广告平台，而不直接参与广告交易的其他数据提供方统称为第三方。\n\n\n三方数据\n\n数据管理平台 (DMP)\n第一方数据的收集和加工是广告市场上非常重要的环节，不过对于没有这方面技术积累的广告主而言，专门设团队进行数据加工是没有必要的，因此市场上出现了\n数据管理平台（DMP), 专门从事此业务，而 DMP 又可划分为第一方 DMP 和第三方\nDMP。\n第一方 DMP\n第一方 DMP\n的目的是对广告主提供的第一方数据（也可结合公开市场第三方数据）进行加工，进而得到广告主指定的用户标签，用于支持网站业务运营和广告投放。\n需要注意的是第一方 DMP\n只能加工第一方数据，而不能使用第一方的数据，也就是不能把数据进行售卖（除非与广告主达成协议）\n因此，第一方的 DMP 的商业模式如下\n\n\n第一方 DMP 商业模式\n\n第三方 DMP\n对于中小网站，其规模不大，没有利用数据的能力，只是单纯想将数据卖给需要数据的广告主，同时也没有加工数据的能力，因此产生了满足中小网站的这项需求的第三方\nDMP。\n第三方 DMP 与第一方 DMP\n的一个不同点在于服务对象的不同，另外一个不同点则是两者的加工标签的逻辑不一样，第一方\nDMP 是根据广告主的需求进行标签的加工，而第三方 DMP 则是根据 DMP\n其自己的逻辑进行加工然后售卖。\n第三方的 DMP 的商业模式如下所示\n\n\n第三方 DMP 的商业模式\n\n数据的交易\n上面提到的 DMP\n的一个重要功能就是售卖标签，实际上就是一种数据交易，这些标签一般售卖的对象是广告主，而广告主往往由于缺乏相应的技术而将手中定向委托给其他平台也就是\nDSP（Demand Side Platform），因此交易发生在 DMP 和 DSP 之间。\n同时由于往往存在着多个 DMP 和多个 DSP，假如 DMP 和 DSP\n间都要一一连接的话，那么通信的代价会非常大，因此在实际中往往是通过广告交易平台也就是\nADX（AD Exchange)\n将两者联系起来，从而降低通信代价。整个数据交易的过程如下所示\n\n\n数据交易怎么做\n\n通过 ADX 进行 DMP 和 DSP 间的通信避免了 DMP 和 DSP\n直接通信的开销，因为实时竞价的时候 ADX 本来就要跟 DSP\n发生通信，因此没有增加二次通信。\n上面简单提到数据交易时的收费是按照实际的广告展示次数付费的，目前来说这种市场化的定价方式是唯一的选择，这种方式并没有限制数据供给次数，直觉上似乎是利润最大化的。\n但是这有可能间接地抬高了流量价格，而低估了数据价格。因为不限量地售卖标签，会导致竞价同一次展示的广告主的数目增加，因为有了标签，各个广告主能够更精准地定向到更多用户，因此更多的广告主的竞价抬高了流量的价格，\n而假如广告主的预算是一定的情况下，购买流量需要更多的钱，因此用于购买数据支出会变少。当然这只是宏观上的探讨，目前业界对此并没有一套完善的理论来指导。\n如果采用限量的售卖，那就要采用竞价的方式，而有了竞价，整个市场的活跃程度和价值会最大化。\n数据隐私\n在数据交易过程中不可避免地会设计到数据隐私的问题。针对数据隐私，欧盟负责隐私保护条例指定的委员会\nA29 制定了以下相关原则\n\nPersonal Identifiable\nInformation（PII）不能使用，PII 指的是可以主动接触到用户的信息，比如手机号、QQ 号、微信号、e-mail 等都不能使用\n用户可以要求系统停止记录和使用自己的行为数据，比如说网站会在网页上说明收集到的用户数据的作用，同时可让用户选择是否允许收集其数据，实际上对商业影响非常小，因为选择不允许收集的用户比例不大\n不能长期保存和使用用户的行为数据，实际上数据也具有时效性，时间太久远的数据基本上无价值\n\n另外一个数据隐私问题就是稀疏的行为数据带来的挑战，一个典型的例子就是\nNetflix\n推荐大赛中，有人从数据集里面发现了自己的同时是同性恋，原因是数据的稀疏性使得个人的行为数据更加容易被熟悉这个人的其他人所辨识。\n目前对于这个领域相关的研究课题是差分隐私（differential\nprivacy）。\n","categories":["计算广告"],"tags":["计算广告"]},{"title":"搜狗、百度、QQ 输入法的词库爬虫","url":"/2016/03/27/%E6%90%9C%E7%8B%97%E3%80%81%E7%99%BE%E5%BA%A6%E3%80%81QQ%E8%BE%93%E5%85%A5%E6%B3%95%E7%9A%84%E8%AF%8D%E5%BA%93%E7%88%AC%E8%99%AB/","content":"本文主要讲述了通过 python\n实现的用于下载搜狗、百度、QQ 三个输入法的词库的爬虫的实现原理。主要利用了 python 自带的 urllib2、Queue、re、threading 模块，并分别通过单线程和多线程实现。最后会给出完整的源码地址。\n\n原理及注意事项\n爬取每个输入法的实现原理都一样，步骤如下：\n（1）获取输入法词库的分类\n（2）下载各个分类并按分类在本地存储\n其中（1）实现的关键点是正则表达式提取网页中的词库分类，（2）实现的关键点是通过广度优先搜索（bfs）来遍历某一类别的所有页面。\n正则表达式提取词库分类\n步骤（1）通过解释词库分类的网页源码来获取具体分类（以搜狗输入法为例，词库分类的网页为 http://pinyin.sogou.com/dict/\n)。先人工观察网页源码的结构，一般同一级的分类的在源码中的\nhref(超链接) 的结构都是一样的，仅仅是 id 或名字不同，这就可以通过正则表达式来提取网页中的分类，并用嵌套的字典来存储多级分类。\nBFS 遍历某一分类的所有页面\n步骤（1）提供了词库的分类，步骤（2）需要考虑的就是怎么下载某一类词库。因为词库文件的数量较多，所以即使是某一类的词库往往也会分多个页面来展示（常见的如通过点击上一页、下一页跳转），所以要完整下载这一类的词库必须遍历这一类词库的所有页面。\n这里采用 BFS 来实现，实现步骤如下：\n1）先通过正则表达式分析当前访问页面的源码，获取当前页面可以跳转到的其他页面的 url，然后将这些 URL 放入到队列中作为待访问的 URL\n2）通过正则表达式获取分析当前访问的页面的源码，获取可以下载的词库文件的 url 存入一个临时的列表（List）中，并开始逐一下载各个文件\n3）将当前页面 url 放入到一个集合（set）中，标记为已访问的 url，防止下一次重复访问\n4）从队列中取出下一个 url，到步骤 3）的集合中检查 url 是否被访问，如果被访问过则继续取下一个，否则将这个 url 设为当前 url 并回到步骤 1）\n按照上面的步骤一直循环执行直到队列为空，这样便可下载了某一类的词库。如法炮制，便可下载所有类的词库了。\n这里需要注意的是，在下载词库文件的时候有可能会出现防盗链的问题。简单来说就是下载词库文件的 http 请求头中的来源（Referer）不属于本站的就拒绝下载，返回 403\nforbidden。这种设计就是为了防止爬虫的整站下载，在爬取过程中发现搜狗输入法中有防盗链，而百度、QQ 输入法则没有。解决方法也很简单，就是在下载文件是构造一个 http 请求头（headers），设置里面的 Refererr 为该站的任一 url 即可。\n单线程下载\n单线程下载的注意事项上面基本提到了，下面贴出 QQ 输入法单线程下载的关键代码，注意这个代码直接执行会报错，可执行的完整代码见文末。\n# 下载某一类别的关键代码，具体代码见文末链接      queue =Queue.Queue()  # 存放待访问url      visited = set()       # 已经访问过的url      downloaded = set()    # 已经下载过的文件      firstURL = ''         # url入口      queue.put(firstURL)      # bfs遍历直至队列为空      while not queue.empty():          currentURL = queue.get()          if currentURL in visited:              continue          else:              visited.add(currentURL)        try:              response = urllib2.urlopen(currentURL)          except :        # 找到链接到其他页面的连接          data = response.read()          pagePattern = re.compile('&amp;page=(\\d+)\"')          pageList = re.findall(pagePattern,data)          for i in pageList:              pageURL = smallCateURL+'&amp;page='+i              queue.put(pageURL)        # 下载当前页面存在的文件          filePattern = re.compile('&lt;a href=\"/dict_detail\\?dict_id=(\\d+)\"&gt;(.*?)&lt;/a&gt;')          fileList = re.findall(filePattern,data)          for id, name in fileList:               fileURL = 'http://dict.qq.pinyin.cn/download?dict_id='+id               filePath = cateDir.decode('utf8')+'/'+name.decode('gbk')+'.qpyd'               # 文件已存在则不下载               if fileURL in downloaded:                   continue               else:                   downloaded.add(fileURL)                   downloadSingleFile(fileURL) #下载这个文件  \n多线程下载\n单线程虽然能够下载词库文件，但是消耗的时间过长，这时候可通过多线程来下载。关于 python 多线程实现以及 python 多线程是否能够提高效率可以参考这篇文章。因为爬虫属于 IO 密集型的任务，所以可以通过多线程来提高下载效率。实测多线程开到 10 个的时候，下载速度比原来快了 7 倍。\n多线程下载实现的思路与单线程的类似，只是在下载某一类别的词库时采用多线程完成，这就需要注意下面几个方面：\n\n线程完成下载的时间不一样，早完成的线程需要等到最后一个线程下载完成才能一起开始下一个类别的下载 , 可通过 python 的 Queue 模块中的 join() 方法和 task_done() 方法实现\n\n需要用线程锁保护可能会被修改的变量，采用 python 的队列数据结构（Queue）则不用，因为 python 的 Queue 模块已经实现了这个锁的功能，具体参见 Queue — A\nsynchronized queue class\n\n关于队列模块中的 join() 方法和 task_done() 方法，官方文档说明如下：\n\nIf a join() is currently blocking, it will resume when all items have\nbeen processed (meaning that a task_done() call was received for every\nitem that had been put() into the queue).\n\n也就是说 Queue 调用了 join 方法后，必须要收到每个取出的 item（这里为 url）返回的 task_done () 消息才会继续执行，否则会一直 block 等待，这就实现了我们说到的早完成的线程需要等到最后一个线程下载完成才能一起开始下一个类别的下载。\n实现的关键代码如下：\n# 构建自己的线程类  queue =Queue.Queue()  # 存放待访问url  visited = set()       # 已经访问过的url  downloaded = set()    # 已经下载过的文件class downloadThread(threading.Thread):      # 重写run函数      def run(self):          while True:              if queue.empty(): # 防止一开始队列内容太少导致后创建的线程退出                  continue              currentURL = queue.get()              # 查看url是否被访问过，需要用锁保护              threadingLock.acquire()              try:                  if currentURL in visited:                      queue.task_done() # 不可少，否则queue调用了join()会一直block下去                      continue                  else:                      visited.add(currentURL)              finally:                  threadingLock.release()            # 解析当前页面              try:                  response = urllib2.urlopen(currentURL)              except :            # 找到链接到其他页面的连接              data = response.read()              pageList = re.findall(pagePattern,data)              for i in pageList:                  pageURL = smallCateURL+'&amp;page='+i                  queue.put(pageURL)            # 下载当前页面存在的文件              fileList = re.findall(filePattern,data)              for id, name in fileList:                  fileURL = 'http://dict.qq.pinyin.cn/download?dict_id='+id                  filePath = downloadDir.decode('utf8')+'/'+name.decode('gbk')+'.qpyd'                # 检查文件是否被下载                  threadingLock.acquire()                  try:                      if fileURL in downloaded:                          continue                      else:                          downloaded.add(fileURL)                  finally:                      threadingLock.release()                  downloadSingleFile.downloadSingleFile(fileURL, filePath, logFile)             #告诉queue当前任务已完成，否则因为queue调用了join，会一直block下去              queue.task_done()  \n最后，下载三个输入法的词库的 python 源码已放到 github 上，链接 https://github.com/WuLC/ThesaurusSpider\n文章为博主个人总结，如有错误，欢迎交流指正\n","categories":["python","爬虫"],"tags":["python","爬虫","广度优先搜索"]},{"title":"搜索相关性：从建模到排序机制","url":"/2025/07/20/%E6%90%9C%E7%B4%A2%E7%9B%B8%E5%85%B3%E6%80%A7%E6%A6%82%E8%BF%B0/","content":"最近一段时间在研究搜索的相关性问题，一个颇有搜索特色的问题。搜索场景下的相关性，指的是展示给用户的内容，跟用户输入的\nquery\n必须满足一定的关联关系，比如说搜 “肯德基”，就不应该出现 “麦当劳” 的内容\n不同于在 feed 场景下，用户对内容基本无预期，feed\n场景的推荐算法可以基于用户历史浏览兴趣、最近热点内容等做\nexploit，或是通过探索用户的一些新兴趣做 explore。但在 search\n场景下，用户主动搜索输入的 query\n往往是有强意图的，出的内容也是要符合用户的这个预期的，否则这些搜索就是无效的，进而会造成搜索留存（LT）的损失。而在用户视角下，如果平台搜索的算法做得足够好，应该在第一页就能够找到自己想要的内容，而这这其实也导致了单\npv 下 search 浏览深度会远比 feed 要低\n场景上的差异，会导致 search 相较于 feed\n的优化目标也有不小差异。比如说搜索 LT\n的度量中，时长并不是最重要的指标；从排序角度，增加了相关性的约束，导致特定\nquery 下可被用来排序的候选有限（相较于\nfeed），同时排序公式中往往也要加入相关性因子来达成相关性目标，对最大化原目标（如广告就是收入）的效率造成干扰\n排序的相关性约束，也是导致了很多在 feed 下有效的 ranking 迭代，在\nsearch 中效果不优甚至无效的原因，比如说这个问题里提到的现象 为什么搜索系统技术文章很少，但推荐系统技术文章很多？，一个很重要的原因是给定\nquery，相关性候选不足导致了 ranking 搜索空间不足，而 ranking\n本身的收益应该是随着候选量增加的边际效率是递增的\n本文主要探讨下搜索场景下的相关性问题的解决思路。如果粗略地划分，相关性往往会涉及到两部分：相关性的建模，以及相关性模型预估分的作用机制，本文尝试对这部分的内容详细展开做一些讨论\n\n相关性建模\n如何判断 query 与内容是否相关？直观来看可以分为主观和客观两类指标\n\n客观指标：就是看一些可基于用户行为来统计的客观指标，如用户搜索后的点击率（一般叫 “有点比”，越大越好）、或者在同一个搜索意图下是否频繁换\nquery（一般叫 “换 q 率”，越小越好）、以及用户的搜索行为是否在减少 (搜索\nLT30)\n\n主观指标：就是偏人工评估的指标，一般依赖约定好的标准，定义某个 query\n下不同内容的相关联的程度，然后人工定期抽检判断当前的搜索的内容的整体相关情况；但这个标准一般在不同\nAPP 是不一样的，即使在同一 APP 内这些标准也是会频繁发生变化的\n\n这里说的相关性建模，针对的是主观指标的建模，而主观指标其实也可以理解为是客观指标的一类中间指标，因为往往是由于搜索结果的相关性不足，导致了用户不点击或频繁换\nquery 重新搜索，甚至离开平台不再搜索\n演进路线\n从技术视角来看，判断 query 与 doc\n内容是否相关，也经历了几个阶段变化，在文章《大众点评搜索相关性技术探索与实践\n- 美团技术团队的文章》中就提到了这相关性建模的演进路线\n阶段一：文本匹配。仅考虑 query 与 doc\n的字面匹配程度，通过 TF-IDF、BM25 等基于 term\n的匹配特征来计算相关性。这种方式计算效率高，但是泛化性比较差，匹配规则比较硬，没法处理一次多义或多词一义的问题\n阶段二：传统语义匹配模型。这种做法不像文本匹配那么粗暴了，而是对于原始的\nquery 和 doc 做一个映射，映射到一个隐式的向量空间，然后在这个空间内计算\nquery 和 doc 的相似性；最常见的做法就是把 query 和 doc 都映射成一个\nembedding，然后基于 embedding\n计算向量距离或空间相似度作为得分（实际上双塔 DNN\n模型隐式地做了这个事情）。比如说 Partial\nLeast Square Regression\n这一类方法；或者是将 Doc 映射到 Query 空间后进行匹配或计算 Doc 翻译成 Query 的概率，典型的方法可以参考这篇\npaper Clickthrough-Based\nTranslation Models for Web Search: from Word Models to Phrase\nModels\n阶段三：深度语义匹配模型。这里相较于阶段二引入了深度神经网络模型\nDNN。同时可以被粗略分为两种范式：基于表示（Representation-based）的方法及基于交互（Interaction-based）的方法\n\n Representation-based 方法\n\n最常见的例子是类似 DSSM\n这一类双塔模型，也是召粗环节最常见的建模范式，基于 Bert 等 encoder\n结构，将原始的文本等特征映射至向量空间，再通过余弦相似度或点积计算最终相关性。在微软的\nBing 搜索的 NRM 模型中，doc 的特征除了基础的标题和内容，还考虑了其他多源\n(Field) 信息，如外链、用户点击这个 doc 的历史 query 等；所以在 Doc\n中有多个 Field，每个 Field 内又有多个\nInstance（文本，如 Query 词）。模型首先学习 Instance 向量，将所有 Instance 的表示向量\npooling 得到一个 Field 的表示向量，将多个 Field 的表示向量 pooling\n得到最终 Doc 的向量，如下图所示\n\nRepresentation-based 方法的好处是 doc\n的向量可离线计算并缓存，线上服务时仅需计算 Query 向量并进行简单的相似度计算，性能好，延迟低，但缺点是\nQuery 和 Doc\n在编码过程中缺乏直接的交互信息，仅依靠最终向量进行相似度计算，可能会丢失一些细粒度的匹配信号，表达能力有一定上限\n\nInteraction-based 方法\n\n这种方法没有显式将 Query 和 Doc 分开来直接学习 Query 和 Doc\n的语义表示向量，而是在底层输入阶段就让 Query 和 Doc 进行交互，如果说前面的双塔是召粗常用的范式，这里的模型是精排常用的范式。ESIM是预训练模型引入之前被业界广泛使用的经典模型，首先对 Query 和 Doc 进行编码得到初始向量，再用 Attention 机制进行交互加权后与初始向量进行拼接，最终分类得到相关性得分\n而引入预训练模型如 BERT 后，通常将 Query 和 Doc\n拼接作为 BERT 句间关系任务的输入，然后输入模型得到最终的相关性得分，如这篇\npaper 的做法就是这样的 Multi-Stage Document Ranking\nwith BERT。\n\n这种方法的优劣刚好跟 representation-based\n方法相反了了，优点是匹配精度高，能捕捉非常细粒度的词语交互关系和深层语义信息；但同时计算开销巨大。线上服务时需要将\nQuery 和 Doc\n实时拼接并送入大型模型进行前向计算，延迟高，对性能挑战大\n多阶段训练\n建模需要解决几个基础问题，包括训练样本获取、特征构造、模型指标等。而在相关性建模任务中，直接的领域打标数据获取是有不小成本的，所以需要考虑因此会通过多阶段训练尽量在成本可控的情况下，提升模型效果\n\n训练样本获取\n\n由于相关性标准是平台制定且会频繁变化的，因此相关性训练样本的 label\n不像 ctr、cvr 这类任务有明确的 ground\ntruth，而是依靠人工标注。且相关性标准如果有变化，label\n也可能会有变化，因此实际中针对某个特定领域，相关性模型训练样本的获取的成本是比较高的\n因此在实际的训练过程中，往往是结合预训练模型 + 大量容易获取 label\n的相关领域数据先做一轮数据，然后再利用上面提到的人工标注的数据来做进一步的\nfinetune。以上面大众点评的文章为例（其他的领域其实大同小异），会先采用用户点击和负采样数据进行第一阶段领域适配的预训练（Continual\nDomain-Adaptive\nPre-training），然后采用人工标注数据进行第二阶段训练（Fine-Tune）\n第一阶段训练：这个阶段将用户是否点击来作为 “是否相关” 的\nlabel，这种做法能够低成本获取很多训练样本。但如果直接将点击样本用于相关性判断会存在较大噪声，因为用户是否点击后受到很多信息的影响（最常见的就是位置信息，越靠前的越容易被点击，靠后的不被点击，但这并不是因为相关性问题），所以做法是引入了多种特征和规则来提高训练样本的准确率。具体规则如下\n对于正样本获取，通过统计是否点击、点击位次、最大点击商户距用户的距离等特征筛选候选样本，将曝光点击率大于一定阈值的作为正例；对于负样本的获取，将位于点击\ndoc 之前且点击率小于阈值的 doc\n才做为负样本，同时采用随机负采样的方式可以为训练样本补充简单负例，但考虑随机负采样时也会引入一些噪声数据，所以利用人工设计的规则对训练数据进行降噪：如当\nQuery 的类目意图与 doc 的类目体系较为一致时或者与 doc\n名高度匹配时，则将其从负样本中剔除\n第二阶段训练：第二阶段就是常见的 fine-tune\n训练了，固定住底层参数，利用人工标准的更准确的 label\n来对顶层参数做微调。但是不同样本被标注的价值也是不太一样，如模型本身就能很好判断的\neasy pair，不如那些模型判断不清楚的 hard\npair，因此在人工标注上，点评不是随机送一批样本给人工去标注的，而是通过难例挖掘和对比样本增强方式生产大量高价值样本交给人工去标注\n\n难例挖掘：包括\n1）将用户点击过但线上旧版模型判定为不相关的作为难例；2）通过边缘采样的方式挖掘具有较高不确定性的样本，如抽取模型预测得分在阈值附近的样本；3）模型或人工识别困难的样本，用当前模型预测训练集，将模型预测结果与标注标签不一致的样本，及人工标注标签有冲突的样本类型重新送标\n\n对比样本增强：借鉴对比学习的思想，为一些高度匹配的样本生成对比样本进行数据增强，并进行人工标注确保样本标签的准确率。通过对比样本之间的差异，模型可以关注到真正有用的信息，同时提升对同义词的泛化能力，这里给了一个这样的例子\n\n\n这里 query “榴莲蛋糕” 与推荐的 “榴莲千层、黑森林蛋糕” 是相关的，但 query\n“鹅肝汉堡” 与 “铁板鹅肝、芝士牛肉汉堡” 是不相关的，为了增强模型对这类高度匹配但结果相反的\ncase\n的识别能力，文章的方法是构造了 “榴莲蛋糕” 与 “榴莲千层”、“鹅肝汉堡” 与 “铁板鹅肝” 这两组对比样本，去掉了与\nquery\n在文本上匹配但对模型判断没有帮助的信息，让模型学到真正决定是否相关的关键信息，同时提升模型对 “蛋糕” 和 “千层” 这类同义词的泛化能力\n\n特征\n\n由于相关性是一个相对客观的标准（即给定一个 query 和 doc，后验的 label\n是明确的），受 context\n信息影响比较小（如位置信息、上下文信息），或者说在 query 和 doc\n不会因为换了位置等信息，相关性就会发生变化。因此相关性模型使用的特征相对来说也比较简单，query\n特征一般是原始的 query 文本，doc 特征一般是 doc 的多模态信息（如\ntitle、image）和 meta 信息（如 doc 所属的类别、类目等）\n而随着大模型的广泛使用，也发展出了利用大模型来构造额外的 query 和 doc\n特征的路线。以 query 为例，基本的做法就是基于 query 构造\nprompt（如基于 query\n和各个平台搜索出来的结果），然后让大模型总结输出更准确和详细的 query\nsummary，然后把这个 query summary 作为额外的 query\n特征输入给相关性模型\n在业界上一般是基于 RAG（Retrieval-Augmented\nGeneration）来完成这个步骤，RAG\n结合了检索（Retrieval）和生成（Generation）两种技术。基本的工作原理是当模型接收到一个\nquery\n时，首先在一个大型的索引中检索相关的文档或信息片段，检索是基于相似性度量完成的，目的是找到与\nquery\n最相关的信息，然后模型使用检索到的文档作为额外的上下文信息输入来生成内容，生成的这一步通常是由大模型来完成。如下图所示是一个基础的流程\n\nRAG 技术通过给 LLM 更详细的输入，能缓解 LLM\n的幻觉问题，也通常被应用在特定的垂直搜索的领域。关于 RAG\n更详细的介绍，可以进一步参考这篇文章 大模型 RAG（检索增强生成）含高级方法\n排序机制\n有了相关性预估分后，需要考虑的就是在线上如何应用这个预估分了。在系统中往往通过以下两个手段来保证相关性：相关性门槛和排序公式增加相关性项\n\n相关性门槛：固定值，用于过滤低相关性广告候选\n\n排序公式增加相关性项：动态变化值，控制投后相关性目标达标\n\n搜索的相关性根本的目标是保证约定的 badcase 率约束不超，而相关性分的\nlabel 如果定义为是否 badcase，那预估分的物理含义即为是否 badcase\n的概率，就成了一个很常规的二分类任务了。则当模型预估准确的时候，通过控制投后的预估均值等于某个目标，即可控制\nbadcase 率在固定值附近。如 badcase 目标是\n5%，相关性预估值准确情况下，让相关性预估均值保持为\n0.95，即可达成这个目标\n那这个在系统中的排序机制应该是怎么样的，下面会针对这个问题提供一个解决思路\n最优排序公式\n首先是最优排序公式的推导，通过下面的推导把问题形式化，这里以广告场景最大化\necpm 为例\n假设有 \\(n\\) 条请求，第 \\(i\\) 条请求曝光的广告的相关性预估分位 \\(predict\\_rel\\_score_i\\)，广告价值为 \\(ecpm_i\\)，相关性均值的目标为\ntarget。则需要求解的问题的可形式化表达为如下形式（其中决策变量 \\(x\\) 为广告选择策略）\n\\[\\begin{align}\n\\max_x &amp;\\sum_{i=0}^{n-1} ecpm_i \\\\\ns.t. \\frac{1}{n} &amp;\\sum_{i=0}^{n-1} predict\\_rel\\_score_i=target\n\\end{align}\\]\n问题建模\n由于每条请求往往会有多条广告，上述问题可进一步细化到如下形式\n假设每个请求 \\(i\\) 有 \\(m_i\\) 个候选广告。对于广告 \\(k\\) 在请求 \\(i\\) 上，定义：\n\n\\(ecpm_{ik}\\)：广告的 eCPM 值\n\n\\(s_{ik}\\)：广告的相关性预估分数（即 \\(predict\\_rel\\_score\\)）\n\n\\(x_{ik}\\)：引入二元决策变量，\\(x_{ik}=1\\) 表示在请求 \\(i\\) 上选择广告 \\(k\\)，否则 \\(x_{ik}=0\\)\n\n则问题可表述成如下形式\n\\[\\begin{align}\n\\max &amp;\\sum_{i=0}^{n-1} \\sum_{k} x_{ik} \\cdot ecpm_i \\\\\ns.t.  &amp;\\sum_{i=0}^{n-1} \\sum_{k} x_{ik} \\cdot s_{ik} =\n(\\sum_{i=0}^{n-1} \\sum_{k} x_{ik}) \\cdot target \\\\\n&amp; x_{ik} \\in \\{0,1\\} \\quad \\forall i,k\n\\end{align}\\]\n问题求解\n1. 拉格朗日松弛\n由于约束是等式且全局，使用拉格朗日乘数法将约束融入目标函数。引入拉格朗日乘数\n\\(\\lambda\\)，构造拉格朗日函数 \\(L\\)：\n\\[\\begin{align}\nL(\\mathbf{x}, \\lambda)\n&amp;= \\sum_{i=0}^{n-1} \\sum_{k} x_{ik} \\cdot \\text{ecpm}_{ik} + \\lambda\n\\left( \\sum_{i=0}^{n-1} \\sum_{k} x_{ik} \\cdot s_{ik} - (\\sum_{i=0}^{n-1}\n\\sum_{k} x_{ik}) \\cdot \\text{target} \\right) \\\\\n&amp;= \\sum_{i=0}^{n-1} \\sum_{k} x_{ik} \\cdot (\\text{ecpm}_{ik} +\n\\lambda \\cdot (s_{ik} - \\text{target}))\n\\end{align}\\]\n这里的 \\(\\lambda\\)\n可以解释为相关性约束的 “影子价格”，表示每单位相关性分数变化对总 eCPM\n的边际影响，则最大化 \\(L\\)\n等价于最大化\n\\[\\begin{align}\n\\max L(\\mathbf{x}, \\lambda) \\iff \\max \\sum_{i=0}^{n-1} \\sum_{k} x_{ik}\n\\cdot (\\text{ecpm}_{ik} + \\lambda \\cdot (s_{ik} - \\text{target}))\n\\end{align}\\]\n2. 问题分解\n对于每个请求 \\(i\\)，最大化 \\(\\sum_{k} x_{ik} \\cdot (\\text{ecpm}_{ik} + \\lambda\n\\cdot (s_{ik} - \\text{target}))\\)，等价于选择广告 \\(k\\) 使得 \\(\\text{ecpm}_{ik} + \\lambda \\cdot (s_{ik} -\n\\text{target})\\) 最大\n\\[\\begin{align}\nk_i^* = \\arg\\max_{k} (\\text{ecpm}_{ik} + \\lambda \\cdot (s_{ik} -\n\\text{target}))\n\\end{align}\\]\n因此，最优决策是对于每个请求 \\(i\\)，独立选择广告 \\(k\\)\n以最大化以下线性组合，同时也是每条请求的排序公式\n\\[\\begin{align}\nscore_{ik}=\\text{ecpm}_{ik} + \\lambda \\cdot (s_{ik} - \\text{target})\n\\end{align}\\]\n3. \\(\\lambda\\) 求解\n在数学上，\\(\\lambda\\) 是拉格朗日乘数，通过求解约束方程获得。实际系统中，\\(\\lambda\\)\n可以通过迭代方法调整（如二分搜索、梯度下降或在线学习），如可以使用二分搜索方法（因为\n\\(s_i(\\lambda)\\) 是关于 \\(\\lambda\\) 是单调递增的函数），步骤如下\n\n初始化 \\(\\lambda_{low}\\) 和 \\(\\lambda_{high}\\)\n 对于每个 \\(\\lambda\\)，计算所有请求的选择（最大化 \\(\\text{ecpm}_{ik} + \\lambda \\cdot s_{ik}\\))\n并计算平均 \\(s_i\\)\n 如果平均 \\(s_i &gt;\n\\text{target}\\)，则减小 \\(\\lambda\\)（降低相关性权重）；反之增大 \\(\\lambda\\)\n 重复直到平均 \\(s_i\\) 收敛到\ntarget\n\n上述方法需要我们获取的所有的流量和候选才能执行，相当于回放过去一段时间的流量得到的历史最优兑换比\n\\(\\lambda\\)。但这跟最优出价比较类似，实际中比较难直接应用。因为这里有两个前提：（1）获取到当天所有流量的数据（2）改变实际竞胜的广告不会影响竞价环境。在实际中这俩往往是难以满足\n更常见的实际做法是基于过去一段时间搜集的相关性预估值均值，然后基于\npid 来做实时调控调整 \\(\\lambda\\)\n的值，pacing 的目标就是相关性均值等于\ntarget。这部分其实跟出价调控比较类似，与计划最优出价类似，这也导致了实际的兑换比与理论最优兑换比有\ngap，需要通过各种手段逼近理论最优的兑换比\n逼近理论最优\n如果进一步分析，会发现实际通过 pid\n等控制器调控的方式，相较于流量回放直接解决最优化问题，差异在于调控过程中是否感知到了流量价值即\necpm\n因为在解决最优化问题时，有最大化 ecpm\n这一目标在约束求解过程，会去寻找刚好满足 target 达成的 \\(\\lambda\\)。但在实际的控制器调控中，只能感知到当前相关性均值是否达标了，当相关性不达标的时候会把\n\\(\\lambda\\) 调得非常大，导致 ecpm\n项在排序中发挥的作用非常小，这就导致了 ecpm 非最优\n比如说在两个连续的时间片内，前一个时间相关性是不达标的状态，但有高\necpm 的候选，这个时候只考虑相关性，会把 \\(\\lambda\\) 调的很大，导致高 ecpm\n候选出不去（因为相关性项占了主导），而接着下一个时间片相关性有缓解了（因为前一个时间片出了高相关性广告），但候选没有高\necpm 的，此时降低相关性项的权重，但出的广告 ecpm\n也不是最优的了；但如果反过来，在前一个时间片降低 \\(\\lambda\\) 后一个时间片升高 \\(\\lambda\\)，是可以做到打平相关性最大化 ecpm\n的，而这需要的就是调控感知流量价值\n在调控 \\(\\lambda\\)\n过程中感知到流量价值即 ecpm，最直观的就是用过去一段时间 \\(t\\)\n内搜集到的流量和广告候选，然后直接通过上面提到的二分法直接求解这个最优化问题，求得最优的兑换比\n\\(\\lambda^*\\)，用作下一个时间片的兑换比。但这里有一个比较强的假设是过去过去一段时间\n\\(t\\)\n的流量和候选分布，跟下一个时间片的类似（或者说差异不大），才能有效，否则约束比较难达成\n除了上面的方法，还有一种更直观的方法，就是在调控过程中直接基于流量价值动态调整兑换比。如果把流量按照价值和相关性两个维度划分，可以划分为如下四个象限\n\n对于这四类流量，如果直观来看，可以先验给出如下的兑换原则\n（1）高价值高相关性的流量，降低兑换比，尽可能多出高 ecpm 广告\n（2）低价值高相关性的流量，提高兑换比，尽可能多出高相关性广告来填补高相关性\n（3）高价值低相关性的流量，降低兑换比，但兑换比要比（1）更高，防止相关性不达标\n（4）低价值低相关性的流量，提高兑换比，兑换比（2）要更高，尽量不出广告\n这里的做法跟前面的理论最优推导出来的结论 “全局最优的兑换比是一个固定的\n\\(\\lambda\\)” 有点矛盾。因为这里相当于是给高价值流量和低价值流量不同的兑换比，而不是全局统一的；但事实上理论最优的假设（看到所有流量）是没法满足的，而我们当前本身就是在做理论最优解的逼近，所以不一定要遵循理论最优的固定的\n\\(\\lambda\\) 的这个结论\n另外这个方法有效也有两个重要假设:1）高价值流量损失的相关性可以从低价值流量上找补回来；2）单位相关性在高价值流量上的兑换效率比低价值流量的更高。1）比较好理解，因为如果找补不回来那相关性就无法达成，而\n2）指的是 ecpm\n和相关性分数的分布在高低价值是不一样的，或者更直观地说：在高价值流量上获取单位相关性损失的\n\\(\\Delta ecpm\\)\n会比低价值流量更大，而这其实取决于实际的库存分布（即 ecpm\n和相关性分的分布），从实际系统来看，这个假设成立的概率还是不小的\n另外，在实际调控中，需要考虑对这些兑换比调整后，大盘的 target\n是能达到的。从这个角度来看，其实方案 2 比方案 1\n是更能达成这一点的，因为可以基于统一的调控系数来做扰动。方案二的解决思路类似出价中的 “保浅优深” 的扰动策略，在保证大盘\ntarget 目标达成的前提下，基于流量价值对 \\(\\lambda\\)\n做扰动；但是与出价不同的是，出价往往是在计划维度做这个事情，但相关性是大盘维度的，没法做在计划维度，一是\ntarget\n是大盘约束，没法很好拆解到各个计划上，二是做在计划维度上也不是最优的，因为做在计划上要求每个计划都有一个\ntarget，这样计划之间就不好做兑换了，或者说要约束更多了，求解空间更小了。关于出价问题上类似的解决思路，可以参考这篇文章《Bid\nOptimization by Multivariable Control in Display Advertising》\n小结\n本文从搜索与推荐场景的差异出发，探讨了相关性建模的技术演进与排序机制中的最优控制策略，试图为这一经典问题提供系统性的解决思路。搜索场景的强意图特性决定了相关性问题的特殊性：与推荐场景的 “无目的性浏览” 不同，用户搜索带有明确预期，这要求结果必须精准匹配查询意图。从技术视角上可以分为相关性建模和排序机制两部分\n在相关性建模中，基本的迭代经历了从文本匹配到深度语义匹配的演进。当前主流方法可分为基于表示（Representation-based）和基于交互（Interaction-based）两类范式，两者在精度与性能间各有权衡。而引入预训练模型及 RAG 技术后，模型对语义的理解深度和泛化能力得到了显著提升。关于相关性建模有两点值得关注，一是基于大模型的语义理解与生成：大语言模型（LLM）在语义理解、意图推理和内容生成方面展现出强大能力，未来有望深入应用 LLM 进行查询意图的深层解析、扩展与归一化，甚至直接生成或增强相关内容摘要，进一步提升相关性判断的准确性和可解释性；二是个性化相关性理解：搜索意图有群体共性，但亦存在个体差异；虽然当前的相关性与\ncontext\n不相关，但未来的相关性模型可能需要更好地融合用户个性化上下文（如历史行为、实时偏好），在保证基础相关性的前提下，提供更契合个体需求的精准结果\n在排序机制中。通过最优化问题形式化推导了带相关性约束的排序公式 \\(score =\\text{ecpm} + \\lambda \\cdot\n\\text{rel_score}\\)。其中的拉格朗日乘数 \\(\\lambda\\)\n可视为相关性约束的 “影子价格”，通过调控 \\(\\lambda\\)\n可实现相关性目标的达成与 ecpm 最大化间的平衡\n当前 \\(\\lambda\\)\n调控多基于大盘均值，难以感知流量价值差异。理想状态应实现分流量价值层的精细化调控，对高价值高相关性流量降低\n\\(\\lambda\\)\n以提升 ecpm，对低价值高相关性流量提高 \\(\\lambda\\)\n以保证相关体验。同时需要考虑在不破坏大盘目标的前提下实现这种动态调控\n搜索广告的相关性，归根结底是在用户意图、广告主诉求和平台价值三者间寻求最佳平衡的艺术与科学。它既需要深入的技术建模与算法优化，也离不开对用户搜索心理和广告主业务目标的深刻洞察。未来的搜索广告系统，或许将更加智能与自适应，能够动态感知不同场景、不同用户对相关性的差异化期望，并精准调控商业与体验的平衡点\n","categories":["计算广告"],"tags":["计算广告","机器学习","搜索"]},{"title":"机器学习中模型优化不得不思考的几个问题","url":"/2017/05/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E4%B8%8D%E5%BE%97%E4%B8%8D%E6%80%9D%E8%80%83%E7%9A%84%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98/","content":"文章为转载，原文链接在这里，文章从业界的角度出发介绍了机器学习如何发挥其价值，非常接地气，值得一看，以下为原文\n\n机器学习工程师的知识图谱\n 图 1 机器学习工程师的知识图谱\n图 1 列出了我认为一个成功的机器学习工程师需要关注和积累的点。机器学习实践中，我们平时都在积累自己的 “弹药库”：分类、回归、无监督模型、Kaggle 上面特征变换的黑魔法、样本失衡的处理方法、缺失值填充…… 这些大概可以归类成模型和特征两个点。我们需要参考成熟的做法、论文，并自己实现，此外还需要多反思自己方法上是否还可以改进。如果模型和特征这两个点都已经做得很好了，你就拥有了一张绿卡，能跨过在数据相关行业发挥模型技术价值的准入门槛。\n在这个时候，比较关键的一步，就是高效的技术变现能力。\n所谓高效，就是解决业务核心问题的专业能力。本文将描述这些专业能力，也就是模型优化的四个要素：模型、数据、特征、业务，还有更重要的，就是它们在模型项目中的优先级。\n模型项目推进的四要素\n项目推进过程中，四个要素相互之间的优先级大致是：业务 &gt; 特征 &gt; 数据 &gt; 模型。\n 图 2 四要素解决问题细分 + 优先级\n业务\n一个模型项目有好的技术选型、完备的特征体系、高质量的数据一定是很加分的，不过真正决定项目好与坏还有一个大前提，就是这个项目的技术目标是否在解决当下核心业务问题。\n业务问题包含两个方面：业务 KPI 和 deadline。举个例子，业务问题是在两周之内降低目前手机丢失带来的支付宝销赃风险。这时如果你的方案是研发手机丢失的核心特征，比如改密是否合理，基本上就死的很惨，因为两周根本完不成，改密合理性也未必是模型优化好的切入点；反之，如果你的方案是和运营同学看\nbad\ncase，梳理现阶段的作案通用手段，并通过分析上线一个简单模型或者业务规则的补丁，就明智很多。如果上线后，案件量真掉下来了，就算你的方案准确率很糟、方法很 low，但你解决了业务问题，这才是最重要的。\n虽然业务目标很关键，不过一般讲，业务运营同学真的不太懂得如何和技术有效的沟通业务目标，比如：\n\n我们想做一个线下门店风险评级的项目，希望运营通过反作弊模型角度帮我们给门店打个分，这个分数包含的问题有：风险是怎么定义的、为什么要做风险评级、更大的业务目标是什么、怎么排期的、这个风险和我们反作弊模型之间的业务关系你是怎么看的？\n做一个区域未来 10min 的配送时间预估模型。我们想通过运营的模型衡量在恶劣天气的时候每个区域的运力是否被击穿（业务现状和排期？运力被击穿可以扫下盲么？运力击穿和配送时间之间是个什么业务逻辑、时间预估是刻画运力紧张度的最有效手段么？项目的关键场景是恶劣天气的话，我们仅仅训练恶劣天气场景的时间预估模型是否就好了？）。\n\n为了保证整个技术项目没有做偏，项目一开始一定要和业务聊清楚三件事情：\n1. 业务核心问题、关键场景是什么。\n2. 如何评估该项目的成功，指标是什么。\n3.\n通过项目输出什么关键信息给到业务，业务如何运营这个信息从而达到业务目标。\n项目过程中，也要时刻回到业务，检查项目的健康度。\n数据与特征\n要说正确的业务理解和切入，在为技术项目保驾护航，数据、特征便是一个模型项目性能方面的天花板。garbage\nin， garbage out 就在说这个问题。\n这两天有位听众微信问我一个很难回答的问题，大概意思是，数据是特征拼起来构成的集合嘛，所以这不是两个要素。从逻辑上面讲，数据的确是一列一列的特征，不过数据与特征在概念层面是不同的：数据是已经采集的信息，特征是以兼容模型、最优化为目标对数据进行加工。就比如通过 word2vec 将非结构化数据结构化，就是将数据转化为特征的过程。\n所以，我更认为特征工程是基于数据的一个非常精细、刻意的加工过程。从传统的特征转换、交互，到 embedding、word2vec、高维分类变量数值化，最终目的都是更好的去利用现有的数据。之前有聊到的将推荐算法引入有监督学习模型优化中的做法，就是在把两个本不可用的高维 ID 类变量变成可用的数值变量。\n观察到自己和童鞋们在特征工程中会遇到一些普遍问题，比如，特征设计不全面，没有耐心把现有特征做得细致…… 也整理出来一套方法论，仅供参考：\n 图 3 变量体系、研发流程\n在特征设计的时候，有两个点可以帮助我们把特征想的更全面：\n1. 现有的基础数据  2.\n业务 “二维图”\n这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应 RD 的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子：\n外卖配送时间业务（维度甲：配送的环节，骑手到点、商家出餐、骑手配送、交付用户；维度乙：颗粒度，订单粒度、商家粒度、区域城市粒度；维度丙：配送类型，众包、自营……）。\n反作弊变量体系（维度甲：作弊环节，登录、注册、实名、转账、交易、参与营销活动、改密……；维度乙：作弊介质，账户、设备、IP、WiFi、银行卡……）。\n通过这些维度，你就可以展开一个 “二维图”，把现有你可以想到的特征填上去，你一定会发现很多空白，比如下图，那么哪里还是特征设计的盲点就一目了然：\n 图 4\n账户维度在转账、红包方面的特征很少；没有考虑 WiFi 这个媒介；客满与事件数据没考虑\n数据和特征决定了模型性能的天花板。deep\nlearning 当下在图像、语音、机器翻译、自动驾驶等领域非常火，但是 deep\nlearning 在生物信息、基因学这个领域就不是热词：这背后是因为在前者，我们已经知道数据从哪里来，怎么采集，这些数据带来的信息基本满足了模型做非常准确的识别；而后者，即便有了上亿个人体碱基构成的基因编码，技术选型还是不能长驱直入 —— 超高的数据采集成本，人后天的行为数据的获取壁垒等一系列的问题，注定当下这个阶段在生物信息领域，人工智能能发出的声音很微弱，更大的舞台留给了生物学、临床医学、统计学。\n模型\n 图 5 满房开房的技术选型、特征工程 roadmap\n模型这件事儿，许多时候追求的不仅仅是准确率，通常还有业务这一层更大的约束。如果你在做一些需要强业务可解释的模型，比如定价和反作弊，那实在没必要上一个黑箱模型来为难业务。这时候，统计学习模型就很有用。\n这种情况下，比拼性能的话，我觉得下面这个不等式通常成立：Glmnet&gt;LASSO&gt;=Ridge&gt;LR/Logistic。相比最基本的 LR/Logistic，ridge 通过正则化约束缓解了 LR 在过拟合方面的问题，lasso 更是通过 L1 约束做类似变量选择的工作。\n不过两个算法的痛点是很难决定最优的约束强度，Glmnet 是 Stanford 给出的一套非常高效的解决方案。所以目前，我认为线性结构的模型，Glmnet 的痛点是最少的，而且在 R、Python、Spark 上面都开源了。\n如果我们开发复杂模型，通常成立第二个不等式\nRF（Random Forest，随机森林）&lt;= GBDT &lt;= XGBoost\n。拿数据说话，29 个 Kaggle 公开的 winner\nsolution 里面，17 个使用了类似 GBDT 这样的 Boosting 框架，其次是 DNN（Deep\nNeural Network，深度神经网络），RF 的做法在 Kaggle 里面非常少见。\nRF 和 GBDT 两个算法的雏形是 CART（Classification And Regression\nTrees），由 L Breiman 和 J\nFriedman 两位作者在 1984 年合作推出。但是在 90 年代在发展模型集成思想 the\nensemble 的时候，两位作者代表着两个至今也很主流的派系：stacking/ Bagging\n&amp; Boosting。\n一种是把相互独立的 CART（randomized variables，bootstrap\nsamples）水平铺开，一种是深耕的 Boosting，在拟合完整体后更有在局部长尾精细刻画的能力。同时，GBDT 模型相比 RF 更加简单，内存占用小，这都是业界喜欢的性质。XGBoost 在模型的轻量化和快速训练上又做了进一步的工作，也是目前我们比较喜欢尝试的模型。\n作者简介\n胡淏，美团算法工程师，毕业于哥伦比亚大学。先后在携程、支付宝、美团从事算法开发工作。了解风控、基因、旅游、即时物流相关问题的行业领先算法方案与流程。\n","categories":["机器学习"],"tags":["机器学习","转载"]},{"title":"最优化计算课程总结","url":"/2017/02/01/%E6%9C%80%E4%BC%98%E5%8C%96%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/","content":"本文主要是最优化计算这门课程的课程总结，参考的教材为《最优化计算》，主要讲述的内容是函数优化，相对于函数优化的另外一种优化是组合优化，两者的主要区别是前者的可行解是连续的，后者的可行解是离散的，或者说前者的可行解是无限的，而后者是有限的。\n\n最优化要解决的问题非常直观，就是给定若干个由若干个变量组成的目标函数，然后使得变量取某一组值时目标函数值最大或最小，这时候变量的取值便称为最优解；有时候变量还有一定的约束，如要满足某些等式或不等式，这时候的约束称为有约束优化，以区别于前面的无约束优化。\n可以说，最优化是一门纯数学的课程，但是现实世界中，很多问题都可以通过建模然后将问题最终转化为求解一个最优化问题，比如说在机器学习中，很多算法往往有个目标函数，这个目标函数可以用与描述预测结果与实际结果的误差，这时候就要最小化这个目标函数 (回归，SVM 等)；而最大化的问题通过改变符号也转为最小化问题。因此，最优化在实际中有广泛应用。\n根据目标函数的形式、数量以及是否有约束条件可以将优化问题分为多种类型。本文主要讲述的是单目标规划，并且将单目标规划进一步分为括线性和非线性，有约束和无约束。\n在讲述具体的优化算法前，先介绍最优化中常用的概念。\n最优化的基本模型为\n\\[\\begin{align}\n\\min\\;f(x)\\\\\ns.t\\quad c(x) \\ge 0\\\\\n\\end{align}\\]\n其中 \\(x = (x_1,x_2,x_3,...x_n)\\)\n是一个变量组成的向量，也就是包含了若干变量，\\(c(x)\\) 则是对各个变量约束的等式和不等式。\n有了基本模型，下面介绍在最优化中的常用概念。\n\n目标函数：就是 \\(f(x)\\)\n 决策变量：目标函数 \\(f(x)\\) 中的所有变量\n约束条件：\\(c(x)\\) 中包含的所有的等式和不等式\n可行域：约束条件在空间围成的区域\n可行解：可行域中的每个点都是原问题的一个可行点\n最优解：能够使目标函数达到最大或最小的可行解\n凸集：集合的一种，用于描述可行域，满足以下性质\n\n\n令 \\(K\\) 为集合，\\(\\forall x_1,x_2 \\in K\\), 若 \\(\\alpha x_1 + (1-\\alpha)x_2 \\in\nK\\), 其中 \\(\\alpha \\in [0,1])\\), 则\n\\(K\\) 为凸集\n\n凸集直观的图像表示如下\n\n\n凸集\n\n线性规划\n数学模型与基本概念\n线性规划就是目标函数和约束条件都是线性的最优化问题，且一般都是带约束的，线性规划的一般模型如下所示\n\n\n线性规划模型\n\n为了模型的一致性，通常会将以上的模型化为标准型，标准型要求\n1）目标函数求最小值\n2）约束条件全为等式\n3）所有的 \\(x\\) 有 \\(x \\ge 0\\)\n化为标准型中的关键点是将约束条件中的不等式变为等式\n1）对于约束条件为 \\(\\le\\) 的情况，在\n\\(\\le\\)\n左边添加一个松弛变量 (非负)。\n2）对于约束条件为 \\(\\ge\\) 的情况，在\n\\(\\ge\\)\n左边减去一个剩余变量 (非负)\n注意：松弛变量、剩余变量在目标函数中的价值系数为 0。\n如下为一个简单的例子\n\n\n化标准型\n\n对于化为标准型的线性规划模型中约束条件的 \\(m × n\\) 系数矩阵 \\(A\\)，从 \\(A\\) 中取大小为 \\(m\n× m\\) 的子矩阵 \\(B\\)，若 \\(Rank(B) = m\\)(即 B 为满秩矩阵)，则称 \\(B\\) 为线性规划问题的一个基矩阵。取 \\(B = (A_1,A_2,···,A_m)\\) ，其中 \\(A_j = (a_{1j},a_{2j},···,a_{mj})^T\\)\n则称 \\(x_1,x_2,···,x_m\\) 为基变量，其它为非基变量。如下所示\n\n\n基变量和非基变量\n\n令所有的非基变量为 0，则得到的解称为基本解，可知基本解的个数\n\\(\\le\nC_n^m\\)。但是基本解不一定满足约束条件，满足约束条件的基本解称为基本可行解，而基本可行解对应的基变量称为可行基。\n基本解，基本可行解，可行解，非可行解的关系如下图所示\n\n\n基本解，基本可行解，可行解，非可行解的关系\n\n关于线性规划有三条重要定理\n\n定理 1 若线性规划存在可行域，则可行域为凸集\n定理 2 线性规划的基本可行解对应于可行域的顶点\n定理 3 若线性规划有最优解，则一定存在基本可行解为最优解。\n\n上面直观解释和定理都是为了说明这个事实：如果线性规划的最优解存在，那最优解一定在可行域的顶点上。下面要介绍的单纯形法就是利用这个性质。\n单纯形法\n单纯形法是解决线性规划的经典方法。其基本思想是先从可行域的一个顶点出发，然后从当前顶点沿着可行域 (可行域是一个凸多面体) 的边找下一个使得目标函数更小的顶点，假如找得到就移动到更优的顶点，找不到就说明当前顶点是线性规划的最优解。\n因此，单纯形法的主要步骤为 1) 确定初始基本可行解\n\n判别当前基本可行解是否是最优解\n从一个基本可行解转换到相邻且改善了的基本可行解\n\n在实际运算的时候，一般通过单纯形表实现。其求解过程如下所示\n对于下面已经化为标准型的线性规划问题\n\n\n标准型\n\n其单纯形表为\n\n\n单纯形表\n\n表中的基指的是基变量，最后一行 \\(\\sigma_j\\)\n是检验数，用于检验当前的基本可行解是否为最优解。\n对应于单纯形表，求解步骤如下\n（1）确定初始基本可行解。\n也就是列出如上图所示的初始的单纯形表，将线性规划化为标准型后，通过矩阵的初等行变换，可以使系数矩阵 A 中包含一个单位阵，而这个单位阵对应的基变量即可作为初始基本可行解。\n（2）判别当前基本可行解是否是最优解。\n通过最后一行的检验数 \\(\\sigma_j\\)\n判断，假如所有非基变量的检验数 \\(\\sigma_j \\ge\n0\\)，则基变量为最优解，计算结束；假如存在 \\(\\sigma_k \\lt 0\\), 且 \\(A_k \\le\n0\\), 则问题为无界解，计算结束；否则转第（3）步\n（3）从一个基本可行解转换到相邻且改善了的基本可行解。\n这一步要确定一个入基变量和一个出基变量，入基变量就是从非基变量中选择的一个变量，然后将其变为基变量，出基变量就是从基变量中选择一个变量，然后将其变为非基变量。实际上就是将这两个变量的位置 (基或非基) 互换，对应于从可行域的一个顶点走到另外一个顶点。\n入基变量的确定: 从所有的检验数中找出最小的 \\(\\sigma_k\\), 对应的 \\(x_k\\) 为入基变量。\n出基变量的确定：通过下式确定的 \\(l\\) 所对应的的 \\(x_l\\) 作为出基变量\n\\[\\begin{align} \\min_{1 \\le i \\le m}\n\\lbrace \\frac{b_i}{a_{ik}} | a_{ik} &gt; 0 \\rbrace\n\\end{align}\\]\n上式中的 \\(a_{ik}\\)\n为入基变量对应系数举证 A 的第 k 列\n找到入基变量 \\(x_k\\) 和出基变量 \\(x_l\\) 后，用入基变量替换出基变量，通过初等行变换使得 \\(x_k\\) 对应系数矩阵 A 的一列成为原单位矩阵中 \\(x_l\\) 对应的那列，其他的值做相应的计算 (见下图)，画出的新的单纯形表如下所示：\n\n\n新的单纯形表\n\n（4）重复步骤（2）和（3），直到找到最优解\n注意除了单纯形法以外，解决线性规划还有另外一类方法：内点法。这里不详细展开，具体可以参考这篇论文\nInterior\nPoint Methods and Linear Programming。\n对偶理论\n在求解一个规划问题（不限于线性规划）的时候，我们常常需要知道这个问题有没有可行解（有时候约束条件很复杂，不要说最优解，找到可行解都很难），或者是估计一下目前的解离最优解还有多远（大型问题多用迭代解法，如果能大致估计当前的解的质量，就对算法什么时候运行结束有一个大致的把握，如果得到了可接受的近似解也可以提前停止），以及判断原问题的最优解是否无界（万一出现这种情况迭代就停不下来了）。\n而对偶问题就是回答这些问题的利器：弱对偶定理给原问题的最优解定了一个界，强对偶定理给出了原问题最优解的一个判定条件。同时，还有很多别的优良性质：例如可以化难为易（把难以求解的约束条件扔到目标函数的位置上去），如果问题的形式合适 (变量少，约束多) 还可以通过把约束变量和对偶变量互换来把大规模问题转换成小规模问题。实际上，很多凸优化问题都是通过解对偶问题来求解的，线性规划只是其中一个特例而已。\n一般地，对于原问题 \\[\\begin{align}\\min\\;z\n=  c^Tx \\quad \\\\\ns.t.\\,Ax \\ge b(x \\ge 0)\\tag{1}\\end{align}\\]\n其对偶问题为 \\[\\begin{align}\\max\\;w\n=  b^Ty \\quad \\\\\ns.t.\\,A^Ty \\le c(y \\ge 0)\\tag{2}\\end{align}\\]\n根据原问题写出其对偶问题时要注意约束条件和变量的符号变化情况，其变换规则如下\n\nmax 问题第 i 个约束取 “≥”，则 min 问题第 i 个变量 ≤ 0\nmin 问题第 i 个约束取 “≤”，则 max 问题第 i 个变量 ≤ 0\n 原问题第 i 个约束取等式，对偶问题第 i 个变量无约束\n max 问题第 j 个变量 ≤ 0 , 则 min 问题第 j 个约束取 “≤”\nmin 问题第 j 个变量 ≤ 0 ，则 max 问题第 j 个约束取 “≥”\n 原问题第 j 个变量无约束，对偶问题第 j 个约束取等式\n\n以上规则是具体的变换规则，实际上其变换规律就是假如原问题不符合以上的给出的 (1) 或 (2) 的标准形状，那么原问题的第\ni 个不符合要求的约束，对应的对偶问题的第 i 个变量要 ≤ 0\n；同样假如原问题的第 i 个变量不符合要求 (就是 \\(x_i\\)≤\n0)，对应的对偶问题的第 i 个约束要改变符号。\n对偶定理包含了一系列的定理，其中主要有弱对偶定理，强对偶定理，最优性定理，互补松弛定理。通过这些定理可以将原来难以解决的原问题通过引入对偶理论从而得以解决，各定理的具体内容如下:\n弱对偶定理 &gt; max\n问题任一可行解的目标值为 min 问题目标值的一个下界； &gt; &gt; min\n问题任一可行解的目标值为 max 问题目标值的一个上界\n强对偶定理 &gt;\n若原问题有最优解，那么对偶问题也有最优解，且两个问题最优解的目标函数值相等。\n无界性 &gt;\n若原问题 (对偶问题) 为无界解，则对偶问题 (原问题) 为无可行解。\n需要注意的是，无界性的逆不存在。若原 (对偶) 问题为无可行解，对偶 (原问题) 问题或为无界解，或为无可行解。\n最优性 &gt; 若 \\(\\overline\nx\\) 和 \\(\\overline y\\)\n分别为原问题和对偶问题的可行解，那么原问题和对偶问题都有最优解，且当\n\\(c^T\\overline x=b^T\\overline\ny\\) 时，\\(\\overline x\\) 和 \\(\\overline\ny\\) 分别为原问题和对偶问题的最优解。\n互补松弛定理 &gt; 若 \\(\\overline x\\) 和 \\(\\overline y\\)\n分别为原问题和对偶问题的可行解，则它们分别是原问题和对偶问题的最优解的充要条件是 \\(\\overline x^T(A^T\\overline y-c)=0\\) 和 \\(\\overline y^T(A\\overline x-b)=0\\)\n通过互补松弛定理，给出原问题 (对偶问题) 的最优解，便可求得其对偶问题 (原问题) 的最优解。\n对应于单纯形法有对偶单纯形法，其解法与单纯形的一样，只是找出基变量和入基变量的方法不一样。\n灵敏度分析\n在许多实际问题中，数据模型的数据未知，需要根据实际情况进行测量、估计和预测，因此这些数据不是十分精确，数据的略微的变化可能会引起问题解的显著变化。所谓灵敏度分析就是研究输入数据的扰动对 LP 最优解的影响，或者说是 LP 最优解对参数变化、约束条件增减、决策变量增减的 Robust (稳健性)。\n灵敏度分析主要就是考虑问题\n\\[\\begin{align}min\\;z =  c^Tx \\quad \\\\\ns.t.\\,Ax \\ge b(x \\ge 0)\\tag{1}\\end{align}\\]\n中，参数 c,b,A 的变化是否会引起最优解的变化。\n\\(c\\) 的变化可分为两种：\\(c\\)\n为非基变量的价值系数和 c 为基变量的价值系数\n1. 非基变量价值系数 \\(c_k\\) 的变化\n假设 \\(\\overline{c_k} = c_k + \\Delta\nc_k\\), 则其在单纯形法中的检验数变为 $ = _k + c_k \\(, 只需要让 \\) $ 即 $_k -c_k\n$ 即可保证最优解不变\n2. 基变量价值系数 \\(c_b\\) 的变化 假设\n\\(\\overline{c_b} = c_b + \\Delta c_b\\),\n则其在单纯形法中的检验数变为 $ = _b - (0,0,0,..c_b,...0,0,0) B^{-1} N \\(, 其中 \\)(0,0,0,..c_b,...0,0,0)\\(为基变量的价值系数的变化量组成的向量，\\)B^{-1}N\n\\(为单纯形表中非基变量对应的系数列组成的矩阵，同样只需要让 \\)\n$ 即可保证最优解不变.\nb 变化的时候需要保证 \\(\\overline{b} =\nB^{-1}(b+\\Delta b) \\ge 0\\), 否则需要将 \\(\\overline{b}\\)\n作为新的 b 的值代入到原来的单纯形表中，让后通过对偶单纯形法进行求解。对偶单纯形法的步骤与原始单纯形法的步骤非常相似，只是选择出基变量和入基变量的方法不同。出基变量选择为\n\\(b_k = min\\;\\lbrace\nb_i,i=1,2,..m\\rbrace\\) 所对应的 \\(x_k\\), 入基变量选择为下面公式对应的 \\(x_l\\),\n\\[\\begin{align} \\frac{\\sigma_l}{a_{kl}} =\nmax\\;[\\frac{\\sigma_j}{a_{kj}}|a_{kj} \\lt 0,j=1,2,...n]\n\\end{align}\\]\n非线性规划\n数学模型与基本概念\n目标函数或约束函数至少有一个不是决策变量的线性函数。即\n\\[\\begin{align}\nmin f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\quad\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\] 其中，\\(f(x),h_i(x),g_j(x)\\)\n中至少有一个是非线性函数。\n梯度与海塞矩阵\n梯度和海塞矩阵是在非线性规划中用得较多的概念，其定义如下：\n梯度 可微函数 \\(f(x)\\) 的梯度，记为 \\(\\nabla f(x)\\), 它是以 \\(f (x)\\) 对 \\(x\\)\n的偏导数为元素的 n 维向量 , 如下所示\n\\[\\begin{align} \\nabla f(x) =\n(\\frac{\\partial f(x)}{\\partial x_1},\\frac{\\partial f(x)}{\\partial\nx_2},....,\\frac{\\partial f(x)}{\\partial x_n}) \\end{align}\\]\n而在某一点的梯度就是将这一点的值代入到上式中，如在点 \\(x_0\\) 上的梯度为\n\\[\\begin{align} \\nabla f(x_0) =\n(\\frac{\\partial f(x_0)}{\\partial x_1},\\frac{\\partial f(x_0)}{\\partial\nx_2},....,\\frac{\\partial f(x_0)}{\\partial x_n}) \\end{align}\\]\n对于一元函数，其梯度就是其一阶导数。\n对于任何函数 \\(f(x)\\)，假如 \\(\\overline x\\) 是 \\(f(x)\\) 的局部极小点且 \\(f(x)\\) 在 \\(\\overline x\\) 处可微，那么必有 \\(\\nabla f(\\overline x) =0\\)\n海塞矩阵\n海塞矩阵的定义与梯度类似，但是求的是二阶偏导，并且结果是一个矩阵，如下所示\n\\[\\begin{align}\n\\nabla^2 f(x) =\n\\begin{pmatrix}\n\\frac{\\partial^2 f(x)}{\\partial^2 x_1} &amp; \\frac{\\partial^2\nf(x)}{\\partial x_1\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2\nf(x)}{\\partial x_1\\partial x_n}\\\\\n\\frac{\\partial^2 f(x)}{\\partial x_2\\partial x_1} &amp; \\frac{\\partial^2\nf(x)}{\\partial^2 x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f(x)}{\\partial\nx_2\\partial x_n}\\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n\\frac{\\partial^2 f(x)}{\\partial x_n\\partial x_1}&amp; \\frac{\\partial^2\nf(x)}{\\partial x_n\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2\nf(x)}{\\partial^2 x_n}\\\\\n\\end{pmatrix}\n\\end{align}\\]\n对于任何函数 \\(f(x)\\)，假如 \\(f(x)\\) 在 \\(\\overline x\\) 处有二阶连续偏导，若 \\(\\nabla f(\\overline x) =0\\) 且海塞矩阵 \\(\\nabla^2 f(\\overline x)\\) 正定，则 \\(\\overline x\\)\n为严格局部最小点。\n凸函数\n凸函数的定义如下\n\n设 \\(f(x)\\) 为定义在 n 维欧氏空间中某个凸集 S 上的函数，若对于任何实数 \\(\\alpha(0&lt;\\alpha&lt;1)\\) 以及 S 中的任意不同两点 \\(x^{(1)}\\) 和 \\(x^{(2)}\\)，均有\n\\[\\begin{align}f(\\alpha x^{(1)}+\n(1-\\alpha)x^{(2)}) \\le \\alpha f(x^{(1)}) + (1-\\alpha)f(x^{(2)})\n\\end{align}\\]\n则称 \\(f(x)\\) 为定义在凸集 S\n上的凸函数。假如上面不等式中的 \\(\\le\\)\n改为 \\(\\lt\\)， 则称其为严格凸函数。\n\n凹函数的定义类似，只需要把上式的不等号方向改变即可。图像直观表示两者如下所示\n\n\n凹函数与凸函数\n\n凸函数的一个重要的性质是其局部极小值点为全局极小值点。\n根据凸函数的定义来判断一个函数是否为凸函数往往比较困难，这里分别通过一阶条件和二阶条件判断凸函数。\n一阶条件 &gt; 设 \\(f(x)\\) 在凸集 S 上有一阶连续偏导数，则 \\(f(x)\\) 为 S 上的凸函数的充要条件为：对于\n任意不同两点 \\(x^{(1)}\\) 和 \\(x^{(2)}\\)，均有 &gt; &gt; \\[\\begin{align} f(x^{(2)}) \\ge f(x^{(1)}) + \\nabla\nf(x^{(1)})^T(x^{(2)} - x^{(1)}) \\end{align}\\]\n二阶条件 &gt; 设 \\(f(x)\\) 在凸集\nS 上有二阶连续偏导数，则 \\(f(x)\\)\n为 S 上的凸函数的充要条件为：\\(f(x)\\)\n的海塞矩阵 \\(\\nabla^2\nf(x)\\) 在 S 上处处半正定 (为凹函数的充要条件为处处半负定)。\n注意：假如海塞矩阵 \\(\\nabla^2\nf(x)\\) 在 S 上处处正定，则 \\(f(x)\\) 为严格凸函数，但是反过来不成立。\n关于正定、半正定、负定的定义及判断方法如下所示\n\n\n正定矩阵和负定矩阵\n\n而顺序主子式的定义如下\n\n\n顺序主子式\n\n通过海塞矩阵判断凸函数例子如下\n\n\n海塞矩阵判断凸函数\n\n凸规划\n凸规划指可行域为凸集，目标函数为凸函数的规划问题。其具体形式如下\n\n\n凸规划模型\n\n注意上面的 \\(g_j(X)\\) 为凹函数，这样围起来的可行域才为凸集。\n因为凸函数和凸集的性质，凸规划有一条重要的性质：凸规划的任一局部极小点为全局极小点。\n一维搜索\n一维搜索就是目标变量只有一个的时候的最优化问题，又称为单变量函数寻优法。\n求解这类问题一般有两种方法，一类是区间收缩法（如黄金分割法），一类是函数逼近法（如三点二次插值法、牛顿法）。下面分别介绍\n单谷函数\n定义：如果函数 \\(f(x)\\) 在区间 [a,b]\n上只有一个极小值点，则称 \\(f(x)\\) 为 [a,\nb] 上的单谷函数。\n单谷函数具有一个重要的消去性质 &gt; 设 \\(f(x)\\) 是区间 [a,b] 上的一个单谷函数，\\(\\overline x \\in\\) [a,b] 是其极小点， \\(x_1\\) 和 \\(x_2\\) 是 [a, b] 上的任意两点，且 \\(a&lt;x_1 &lt;x_2&lt;b\\)，那么比较 \\(f(x_1)\\) 与 \\(f(x_2)\\) 的值后，可得出如下结论：\n1) 若 \\(f(x_1)≥f(x_2),\\overline x\\in\n[x_1,b]\\)\n2) 若 \\(f(x_1) &lt; f(x_2),\\overline x \\in\n[a,x2]\\)\n两种情况如下图所示 \n外推内插法\n在一维搜索中的区间收缩法中需要用到单谷区间，而寻找单谷区间的方法就是下面要介绍的外推内插法。\n其思路为从某个初始点出发，沿函数值下降的方向前进，直至发现函数值上升为止。而由两边高，中间低的三点，可确定极小点所在的初始区间。\n进退算法 1. 选定初始点 a 和步长 h;\n2. 计算并比较 \\(f(a)\\) 和 \\(f(a+h)\\)；有前进和后退两种情况：\n1) 前进算法：若 \\(f(a) \\ge\nf(a+h)\\) 则步长加倍，计算 \\(f(a+3h)\\)。若 \\(f(a+h) \\le f(a+3h)\\)，\\(a_1=a, a_2=a+3h\\),\n停止运算；否则将步长加倍，并重复上述运算。\n2) 后退算法：若 \\(f(a) \\lt\nf(a+h)\\) 则步长改为 \\(-h\\)，计算 \\(f(a-h)\\)。若 \\(f(a-h) \\ge f(a)\\)，\\(a_1=a-h, a_2=a+h\\),\n停止运算；否则将步长加倍，继续后退。\n3. 得到的满足 “中间小两头大” 的三点已经可以作为单谷区间，但是当这个区间太大的时候，也可以进行缩短，缩短的方法如下：\n\n假如得到的三点 \\(a&lt;b&lt;c\\)，在\n\\(b，c\\) 之间内插一点 \\(\\overline b =\n(b+c)/2\\)。这样得到四个点：\\(a\n，b，\\overline\nb，c\\)。比较这 4 个点的函数值，令其中函数值最小的点为 \\(x_2\\) , \\(x_2\\) 的左右邻点为 \\(x_1\\) 和 \\(x_3\\)，至此得到了更小的极值点存在区间\n[\\(x_1,x_3\\)]，且 \\(x_1、x_2、x_3\\)\n三点，满足 “两头大中间小” 的条件。依照此方法可以划出更小的单谷区间。\n\n黄金分割法\n黄金分割法的思想是：反复使用单谷函数的消去性质，不断缩小包含极小点的搜索区间，直到满足精度为止。\n该方法的优点是需计算函数值，通用性强。\n三点二次插值法\n三点二次插值法的思想是：形式复杂的函数进行数学运算时不方便，因此找一个近似的、解析性能好、便于计算的简单函数来代替，用近似函数的极小点作为原函数极小点的近似，常用于近似的简单函数是二次函数。\n三点二次插值多项式近似法（抛物线法）的基本原理为：设目标函数 \\(f(x)\\) 在三点 \\(x_1 &lt; x_2 &lt;x_3\\)\n上的函数值分别为 \\(f_1,f_2,f_3\\), 假设相应的二次插值多项式为 \\(P_2(x)=c＋bx + ax^2\\), 令 \\(P_2(x)\\) 和 \\(f(x)\\) 在三点上的函数值相等，进而求出 \\(P_2(x)\\) 中三个未知参数 \\(a、b、c\\) 的值以及 \\(P_2(x)\\) 的稳定点 \\(\\overline x =\n-\\frac{b}{2a}\\)。而这个方法的命名也源于其原理，通过三个点用二次函数去逼近原函数。其图像直观表示如下\n\n\n三点二次插值\n\n需要注意的是选取的三个点要有一定的要求，若任意取这三个点，则求出的 \\(\\overline x\\)\n可能位于给定区间之外或误差太大，见下图\n\n\n三点二次插值不符合\n\n因此，最初的三个点 \\(x_1&lt;x_2&lt;x_3\\)\n应构成一个两边高，中间低的极小化框架。\n在完成一次计算后，得到近似的 \\(\\overline\nx\\), 要进行搜索区间的收缩，然后在新区间中重新构造三点组成的 “极小化框架”\n。构造的方法为比较 \\(f(\\overline\nx),f(x_2)\\)，以函数值较小的点为中间点，加上左右两点。\n最后终止准则可以采用目标函数值的相对误差或绝对误差来判断。\n牛顿法\n牛顿法是一种函数逼近法，其基本思想是在极小点附近用二阶泰勒多项式近似代替目标函数，求解二阶泰勒多项式的极小点作为目标函数的极小点。牛顿法在数值分析中是用于求解方程的根，而求解函数极值点等价于求其导函数为 0 时的根 (假如函数可微)。\n将 \\(f(x)\\) 在点 \\(x_k\\) 处进行泰勒展开，取前三项有 \\[\\begin{align} f(x) \\approx \\varphi(x) =\nf(x_k)+f^{'}(x_k)(x-x_k)+\\frac{1}{2}f^{\"}(x_k)(x-x_k)^2\n\\end{align}\\]\n求 \\(\\varphi^{'}(x) = 0\\)\n的根，可得 \\[\\begin{align} x_{k+1} = x_k -\n\\frac{f^{'}(x_k)}{f^{\"}(x_k)} \\end{align}\\]\n通过上面公式进行迭代直至 \\(|f^{'}(x_k)|\n\\lt \\epsilon\\) (\\(\\epsilon\\) 为很小的正数)。\n无约束非线性规划\n前面介绍的一维搜索虽然也属于无约束非线性规划，只是仅有一个约束变量。但是由于实际问题中变量的个数往往不止一个，因此多个变量的无约束非线性规划在实际中使用更为广泛。下面主要介绍多变量无约束非线性规划的解决方法。无约束问题的最优化方大致分为两类：\n1.\n直接法：求解过程中只用到目标函数值，无须计算导数。如变量轮换 (坐标轮换)，模式搜索等。\n2.\n解析法：用函数的解析性（一阶、二阶导数），即在计算过程中需要计算目标函数的导数。如：梯度法、共扼梯度法、牛顿法等\n一般来说，解析法的收敛速率较高，直接法的可靠性较高。\n直接法\n坐标 (变量) 轮换法\n坐标轮换法属于直接法，既可以用于无约束优化问题的求解，又可以经过适当处理用于约束优化问题求解。\n坐标轮换法是每次搜索只允许一个变量变化，其余变量保持不变，即沿坐标方向轮流进行搜索的寻优方法。它把多变量的优化问题轮流地转化成单变量（其余变量视为常量）的优化问题，因此又称这种方法为变量轮换法。此种方法只需目标函数的数值信息而不需要目标函数的导数。\n如下图为只有两个变量时进行坐标轮换的搜索过程，可以看到，每一个的移动都只在一个方向上改变\n\n\n坐标轮换法\n\n判断其收敛 (终止) 的可采用点距准则或函数值准则，当点距或函数值只差小于指定的值时则收敛。其中采用的点应该是一轮迭代的始点和终点，而不是某搜索方向的前后迭代点。\n其流程图如下所示 \n坐标轮换法程序简单，易于掌握。但是计算效率比较低，尤其是当优化问题的维数较高时更为严重。一般把此种方法应用于维数小于 10 的低维优化问题。\n模式搜索法\n模式搜索法的思想是算法从初始基点开始，交替实施两种搜索: 轴向搜索和模式搜索。轴向搜索一次沿着 n 个坐标轴方向进行，用来确定新的迭代点和有利于函数值下降的方向。模式搜索则沿着相邻两个迭代点的连线方向进行，试图使函数值下降得更快。\n其搜索过程与坐标轮换法类似，其中轴向搜索其实就是进行了一轮的坐标搜索，与坐标轮换法不同点在于在进行了一轮坐标搜索后会进行模式搜索。如下所示\n模式搜索法的具体过程为 \n通过图像直观表示如下 \n可变单纯形法\n可变单纯形法也称可变多面体搜索法，是一种传统的处理无约束最优化问题的直接算法.\n首先在 n 欧氏空间中构造一个包含 n+1 个顶点的凸多面体，求出各顶点的函数值，并确定其中的最大值、次大值和最小值，然后通过反射、扩张、内缩、缩边等策略求出一个较好解，用之取代最大 (差) 点，从而构成新的多面体，如此多次迭代则可逼近一个性能较好的极小点。\n算法简单、计算量小、优化快速，且不要求函数可导，因而适用范围较广。但它对初始解依赖性较强，容易陷入局部极小点，而且优化效果随函数维数的增加明显下降。\nLagarias (1998) 研究了可变单纯形法求解低维函数时的收敛特性，但结论难以推广到高维问题，也即单一可变单纯形法难以保证对高维复杂函数具有较好的优化效果。\n解析法\n解析法是利用了函数的导数信息的一类方法，其主要思想是通过导数信息找到函数下降的方向，让后沿着这个方向往下走直到走到最小值。\n最速下降法\n最速下降法利用了函数在某点上的负梯度方向是函数在该店下降最快的方向这一结论。其迭代公式为\n\\[\\begin{align} x^{(k+1)} = x^{(k)} +\n\\alpha^{(k)}d^{(k)} \\end{align}\\]\n其中 \\(d^{(k)} = - \\nabla\nf(x^{(k)})\\) 为下降方向，\\(\\alpha^{(k)}\\) 为步长，其求解方法是对 \\(\\alpha^{(k)}\\) 进行一维搜索 (因为此时 \\(x^{(k)},d^{(k)}\\) 已知)，即\n\\[\\begin{align} f(x^{(k)} +\n\\alpha^{(k)}d^{(k)}) =min\\; f(x^{(k)} + \\alpha d^{(k)}) =\nmin\\;\\varphi(\\alpha) \\end{align}\\]\n令 \\(\\varphi^{'}(\\alpha) =\n0\\), 求出的 \\(\\alpha\\)\n的值即为步长。\n其收敛的判断准则是梯度足够小，即其二阶范数 \\(||\\nabla f(x^{(k)})|| \\lt \\epsilon\\)\n在最速下降法中相邻的两个迭代点的梯度是彼此正交的。也即在梯度的迭代过程中，相邻的搜索方向相互垂直。\n因此最速下降法向极小点的逼近路径是锯齿形路线，越接近极小点，锯齿越细，前进速度越慢。这是因为梯度是函数的局部性质，从局部上看，在该点附近函数的下降最快，但从总体上看则走了许多弯路，因此函数值的下降并不快。其示意图如下所示\n\n\n最速下降法示意图\n\n牛顿法\n牛顿法跟最速下降法的思想是一样的，都是找一个能够使函数值下降的方向前进，只是最速下降法找的方向是负梯度方向，而牛顿法找的是牛顿方向。根据每次迭代的步长是否固定，可以将牛顿法分为原始牛顿法和阻尼牛顿法两种，实际中应用较多的是阻尼牛顿法。\n原始牛顿法\n原始牛顿法的思想在一维搜索中已经提到，只是一维搜索中是只有一个 \\(x\\) 变量，而这里处理的是多个 \\(x\\) 变量，相应地用梯度和海塞矩阵代替原来的一阶导数和二阶导数。\n其思想就是在第 k 次迭代的迭代点 \\(x^{(k)}\\)\n邻域内，通过泰勒展开用一个二次函数去近似代替原目标函数 \\(f(x)\\)，然后求出该二次函数的极小点作为对原目标函数求优的下一个迭代点，依次类推，通过多次重复迭代，使迭代点逐步逼近原目标函数的极小点。\n其主要步骤为\n设目标函数 \\(f(x)\\) 具有连续的一、二阶导数，在 \\(x^{(k)}\\) 点邻域内取 \\(f(x)\\) 的二次泰勒多项式作近似式，对于只有一个变量 \\(x\\) 时为\n\\[\\begin{align} f(x) \\approx \\varphi(x) =\nf(x_k)+f^{'}(x_k)(x-x_k)+\\frac{1}{2}f^{''}(x_k)(x-x_k)^2\n\\end{align}\\]\n而有多个变量 \\(x\\) 时，有\n\\[\\begin{align} f(x) \\approx \\varphi(x) =\nf(x^{(k)})+\\nabla f(x^{(k)})^T\\Delta x+\\frac{1}{2}\\Delta x^TH_k\\Delta x\n\\end{align}\\]\n令 \\(x^{(k+1)}\\)\n为函数的极小点，则应有 \\(\\nabla\n\\varphi(x^{(k+1)}) = 0\\)\n令 $ (x)= f (x^{(k)})+H_kx=0$, 且 \\(\\Delta x\n= x^{(k+1)}-x^{(k)}\\)\n则有 \\(x^{(k+1)} = x^{(k)}-H_k^{-1}\\nabla\nf(x^{(k)})\\)，而 \\(-H_k^{-1}\\nabla\nf(x^{(k)})\\) 则为点 \\(x^{(k)}\\)\n处的牛顿方向，也就是该点的下降方向。通过该公式进行迭代直至该点的梯度收敛，即其梯度的二阶范数 \\(||\\nabla f(x^{(k)})|| \\lt \\epsilon\\)。\n牛顿法是具有二次收敛性的算法。若用原始牛顿法求某二次目标函数的最优解，则构造的逼近函数与原目标函数是完全相同的二次式，其等值线完全重合，故从任一点出发，一定可以一次达到目标函数的极小点。\n其优点是：对于二次正定函数，迭代一次即可以得到最优解，对于非二次函数，若函数二次性较强或迭代点已经进入最优点的较小邻域，则收敛速度也很快。\n其缺点是：由于迭代点的位置是按照极值条件确定的，并未沿函数值下降方向搜索，因此，对于非二次函数，有时会使函数值上升，即\n\\(f(x_{k+1}) &gt;\nf(x_k)\\)，而使计算失败。\n阻尼牛顿法\n阻尼牛顿法对原始牛顿法进行了改进，每次迭代加入了步长\n \\(\\alpha^{(k)}\\)，即将迭代公式从\n\\(x^{(k+1)} = x^{(k)}-H_k^{-1}\\nabla\nf(x^{(k)})\\)\n变为了\n\\(x^{(k+1)} =\nx^{(k)}-\\alpha^{(k)}H_k^{-1}\\nabla f(x^{(k)})\\)\n最优步长 \\(\\alpha^{(k)}\\)\n也称为阻尼因子，其求解方法也类似于最速下降中通过一维搜索得到的最优步长。\n其优点是：\n由于阻尼牛顿法每次迭代都在牛顿方向进行一维搜索，避免了迭代后函数值上升的现象，从而保持了牛顿法的二次收敛性，而对初始点的选择没有苛刻的要求。\n缺点是：\n1）对目标函数要求苛刻，要求函数具有连续的一、二阶导数；为保证函数的稳定下降，海赛矩阵必须正定；为求逆阵要求海赛矩阵非奇异。\n2）计算复杂且计算量大，存储量大\n拟牛顿法 (变尺度法)\n从前面介绍的最速下降法和牛顿法可知，梯度法的搜索方向只需计算函数的一阶偏导数，计算量小，当迭代点远离最优点时，函数值下降很快，但当迭代点接近最优点时收敛速度极慢。牛顿法的搜索方向不仅需要计算一阶偏导数，而且要计算二阶偏导数及其逆阵，计算量很大，但牛顿法具有二次收敛性，当迭代点接近最优点时，收敛速度很快。\n若迭代过程先用梯度法，后用牛顿法并避开牛顿法的海赛矩阵的逆矩阵的烦琐计算，则可以得到一种较好的优化方法，这就是 “拟牛顿法” 产生的基本构想。为此，综合梯度法和牛顿法的优点，提出拟牛顿法。\n拟牛顿法的迭代公式与最速下降和阻尼牛顿法类似，\n\\(x^{(k+1)} = x^{(k)}-\\alpha^{(k)}A_k\\nabla\nf(x^{(k)})\\)\n其中 \\(A_k\\) 为构造的构造的\nn×n 阶对称矩阵，拟牛顿方向即为 \\(-A_k\\nabla f(x^{(k)})\\)。\n\n当 \\(A_k =\nI\\) 时，上式为最速下降法的迭代公式 当 \\(A_k =\nH_k^{-1}\\) 时，上式为阻尼牛顿法的迭代公式\n\n拟牛顿法原来使通过 DFP 法构造 \\(A_k\\)，构造过程中避开了二阶导数的计算，因此收敛速度也比较快；但是 DFP 算法由于舍入误差和一维搜索的不精确，有可能导致 \\(A_k\\) 奇异，而使数值稳定性方面不够理想。所以 1970 年提出更稳定的算法，称为 BFGS 算法。这里不详细展开讨论这两种算法。\n共轭梯度法\n共轭梯度法的搜索方向采用梯度法基础上的共轭方向，如图所示，\n\n\n共轭梯度法\n\n目标函数 \\(f(x)\\) 在迭代点 \\(x^{(k+1)}\\) 处的负梯度为 \\(-\\nabla\nf(x^{(k+1)})\\)，该方向与前一搜索方向  \\(S^k\\)\n互为正交，在此基础上构造一种具有较高收敛速度的算法，该算法的搜索方向要满足以下两个条件：\n1）以 \\(x^{(k+1)}\\) 点出发的搜索方向\n\\(S^{k+1}\\) 是 \\(-\\nabla f(x^{(k+1)})\\) 与 \\(S^k\\) 的线性组合。即 \\[\\begin{align}  S^{k+1} = -\\nabla f(x^{(k+1)}) +\n\\beta_kS^k \\end{align}\\]\n\\[\\begin{align} \\beta_k = (\\frac{||\\nabla\nf(x^{(k+1)})||}{||\\nabla f(x^{(k)})||})^2 \\end{align}\\]\n2）\\([S^{k+1}]^TGS^k=0\\)\n除了计算下降方向方法不同，其迭代公式与前面的方法类似，为\n\\(x^{(k+1)} =\nx^{(k)}+\\alpha^{(k)}S^{(k)}\\)\n收敛的判断也是判断梯度的二阶范数\n\\(||\\nabla f(x^{(k+1)})|| \\lt\n\\epsilon\\) 是否成立即可\n共轭梯度法属于解析法，其算法需求一阶导数，所用公式及算法简单，所需存储量少该方法以正定二次函数的共轭方向理论为基础，对二次型函数可以经过有限步达到极小点，所以具有二次收敛性。但是对于非二次型函数，以及在实际计算中由于计算机舍入误差的影响，虽然经过\nn\n次迭代，仍不能达到极小点，则通常以重置负梯度方向开始，搜索直至达到预定精度，其收敛速度也是较快的。\n方法对比\n为了比较各种优化方法的特性，必须建立合理的评价准则。\n无约束优化方法的评价准则主要包括以下几个方面\n1、可靠性。即在合理的精度要求下，在一定允许时间内能解出各种不同类型问题的成功率。能够解出的问题越多，则算法的可靠性越好\n2、有效性。即算法的解题效率。它有两个衡量标准。其一是对同一题目，在相同精度和初始条件下，比较机时多少。其二是在相同精度下，计算同一题目所需要的函数的计算次数。\n3、简便性。一方面指实现该算法的准备工作量的大小。另一方面指算法占用存储单元的数量。\n各个算法的性能对比如下：\n可靠性：牛顿法较差，因为它对目标函数要求太高，解题成功率较低。\n有效性：坐标变换法和梯度法的计算效率较低，因为它们从理论上不具有二次收敛性。\n简便性：牛顿法和拟牛顿法的程序编制较复杂，牛顿法还占用较多的存储单元。\n在选用无约束优化方法时，一方面要考虑优化方法的特点，另一方面要考虑目标函数的情况。\n1、一般而言，对于维数较低或者很难求得导数的目标函数，使用坐标轮换法较合适。\n2、对于二次性较强的目标函数，使用牛顿法效果好。\n3、对于一阶偏导数易求的目标函数，使用梯度法可使程序编制简单，但精度不宜过高\n4、综合而言，共轭梯度法和 DFP 法具有较好的性能。\n约束非线性规划\n约束条件下的非线性规划模型如下所示\n\\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\quad\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\]\n在约束非线性规划中有几个比较重要的概念\n起作用约束和不起作用约束\n对于约束条件 \\(g_j(x) \\ge\n0(j=1,2,...,l)\\), 满足它有两种可能：其一是 \\(g_j(x) \\gt 0\\), 这时，\\(x\\)\n不是处于由这一约束条件形成的可行域的边界上，因此当点不论沿什么方向稍微离开时，都不会违背这一约束条件，这样的约束就称为不起作用约束，即它对\n的微小扰动不起限制作用 , 也就是约束中的所有等式约束均是起作用约束，包括\n所有的 \\(h(x)\\) 和 \\(g(x)\\)\n中取等号的约束。注意，不起作用约束并不是无效约束！\n有效集 (积极集)\n有效集定义为不等式约束中符号为等号的那些约束条件的下标，如对于上面的带约束的非线性规划模型，其有效集为\n\\(I(x) = \\lbrace j|g_j(x)=0, 1 \\le j \\le l\n\\rbrace\\)\nKT 条件\n在不同的资料中，KT (Kuhn-Tucker) 条件也会被称为\nKKT (Karush-Kuhn-Tucker) 条件。原因是这个理论是 Karush (1939 年) 以及\nKuhn 和 Tucker (1951)\n先后独立发表出来的。而且是在 Kuhn 和 Tucker 发表之后才逐渐受到重视，所有很多教材都会将这一条件称为 KT 条件，我们这里只需要知道这 KT 跟 KKT 是一样的东西就可以了。\nKT 条件是非线性规划领域中最重要的理论成果之一，其重要的意义在于它是确定某点是最优点的一阶必要条件，只要是最优点就必须满足这个条件。但一般来说它不是充分条件，即满足 KT 条件的点并不一定是最优点。但是对于凸规划，KT 条件是最优点存在的充要条件。也就是说在凸规划中通过 KT 条件可以找到最优解。\n对于非线性规划模型 \\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\quad\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\]\n其 KT 条件为 \\[\\begin{align}\n\\nabla f(x) - \\sum_{i=1}^mr_i\\nabla h_i(x) - \\sum_{j=1}^l\\lambda_j\\nabla\ng_j(x) = 0\\\\\n\\lambda_j g_j(x) = 0\\quad (j=1,2,3...l)\\\\\n\\lambda_j \\ge 0 \\quad (j=1,2,3...l)\\\\\n\\end{align}\\]\n求解上面的 KT 条件组成的方程组得到的 \\(x\\)\n值就是 KT 点，也就是最优点 (对于凸规划而言)。另外，下式\n$f(x) - {i=1}^mr_i h_i(x) - {j=1}^l_j g_j(x) $\n通常被称为上面非线性规划问题的拉格朗日函数，对应的\n\\((r_1,r_2,...r_m)\\) 和 \\((\\lambda_1,\\lambda_2,...\\lambda_l)\\)\n称为拉格朗日乘子。\n罚函数法\n罚函数法的思想是借助惩罚函数将约束问题装化为无约束问题进行求解，根据惩罚函数的不同，罚函数法又分为外点法，内点法和乘子法。\n外点法\n外点法可以用来解决只有等式约束、只有不等式约束或同时含有等式和不等式约束问题。\n先考虑只有等式约束的问题如下 \\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\end{align}\\]\n则可以将上面的带约束的问题等价于下面的无约束问题\n\\[\\begin{align} min\\;p(x,M) = f(x) +\nM\\sum_i^m [h_i(x)]^2 \\end{align}\\]\n其中 M 是充分大的正数 (求解时一般让其趋于无穷大), 被称为罚因子，而\n\\(p(x,M)\\) 称为惩罚函数，\\(M\\sum_i^m [h_i(x)]^2\\)\n为惩罚项，其作用就是当 \\(x\\)\n不满足任一等式约束 \\(h(x) = 0\\)\n时，罚项就会变得很大从而使解不能满足最小，也就是惩罚了这个解。\n同样对于不等式约束问题 \\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\]\n构造的惩罚函数为\n\\[\\begin{align} p(x,M) = f(x) + M\\sum_j^l\n[min(0, g_j(x))]^2 \\end{align}\\]\n其含义也是当 \\(x\\)\n满足不等式约束条件时，罚项为 0，不满足是罚项就会变得很大，从而使当前的\n\\(x\\) 不忙足目标函数值最小。\n对于同时含有等式约束和不等式约束的规划问题，只要将上面的等式约束和不等式约束中的罚项加在一起即可构造惩罚函数：\n\\[\\begin{align} p(x,M) = f(x) + M( \\sum_i^m\n[h_i(x)]^2 + \\sum_j^l [min(0, g_j(x))]^2) \\end{align}\\]\n求解时只需要用无约束问题的求解方法求解惩罚函数的最小点即可，如下为一个求解的例子\n\n\n外点法求解例子\n\n让 \\(M_k \\rightarrow \\infty\\)\n即可得到最优解为 \\(x =(2,1)^T\\)。\n但是假如让 M 逐步变化，可以得到以下表格\n\n\n外点法渐变过程\n\n从上面的表格可知，当 \\(M_k\\) 从 \\(1 \\rightarrow \\infty\\)\n过程中，罚函数的一系列无约束极小点是从可行域的外部趋近最优解的，因此，这也是外点法名称的来历。\n内点法\n这里讲述的内点法只考虑不等式约束的问题，对于问题\n\\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\]\n其严格内点集合 (又称可行域内部) 定义为\n\\(H = \\lbrace g_j(x) &gt; 0\n|j=1,2,...,l\\rbrace\\)\n内点法就是通过在严格内点集合中进行迭代得到最优解，这也是内点法这一说法的来历，需要注意的是内点法得到的最优解并不一定是全局最优解，因为内点法只是在严格内点中迭代，而全局最优解有可能落在边界上。但是对于最优解不落在边界的问题，内点法能够得到最优解，并且对于那些最优解落在边界上的问题，内点法也能够获得较好的近似解。\n对于上面的模型可以构造障碍函数\n\\[\\begin{align} p(x,r_k) = f(x) + r_k\n\\sum_j^l \\frac{1}{g_j(x)} \\end{align}\\]\n或\n\\[\\begin{align} p(x,r_k) = f(x) - r_k\n\\sum_j^l ln(g_j(x)) \\end{align}\\]\n求解障碍函数的最小值就相当于求解原来的带约束问题；障碍函数类似于外点法中的惩罚函数，其中\n\\(r_k \\sum_j^l \\frac{1}{g_j(x)}\\) 或\n\\(- r_k \\sum_j^l ln(g_j(x))\\)\n被称为障碍项，\\(r_k\\)\n称为障碍因子。\n障碍函数的作用是惩罚靠近可行域边界的 \\(x\\) 点，即那些使 \\(g(x) = 0\\) 的点，当 \\(x\\)\n靠近这些边界的时候，障碍项会变得很大，从而使得其不满足障碍函数最小。\n其求解的一个例子如下\n\n\n内点法求解例子\n\n乘子法\n乘子法类似于上面提到的外点法和内点法，也是通过引入乘子罚函数使约束问题变为无约束问题。但是与前面不同的地方是乘子罚函数是在罚函数的基础上增加了拉格朗日乘子项，从而称为増广拉格朗日函数。这里只讨论等式约束的情况。\n对于问题 \\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\end{align}\\]\n定义增广拉格朗日函数 (乘子罚函数) 为\n\\[\\begin{align} \\varphi (x,\\lambda, M) =\nf(x) - \\sum_{i=0}^m \\lambda_i h_i(x) + \\frac{M}{2} \\sum_{i=1}^m\n[h_i(x)]^2 \\end{align}\\]\n其中，\\(\\overrightarrow \\lambda =\n(\\lambda_1,\\lambda_2,...\\lambda_m)^T\\)\n为拉格朗日乘子向量。则原问题的求解转为了求解增广拉格朗日函数的极小点。\n乘子罚函数 \\(\\varphi (x,\\lambda,\nM)\\) 与普通拉格朗日函数的区别是增加了罚项\n\\(\\frac{M}{2} \\sum_{i=1}^m\n[h_i(x)]^2\\), 与罚函数的区别是增加了乘子项 \\(-\\sum_{i=0}^m \\lambda_i h_i(x)\\).\n假如知道拉格朗日乘子 \\(\\overrightarrow\n\\lambda\\),\n再给定一个足够大的罚因子 M (M 不必趋于无穷大)，就可以通过极小化 \\(\\varphi (x,\\lambda, M)\\)\n得到问题的局部最优解。但是由于 \\(\\overrightarrow \\lambda\\)\n事先无法知道，所以给定一个足够大的 \\(M\\) 和初始的估计量 \\(\\overrightarrow\n{\\lambda^{(1)}}\\)，每次通过下面的公式对 \\(\\overrightarrow \\lambda\\) 进行修正。\n\\[\\begin{align}  \\overrightarrow\n{\\lambda^{(k+1)}} = \\overrightarrow {\\lambda^{(k)}} - Mh_i(x^{(k)})\n\\end{align}\\]\n上式中的 \\(x^{(k)}\\) 是第 k\n次迭代拉格朗日乘子为 \\(\\overrightarrow\n{\\lambda^{(k)}}\\) 时得到的极小点。通过上式进行迭代直至 \\(\\overrightarrow \\lambda\\) 收敛。\n在乘子法中，罚因子 \\(M\\)\n不必趋于无穷大，只要足够大，就可以通过极小化乘子罚函数，得到原来约束问题的局部最优解，而这避免了罚函数法中的病态问题。实践证明，乘子法优于罚函数法，使用范围比罚函数要广。\n下面为一个求解的简单例子\n\n\n乘子法的例子\n\n得到上面的等式后假设 \\(\\overrightarrow\n{\\lambda^{(k)}}\\) 的收敛值为 \\(\\alpha\\), 则有 \\(\\alpha = \\frac{7}{23} \\alpha +\n\\frac{28}{23}\\)\n二次规划\n二次规划是特殊的非线性规划，形式简单，既可以使用求耳机非线性规划的一般方法，又可以使用特定的解法。在实际中有广泛应用，如支持向量机（SVM）本质上就是一个二次规划问题。\n二次规划问题的一般模型也如下 \\[\\begin{align}\nmin\\;f(x)\\\\\ns.t\\quad h_i(x) = 0(i=1,2,...,m)\\\\\n\\quad\\quad g_j(x) \\ge 0(j=1,2,...,l)\\\\\n\\end{align}\\]\n但是要求 \\(f(x)\\)\n是二次函数，而 \\(h_i(x)\\) 和 \\(g_j(x)\\) 是线性函数。\n因此可以展开写成如下的形式：\n\\[\\begin{align}\nmin\\;f(x) = \\frac{1}{2}x^TGx + r^Tx\\\\\ns.t\\quad A_i^Tx - b_i = 0(i=1,2,...,m)\\\\\n\\quad\\quad\\quad A_i^Tx-b_i \\ge 0(i=m+1,...,m+l)\\\\\n\\end{align}\\]\n其中 G 为 \\(n × n\\) 阶对称矩阵 (\n\\(n\\) 为未知变量个数)，\\(r,A_i\\) 为 n 位列向量，\\(b_i\\) 为实数。若矩阵 G\n为 (正定) 半正定矩阵，那么将问题称为严格) 凸二次规划。前面提到，对于凸规划，\\(\\overline x\\)\n为全局极小点的充要条件是该点满足如下的 KT 条件。\n\\[\\begin{align}\nG\\overline x + r - \\sum_{i=1}^{m+l}\\lambda_iA=0\\\\\n\\lambda_i (A_i^Tx-b_i) = 0(i=m+1,...,m+l)\\\\\n\\lambda_i \\ge 0(i=m+1,...,m+l)\n\\end{align}\\]\n通过求解 KT 条件组成的方程组，能够解出二次规划的最优化问题，这只是求解非线性规划的一般方法，还有一些专门用来求解二次规划的方法，对于只含有等式约束的二次规划问题可采用消去法，而同时含有等式约束和不等式约束的解决方法是有效集法，这是更一般的解决方法，下面主要介绍有效集法的原理和过程。\n有效集法又称积极集法，其基本思想是通过求解有限个等式约束二次规划问题来求解一般约束下的二次规划问题。从直观上理解，不起作用的约束在解的附近不起任何作用，可以去掉不考虑，而起作用 (积极) 的不等式约束由于在解处等于 0，故可以用等式约束来代替不等式约束。\n有效集方法中的有效集指的是约束中取等号的那些约束条件，对于上面的问题，定义\n$I(x) = i| A_i^Tx =b_i, m+1 i m+l$\n则有效集为\n\\(E = \\lbrace 1,2,...m\\rbrace \\bigcup\nI(x)\\)\n有效集方法就是将其转化为以下问题进行求解，并对得到的解进行讨论 \\[\\begin{align}\nmin\\;f(x) = \\frac{1}{2}x^TGx + r^Tx\\\\\ns.t\\quad A_i^Tx = b_i (i \\in E)\\\\\n\\end{align}\\]\n其具体计算步骤如下\n \n","categories":["数学"],"tags":["数学"]},{"title":"机器学习中样本比例不平衡的处理方法","url":"/2017/07/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A0%B7%E6%9C%AC%E6%AF%94%E4%BE%8B%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/","content":"在机器学习中，常常会遇到样本比例不平衡的问题，如对于一个二分类问题，正负样本的比例是\n10:1。这种现象往往是由于本身数据来源决定的，如信用卡的征信问题中往往就是正样本居多。样本比例不平衡往往会带来不少问题，但是实际获取的数据又往往是不平衡的，因此本文主要讨论面对样本不平衡时的解决方法。\n\n样本不平衡往往会导致模型对样本数较多的分类造成过拟合，即总是将样本分到了样本数较多的分类中；除此之外，一个典型的问题就是\nAccuracy\nParadox，这个问题指的是模型的对样本预测的准确率很高，但是模型的泛化能力差。其原因是模型将大多数的样本都归类为样本数较多的那一类，如下所示\n\n\n\ncategory\nPredicted Negative\nPredicted Positive\n\n\n\n\nNegative Cases\n9700\n150\n\n\nPositive Cases\n50\n100\n\n\n\n准确率为 \\[\\frac{9700+100}{9700 + 150 + 50\n+ 100} = 0.98\\]\n而假如将所有的样本都归为预测为负样本，准确率会进一步上升，但是这样的模型显然是不好的，实际上，模型已经对这个不平衡的样本过拟合了。\n针对样本的不平衡问题，有以下几种常见的解决思路\n\n搜集更多的数据\n改变评判指标\n对数据进行采样\n合成样本\n改变样本权重\n\n搜集更多的数据\n搜集更多的数据，从而让正负样本的比例平衡，这种方法往往是最被忽视的方法，然而实际上，当搜集数据的代价不大时，这种方法是最有效的。\n但是需要注意，当搜集数据的场景本来产生数据的比例就是不平衡时，这种方法并不能解决数据比例不平衡问题。\n改变评判指标\n改变评判指标，也就是不用准确率来评判和选择模型，原因就是我们上面提到的\nAccuracy Paradox\n问题。实际上有一些评判指标就是专门解决样本不平衡时的评判问题的，如准确率，召回率，F1 值，ROC（AUC），Kappa 等。\n根据这篇文章，ROC\n曲线具有不随样本比例而改变的良好性质，因此能够在样本比例不平衡的情况下较好地反映出分类器的优劣。\n关于评判指标更详细的内容可参考文章： Classification\nAccuracy is Not Enough: More Performance Measures You Can Use\n对数据进行采样\n对数据采样可以有针对性地改变数据中样本的比例，采样一般有两种方式：over-sampling\n和\nunder-sampling，前者是增加样本数较少的样本，其方式是直接复制原来的样本，而后者是减少样本数较多的样本，其方式是丢弃这些多余的样本。\n通常来说，当总样本数目较多的时候考虑\nunder-sampling，而样本数数目较少的时候考虑\nover-sampling。\n关于数据采样更详细的内容可参考 Oversampling\nand undersampling in data analysis\n合成样本\n合成样本 (Synthetic\nSamples) 是为了增加样本数目较少的那一类的样本，合成指的是通过组合已有的样本的各个\nfeature 从而产生新的样本。\n一种最简单的方法就是从各个 feature\n中随机选出一个已有值，然后拼接成一个新的样本，这种方法增加了样本数目较少的类别的样本数，作用与上面提到的\nOver-sampling\n方法一样，不同点在于上面的方法是单纯的复制样本，而这里则是拼接得到新的样本。\n这类方法中的具有代表性的方法是 SMOTE（Synthetic Minority\nOver-sampling Technique），这个方法通过在相似样本中进行 feature\n的随机选择并拼接出新的样本。\n关于 SMOTE 更详细的信息可参考论文 SMOTE: Synthetic\nMinority Over-sampling Technique\n改变样本权重\n改变样本权重指的是增大样本数较少类别的样本的权重，当这样的样本被误分时，其损失值要乘上相应的权重，从而让分类器更加关注这一类数目较少的样本。\n参考：\n8\nTactics to Combat Imbalanced Classes in Your Machine Learning\nDataset In\nclassification, how do you handle an unbalanced training set?\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"机器学习基石 -- 学习的可行性","url":"/2017/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/","content":"本文是《机器学习基石》第四讲 Feasibility of Learning\n的课程笔记。通过 Hoeffding\n不等式，引出了机器学习中学习的可行性。\n\n刚刚接触机器学习的时候，往往会被各种机器学习算法搞得昏头涨脑，却往往忽略了一个问题：那就是机器学习中的 “学习” 二字到底指的是什么，或者说机器为什么能够学习，到底学到了什么东西？\n要回答这个问题，首先要确认一个大前提：数据集（包括训练集和测试集）是从相同分布中产生的，也就是说产生数据的环境应该是一致的，否则假如训练集与数据集的产生方式不一样，那么从训练集中是不可能学习到训练集中相关的知识的\n有了上面这个大前提，从概率论的角度来讲，机器学习学的就是这个概率分布，；或者说是模式识别中的模式（pattern）；然后用学习到的概率分布去预测未见过但是也是在这个概率分布下产生的样本，这样一来，便称机器能够 “学习” 了。\n下面的内容便是将这一过程通过数学来严谨化\nHoeffding 不等式\n为了引入 Hoeffding\n不等式，首先来看一下概率论中一个简单的例子：假如一个罐子中有绿色和橙色两种弹珠，现在想知道罐子中橙色弹珠的比例，该怎么做？\n最直观的方法就是将罐子中所有的弹珠分类并计数，然后计算橙色弹珠的比例。但是当罐子中的弹珠数目变得很大的时候，在实际中显然是无法将所有弹珠都数一遍。\n这时便需要进行抽样并从抽出的样本（sample）中估计橙色弹珠的比例，但是抽样一定会带来一定的误差的，而且直观上来看，抽样的样本数目越多，误差越小。而\nHoeffding\n不等式就是描述这个误差跟抽样数目的关系，假如橙色弹珠的真实比例为 \\(\\mu\\) , 而从样本中估计出的比例为 \\(\\nu\\)， 样本大小为 \\(N\\), 则对应的 Hoeffding 不等式如下\n\\[\\begin{align} p(|\\nu - \\mu| \\gt\n\\epsilon) \\le 2\\exp(-2\\epsilon^2N) \\end{align}\\]\n上式中的 \\(\\epsilon\\)\n表示允许的误差范围\n从 Hoeffding\n不等式到机器学习\n假如将上面的罐子中的一个弹珠抽象为机器学习中的一个样本，考虑一个二分类问题，绿色弹珠表示样本标签与我们的模型\n\\(h\\)\n预测出的标签一致，而橙色弹珠则表示样本标签与预测标签不一致。则橙色弹珠的比例就是模型\n\\(h\\) 的错误率。同时将模型 \\(h\\) 在全部弹珠中的错误率记为 \\(E_{out}(h)\\), 而在样本中的错误率记为 \\(E_{in}(h)\\)，则根据 Hoeffding 不等式有\n\\[\\begin{align} p(|E_{in}(h) - E_{out}(h)|\n\\gt \\epsilon) \\le 2\\exp(-2\\epsilon^2N) \\end{align}\\]\n也就是说，训练样本的数目越大，\\(E_{in}(h)\\) 和 \\(E_{out}(h)\\)，\n也就是训练误差和泛化误差越接近。\n这一等式实际上代表了 PAC\n(probaly approximately correct) 学习理论中的 probably 部分，PAC\n理论简单描述如下 (摘自 Wikipedia)\n\nIn this framework, the learner receives samples and must select a\ngeneralization function (called the hypothesis) from a certain class of\npossible functions. The goal is that, with high probability (the\n\"probably\" part), the selected function will have low generalization\nerror (the \"approximately correct\" part).\n\n上面的 Hoeffding\n不等式只是说明了训练误差和泛化误差可以很接近，但是这一接近必须要在训练误差也就是\n\\(E_{in}(h)\\)\n很小的情况下才有意义，否则大的训练误差就有大的范化误差，而大的范化误差的模型实际上是没有意义的。而小的训练误差对应到\nPAC 中的 AC (approximately correct) 部分。\n从一个 hyposthesis 到多个\nhypothesis\n上面的 Hoeffding 不等式描述的是一个 hypothesis 也就是一个 \\(h\\) 的情况，但是实际中往往有多个 hypothesis\n可选，这个时候的 Hoeffding 不等式又会变成怎样？\n首先回顾一下单个 \\(h\\) 的 Hoeffding\n不等式，它告诉我们下面这个事情发生的概率很大：\\(h\\)\n在抽取的样本上得到的样本误差（也就是训练误差）跟 \\(h\\)\n的总体误差（也就是泛化误差）很接近。\n从另一个角度来讲，也就是说还是有很小的概率抽出一些样本，使得 \\(h\\)\n在样本上得到的误差与其在总体上得到的误差相差很大（实际上就是抽出的样本不能很好反映总体），讲义中将这部分的\nsample 称为 bad data，就是使得 \\(h\\) 的\n\\(E_{in}(h)\\) 很小，\\(E_{out}(h)\\)\n很大的样本。如下图所示，如果进行多次抽样，那么肯定有一些样本会导致 \\(E_{in}(h)\\) 和 \\(E_{out}(h)\\) 的差距较大。\n\n\nbad data for h\n\n\n注：这里 \\(E_{in}(h)\\) 很小，\\(E_{out}(h)\\)\n很大其实已经是我们常听到的过拟合现象，影响过拟合的因素有很多，而抽样的数据的分布是否能够代表整体数据的分布则是其中一个因素。下面是一个简单的例子：对于一个高斯分布产生的数据，如果抽样数据是图中的黑色点，那么拟合出来的曲线可能是图中的黑线，也就是说假如抽样数据的分布如果跟原始数据分布不一致，我们的模型拟合了抽样的数据，对于原始数据而言，自然没有预测能力，也就是 \\(E_{in}(h)\\) 很小，\\(E_{out}(h)\\)\n很大，可以说是过拟合了抽样的数据。\n\n\n\ngaussian_distribution.png-32.7kB\n\n回到讨论的话题，如果对于 \\(M\\)\n个 hypothesis 呢？上图可以改为如下形式\n\n\nbad data for many h\n\n在上图中，由于每个 hypothesis 都不同，因此对各个 hypothesis 而言其\nbad data 也不同，只有当样本对各个 hypothesis 而言都不是 bad\n的时候，才不会泛化误差和训练误差差距很大的情况。在有 \\(M\\) 个 hypothesis 的时候，用 Hoeffding\n不等式表示选择了 bad data 的概率为\n\n\nHoeffding 对多个不等式的情况\n\n上面的不等式表明，对于有限多个 hypothesis 而言，\\(E_{in}(h) \\approx E_{out}(h)\\) 还是 PAC\n的，只是两者误差的 upper bound 变大了，但是数据量 \\(N\\) 的增大能够抵消这一影响。\n在上面的前提下，在有多个 hypothesis 的情况下，只需要选择 \\(E_{in}(h)\\) 小的，就能拿保证 \\(E_{out}(h)\\)\n也是小的，也就是学习是可行的。\n小结\n本文主要是通过 Hoeffding 不等式证明了当模型的所有 hypothesis\n的个数 \\(M\\) 为有限个时，样本数目 \\(N\\) 足够大时，就能够保证泛化误差 \\(E_{out}(h)\\) 和训练误差 \\(E_{in}(h)\\) 很接近。\n这时候只要找到一个 hypothesis 使得 \\(E_{in}(h)\\) 很小，那么 \\(E_{out}(h)\\)\n也会很小，从而达到学习的目的。\n当然有一个大前提就是训练样本和测试样本必须要在同一分布下产生，否则学习无从谈起。\n上面的内容可通过下图进行描述\n\n\n模型图\n\n但是还有一个问题，就是实际中某个模型空间里的 hypothesis\n往往是无限多个的，这种情况下又该如何通过数学描述？这部分内容将在后面讲述。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"机器学习基石 --PLA","url":"/2017/02/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--PLA/","content":"本文是《机器学习基石》第二讲 Learning to Answer Yes/No\n课程的笔记。主要介绍了机器学习的基本概念，以及感知机及其训练算法\nPLA。\n\n机器学习的基本概念\n该讲以根据客户的特征来决定是否给客户发放信用卡的例子引出了机器学习要解决的问题：对于我们的问题，存在着一个未知的理想目标函数\n\\(f\\)\n能够满足我们的决策需求 (在该例子中就是根据给定用户的特征，输出是否给用户发放信用卡)，但是这个函数是未知的，我们只能从观测到的数据集\n\\(D\\) 中通过算法 \\(A\\) 提取出一个近似的函数 \\(g\\) 来逼近理想的目标函数 \\(f\\)。而当选定了算法 \\(A\\) 后，选取 \\(g\\) 的集合 \\(H\\)\n实际上也确定了 (例如目标函数是线性或非线性的)。\n下图展示了上面提到的过程\n\n\n机器学习基本概念\n\n从上面概括了关于机器学习中的基本过程。但是在实际中通过机器学习解决问题的过程往往是的论述可知，机器学习中首先需要确定目前的一个问题，然后根据问题提出假设并依照假设去搜集数据，然后对数据进行特征提取、转换等，接着尝试通过不同算法去建模并验证。在这门课程中往往更关心数据特征的提取以及不同算法的研究，这固然很重要，但是实际中确认问题以及搜集相应的数据也是一个非常重要的步骤，不可忽视。\n下面要介绍这门课程的第一个算法：\n感知机\n感知机是神经网络的基础，与线性回归（Linear\nRegression），逻辑回归（Logistics\nRegression）等模型也非常类似，是一种非常典型的线性模型。\n原始的感知机算法用于解决二分类问题，其思想如下：假设样本有 \\(d\\)\n个特征，但是每个特征的重要性不一样，因此各个特征的权重也不一样，对其进行加权后得到的总和假如大于某个阈值则认为归为其中一类，反之归为另一类。如在信用卡的例子中，通过感知机有如下的结果\n\n\n感知机的信用卡发放问题\n\n将上面的化为常数项，即可将得到更一般的表达形式如下\n\n\n感知机更一般表示\n\n上面的 \\(w\\) 和 \\(x\\) 均是一个列向量。\nPLA\n上面只是说明了感知机这一算法的基本模型，但是感知机还要通过学习才能对样本进行正确的分类，这个学习的过程就是我们下面要讲的\nPLA (Perceptron Learning Algorithm)。PLA 的过程如下\n1）随机初始化参数 \\(w\\) 2）利用参数\n\\(w\\)\n预测每个样本点的值并与其实际的值比较，对于分类错误的样本点 \\((x_n, y_n)\\)，利用公式 \\(w = w + y_nx_n\\) 更新参数 \\(w\\) 的值\n3）重复上面的过程直到所有的样本点都能够被参数 \\(w\\) 正确预测。\n对于某个被预测错误的样本点，参数 \\(w\\) 更新的过程如下所示\n\n\n参数 w 在错误点的更新\n\n注意上面的算法的前提是所有的样本点都必须线性可分，假如样本点线性不可分，那么 PLA 按照上面的规则会陷入死循环中。如下是线性可分与线性不可分的例子)\n\n\n线性可分与不可分的例子\n\n收敛性\n上面提到只有当所有的样本均为线性可分时，PLA 才能将所有的样本点正确分类后再停下了，但是这仅仅是定性的说明而已，并没有严格的数学正面来支撑其收敛性，下面要讲的便是通过数学证明来说明\nPLA 算法的收敛性。\n建议中通过下面两页 PPT 来说明 PLA 的收敛性\n\n\nPLA 收敛性 1\n\n上面讲的是随着参数 \\(w\\)\n的更新，\\(w_f^Tw_{t+1}\\)\n的值越来越大，也就是两者越来越相似（衡量两个向量相似性的一种方法就是考虑他们的内积，值越大，代表两者约接近，但是这里还没对向量归一化，所以证明并不严格，但是已经说明了两者具有这个趋势，下面是更严格的过程）\n\n\nPLA 收敛性 2\n\n上面似乎只是说明了经过 T 次的纠错，\\(w_t\\)\n的值会限制在一个范围内，但是并没有给出最终结论 \\[\\frac{w_f}{||w_f|| }\\frac{w_T}{||w_T||} \\ge\n\\sqrt{T} *\nconstant\\] 的证明过程，因此这里对其推导过程进行描述。(注：这里的\n\\(w_f\\) 是不变的，因此 \\(w_f\\) 与 \\(w_f^T\\) 是一样的)\n假设经过了 T 次纠错，那由第一张 PPT 可知 \\[\\begin{align} w_f^Tw_T \\ge w_f^Tw_{T-1} + \\min_n\ny_nw_f^Tx_n \\ge T\\min_n y_nw_f^Tx_n \\end{align}\\]\n而由第二张 PPT 可知\n\\[\\begin{align}||w_T||^2 \\le ||w_{T-1}||^2\n+ \\max_n||x_n||^2 \\le T\\max_n||x_n||^2\\\\\n||w_T|| \\le \\sqrt{T} \\max_n||x_n||\\end{align}\\]\n综合上面的两条式子有\n\\[\\begin{align}\n\\frac{w_f^T}{||w_f^T||}\\frac{w_T}{||w_T||} \\ge \\frac{T\\min_n\ny_n^Tw_f^Tx_n}{||w_f^T||\\sqrt{T} \\max_n||x_n||} = \\sqrt{T} \\frac{\\min_n\ny_n\\frac{w_f^T}{||w_f^T||}x_n}{\\max_n||x_n||} = \\sqrt{T} * constant\n\\end{align}\\]\n因此上面的命题得证。至此，已经可知道犯错误的次数 T\n是受到某个上限的约束的。下面会给出这个具体的上限是多少。\n又因为 \\[1 \\ge\n\\frac{w_f^T}{||w_f^T||}\\frac{w_T}{||w_T||} \\ge \\sqrt{T} * constant\\\\\\\n\\frac{1}{constant^2} \\ge T\\]\\[\\begin {align}\n\n即犯错的次数的上限为 $\\frac {1}{constant^2} $, 假设令\n$$\\begin {align}  \\max_n||x||^2 = R^2, \\rho = \\min_n\ny_n\\frac {w_f^T}{||w_f^T||} x_n \\end {align}\\]\\[\n则有\n\\]\\[\\begin{align} T \\le\n\\frac{R^2}{\\rho^2} \\end{align}\\]$$\n这也说明了 PLA 会在有限步内收敛，而这也是后面的练习题里面的答案。\n优缺点及改进\nPLA\n的优点和缺点都非常明显，其中优点是简单，易于实现，但是缺点是假设了数据是线性可分的，然而事先并无法知道数据是否线性可分的。正如上面提到的一样，假如将 PLA\n用在线性不可分的数据中时，会导致 PLA 永远都无法对样本进行正确分类从而陷入到死循环中。\n为了避免上面的情况，将 PLA\n的条件放宽一点，不再要求所有的样本都能正确地分开，而是要求犯错的的样本尽可能的少，即将问题变为了 \\[arg\\min_w \\sum_{n=1}^{N} 1\\lbrace y_n \\ne\nsign(w^Tx_n) \\rbrace\\]\n这个最优化问题是个 NP-hard\n问题，无法求得其最优解，因此只能求尽可能接近其最优解的近似解。讲义中提出的一种求解其近似解的算法\nPocket Algorithm。其思想就是每次保留当前最好的 \\(w\\), 当遇到错误的样本点对 \\(w\\) 进行修正后，比较修正后的 \\(w\\) 与原来最好的 \\(w\\)\n在整个样本点上的总体效果再决定保留哪一个，重复迭代足够多的次数后返回当前得到的最好的\n\\(w\\)。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"机器学习基石 -- 机器学习的分类","url":"/2017/02/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB/","content":"本文是《机器学习基石》第三讲 Types of Learning\n课程的笔记。主要概括性介绍了机器学习的几个分类标准及其具体分类。\n\n根据输出来分\n根据模型输出来对机器学习分类是最常见的分类方法，往往可以分为回归（regression）和分类（classification）两大问题，而根据类比数量的不同，分类问题又往往又有二分类和多分类两种。\n除了分类和回归，讲义中还提到了一种\nStructured Learning, 这一类型的输出的信息之间有结构化的信息。如输入\n\"I love ML\",\n输出该句子中各个词对应的词性（如 PVN，PVP，NVN 等），可以粗略将其看作是一个多分类问题，只是这个多分类问题的类别非常多且类比没有明确的定义。用到这一类型学习方法的例子还包括语音识别（语音 -&gt; 句子）等\n根据样本来分\n根据给出的样本是否有标记对机器学习进行分类也是一种常见的分类方法，可分为有监督学习（supervised），无监督学习（unsupervised）和半监督学习（semi-supervised）。\n有监督学习就是除了给出样本的属性 \\(x\\) 外，还给出了样本的标记 \\(y\\), 这个标记可以是样本的分类等。常见的分类、回归等一般都是有监督学习。\n而无监督学习则是只给出给出样本的属性 \\(x\\)，让后要求找出这些样本内在属性及联系。其代表的应用是聚类，如给出若干无标记的文章，根据其主题将其聚成不同的类，最常见的聚类方法是 K-Means，但是解决文本聚类问题一般通过主题模型，常见的有 LSA，pLSA，LDA，HDP 等；除此之外，无监督学习还应用到密度估计（density\nestimation）中，如根据交通事故的发生地点做密度估计，从而得到危险的地段，一般解决这类问题可通过混合高斯模型等；无监督学习还可应用在异常点检测（outlier\ndetection）中，例如从海量的用户日志中找到某个可疑的用户操作，解决这类问题的方法也有很多，比如通过 PCA 映射到低维度后通过可视化来找。\n无监督学习解决了有监督学习中需要获取大量标记样本带来的困难，而半监督学习则是介于两者之间的一种方法。半监督学习主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。如\nactive\nlearning\n就是半监督学习的一种，其思想就是让机器标注数据然后对于有疑问的标记进行 “提问”。\n除了上面提到的三种的学习方法，讲义还提到了另外一种不那么明显依赖于样本的学习方法\nReinforcement Learning：强化学习 (增强学习)，这种学习方法通过奖励或惩罚来训练当前的算法。常见的应用有机器人以及各种\nAI Game（如 AlphaGo，Fine Art 等）\n根据训练方式来分\n这里根据训练方式主要是指训练时数据的输入方式，根据数据是否一次性送入到模型中训练将其分为\nbatch learning 和 online learning。\nbatch learning 指数据整批输进去，训练出一个模型用于预测等。\n而 online learning 指每次有新样本的时候就用来训练更新\nhypothesis，常见的比如说有垃圾邮件分类系统，这里有两点需要注意：\n一是这种方法往往是依赖于训练算法的，如 SGD 等就适用在 online learning\n中，因为其每次重新训练只需要依靠新的样本即可，而其他一些算法如果要加入新的数据就需要将所有的数据重新进行训练，这样的算法如果用在\nonline learning 中的代价就太大了。\n二是虽然说每次有新的样本就训练更新\nhypothesis，但是也不是来一个就更新一下，这样的训练成本也很高，实际中往往是等样本数积累到一定数量的时候才对这一批进行一个训练和更新。就像\ngradient descent 中的 mini-batch。\n根据输入来分\n根据输入的样本的特征来分也可以分为下面三类（虽然这中分类方法并不常见）：concrete\nfeatures，raw feaures 和 abstract features。\nconcrete features\n指输入的样本已经明确给出了其各种特征，如信用卡例子中顾客的各项资料等。\nraw feaures\n一般指图像或音频中的图像或声波，这些信息是原始的信号，需要进行一些转换才能使用。\nabstract features 并没有一个严格定义，原讲义给出了 KDDCup 2011\n的例子：\n\ngiven previous (userid, itemid, rating) tuples, predict the rating\nthat some userid would give to itemid\n\n这种按照输入样本的 features\n进行分类的方法在实际中并不常用，因为输入的样本往往是各种\nfeatures 交杂在一起的，不同问题需要与其相应的 features\n才能得到好的效果，features 对结果的影响比较大。因此机器学习中也产生了\nfeature engineering 一说。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"概率论与数理统计知识整理 (2)-- 二维随机变量的分布","url":"/2016/10/08/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(2)--%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%88%86%E5%B8%83/","content":"前面讲到的随机变量都是一维的，但是某些试验中随机变量可能有多个，这里主要讨论二维的随机变量。\n\n联合分布函数\n假设 \\(X\\) 和 \\(Y\\)\n都是随机变量，那么我们定义其分布函数如下：\n\\[\\begin{align}  F(x,y) = P ((X \\le\nx)\\cap(Y \\le y)) =  P (X \\le x, Y \\le y ) \\end{align}\\]\n上面的 \\(F(x,y)\\)\n称作随机变量 (X,Y) 的分布函数，也叫作联合分布函数。\n离散型随机变量联合分布\n如果上面的 \\(X\\) 和 \\(Y\\)\n都是离散随机变量，那么对于 \\((X,Y)\\) 的所有取值可记为\n\\[\\begin{align} P(X=x_i, Y=y_i) =\np_{ij},i,j=1,2,.... \\end{align}\\]\n上面的所有 P 的取值为二维离散随机变量的分布律，也叫联合分布律。直观用表格表示如下所示\n\n连续型随机变量联合分布\n类似地，如果上面的 X 和 Y 都是连续随机变量，那么分布函数可定义为\n\\[\\begin{align}  F(x,y) =\n\\int_{-\\infty}^y\\int_{-\\infty}^xf(u,v)dudv  \\end{align}\\]\n其中 \\(f(x,y)\\)\n被称为概率密度函数，也叫联合概率密度函数。\n其性质与一维随机变量的概率密度函数非常相似\n\n\\[\\begin{align}f(x,y) \\ge\n0\\]\\begin{align}\n\\end{align}\\[\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f(x,y)dxdy\n= F(\\infty,\\infty)\\end{align}\\]\n\n3. 设 \\(G\\) 是 \\(xOy\\) 平面上的区域，点 \\((X,Y)\\) 落在 G 内的概率为\n\\[\\begin{align} P((X,Y)\\in G) = \\int\\int\nf(x,y)dxdy \\end{align}\\]\n4. 若 \\(f(x,y)\\) 在点 \\((x, y)\\) 连续，则\n\\[\\begin{align}\n\\frac{\\partial^2F(X,Y)}{\\partial x \\partial y} = f(x, y)\n\\end{align}\\]\n边缘分布函数\n二维随机变量 \\((X,Y)\\)\n作为一个整体的时候，其分布函数为联合分布函数，但是 \\(X\\) 和 \\(Y\\)\n是随机变量，各自也有分布函数，将其分别记为 \\(F_X(x),F_Y(y)\\)，称为随机变量 \\((X,Y)\\) 关于 \\(X\\) 和关于 \\(Y\\) 的边缘分布函数。\n边缘分布函数可通过联合分布函数确定，关系如下\n\\[\\begin{align} F_X(x) = P(X \\le x) = P(X\n\\le x,Y \\lt \\infty) = F(x, \\infty) \\end{align}\\]\n即\n\\[\\begin{align} F_X(x) = F(x,\\infty)\n\\end{align}\\]\n也就是说在联合分布函数 \\(F(x,y)\\)\n中令 \\(y \\rightarrow \\infty\\)\n即可得到边缘分布 \\(F_X(x)\\), 同理 \\[F_Y(y) = F(\\infty, y)\\]\n下面分别以离散型随机变量和连续性随机基量为例说明\n离散型随机变量边缘分布\n假如 \\(X\\) 和 \\(Y\\) 是离散型随机变量，那么随机变量 \\((X,Y)\\) 关于 \\(X\\) 和关于 \\(Y\\) 的边缘分布定义下\n\\[\\begin{align} p_{i.} =\n\\sum_{j=1}^{\\infty} p_{ij} = P(X = x_i), i=1,2,3.....n\n\\end{align}\\]\n\\[\\begin{align} p_{.j} =\n\\sum_{i=1}^{\\infty} p_{ij} = P(Y = y_j), j=1,2,3.....n\n\\end{align}\\]\n上面的式子分别称为随机变量 \\((X,Y)\\)\n关于 \\(X\\) 和关于 \\(Y\\) 的边缘分布率。\n连续型随机变量边缘分布\n假如 \\(X\\) 和 \\(Y\\) 分别是连续性随机变量，那么随机变量\n\\((X,Y)\\) 关于 \\(X\\)\n的边缘分布函数定义为\n\\[\\begin{align} F_X(x) = F(x,\\infty) =\n\\int_{-\\infty}^{x}(\\int_{-\\infty}^{\\infty}f(x,y)dy)dx\n\\end{align}\\]\n而\n\\[\\begin{align}  f_X(x) =\n\\int_{-\\infty}^{\\infty}f(x,y)dy \\end{align}\\]\n则被称为随机变量 \\((X,Y)\\) 关于\n\\(Y\\) 的\n边缘概率密度函数\n条件分布\n由条件概率可以比较容易推导出条件分布的含义，其定义如下：\n离散型随机变量的条件分布\n对于离散型随机变量，条件分布的定义如下：\n\n设 \\((X,Y)\\)\n是二维离散型随机变量，对于固定的 \\(j\\)，若 \\(P(Y=y_j) \\gt 0\\), 则称 \\[\\begin{align} P(X = x_i|Y= y_j) = \\frac{P(X =\nx_i, Y=y_j)}{P(Y=y_j)} = \\frac{p_{ij}}{p_{.j}}, i = 1,2,3\n\\end{align}\\] 为在 \\(Y=y_j\\)\n条件下随机变量 X 的条件分布律。同理，交换 \\(X\\) 和 \\(Y\\) 的位置得到的是在 \\(X=x_i\\) 条件下随机变量 \\(Y\\) 的条件分布律。\n\n连续型随机变量的条件分布\n对于连续型的随机变量，条件分布的定义如下：\n\n设二维随机变量 \\((X,Y)\\)\n的概率密度函数为 \\(f(x,y),(X,Y)\\) 关于\n\\(Y\\) 的边缘概率密度为 \\(f_Y(y)\\) . 若对于固定的 \\(y，f_Y(y) &gt;0\\) ，则称 \\(\\frac{f(x,y)}{f_Y(y)}\\) 为在 \\(Y=y\\) 的条件下 \\(X\\) 的条件概率密度。记为 \\[\\begin{align} f_{X|Y}(x|y) =\n\\frac{f(x,y)}{f_Y(y)} \\end{align}\\]\n\n有了条件概率密度 (就是条件概率密度函数)，我们也可以定义出条件分布函数如下\n\\[\\begin{align} \\int_{-\\infty}^x\nf_{X|Y}(x|y)dx = \\int_{-\\infty}^x \\frac{f(x,y)}{f_Y(y)}dx\n\\end{align}\\]\n上面的函数为在 \\(Y=y\\) 的条件下\n\\(X\\) 的条件分布函数，记为\n\\(F_{X|Y}(x|y) = P(X \\le x|\nY=y)\\)\n相互独立的随机变量\n两个随机变量 \\(X,Y\\)\n相互独立的充要条件如下：\n\\(F(x,y) = F_X(x)F_Y(y)\\)\n上面的 \\(F(x,y),F_X(x),F_Y(y)\\)\n分别是二维随机变量的联合分布函数及关于 \\(X\\) 和 \\(Y\\) 的边缘分布函数。\n除了通过分布函数，对于具体的连续型随机变量或离散型随机变量，还可通过概率密度函数和分布律来定义相互独立的条件。\n对于连续型随机变量，上面的式子等价于\n\\(f(x,y) = f_X(x)f_Y(y)\\)\n式子中的 \\(f(x,y),f_X(x),f_Y(y)\\)\n分别为 随机变量 \\((X,Y)\\)\n的条件概率密度函数和边缘概率密度函数。\n对于离散型随机变量则有：\n\\(P(X = x_i, Y = y_j) =\nP(X=x_i)P(Y=y_j)\\)\n二维随机变量的函数的分布\n在讨论一维随机变量的分布函数的时候，也讨论了一维随机变量的函数的分布函数，同样对于二维随机变量，我们也可以讨论其函数的分布函数。下面主要讨论\n\\(Z=X+Y\\)，\\(Z=XY\\)，\\(Z=Y/X\\)，\\(M=max(X,Y)\\)，\\(N=min(X,Y)\\) 这几个函数的分布函数（\\(X，Y\\)\n为相互独立的随机变量），这里主要给出具体的公式，证明省略。\n\\(Z = X + Y\\)\n的分布\n设 \\((X,Y)\\)\n是二维连续型随机变量，其概率密度函数为 \\(f(x,y)\\)， \\(Z =\nX+Y\\) 仍然为连续性随机变量，其概率密度函数为\n\\[\\begin{align} f_{X+Y}(z) =\n\\int_{-\\infty}^{\\infty} f(z-y,y)dy \\end{align}\\] 或 \\[\\begin{align} f_{X+Y}(z) =\n\\int_{-\\infty}^{\\infty} f(x,z-x)dx \\end{align}\\]\n当 \\(X,Y\\)\n相互独立时，其边缘概率密度函数具有以下性质\n\\(f(x,y) = f_X(x)f_Y(y)\\)\n因此上面的式子也可以化成下面的形式\n\\[\\begin{align} f_{X+Y}(z) =\n\\int_{-\\infty}^{\\infty} f_X(z-y)f_Y(y)dy \\end{align}\\]\n\\[\\begin{align} f_{X+Y}(z) =\n\\int_{-\\infty}^{\\infty} f_X(x)f_Y(z-x)dx \\end{align}\\]\n\\(Z=XY\\) 和\n\\(Z=Y/X\\) 的分布\n设 \\((X,Y)\\)\n是二维连续型随机变量，其概率密度函数为 \\(f(x,y)\\)， \\(Z =\n\\frac{Y}{X},Z = XY\\) 仍然为连续性随机变量，其概率密度函数为\n\\[\\begin{align} f_{Y/X}(z) =\n\\int_{-\\infty}^{\\infty} |x|f(x,xz)dx \\end{align}\\]\n\\[\\begin{align} f_{XY}(z) =\n\\int_{-\\infty}^{\\infty} \\frac{1}{|x|}f(x,z/x)dx \\end{align}\\]\n当 \\(X,Y\\)\n相互独立时，同样有下面的性质\n\\[\\begin{align} f_{Y/X}(z) =\n\\int_{-\\infty}^{\\infty} |x|f_X(x)f_Y(xz)dx \\end{align}\\]\n\\[\\begin{align} f_{XY}(z) =\n\\int_{-\\infty}^{\\infty} \\frac{1}{|x|}f_X(x)f_Y(z/x)dx\n\\end{align}\\]\n\\(M =\nmax(X,Y)\\) 和 \\(N = min(X,Y)\\)\n的分布\n讨论 \\(max(X,Y)\\) 和 \\(min(X,Y)\\) 的分布的时候， 一般假设 \\(X, Y\\)\n相互独立，因为这样才有下面的性质。\n对于 \\(M = max(X,Y)\\) 的分布有\n\\(F_{max}(z) = P(M \\le z) = P(X \\le z, Y\n\\le z) = P(X \\le z)P(Y \\le z)\\)\n由于 \\(X\\) 和 \\(Y\\) 相互独立，因此有\n\\(F_{max}(z) = F_X(z)F_Y(z)\\)\n同样对 \\(N = min(X,Y)\\) 有\n\\(F_{min}(z) = P(N \\le z) = 1 - P(N \\gt z)\n= 1 - P(X &gt; z)P(Y&gt;z)\\)\n即\n\\(F_{min}(z) = 1 - (1 - F_X(z))(1 -\nF_Y(z))\\)\n推广到 \\(n\\)\n个相互独立的随机变量有下面的性质\n\\(M = max \\lbrace X_1,X_2...,X_n\n\\rbrace\\) 及 \\(N = min\\lbrace\nX_1,X_2...,X_n \\rbrace\\) 的分布函数分别为\n\\[\\begin{align} F_{max}(z) =\nF_{X_1}(z)F_{X_2}(z)...F_{X_n}(z) \\end{align}\\]\n\\[\\begin{align} F_{min}(z) = 1 - (1 -\nF_{X_1}(z))(1 - F_{X_2}(z))...(1 - F_{X_n}(z)) \\end{align}\\]\n而当 $ X_1,X_2...,X_n $ 独立同分布的时候，上式变为如下所示\n\\[\\begin{align} F_{max}(z) = [F(z)]^n\n\\end{align}\\]\n\\[\\begin{align} F_{min}(z) = 1 - (1 -\nF(z))^n \\end{align}\\]\n","categories":["数学"],"tags":["数学"]},{"title":"梯度裁剪及其作用","url":"/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/","content":"本文简单介绍梯度裁剪 (gradient clipping) 的方法及其作用，最近在训练 RNN\n过程中发现这个机制对结果影响非常大。\n\n梯度裁剪一般用于解决 梯度爆炸 (gradient explosion)\n问题，而梯度爆炸问题在训练 RNN 过程中出现得尤为频繁，所以训练 RNN\n基本都需要带上这个参数。常见的 gradient clipping 有两种做法\n\n根据参数的 gradient 的值直接进行裁剪\n根据若干参数的 gradient 组成的 vector 的 L2 norm 进行裁剪\n\n第一种做法很容易理解，就是先设定一个 gradient 的范围如 (-1, 1), 小于\n-1 的 gradient 设为 -1， 大于这个 1 的 gradient 设为 1.\n第二种方法则更为常见，先设定一个 clip_norm,\n然后在某一次反向传播后，通过各个参数的 gradient 构成一个\nvector，计算这个 vector 的 L2 norm（平方和后开根号）记为\nLNorm，然后比较 LNorm 和\nclip_norm 的值，若 LNorm &lt;=\nclip_norm 不做处理，否则计算缩放因子\nscale_factor = clip_norm/LNorm\n，然后令原来的梯度乘上这个缩放因子。这样做是为了让 gradient\nvector 的 L2 norm 小于预设的 clip_norm。\n关于 gradient clipping 的作用可更直观地参考下面的图，没有 gradient\nclipping 时，若梯度过大优化算法会越过最优点。\n\n\ngradient clipping\n\n而在一些的框架中，设置 gradient clipping 往往也是在 Optimizer\n中设置，如 tensorflow 中设置如下\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)gvs = optimizer.compute_gradients(cost)capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]train_op = optimizer.apply_gradients(capped_gvs)\nKeras 中设置则更为简单\noptimizer = optimizers.SGD(lr=0.001, momentum=0.9, clipnorm=1.),\n除此之外，调试 RNN 是个比较 tricky 的活，可参考知乎上这个问题：你在训练 RNN 的时候有哪些特殊的 trick？\n另外，与 grdient explosion 相反的问题 gradient vanishing,\n解决方法跟上面不同，不能简单地采用 scaling 的方法，具体可参考这个问题 梯度消失问题为什么不通过\ngradient scaling 来解决？，实际的处理方法一般是采用 LSTM 或 GRU\n这类有记忆的 RNN 单元。\n\n参考：\n\nGradient\nClipping\ncaffe 里的 clip\ngradient 是什么意思？\nWhat\nis gradient clipping and why is it necessary?\n\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"概率论与数理统计知识整理 (1)-- 一维随机变量的分布类型","url":"/2016/10/03/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(1)--%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%88%86%E5%B8%83%E7%B1%BB%E5%9E%8B/","content":"本文主要讲述三种离散型随机变量的分布 (伯努利分布，二项分布，泊松分布) 和三种连续型随机变量的分布 (均匀分布，指数分布，正态分布)。\n\n离散型随机变量的分布\n伯努利分布\n伯努利分布又名两点分布或者 0-1 分布，只能取两种结果，一般记为 0 或 1。设取 1 的概率为\n\\(p\\)，其分布规律为\n\\(P(X=k) = p^k(1-p)^{1-k}, k = 0,1\n(1&lt;p&lt;1)\\)\n如果试验 E 只有两种结果，则称试验 E 为伯努利试验。如果独立的进行\n\\(n\\)\n次试验 E，则称为 n 重伯努利试验。\n二项分布\n以 \\(X\\) 表示 \\(n\\) 重伯努利试验中事件 A 出现的次数，则\n\\(X\\) 为一个随机变量，令 \\(p\\) 为事件 A 出现的概率，则事件 \\(X\\) 服从以下分布：\n\\(P(X=k) = C_n^k p^k q^{n-k}, k =\n0,1,2,...,n\\)\n其中 \\(q = 1-p\\), 则我们称 X\n服从参数为 n,p 的二项分布，记为 \\(X\\)~\\(b(n,p)\\)。其概率和根据以下公式为 1\n\\(\\sum_{k=1}^nP(X=k) = \\sum_{k=1}^nC_n^k\np^k q^{n-k} = (p+q)^n = 1\\)\n泊松分布\n随机变量 \\(X\\)\n的所有可能取值为 0,1,2,3...., 且各个取值的概率为\n\\(P(X=k) = \\frac\n{\\lambda^ke^{-\\lambda}}{k!}, k=0,1,2....\\)\n则我们称 \\(X\\) 服从参数为 \\(\\lambda\\) 的泊松分布，记为 \\(X\\)~\\(\\pi(\\lambda)\\) 其中 \\(\\lambda\\) 为 \\(X\\) 的期望，即 \\(E(X)\\)\n其分布规律通过图像直观表示为\n\n泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA 序列的变异数、放射性原子核的衰变数等等。从上面的图像也可以看出，对于给定泊松分布的强度\n\\(\\lambda\\)\n, 其在单位时间内发生的次数的概率有一个峰值，也就是说发生的次数很多或很少的可能性都不大，且当强度越大，其最大可能发生的次数的值也越大。\n下面介绍用泊松分布来逼近二项分布的定理，也就是泊松定理。\n\n泊松定理：设 \\(\\lambda &gt;\n0\\) 是一个常数，\\(n\\)\n是任意正整数，设 \\(np =\n\\lambda\\), 则对于任一非负整数 \\(k\\)，有 \\[\\begin{align} \\lim_{n \\to \\infty}\nC_n^kp^k(1-p)^{n-k} = \\frac{\\lambda^ke^{-\\lambda}}{k!}\n\\end{align}\\]\n\n即当 \\(n\\) 很大且 \\(p\\)\n很小的时候，我们可以使用上面公式等号右边部分来逼近左边部分，从而简化计算。\n连续型随机变量的分布\n分布函数和概率密度函数\n首先需要清楚，分布函数是针对随机变量的，离散或连续都可以；而概率密度函数是针对连续随机变量的。下面是这两者的定义以及联系\n分布函数的定义如下：\n\\(X\\) 是一个随机变量，\\(x\\) 是任意实数，则 \\(X\\) 的分布函数定义为\n\\(F(x) = P(X \\le x),\n(-\\infty&lt;x&lt;\\infty)\\)\n则对于任意实数 \\(x_1,x_2(x_1 &lt;\nx_2)\\), 有\n\\(P(x_1 &lt; X \\le x_2) = p(X \\le x_2) -\np(X \\le x_1) = F(x_2) - F(x_1)\\)\n即如果我们知道了 \\(X\\)\n的分布函数，则可以知道 \\(X\\)\n落在任一区间上的概率，也就是说通过分布函数可以完整描述随机变量的统计规律特性。\n概率密度函数定义如下：\n对于分布函数 \\(F(X)\\)，假如存在\n\\(f(x)\\) 使得以下公式成立：\n\\(F(x) = \\int_{-\\infty}^x f(t)\ndt\\)\n则 X 为连续型随机变量，\\(f(x)\\) 为随机变量的概率密度函数。\\(f(x)\\) 具有以下性质\n\n\\(f(x) \\ge 0\\)\n\\(\\int_{-\\infty}^{\\infty} f(x) dx =\n1\\)\n\n3. 对于任意实数 \\(x_1,x_2(x_1 &lt;\nx_2)\\)，有 \\[P(x_1&lt; X \\le x_2) =\nF(x_2) - F(x_1) = \\int_{x_1}^{x_2} f(x) dx\\]\n且当 \\(f(x)\\) 在点 \\(x\\) 连续的时候，有\n\\(F'(x) = f(x)\\)\n通过上面这个性质可以推导出下面的约等式 \\(P(x &lt;X \\le x+\\Delta x) \\approx f(x)\\Delta\nx\\)\n也就是说概率密度函数在某点的值的大小一定程度上反映了随机变量落在该点附近的概率的大小。\n上面讲了随机变量的分布函数，下面讲一下随机变量的函数的分布函数，例如\n\\(X\\) 为一个随机变量，则 \\(Y = (X-1)^2\\) 是随机变量 \\(X\\) 的函数，且 \\(Y\\) 也是一个随机变量， 因此 \\(Y\\)\n也有自己的分布律。下面是两个例子，其中一个是离散型随机变量，一个是连续型随机变量。\n\n\n离散型随机变量的函数的分布律容易求，对于连续型随机变量的函数的分布律的求法一般是 先按定义写出\n\\(Y\\) 的分布函数 \\(F(Y&lt;=y)\\), 然后替换成 \\(F(g(X)&lt;=Y)\\), 再转换成 \\(X\\) 的分布函数和概率密度函数。\n均匀分布\n若随机变量 \\(X\\)\n的概率密度函数为\n\\[\\begin {align} f (x) =\n\\begin {cases}  \\frac {1}{b-a} &amp;{a&lt;x&lt;b} \\\\\\\n0&amp;{其他}\\end {cases} \\end {align}\\]\n则称 X 在区间 \\((a,b)\\)\n上服从均匀分布，记为 \\(X\\)~\\(U(a,b)\\)\n指数分布\n若随机变量 \\(X\\)\n的概率密度函数为\n\\[\\begin {align} f (x) =\n\\begin {cases}  \\frac {1}{\\theta} e^{-x / \\theta} &amp;{x&gt;0} \\\\\\\n0&amp;{其他}\\end {cases} \\end {align}\\]\n则称 \\(X\\) 服从参数为 \\(\\theta\\) 的指数分布。\n其图像如下所示:\n\n其分布函数为\n\\[\\begin {align} F (x) =\n\\begin {cases}  1-e^{-x / \\theta} &amp;{x&gt;0} \\\\\\\n0&amp;{其他}\\end {cases} \\end {align}\\]\n如果说上面的泊松分布是描述某个时间段内事件发生次数的概率分布，那么指数分布描述的就是事件发生的时间间隔的概率分布。指数分布是连续的分布，反映在其实际意义上就是时间是连续的。更详细的描述可查看这里\n关于指数分布的一个有趣的性质为：\n\\(P(X&gt;s+t | X&gt;s) =\nP(X&gt;t)\\)\n该性质也称为无记忆性，假设 \\(X\\)\n是某一原件的寿命，上面的式子表示的就是该元件在使用了 s\n个小时后，至少还能使用 t\n个小时的条件概率。而这一条件概率又等于该元件从刚开始使用的算起至少能使用\nt\n个小时的概率。也就是说原件对使用过的 s 个小时无记忆性，这个特性与随机过程中的平稳过程非常相似，而这个特性也是指数分布有广泛应用的重要原因。\n正态分布\n正态分布也叫高斯分布，其概率密度函数为\n\\[\\begin{align} f(x) =\n\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\end{align}\\]\n则称 \\(X\\) 服从参数为 \\(\\mu, \\sigma\\) 的正态分布，记为 \\(X\\)~\\(N(\\mu,\\sigma^2)\\)，而且 \\(\\mu, \\sigma\\)\n分别是正态分布的期望和标准差。其图像如下所示\n\n从图像可知，当 $ X =\n$ 时，取值最大，也就是说随机变量落在这个值附近的概率最大，而这个值也就是正态分布的期望。\n","categories":["数学"],"tags":["数学"]},{"title":"概率论与数理统计知识整理 (4)-- 大数定律和中心极限定理","url":"/2016/10/18/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(4)--%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86/","content":"大数定律和中心极限定理都是与极限有关的定理，其中大数定律主要描述了当样本的数量足够多时，其均值 (频率) 可以用来逼近总体的期望（概率）；而中心极限定理则描述了在某些条件下，大量独立同分布的随机变量的和的分布逼近于正态分布。\n\n大数定律\n大数定律有弱大数定律和强大数定律，两者描述的都是样本数量越多，则其平均就越趋近期望值。两个简单的区别就是弱大数定律表示样本均值依概率收敛于总体均值，而强大数定律表示了样本均值可以以概率为 1 收敛于总体均值。弱大数定律比较早被证明出来，强大数定律是比较晚被证明出来的，通俗来说就是数学家先证明了弱大数定律，后来在没有改变前提的情况下把弱大数定律推进了一步，更加确定了这个收敛，也就是强大数定律。\n这里主要讲几个弱大数定律的定义\n\n弱大数定理（辛钦大数定理） 设 \\(X_1,X_2,...\\)\n是独立同分布的随机变量序列，且具有数学期望 \\(E(X_k) = \\mu(k=1,2,,....)\\), 取前 n\n个变量的算术平均 \\(\\frac{1}{n}\n\\sum_{k=1}^{n}X_k\\), 对于任意的 \\(\\epsilon\\), 有 \\[\\begin{align} \\lim_{n \\rightarrow \\infty}\nP(|\\frac{1}{n} \\sum_{k=1}^{n}X_k - \\mu| &lt; \\epsilon) = 1\n\\end{align}\\]\n\n定义描述的就是当样本数 n 足够大时，样本均值与总体期望的差可以无限小，也就是可以通过样本均值估计总体期望。基于上面的辛钦弱大数定理可以推出下面的伯努利大数定理\n\n伯努利大数定理 设 \\(f_A\\) 是 n 次独立重复试验中事件 \\(A\\) 发生的次数，\\(p\\) 是事件 \\(A\\) 在每次试验中发生的概率，则对于任意正数\n$&gt; 0 \\(, 有 \\)\\(\\lim_{n \\rightarrow \\infty}P(|\\frac{f_A}{n} - p|\n&lt; \\epsilon) = 1\\)\\(或 \\)\\(\\lim_{n \\rightarrow \\infty}P(|\\frac{f_A}{n} - p|\n\\ge \\epsilon) = 0\\)$\n\n伯努利大数定理主要描述当样本数足够大时，可以用样本的频率来估计总体的概率，其本质跟辛钦弱大数定理是一样的。\n中心极限定理\n一般来说，n 个独立同分布的随机变量的和的分布函数是比较难求的，而通过中心极限定理，可以描述当 n 足够大的时候，这些随机变量的和的分布近似服从正态分布。下面主要讲述两条中心极限定理的\n\n独立同分布的中心极限定理 随机变量 \\(X_1,X_2,...X_n\\)\n独立同分布，且具有数学期望 \\(E(X_k) =\n\\mu\\), 和方差 \\(D(X_k) = \\sigma^2 &gt;\n0(k=1,2,3...)\\), 则随机变量之和 \\(\\sum_{k=1}^n X_k\\) 的标准化变量 \\[Y_n = \\frac{\\sum_{k=1}^n X_k - E(\\sum_{k=1}^n\nX_k)}{\\sqrt{D(\\sum_{k=1}^n X_k)}} = \\frac{\\sum_{k=1}^n X_k -\nn\\mu}{\\sqrt{n}\\sigma}\\] 的分布函数 \\(F_n(x)\\) 对于任意 \\(x\\) 满足 \\[\\begin{align} \\lim_{n \\rightarrow \\infty} F_n(x)\n= \\lim_{n \\rightarrow \\infty}P(\\frac{\\sum_{k=1}^n X_k -\nn\\mu}{\\sqrt{n}\\sigma}\\le x)  = \\int_{-\\infty}^x\\frac{1}{\\sqrt{2\\pi}}\ne^{-t^2/2} dt \\end{align}\\]\n\n也就是说，当上面的 \\(n\\)\n充分大的时候，\\(\\frac{\\sum_{k=1}^n X_k -\nn\\mu}{\\sqrt{n}\\sigma}\\) 服从正态分布 \\(N(0,1)\\)\n也可以将上面分布写成下面的形式\n\\(\\frac{ \\overline X-\n\\mu}{\\sigma/\\sqrt{n}}\\)~\\(N(0,1)\\) 或 \\(\\overline X\\)~\\(N(\\mu, \\sigma^2/n)\\)\n也就是说，当样本的数量 n 足够大的时候，样本均值服从均值为 \\(\\mu\\), 方差为 \\(\\sigma^2/n\\) 的正态分布， 其中 \\(\\mu\\) 和 \\(\\sigma\\)\n分别是原来随机变量的所服从的分布的期望和方差，这一结果是数理统计中大样本统计推断的基础。\n上面的独立同分布中每个随机变量都是同分布的，也就是具有同样的期望和方差，那么如果随机变量的分布独立呢？下面是对应这种情况的中心极限定理。\n\n李雅普诺夫定理 设随机变量 \\(X_1,X_2,...X_n\\)\n相互独立，具有数学期望和方差 \\[E(X_k) = \\mu_k,\nD(X_k) = \\sigma_k^2 &gt; 0,k=1,2,...\\], 记 \\[B_n^2 = \\sum_{k=1}^n \\sigma_k^2\\]\n若存在正数 \\(\\delta\\), 使得当 \\(n \\rightarrow \\infty\\) 时，\\[\\frac{1}{B_n^{2+\\delta}}\\sum_{k=1}^{n} E(|X_k -\n\\mu_k|^{2+\\delta}) \\rightarrow 0\\] 定义随机变量 \\(Z_n\\) 为 \\[Z_n =\n\\frac{\\sum_{k=1}^n X_k - \\sum_{k=1}^n  \\mu_k}{B_n}\\]\n那么当 n 很大时，只要满足定理中的条件，那么随机变量 \\(Z_n\\) 服从正态分布 \\(N(0,1)\\)。\n\n也就是说当 n 很大的时候，随机变量的和 \\(\\sum_{k=1}^{n}X_k\\) 近似服从正态分布 \\(N(\\sum_{k=1}^n\\mu_k, B_n^2)\\)\n下面是应用中心极限定理的一个例子\n\n","categories":["数学"],"tags":["数学"]},{"title":"概率论与数理统计知识整理 (3)-- 随机变量的统计特征","url":"/2016/10/14/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(3)--%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81/","content":"随机变量的统计特征主要包括期望，方差，协方差以及相关系数。\n\n期望\n离散型随机变量：\\[E(X) = \\sum_{k=1}^{\n+\\infty}p_kx_k\\]\n连续型随机变量：\\[E(X) = \\int_{-\\infty}^{\n+\\infty} {xf(x)dx} \\]\n期望有以下性质 (C 为常数，其他均为随机变量):\n\\(E(C) = C\\)\n\\(E(CX) = CE(X)\\)\n\\(E(X+Y) = E(X)+E(Y)\\)\n$E(XY) = E(X)E(Y) $ （\\(X,Y\\)\n相互独立）\n前面讨论随机变量的分布函数时，同时讨论了随机变量的函数的分布函数，这里同样对于随机变量\n\\(X\\)\n的函数的期望进行讨论，其定义及求法如下所示。\n设 Y 是随机变量 X 的函数：\\(Y=g(X)\\)(g 是连续函数)\n\n如果 \\(X\\)\n是离散型随机变量，它的分布律为 \\[\\begin{align} P(X=x_k) = p_k, k = 1,2,...\n\\end{align}\\] 若 $_{k=1}^{} g (x_k) p_k \\(绝对收敛，则有 \\)\\(\\begin{align} E(Y) = E[g(X)] =\n\\sum_{k=1}^{\\infty}g(x_k)p_k  \\end{align}\\)$\n 如果 X 是连续型随机变量，它的概率密度函数为 \\(f(x)\\), 若 \\(\\int_{-\\infty}^{\\infty}g(x)f(x)dx\\)\n绝对收敛，则有 \\[\\begin{align} E(Y) = E[g(X)]\n= \\int_{-\\infty}^{\\infty}g(x)f(x)dx \\end{align}\\]\n\n这个定理的重要意义在于求 \\(E(Y)\\)\n的时候，不用再求 Y 的分布律或概率密度函数，直接利用 X\n的分布律或概率密度函数即可。\n方差\n方差的原始定义为\n\\(D(X) = E[(X-E(X))^2] = E(X^2) -\nE(X)^2\\)\n方差有以下性质：\n\\(D(C) = 0\\)\n\\(D(CX) = C^2D(X)\\)\n$D(X+Y) = D(X) + D(Y) + 2E([X-E(X)][Y-E(Y)]) $\n如果 \\(X，Y\\)\n是相互独立的，那么 \\(E([X-E(X)][Y-E(Y)]) =\n0\\), 当这一项不为 0 的时候，称作变量 \\(X,Y\\) 的协方差。\n常见分布的期望和方差\n前面我们提到了若干种典型的离散分布和连续分布，下面是这几种分布的期望和方差，记住这些常用的期望和方差能够在使用的时候省去推导过程。\n\n\n\n\n\n\n\n\n\n分布类型\n概率密度函数\n期望\n方差\n\n\n\n\n伯努利分布～\\(B(1,p)\\)\n\\(p = p^x(1-p)^{1-x}\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\n 二项分布～\\(B(n,p)\\)\n\\(p_i = C_n^i\np^i(1-p)^{n-i}(i=1,2,3...)\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\n 泊松分布～\\(P(\\lambda)\\)\n$p_i = (i = 1,2,3,) \\(|\\)\\(|\\)$\n\n\n\n\n 均匀分布～\\(U(a,b)\\)\n\\(f(x) = \\frac{1}{b-a}\\)\n\\(\\frac{a+b}{2}\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\n\n 正态分布～\\(N(\\mu,\\sigma^2)\\)\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\ne^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\n\n 指数分布～\\(E(\\lambda)\\)\n\\[f (x) = \\begin {cases}  \\lambda\ne^{-x\\lambda} &amp;{x&gt;0} \\\\\\ 0&amp;{其他}\\end {cases}\\]\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\n\n\n切比雪夫不等式\n切比雪夫不等式的定义如下：\n\n设随机变量 \\(X\\) 具有数学期望 \\(E(X) = \\mu\\), 方差 \\(D(X) = \\sigma^2\\), 则对于任意正数 \\(\\epsilon\\), 下面的不等式成立 \\[P(|X-\\mu|\\ge \\epsilon) \\le\n\\frac{\\sigma^2}{\\epsilon^2}\\]\n\n从定义可知，切比雪夫不等式也可写成如下的形式：\n\\[\\begin{align} P(|X-\\mu| \\le \\epsilon)\n\\ge 1 - \\frac{\\sigma^2}{\\epsilon^2} \\end{align}\\]\n切比雪夫不等式的一个重要意义在于当随机变量 X\n的分布未知，只知道 \\(E(X)\\) 和 \\(D(X)\\) 的情况下，对于事件 $(|X-|) $\n概率的下限的估计。\n协方差\n协方差表达了两个随机变量的相关性，正的协方差表达了正相关性，负的协方差表达了负相关性。协方差为 0\n表示两者不相关，对于同样的两个随机变量来说，计算出的协方差的绝对值越大，相关性越强。\n协方差的定义入下:\n\\(Cov(X,Y) =\nE\\{[X-E(X)][Y-E(Y)]\\}\\)\n由定义可以知下面等式成立:\n\\(Cov(X,Y) = Cov(Y,X)\\) \\(Cov(X,Y) = E(XY) - E(X)E(Y)\\)\n协方差有以下性质：\n\\(Cov(aX,bY) =\nabCov(X,Y)\\)（a，b 是常数）\n\\(Cov(X_1+X_2, Y) = Cov(X_1, Y) +\nCov(X_2,Y)\\)\n假如我们现在有身高和体重这两个未知变量，对于一系列的样本我们算出的的协方差为 30，那这究竟是多大的一个量呢？如果我们又发现，身高与鞋号的协方差为 5，是否说明，相对于鞋号，身高与体重的的相关性更强呢？\n为了能进行这样的横向对比，我们计算相关系数 (correlation coefficient)，\n相关系数相当于是 “归一化” 的协方差。\n\\[\\begin{align} \\rho_{XY} =\n\\frac{Cov(X,Y)}{\\sqrt{D(X)D(Y)}} \\end{align}\\]\n相关系数是用协方差除以两个随机变量的标准差。相关系数的大小在 - 1 和 1 之间变化，等于 0 表示不相关。再也不会出现因为计量单位变化，而数值变化较大的情况，而相关系数的大小的含义与协方差是一样的。\n需要注意的是上面提到的相关均指线性相关，\\(X, Y\\) 不相关是指 \\(X,Y\\)\n之间不存在线性关系，但是他们还可能存在除线性关系以外的关系。因此，有以下结论:\n\\(X,Y\\) 相互独立则 \\(X,Y\\) 一定不相关；反之 \\(X,Y\\)\n不相关，两者不一定相互独立。\n简单的证明如下： 当 \\(X,Y\\)\n相互独立的时候有 \\(E(XY) = E(X)E(Y)\\)\n， 根据上面协方差的展开式\n\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\)\n此时协方差为零，两者不相关。\n而当 \\(X, Y\\)\n不相关的时候举一个反例如下：\n\n\n不相关但是不独立的例子\n\n上面的例子来源于 https://www.zhihu.com/question/26583332,\n可知计算出来的协方差为 0，即两者不相关，但是 \\(P(XY) \\neq P(X)P(Y)\\), 即 两者不独立，注意\n\\(E(XY) = E(X)E(Y)\\) 不是 \\(X,Y\\) 独立的充分条件。\n矩和协方差矩阵\n下面介绍概率论中几种矩的定义\n\n设 \\(X,Y\\) 为随机变量，则\n\\(E(X^k), k=1,2,3....\\) 称为 \\(X\\) 的 \\(k\\) 阶原点矩，简称 \\(k\\) 阶矩\n\\(E((X-E[X])^k), k=1,2,3....\\) 称为\n\\(X\\) 的 \\(k\\) 阶中心距\n\\(E(X^kY^l),k,l=1,2,...\\) 称为 \\(X\\) 和 \\(Y\\) 的 \\(k+l\\) 阶混合矩\n\\(E((X-E[X])^k(Y-E[Y])^l)),k,l=1,2,...\\) 称为 \\(X\\) 和 \\(Y\\) 的 \\(k+l\\) 阶混合中心矩\n\n由以上定义我们可以知道，随机变量的期望是其一阶原点矩，方差是其二阶中心距，协方差是其二阶混合中心矩。\n除此之外，另外一个常用的概念是协方差矩阵， 其定义如下：\n对于 \\(n\\) 维随机变量 \\((X_1,X_2,X_3...,X_n)\\) 构成的矩阵\n\\[\\begin{align}C=\n\\begin{bmatrix}\nc_{11} &amp; c_{12} &amp; \\cdots &amp; c_{1n} \\\\\\\nc_{21} &amp; c_{22} &amp; \\cdots &amp; c_{2n} \\\\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\\\nc_{n1} &amp; c_{n2} &amp; \\cdots &amp; c_{nn} \\\\\\\n\\end{bmatrix}\n\\end{align}\\]\n其中各个元素为 \\[c_{ij} = Cov(X_i,X_j) =\nE((X_i - E[X_i])(X_j - E[X_j]))，i,j=1,2,3..n\\]\n则称矩阵 \\(C\\)\n为协方差矩阵，由于 \\(c_{ij} = c_{ji}\\)\n， 因此上面的矩阵为一个对称矩阵。\n协方差矩阵其实是将二维随机变量的协方差一般化后拓展到了 \\(n\\)\n维随机变量上的一种表示形式，但是除了作为一种表示形式以外，协方差矩阵还存在着某些性质使得其在多个领域均有应用，如主成成分分析。\n","categories":["数学"],"tags":["数学"]},{"title":"概率论与数理统计知识整理 (5)-- 样本及抽样分布","url":"/2016/11/18/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(5)--%E6%A0%B7%E6%9C%AC%E5%8F%8A%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/","content":"前面几部分主要总结了概率论中的一些知识，后面主要讲述与数理统计相关的知识。\n概率论与数理统计的主要区别为，在概率论中所研究的随机变量，其分布都是假设已知的，在这一前提下去研究它的性质（数字特征，分布函数等）；而在数理统计中研究的随机变量其分布是未知的，通过对所研究的随机变量进行重复独立的试验和观察，得到许多观察值，再对观察值进行分析，从而对所研究的随机变量的分布做出各种推断。\n\n因此数理统计的主要内容包括两部分，一是如何收集，整理数据资料，二是如何对得到的数据资料进行分析和研究，从而对所研究的对象的性质和特点做出推断。第二部分其实就是统计推断的问题，也是后面主要讲述的内容。本文主要讲述数理统计中的两个基本概念：样本和抽样分布。\n样本\n从前面可知，数理统计就是通过数据来推断变量的分布，比如说现在要求求出全国成年男人的身高的一个分布，那只需要测出每个成年男人的身高后进行统计即可。\n但是在实际中，受限于人力物力和测试的难度，我们往往不会对每个成年男人进行身高的测试，而是在全国男人中选择部分的男人进行测试 (如根据每个地区的人口数量按比例测试)，然后用这部分男人的身高分布来推断全国男人的分布，这样的推断肯定会存在误差，但是通过增加样本的数量，可以减少这种误差 (大数定理)。\n上面其实就是一个很简单的数理统计过程，当中有几个概念需要注意，例子中的全国男人的身高是一个总体，选择出来实际测试身高的男人是一个样本，测试得到的身高称为样本值（观测值），总体和样本中的数目分别称为他们的容量。\n其严格定义如下：\n\n设 \\(X\\) 是具有分布函数 \\(F\\) 的随机变量，若 \\(X_1, X_2, ...,X_n\\) 是具有同一分布函数\n\\(F\\) 的相互独立的随机变量，则称 \\(X_1, X_2,...X_n\\) 为从分布函数 \\(F\\)\n得到的容量为 n 的简单随机样本，简称样本，他们的观测值 \\(x_1, x_2,...x_n\\) 称为样本值，又称为 \\(X\\) 的 \\(n\\) 个独立的观测值。\n\n由定义可知样本 $ X_1,X_2,...,X_n $ 相互独立，且他们的分布函数均为\n\\(F\\), 所以 $ (X_1,X_2,...,X_n)\n$ 的分布函数为\n\\[\\begin{align}  F^*(x_1,x_2,...,x_n) =\n\\prod_{i=1}^nF(x_i) \\end{align}\\]\n同样，$ (X_1,X_2,...,X_n) $ 的概率密度函数为：\n\\[\\begin{align}  f^*(x_1,x_2,...,x_n) =\n\\prod_{i=1}^nf(x_i)  \\end{align}\\]\n抽样分布\n统计量\n样本是进行统计推断的依据，但是在应用中，往往不是直接使用样本本身，而是针对不同问题构造适当的样本的函数，利用这些样本的函数进行统计推断。\n当这些样本的函数中不含未知变量时，我们称其为统计量，如下面就是几个常用的统计量，其中\n$ X_1,X_2,....,X_n $ 为总体的一个样本。\n样本平均值： \\[\\overline X = \\frac{1}{n}\n\\sum_{i=1}^{n} X_i\\]\n样本方差：\\[S^2 = \\frac{1}{n-1}\n\\sum_{i=1}^{n} (X_i - \\overline X)^2\\]\n样本标准差：\\[S = \\sqrt {S^2}\\]\n样本 k 阶原点矩： \\[A_k = \\frac{1}{n}\n\\sum_{i=1}^{n} X_i^k (k=1,2,...)\\]\n样本 k 阶中心矩: \\[B_k = \\frac{1}{n}\n\\sum_{i=1}^{n}(X_i - \\overline X)^k (k=2,3,4.....)\\]\n这些统计量的定义与概率论中的基本相似，唯一比较奇怪的是为什么样本方差的分母是\n\\(n - 1\\) 而不是\nn，原因是通过数学证明可以得到只有当分母取 n-1 时，用样本来估计总体才是无偏的 (无偏指的是估计量的期望与总体的参数一致)，下面是分母取 n 时得到的有偏估计的证明过程（\\(S_1^2\\) 为样本方差）\n\n更多的解释可参考这里\n除了上面的统计量还定义了与总体分布函数 \\(F(x)\\)\n相应的统计量 -- 经验分布函数。其定义如下\n\n设 $ X_1,X_2,....,X_n $ 为总体的一个样本，$ S (x)(-&lt; x &lt; )\n$ 表示为样本中不大于 \\(x\\)\n的随机变量的个数，则经验分布函数 \\(F_n(x)\\) 的定义如下 \\[F_n(x) = \\frac{1}{n}S(x) (-\\infty &lt; x &lt;\n\\infty)\\]\n\n下面是关于经验分布函数的一个简单例子。\n设总体 \\(F\\)\n具有一个样本值 1,2,3，则经验分布函数 \\(F_3(x)\\) 的观测值为\n\\[f(y) = \\begin{cases}  \n0 &amp;{x&lt;1} \\\\\\\n\\frac{1}{3} &amp;{1\\le x&lt;2}\\\\\\\n\\frac{2}{3} &amp;{2\\le x&lt;3}\\\\\\\n1 &amp;{x \\ge 3}\\end{cases}\\]\n经验分布函数的意义在于当 n 充分大的时候，\\(F_n(x)\\) 以概率 1 收敛于总体分布函数 \\(F(x)\\)\n统计量的分布\n使用统计量进行统计推断时，常常需要知道其分布，统计量的分布也称为抽样分布，下面介绍三种来自正态分布的抽样分布：\n\\(\\chi^2\\) 分布，\\(t\\) 分布和 \\(F\\) 分布。\n\\(\\chi^2\\) 分布\n\\(\\chi^2\\) 分布的定义如下\n\n设 \\(X_1, X_2,...X_n\\) 是来自总体\n\\(N(0,1)\\) 的样本，则称统计量 \\[\\chi^2 = X_1^2 + X_2^2\n+....X_n^2\\] 为服从自由度为 \\(n\\)\n的 \\(\\chi^2\\) 分布\n\n上面的自由度指的是右端独立变量的个数。\n\\(\\chi^2(n)\\) 的概率密度函数为\n\\[\\begin {align} f (y) =\n\\begin {cases}  \\frac {1}{2^{\\frac {n}{2}}\\Gamma (n/2)} y^{n/2-1} e^{-y/2}\n&amp;{y&gt;0} \\\\\\ 0&amp;{其他}\\end {cases} \\end {align}\\]\n上式的 \\(\\Gamma\\) 函数定义为\n\\[\\begin{align} \\Gamma = \\int_{0}^{\\infty}\n\\frac{t^z - 1}{e^t} dt \\end{align}\\]\n\\(f(y)\\) 的图像如下所示\n\n\n卡方分布的概率密度函数\n\n关于 \\(\\chi^2(n)\\)\n有以下几个有用的结论：\n\n可加性\n\n设 \\(\\chi_1^2\\)~\\(\\chi^2(n_1), \\chi_2^2\\)~\\(\\chi^2(n_2)\\), 并且 \\(\\chi_1^2, \\chi_2^2\\) 相互独立，则有 \\[\\chi_1^2 + \\chi_2^2 \\sim \\chi^2(n_1 +\nn_2)\\]\n\n期望和方差\n\n若 \\(\\chi^2\\)~\\(\\chi^2(n)\\)，则 \\(\\chi^2\\) 的期望和方差如下所示 \\[\\begin{align} E(\\chi^2) = n, D(\\chi^2)=2n\n\\end{align}\\]\n\n分位点\n\n分位点的定义如下，给定正数 \\(a,\n0&lt;a&lt;1\\), 称满足下面条件\n\\[\\begin{align} P(\\chi^2 \\gt \\chi_a^2(n))\n= \\int_{\\chi_a^2(n)}^{\\infty}f(y)dy= a \\end{align}\\]\n的 \\(\\chi_a^2(n)\\) 为 \\(\\chi^2(n)\\) 上的 a 分位点，其图像如下所示\n\n由定义可知，分位点由 \\(a,n\\)\n共同决定，因此对于不同的 \\(a，n\\)\n可以查阅表格得到其 \\(a\\) 分位点。\n\\(t\\) 分布\n\\(t\\) 分布的定义如下： &gt; 设 \\(X \\sim N(0,1), Y \\sim \\chi^2(n)\\), 且 \\(X,Y\\) 相互独立，则称随机变量 \\[ t = \\frac{X}{\\sqrt{Y/n}}\\] 服从自由度为\n\\(n\\) 的 \\(t\\) 分布，记为 \\(t \\sim t(n)\\)\n其概率密度函数和对应的图像如下所示：\n\n\nt 分布的概率密度函数和图像\n\n其分位点的定义与上面讲述的一样，\n\\[\\begin{align} P(t \\gt t_a(n)) =\n\\int_{t_a(n)}^{\\infty}h(t)dt= a \\end{align}\\]\n\n且由于其概率密度函数的对称性可知，总是存在这样对称的两个分位点 ：\n\\(t_{1-a}(n) = -t_a(n)\\)\n\\(F\\) 分布\nF 分布的定义如下 &gt; 设 \\(U \\sim\n\\chi^2(n_1), V \\sim \\chi^2(n_2)\\)， 且 \\(U,V\\) 相互独立，则称随机变量 \\[F = \\frac{U/n_1}{V/n_2}\\] 服从自由度为 \\((n_1,n_2)\\) 的 \\(F\\) 分布，记为 \\(F\n\\sim F(n_1, n_2)\\)\n其概率密度函数为：\n\\[\\begin {align} \\psi (y) =\n\\begin {cases}  \\frac {\\Gamma ((n_1+n_2)/2)(n_1/n_2)^{n_1/2} y^{n_1/2-1}}{\\Gamma (n_1/2)\\Gamma (n_2/2)[1+(n_1y/n_2)]^{(n_1+n_2/) 2}}\n&amp;{y&gt;0} \\\\\\ 0&amp;{其他}\\end {cases} \\end {align}\\]\n概率密度函数的图像如下所示\n\n\nF 分布的概率密度函数\n\n其分位点定义同上 \\[\\begin{align} P(F \\gt\nF_a(n_1,n_2)) = \\int_{F_a(n_1,n_2)}^{\\infty}\\psi(y)dy= a\n\\end{align}\\]\n\n\nF 分布的分为点\n\n且具有以下性质 \\[\\begin{align}\nF_{1-a}(n_1,n_2) = \\frac{1}{F_a(n_2,n_1)} \\end{align}\\]\n上面只是简单地介绍了三大抽样分布，并未介绍其作用，实际上三大抽样分布主要用于参数的区间估计中，而这主要基于从正态分布中抽取的样本所构造的统计量服从这三大分布这一事实，从下面要介绍的定理中可以看到了这三大抽样分布的作用。更详细的作用会在区间估计中进一步体现。\n正态总体的样本均值与样本方差的分布\n由于正态分布的普遍性，这里特意指出从服从正态分布的总体中抽取出的样本的所服从的分布。\n假设上面的 \\(X\\) 服从正态分布 \\(N(\\mu, \\sigma^2)\\),\n则有以下几条定理，这几条定理在数理统计的区间估计中起了重要作用。\n\n定理一： 设 \\(X_1,\nX_2,....X_n\\) 服从 \\(N(\\mu,\n\\sigma^2)\\)，\\(\\overline\nX\\) 是样本均值，则有 \\[\\overline X \\sim\nN(\\mu,\\sigma^2/n)\\]\n\n证明如下： \\[\\begin{align} E(\\overline X)\n= E(\\frac{1}{n}\\sum_{i=1}^{n} X_i) = \\frac{1}{n}E(\\sum_{i=1}^{n} X_i) =\n\\frac{1}{n}n E(X) = \\mu \\end{align}\\]\n\\[\\begin{align} D(\\overline X) =\nD(\\frac{1}{n}\\sum_{i=1}^{n} X_i) = \\frac{1}{n^2}D(\\sum_{i=1}^{n} X_i) =\n\\frac{1}{n}n D(X) = \\sigma^2/n \\end{align}\\]\n定理一通常用于区间估计中已知总体（服从正态分布）的期望 \\(\\mu\\) 来估计其未知的方差 \\(\\sigma^2\\) , 或已知方差 \\(\\sigma^2\\) 来估计未知的期望 \\(\\mu\\)。\n\n定理二 ：设 \\(X_1,\nX_2,....X_n\\) 服从 \\(N(\\mu,\n\\sigma^2)\\)，\\(\\overline\nX\\) 是样本均值，\\(S^2\\)\n是样本的方差，则 \\(\\overline X\\) 和\n\\(S^2\\) 相互独立，且有 \\[\\frac{(n-1)S^2}{\\sigma^2} \\sim\n\\chi^2(n-1)\\]\n\n由于该定理的证明部分较为冗长，这里略去证明过程，感兴趣的读者可参考相关书籍。定理二主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其方差的范围，这也是\n\\(\\chi^2\\)\n分布的作用之一。\n\n定理三：设 \\(X_1,\nX_2,....X_n\\) 服从 \\(N(\\mu,\n\\sigma^2)\\)，\\(\\overline\nX\\) 是样本均值，\\(S^2\\)\n是样本的方差，则 \\[\\frac{\\overline X -\n\\mu}{S/\\sqrt{n}} \\sim t(n-1)\\]\n\n证明： 根据定理一，易知 \\(\\overline X - \\mu\n\\sim N(0, \\sigma^2/n)\\), 则 \\(\\frac{\\overline X - \\mu}{\\sqrt{\\sigma^2/n}} \\sim\nN(0,1)\\), 从定理二可知 \\[\\begin{align}\n\\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1) \\end{align}\\]\n则根据 t 分布的定义有 \\[\\begin{align}  \\frac{\\frac{\\overline X -\n\\mu}{\\sqrt{\\sigma^2/n}}} {\\sqrt{\\frac{(n-1)S^2}{\\sigma^2(n-1)}}} \\sim\nt(n-1) \\end{align}\\]\n化简可得 \\[\\begin{align} \\frac{\\overline X\n- \\mu}{\\sqrt{\\sigma^2/n}} \\sim t(n-1) \\end{align}\\]\n定理三主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其期望的范围，这也是\n\\(t\\)\n分布的作用之一，注意前面讲到的 \\(\\chi^2\\) 分布估计的是方差。\n\n定理四：设 \\(X_1, X_2...X_n\\) 与 \\(Y_1,Y_2,...Y_n\\) 分别是来自正态总体 \\(N(\\mu_1, \\sigma_1^2)\\) 和 \\(N(\\mu_2, \\sigma_2^2)\\) 的样本，\\(\\overline X, \\overline\nY\\) 分别是其样本均值，\\(S_1^2,\nS_2^2\\) 分别是其样本方差。则有 \\[\\frac{S_1^2/S_2^2}{\\sigma_1^2/ \\sigma_2^2} \\sim\nF(n_1 - 1, n_2 - 1)\\] 且当 \\(\\sigma_1^2\n= \\sigma_2^2 = \\sigma^2\\) 时，\\[\\frac{(\\overline X - \\overline Y) - (\\mu_1 -\n\\mu_2)}{S_w\\sqrt{1/n_1+1/n_2}} \\sim t(n_1+n_2-2)\\] 其中，\\(S_w = \\sqrt{\\frac{(n_1 -1)S_1^2+(n_2\n-1)S_2^2}{n_1+n_2-2}}\\)\n\n证明如下：由定理二可知 \\[\\frac{(n_1-1)S_1^2}{\\sigma_1^2} \\sim\n\\chi^2(n_1-1), \\frac{(n_2-1)S_2^2}{\\sigma_2^2} \\sim\n\\chi^2(n_2-1)\\]\n由 F 分布的定义可知 \\[\\begin{align}  \\frac{(n_1-1)S_1^2}{\\sigma_1^2(n_1-1)}\n/ \\frac{(n_2-1)S_2^2}{\\sigma_2^2(n_2-1)} \\sim F(n_1-1,\nn_2-1)\\] 化简可得 \\[\\frac{S_1^2/S_2^2}{\\sigma_1^2/ \\sigma_2^2} \\sim\nF(n_1 - 1, n_2 - 1) \\end{align}\\]\n当 \\(\\sigma_1^2 = \\sigma_2^2 =\n\\sigma^2\\) 时，\n易知 \\((\\overline X - \\overline Y) \\sim\nN(\\mu_1 - \\mu_2,\\sigma_1^2/n_1 + \\sigma_2^2/n_2)\\)\n则 \\(\\frac{(\\overline X - \\overline Y)-\n(\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}} \\sim\nN(0,1)\\)\n由定理二可知 \\[\\frac{(n_1-1)S_1^2}{\\sigma_1^2} \\sim\n\\chi^2(n_1-1), \\frac{(n_2-1)S_2^2}{\\sigma_2^2} \\sim\n\\chi^2(n_2-1)\\], 由 \\(\\chi^2\\)\n分布的可加性可知： \\[\\begin{align}\n\\frac{(n_1-1)S_1^2}{\\sigma_1^2} + \\frac{(n_2-1)S_2^2}{\\sigma_2^2} \\sim\n\\chi^2(n_1+n_2-2) \\end{align}\\]\n由 t 分布的定义可知： \\[\\begin{align}\n\\frac{(\\overline X - \\overline Y)- (\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1\n+ \\sigma_2^2/n_2}} / (\\sqrt{(\\frac{(n_1-1)S_1^2}{\\sigma_1^2} +\n\\frac{(n_2-1)S_2^2}{\\sigma_2^2})/(n_1+n_2-2)}) \\sim t(n_1+n_2-2)\n\\end{align}\\]\n将 \\(\\sigma_1^2 = \\sigma_2^2 =\n\\sigma^2\\) 代入到上式化简即可得到 \\[\\frac{(\\overline X - \\overline Y) - (\\mu_1 -\n\\mu_2)}{S_w\\sqrt{1/n_1+1/n_2}} \\sim t(n_1+n_2-2)\\] 其中，\\(S_w = \\sqrt{\\frac{(n_1 -1)S_1^2+(n_2\n-1)S_2^2}{n_1+n_2-2}}\\)\n定理四的作用是在区间估计时估计两个均服从正态分布的总体的方差的比值（期望未知）以及两者期望的差距（方差未知）\n小结\n本文主要介绍了数理统计的概念，数理统计主要做的事情就是通过有限的样本构造的统计量去推断总体分布的参数。同时介绍了数理统计中的三大分布\n\\(\\chi^2\\) 分布， \\(t\\) 分布和 \\(F\\) 分布，这三大分布与前面讲的随机变量的分布不同（伯努利分布，泊松分布，正态分布等等），随机变量的分布可以认为是整体的分布，而三大分布则是描述样本的分布情况。这三大分布在区间估计中有重要作用：\n其中 \\(\\chi^2\\) 分布主要解决总体期望未知时估计其方差的问题，\n\\(t\\) 分布主要解决总体方差未知时估计其期望的问题，\\(F\\) 主要解决期望未知时两个正态分布的方差比值问题。需要注意的是上面估计的前提是总体服从正态分布。\n","categories":["数学"],"tags":["数学"]},{"title":"浏览器缓存机制","url":"/2016/02/12/%E6%B5%8F%E8%A7%88%E5%99%A8%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/","content":"最近看到一篇比较好的关于浏览器缓存的文章，原文链接, 原文内容如下\n\n浏览器缓存机制，其实主要就是 HTTP 协议定义的缓存机制（如： Expires；\nCache-control 等）。但是也有非 HTTP 协议定义的缓存机制，如使用 HTML Meta\n标签，Web 开发者可以在 HTML 页面的 &lt;head&gt; 节点中加入 &lt; meta\n&gt; 标签，代码如下：\n&lt;META HTTP-EQUIV=\"Pragma\" CONTENT=\"no-cache\"&gt;  \n上述代码的作用是告诉浏览器当前页面不被缓存，每次访问都需要去服务器拉取。使用上很简单，但只有部分浏览器可以支持，而且所有缓存代理服务器都不支持，因为代理不解析 HTML 内容本身。\n下面我主要介绍 HTTP 协议定义的缓存机制。\nExpires 策略\nExpires 是 Web 服务器响应消息头字段，在响应 http 请求时告诉浏览器在过期时间前浏览器可以直接从浏览器缓存取数据，而无需再次请求。\n下面是宝宝 PK 项目中，浏览器拉取 jquery.js web 服务器的响应头：\n\n【注：Date 头域表示消息发送的时间，时间的描述格式由 rfc822 定义。例如，Date:\nMon,31 Dec 2001 04:25:57GMT。】\nWeb 服务器告诉浏览器在 2012-11-28\n03:30:01 这个时间点之前，可以使用缓存文件。发送请求的时间是 2012-11-28\n03:25:01，即缓存 5 分钟。\n不过 Expires 是 HTTP 1.0 的东西，现在默认浏览器均默认使用 HTTP\n1.1，所以它的作用基本忽略。\nCache-control 策略（重点关注）\nCache-Control 与 Expires 的作用一致，都是指明当前资源的有效期，控制浏览器是否直接从浏览器缓存取数据还是重新发请求到服务器取数据。只不过 Cache-Control 的选择更多，设置更细致，如果同时设置的话，其优先级高于 Expires。\nhttp 协议头 Cache-Control ：\n值可以是 public、private、no-cache、no-\nstore、no-transform、must-revalidate、proxy-revalidate、max-age\n各个消息中的指令含义如下：\n| 值 | 含义 |\n|:--|:--:|\n|public | 指示响应可被任何缓存区缓存 |\n|private | 指示对于单个用户的整个或部分响应消息，不能被共享缓存处理。这允许服务器仅仅描述当用户的部分响应消息，此响应消息对于其他用户的请求无效 |\n|no-cache | 指示请求或响应消息不能缓存 |\n|no-store | 用于防止重要的信息被无意的发布。在请求消息中发送将使得请求和响应消息都不使用缓存 |\n|max-age | 指示客户机可以接收生存期不大于指定时间（以秒为单位）的响应 |\n|min-fresh | 指示客户机可以接收响应时间小于当前时间加上指定时间的响应 |\n|max-stale | 指示客户机可以接收超出超时期间的响应消息。如果指定 max-stale 消息的值，那么客户机可以接收超出超时期指定值之内的响应消息 |\n还是上面那个请求，web 服务器返回的 Cache-Control 头的值为 max-age=300，即 5 分钟（和上面的 Expires 时间一致，这个不是必须的）。\n\nLast-Modified/If-Modified-Since\nLast-Modified/If-Modified-Since 要配合 Cache-Control 使用。\n\nLast-Modified：标示这个响应资源的最后修改时间。web 服务器在响应请求时，告诉浏览器资源的最后修改时间。\n\nIf-Modified-Since：当资源过期时（使用 Cache-Control 标识的 max-age），发现资源具有 Last-Modified 声明，则再次向 web 服务器请求时带上头\nIf-Modified-Since，表示请求时间。web 服务器收到请求后发现有头 If-Modified-Since\n则与被请求资源的最后修改时间进行比对。\n\n若最后修改时间较新，说明资源又被改动过，则响应整片资源内容（写在响应消息包体内），HTTP\n200；若最后修改时间较旧，说明资源无新修改，则响应 HTTP 304\n(无需包体，节省浏览)，告知浏览器继续使用所保存的 cache。\nEtag/If-None-Match\nEtag/If-None-Match 也要配合 Cache-Control 使用。\n\nEtag：web 服务器响应请求时，告诉浏览器当前资源在服务器的唯一标识（生成规则由服务器决定）。Apache 中，ETag 的值，默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行 Hash 后得到的。\n\nIf-None-Match：当资源过期时（使用 Cache-Control 标识的 max-age），发现资源具有 Etage 声明，则再次向 web 服务器请求时带上头 If-None-Match\n（Etag 的值）。web 服务器收到请求后发现有头 If-None-Match\n则与被请求资源的相应校验串进行比对，决定返回 200 或 304。\n\n既生 Last-Modified 何生 Etag？\n你可能会觉得使用 Last-Modified 已经足以让浏览器知道本地的缓存副本是否足够新，为什么还需要 Etag（实体标识）呢？HTTP1.1 中 Etag 的出现主要是为了解决几个 Last-Modified 比较难解决的问题：\n\nLast-Modified 标注的最后修改只能精确到秒级，如果某些文件在 1 秒钟以内，被修改多次的话，它将不能准确标注文件的修改时间\n\n如果某些文件会被定期生成，当有时内容并没有任何变化，但 Last-Modified 却改变了，导致文件没法使用缓存\n\n有可能存在服务器没有准确获取文件修改时间，或者与代理服务器时间不一致等情形\n\nEtag 是服务器自动生成或者由开发者生成的对应资源在服务器端的唯一标识符，能够更加准确的控制缓存。Last-Modified 与 ETag 是可以一起使用的，服务器会优先验证 ETag，一致的情况下，才会继续比对 Last-Modified，最后才决定是否返回 304。\n用户行为与缓存\n浏览器缓存行为还有用户的行为有关！！！\n\n\n\n用户操作\n Expires/Cache-Control\nLast-Modified/Etag\n\n\n\n\n 地址栏回车\n有效\n有效\n\n\n页面链接跳转\n有效\n有效\n\n\n新开窗口\n有效\n有效\n\n\n前进、后退\n有效\n有效\n\n\n F5 刷新\n无效\n有效\n\n\n Ctrl+F5 刷新\n无效\n无效\n\n\n\n总结\n浏览器第一次请求：\n\n浏览器再次请求时：\n\n","categories":["杂"],"tags":["web"]},{"title":"概率论与数理统计知识整理 (6)- 参数估计","url":"/2017/02/18/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(6)-%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/","content":"在数理统计中，常常需要通过样本来估计总体的参数，估计可划分为两大类：点估计和区间估计。点估计就是估计总体中某个参数的值，而区间估计是估计总体的某个参数落在某个区间的概率大小。本文主要讲述点估计中的矩估计法和最大似然估计法，以及针对服从正态分布的期望和方差进行区间估计。\n\n点估计\n点估计一般解决的问题是总体 \\(X\\)\n的分布函数 \\(F(X,\\theta)\\)\n形式为已知，但是 \\(\\theta\\)\n参数未知。点估计的目的就是通过样本 \\(X_1,X_2,...X_n\\) 构造一个适当的统计量 \\(\\theta'(X_1,X_2,...X_n)\\)，用于作为未知参数\n\\(\\theta\\) 的近似值。由于 \\(\\theta'\\)\n是样本的函数，因此对于不同的样本，\\(\\theta'\\) 的值一般不同。\n点估计中一般用到的方法包括矩估计法和最大似然估计法。\n矩估计法\n矩估计法的核心思想是样本矩总是依概率收敛于相应的总体矩，因此可通过样本矩作为相应的总体矩的估计量，进而根据总体矩与待估参数的关系求出待估参数。\n矩估计法的一般描述如下： 设 \\(X\\)\n为连续型随机变量，其概率密度函数为 \\(f(x;\\theta_1,\n\\theta_2,..\\theta_k)\\)；离散型随机变量，其分布律为 \\(P(X=x) = p(x; \\theta_1,\n\\theta_2,..\\theta_k)\\)；则总体的 \\(k\\) 阶矩分别为 \\[\\begin{align} \\mu_k = E(X^k)\n=  \\int_{-\\infty}^{\\infty} x^kf(x;\\theta_1, \\theta_2...\\theta_k) dx\n\\end{align}\\] \\[\\begin{align} \\mu_k =\nE(X^k) =  \\sum_{x \\in R_x} x^kp(x;\\theta_1, \\theta_2....\\theta_k)\n\\end{align}\\]\n上式中的 \\(R_x\\) 是 \\(X\\) 可能取值的范围；上面是总体的 k\n阶矩的定义，但是实际估计时，往往到只需要使用其一阶矩和二阶矩，也就是\n\\(E(X)\\) 和 \\(E(X^2)\\)。\n而样本 \\(X_1, X_2...X_n\\) 的 \\(k\\) 阶矩的定义为 \\[A_k = \\frac{1}{n}\n\\sum_{i=1}^{n}X_i^k\\]\n由于总体的 k 阶矩往往是未知参数 \\(\\theta\\) 的函数，因此常常先用总体的 k 阶\n\\(\\mu_k\\) 矩将参数 \\(\\theta\\) 表示出来，然后用样本矩 \\(A_k\\) 代替 \\(\\mu_k\\), 进而得出估计的 \\(\\theta\\) 的值。下面是一个简单的例子\n\n\n矩估计法的例子\n\n最大似然估计法\n最大似然估计的思想是既然当前取得了这组样本，那么有理由相信已取得的样本出现的概率是很大的。因此通过极大化这组样本的联合概率来估计未知参数的值。\n离散型总体\n单总体为离散型的时候，设当前样本为 \\(X_1,X_2,...X_n\\)， 则其联合概率为 \\(\\prod_{i=1}^{n} p(x_i;\\theta)\\), 其中 \\(x_i\\) 是 \\(X_i\\)\n相应的观测值，则上面的联合概率实际上是参数 \\(\\theta\\) 的函数，记为 \\[L(\\theta) = \\prod_{i=1}^{n}\np(x_i;\\theta)\\] 上面的 \\(L(\\theta)\\)\n被称为样本的似然函数。\n选择 \\(\\theta\\) 的值使得 \\(L(\\theta)\\)\n最大便是最大似然估计做的事情。一般通过对\n似然函数求导便可求得其最大值对应的 \\(\\theta\\)。如下是一个简单的例子\n\n\n离散型极大似然估计\n\n上面最后求解的结果是 \\(p' = \\overline\nx\\)。同时也注意到求解似然函数最大化时会先对似然函数取 \\(log\\) ,\n目的是将连乘变为连加，方便运算，同时这种方法也被称为对数极大似然估计。\n连续型总体\n若总体是连续型，设其概率密度函数为 \\(f(x,\\theta)\\)，则当前样本 \\(X_1,X_2,...X_n\\) 的联合概率密度函数为 \\[\\begin{align} \\prod_{i=1}^{n}f(x_i;\\theta)\n\\end{align}\\] 其中 \\(x_1,x_2,...x_n\\)\n是相应于样本的一个样本值，则随机点落在 （\\(x_1,x_2,...x_n\\)）的领域（边长为 \\(dx_1,\ndx_2,...dx_n\\) 的 n 维立方体）内的概率近似为 \\[\\prod_{i=1}^{n}f(x_i;\\theta)dx_i\\]\n同样我们要让上式取到最大，但是因子 \\(\\prod_{i=1}^{n}dx_i\\) 不随 \\(\\theta\\) 改变，因此只需考虑函数 $ L () =\n_{i=1}^{n} f (x_i;)$ 最大即可，这里 \\(L(\\theta)\\)\n被称为似然函数，极大化也是通过求导来解决。\n下面是一个连续型总体进行极大似然估计的例子\n\n\n正态分布\n\n评选标准\n对于同一参数，不同的估计方法求出的估计量可能不一样，那么如何判断不同的估计量之间的优劣，无偏性，有效性和相合性是常用的三个指标。\n无偏性\n无偏性指的是从样本中得到的估计量 \\(\\theta'\\) 的期望与总体的参数 \\(\\theta\\) 相等，也就是 \\[E(\\theta') = \\theta\\] 此时称 \\(\\theta'\\) 是 \\(\\theta\\)\n的无偏估计量。无偏估计量的意义是对于某些样本值，这一估计量得到的估计值比真实值要打，而对于另外一些样本则偏小，反复将这一估计量使用多次，就平均来说其偏差为零。\n有效性\n当两个估计量 \\(\\theta_1',\n\\theta_2'\\)\n均是无偏估计量时，就要通过比较他们的有效性来决定选取哪个估计量。有效性指的是在样本容量\n\\(n\\) 相同的情况下，假如 \\(\\theta_1'\\) 的观察值较 \\(\\theta_2'\\) 的值更密集在真值 \\(\\theta\\) 附近，那么认为 \\(\\theta_1'\\) 比 \\(\\theta_2'\\) 更为理想。\n实际上，上面比较的就是两个估计量的方差大小，方差越小，则越有效，因此当两个总体的样本数相同的时候，若\n\\(D(\\theta_1') &lt;\nD(\\theta_2')\\) 时， 就称 \\(\\theta_1'\\) 比 \\(\\theta_2'\\) 更有效。\n相合性\n当样本数目 \\(n \\rightarrow \\infty\\)\n时，估计量 \\(\\theta'(X_1，X_2...X_n)\\)\n依概率收敛于真正的 \\(\\theta\\) , 则称\n\\(\\theta'\\) 为 \\(\\theta\\)\n的相合估计量。即有以下式子成立 \\[ \\lim_{n\n\\rightarrow \\infty}P(|\\theta' - \\theta| &lt; \\epsilon) =\n1\\]\n相合性是一个估计量的基本要求，如果估计量没有相合性，那么无论样本数量\nn 取多大，这些估计量都无法准确估计正确参数，都是不可取的。\n区间估计\n对于总体中的未知参数，我们的估计总是存在着一定的误差的，如何去衡量这个误差是一个需要考虑的事情。同时，除了上面的点估计，在实际中我们往往还希望估计出参数的一个范围，同时参数落在这个范围的概率，或者是说可信程度。\n估计参数落在某个范围以及落在这个范围的可信程度就是区间估计干的事情。\n其严格定义如下 &gt; 设总体的分布中存在一个未知参数 \\(\\theta\\), 对于给定的值 \\(\\alpha(0 &lt; \\alpha &lt;1)\\), 若通过样本\n\\(X_1,X_2,X_3...X_n\\) 估计的两个统计量\n\\(\\theta'_1\\) 和 \\(\\theta'_2\\) 满足下面不等式时 \\[P(\\theta'_1 &lt; \\theta &lt; \\theta'_2)\n\\ge 1 - \\alpha\\] 则称区间 \\((\\theta'_1, \\theta'_2)\\) 是参数\n\\(\\theta\\) 置信水平为 \\(1-\\alpha\\) 的置信区间，\\(\\theta'_1, \\theta'_2\\)\n分别称为置信下限和置信上限。\n上面式子的含义是若反复抽样多次（每次得到的样本的容量相等），每个样本值确定一个区间 \\((\\theta'_1,\n\\theta'_2)\\)，这个区间要么包含 \\(\\theta\\) 的真值，要么不包含 \\(\\theta\\) 的真值，在这么多的区间中，包含\n\\(\\theta\\) 真值的约占 \\(1-\\alpha\\).\n正态分布均值与方差的区间估计\n由于正态分布的普遍性，下面主要讲述对正态分布的期望和方差进行区间估计的方法，而这里会用到我们前面讲到的统计量的三大分布：\n\\(\\chi^2\\) 分布， \\(t\\) 分布， \\(F\\)\n分布，以及对其拓展的一些定理，具体的定理及其证明可参考这篇文章。\n下面会讲述单个正态分布的期望和方差的估计，以及两个正态分布的期望差和方差比的估计。\n单个正态分布\n下面的关于单个正态分布的讨论都是基于以下假设：给定置信水平为 \\(1-\\alpha\\), 设 \\(X_1,X_2,X_3...X_n\\) 为总体 \\(N(\\mu, \\sigma^2)\\) 的样本，\\(\\overline X，S^2\\)\n分别是样本的期望和方差。\n估计期望 \\(\\mu\\) 的置信区间\n通过样本 \\(X_1,X_2,X_3...X_n\\)\n估计总体 \\(N(\\mu, \\sigma^2)\\) 的期望\n\\(\\mu\\) 时可以分为两种情况：\n\n总体的方差 \\(\\sigma^2\\) 已知\n总体的方差 \\(\\sigma^2\\) 未知\n\n总体的方差 \\(\\sigma^2\\)\n已知 若已知总体的方差，则因为 \\(\\overline X \\sim N(\\mu , \\sigma^2/n)\\),\n即 \\(\\frac{\\overline X -\n\\mu}{\\sqrt{\\sigma^2/n}} \\sim N(0,1)\\),\n下面都会这样不加证明给出这些统计量服从的分布，具体的证明参考这篇文章。\n按照标准正态分布的上 \\(\\alpha\\)\n分位点的定义有 \\[\\begin{align}\nP(|\\frac{\\overline X - \\mu}{\\sqrt{\\sigma^2/n}} | &lt; z_{\\alpha/2}) = 1\n- \\alpha \\end{align}\\]\n从概率密度函数上直观看为：\n\n\n正态分布分位点\n\n进一步化简有 \\[\\begin{align} P(\\overline X\n- \\frac{\\sigma}{\\sqrt{n}}z_{\\alpha/2} &lt; \\mu &lt; \\overline X +\n\\frac{\\sigma}{\\sqrt{n}}z_{\\alpha/2}) = 1 - \\alpha\n\\end{align}\\]\n给定 \\(\\alpha, z_{\\alpha/2}\\)\n的值可以通过查表获得。这样便得到了期望 \\(\\mu\\) 的一个估计区间为 \\((\\overline X -\n\\frac{\\sigma}{\\sqrt{n}}z_{\\alpha/2}, \\overline X +\n\\frac{\\sigma}{\\sqrt{n}}z_{\\alpha/2})\\), 其置信度为 \\(1 - \\alpha\\)。注意置信水平为 \\(1 - \\alpha\\)\n的置信区间并不是唯一的，假如说给定 \\(\\alpha =\n0.05\\), 则上面的式子可写为 \\[P(\\overline X - \\frac{\\sigma}{\\sqrt{n}}z_{0.025}\n&lt; \\mu &lt; \\overline X + \\frac{\\sigma}{\\sqrt{n}}z_{0.025}) = 1 -\n\\alpha\\]\n同时也可写为 \\[\\begin{align} P(\\overline X\n- \\frac{\\sigma}{\\sqrt{n}}z_{0.04} &lt; \\mu &lt; \\overline X +\n\\frac{\\sigma}{\\sqrt{n}}z_{0.01}) = 1 - \\alpha \\end{align}\\]\n但是写成不对称的形式计算出来的区间长度要更长，显然，置信度相同的情况下，置信区间肯定是越小越好，所以对于正态分布的分位点往往选择对称形式。\n下面的求解方法与这方法类似，只是构造的统计量不同，因而服从的分布也不同。\n总体的方差 \\(\\sigma^2\\)\n未知\n当总体方差未知时，就无法利用上面标准正态分布。但是回忆 \\(t\\) 分布的作用及其定理，可知\n\\[\\begin{align} \\frac{\\overline X -\n\\mu}{S/\\sqrt{n}} \\sim t(n-1) \\end{align}\\]\n同样按照 \\(t\\) 分布的上 \\(\\alpha\\) 分位点的定义有 \\[\\begin{align} P(|\\frac{\\overline X -\n\\mu}{S/\\sqrt{n}}| &lt; t_{\\alpha/2}(n-1)) = 1 - \\alpha\n\\end{align}\\]\n其对应的概率密度函数如下所示\n\n\nt 分布的概率密度函数\n\n进一步化简可得 \\[\\begin{align} P(\\overline\nX - \\frac{S}{\\sqrt{n}}t_{\\alpha/2}(n - 1) &lt; \\mu &lt; \\overline X +\n\\frac{S}{\\sqrt{n}}t_{\\alpha/2}(n - 1)) = 1 - \\alpha\n\\end{align}\\]\n则期望 \\(\\mu\\) 的一个置信水平为\n\\(1-\\alpha\\) 的置信区间为 \\[\\begin{align} (\\overline X -\n\\frac{S}{\\sqrt{n}}t_{\\alpha/2}(n - 1), \\overline X +\n\\frac{S}{\\sqrt{n}}t_{\\alpha/2}(n - 1)) \\end{align}\\]\n估计方差 \\(\\sigma^2\\) 的置信区间\n估计方差 \\(\\sigma^2\\)\n的置信区间也可分为两种情况\n\n总体的期望 \\(\\mu\\) 已知\n总体的期望 \\(\\mu\\) 未知\n\n总体的期望 \\(\\mu\\)\n已知\n当期望 \\(\\mu\\) 已知时，求解方差\n\\(\\sigma^2\\)\n的置信区间的方法跟上面已知方差 \\(\\sigma^2\\) 求解期望 \\(\\mu\\) 的一样，都是利用 \\(\\frac{\\overline X - \\mu}{\\sqrt{\\sigma^2/n}} \\sim\nN(0,1)\\)，然后写出对应未知量的区间，这里就不详细讲述已知 \\(\\mu\\) 求解方差 \\(\\sigma^2\\) 的详细过程了。\n总体的期望 \\(\\mu\\)\n未知\n当期望 \\(\\mu\\) 未知时，求解方差\n\\(\\sigma^2\\)\n的区间估计就再也不能利用上面的 \\(\\frac{\\overline X - \\mu}{\\sqrt{\\sigma^2/n}} \\sim\nN(0,1)\\)。结合 \\(\\chi^2\\)\n分布的特性及其推导的定理可知\n\\[\\begin{align} \\frac{(n-1)S^2}{\\sigma^2}\n\\sim \\chi^2(n-1) \\end{align}\\]\n同样按照 \\(\\chi^2\\) 分布的 \\(\\alpha\\) 分位点的定义有\n\\[\\begin{align} P( \\chi^2_{1 -\n\\alpha/2}(n-1) &lt; \\frac{(n-1)S^2}{\\sigma^2} &lt;\n\\chi^2_{\\alpha/2}(n-1)) = 1 - \\alpha \\end{align}\\]\n注意这里不能用绝对值了，原因是 \\(\\chi^2\\)\n分布的概率密度函数不像标准正态分布或 \\(t\\)\n分布那样是对称的。其对应的概率密度函数如下所示\n\n\n卡方分布的概率密度函数\n\n进一步化简可得\n\\[\\begin{align}\nP(\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}(n-1)} &lt; \\sigma^2 &lt;\n\\frac{(n-1)S^2}{\\chi^2_{1 - \\alpha/2}(n-1)}) = 1 - \\alpha\n\\end{align}\\]\n即给定样本，总体期望 \\(\\mu\\)\n未知的时候，总体方差 \\(\\sigma^2\\)\n的一个置信水平为 \\(1-\\alpha\\)\n的置信区间为\n\\[\\begin{align}\n(\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}(n-1)}, \\frac{(n-1)S^2}{\\chi^2_{1 -\n\\alpha/2}(n-1)}) \\end{align}\\]\n实际上， \\(\\chi^2\\)\n分布的一个作用就是在正态总体分布中期望未知时估计其方差的置信区间。\n两个正态分布\n下面讲述两个正态分布的期望差值的区间估计以及方差比的估计。考虑以下问题：已知产品的某一质量指标服从正态分布，但由于原料、操作人员不同，或工艺过程的改变等因素，引起总体均值、方差有所变化。我们需要知道这些变化有多大，就需要考虑两个正态分布均值差或方差比的估计问题。\n下面的讨论都是假设给定了置信水平为 \\(1-\n\\alpha\\), 并设 \\(X_1,\nX_2,....X_n\\) 是来自第一个总体 \\(N_1(\\mu_1, \\sigma_1^2)\\) 的样本，\\(Y_1, Y_2,....Y_n\\) 是来自第二个总体 \\(N_2(\\mu_2, \\sigma_2^2)\\) 的样本，并假设\n\\(\\overline X, \\overline Y\\)\n是第一、第二个样本的均值，\\(S_1^2,\nS_2^2\\) 是第一、第二个样本的方差。\n估计 \\(\\mu_1 - \\mu_2\\) 的置信区间\n估计 \\(\\mu_1 - \\mu_2\\)\n的置信区间时也可以分为两种情况\n\n总体的方差 \\(\\sigma_1^2,\n\\sigma_2^2\\) 已知\n总体的方差 \\(\\sigma_1^2,\n\\sigma_2^2\\) 未知，但是知道 \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\)（\\(\\sigma\\) 未知）\n\n总体的方差 \\(\\sigma_1^2,\n\\sigma_2^2\\) 已知 由 \\(\\overline X \\sim N(\\mu_1, \\sigma_1^2/n_1),\n\\overline Y \\sim N(\\mu_2, \\sigma_2^2/n_2)\\) 可知 \\[\\overline X - \\overline Y \\sim N(\\mu_1 - \\mu_2,\n\\sigma_1^2/n_1 + \\sigma_2^2/n_2)\\\\ \\frac{(\\overline X - \\overline Y) -\n(\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}} \\sim N(0,\n1)\\]\n与上面相同，按照标准正态分布的上 \\(\\alpha\\) 分位点的定义有 \\[\\begin{align} P(|\\frac{(\\overline X - \\overline\nY) - (\\mu_1 - \\mu_2)}{\\sqrt{\\sigma_1^2/n_1 + \\sigma_2^2/n_2}} | &lt;\nz_{\\alpha/2}) = 1 - \\alpha \\end{align}\\]\n同样可解得 \\(\\mu_1 - \\mu_2\\)\n置信度为 \\(1-\\alpha\\) 的区间。\n总体的方差 \\(\\sigma_1^2,\n\\sigma_2^2\\) 未知，但 \\(\\sigma_1^2 =\n\\sigma_2^2 = \\sigma^2\\)（\\(\\sigma\\) 未知）\n根据 \\(t\\)\n分布的作用及其推导的定理可知\n\\[\\begin{align} \\frac{(\\overline X -\n\\overline Y) - (\\mu_1 - \\mu_2)}{S_w\\sqrt{1/n_1+1/n_2}} \\sim t(n_1+n_2-2)\n\\end{align}\\]\n其中 \\(S_w = \\sqrt{\\frac{(n_1 -1)S_1^2+(n_2\n-1)S_2^2}{n_1+n_2-2}}\\)\n同样根据 \\(t\\) 分布的上 \\(\\alpha\\) 分位点的定义有 \\[\\begin{align} P(|\\frac{(\\overline X - \\overline\nY) - (\\mu_1 - \\mu_2)}{S_w\\sqrt{1/n_1+1/n_2}}| &lt;\nt_{\\alpha/2}(n_1+n_2-2)) = 1 - \\alpha \\end{align}\\]\n通过查表同样可以求出 \\(\\mu_1 -\n\\mu_2\\) 置信度为 \\(1-\\alpha\\)\n的区间，结合上面 \\(t\\)\n分布在单个正态总体分布参数估计的问题可知， \\(t\\)\n分布专门用于解决正态分布中方差未知时估计其期望的问题。\n估计 \\(\\sigma_1^2 / \\sigma_2^2\\) 的置信区间\n估计 \\(\\sigma_1^2 / \\sigma_2^2\\)\n的置信区间同样可以分为两种情况 1. 总体期望 \\(\\mu_1, \\mu_2\\) 已知 2. 总体期望 \\(\\mu_1, \\mu_2\\) 未知\n总体期望 \\(\\mu_1, \\mu_2\\)\n已知时可以先通过标准正态分布求出 \\(\\sigma_1^2,\n\\sigma_2^2\\) 各自的范围，然后求解 \\(\\sigma_1^2 / \\sigma_2^2\\)\n的范围。下面主要讲总体期望 \\(\\mu_1,\n\\mu_2\\) 未知时，如何估计 \\(\\sigma_1^2 /\n\\sigma_2^2\\) 的范围。\n由 \\(F\\)\n分布的定义以及推导的定理可知\n\\[\\begin{align}\n\\frac{S_1^2/S_2^2}{\\sigma_1^2/ \\sigma_2^2} \\sim F(n_1 - 1, n_2 - 1)\n\\end{align}\\]\n根据 \\(F\\) 分布的 \\(\\alpha\\) 分位点的定义有\n\\[\\begin{align} P( F_{1 - \\alpha/2}(n_1 -\n1, n_2 - 1) &lt; \\frac{S_1^2/S_2^2}{\\sigma_1^2/ \\sigma_2^2} &lt;\nF_{\\alpha/2}(n_1 - 1, n_2 - 1)) = 1 - \\alpha \\end{align}\\]\n化简可得\n\\[\\begin{align} P(\n\\frac{S_1^2}{S_2^2}\\frac{1}{F_{\\alpha/2}(n_1 - 1, n_2 - 1)} &lt;\n\\frac{\\sigma_1^2}{ \\sigma_2^2} &lt; \\frac{S_1^2}{S_2^2}\\frac{1}{F_{1 -\n\\alpha/2}(n_1 - 1, n_2 - 1)}) = 1 - \\alpha \\end{align}\\]\n即 \\(\\sigma_1^2 / \\sigma_2^2\\)\n一个置信度为 \\(1-\\alpha\\) 的置信区间为\n\\[\\begin{align}\n(\\frac{S_1^2}{S_2^2}\\frac{1}{F_{\\alpha/2}(n_1 - 1, n_2 - 1)} &lt;\n\\frac{\\sigma_1^2}{ \\sigma_2^2}, \\frac{S_1^2}{S_2^2}\\frac{1}{F_{1 -\n\\alpha/2}(n_1 - 1, n_2 - 1)}) \\end{align}\\]\n小结\n在上面对正态分布总体进行参数估计中，用到了数理统计中的三大分布：\n\\(\\chi^2\\) 分布， \\(t\\) 分布和 \\(F\\) 分布， 其中 \\(\\chi^2\\)\n分布主要解决总体期望未知时估计其方差的问题， \\(t\\)\n分布主要解决总体方差未知时估计其期望的问题，\\(F\\)\n主要解决期望未知时两个正态分布的方差比值问题。\n单侧置信区间\n上面均是讨论未知参数 \\(\\theta\\)\n的双侧置信区间，但是在实际问题中，往往考虑的只是一个上限或下限，比如说设备、原件的寿命我们关心的是平均寿命\n\\(\\theta\\)\n的下限。这就引出了单侧置信区间的概念。单侧置信区间跟双侧置信区间的概念非常类似。\n\n总体的参数 \\(\\theta\\) 未知，\n对于给定的 \\(\\alpha\\) , 若由样本 \\(X_1, X_2..X_n\\) 确定的统计量 \\(\\theta'\\) 满足 \\[P(\\theta &gt; \\theta') = 1 -\n\\alpha\\] 则称 \\((\\theta',\n\\infty)\\) 是参数 \\(\\theta\\)\n的置信水平为 \\(1 - \\alpha\\)\n的单侧置信区间，而 \\(\\theta'\\)\n是单侧置信下限，将 \\(\\theta &gt;\n\\theta'\\) 变为 \\(\\theta &lt;\n\\theta'\\) 后，相应地变为单侧置信上限。\n\n单侧置信区间的计算方法与上面提到的双侧置信区间的计算方法已知，都是根据给定的\n\\(\\alpha\\)\n值和统计量服从的分布去查表，找到相应的分位点后带入不等式求解目标估计量的范围即可。\n","categories":["数学"],"tags":["数学"]},{"title":"浅谈算力优化","url":"/2022/10/30/%E6%B5%85%E8%B0%88%E7%AE%97%E5%8A%9B%E4%BC%98%E5%8C%96/","content":"当前的推荐或广告系统基本都是做到请求级别的预估和优化，在效果最大化的同时带来的问题是机器成本的上升；而流量分布的不均匀使得这个问题更为严峻，比如说对于抖音或美团，一天内流量往往有两个峰：午高峰和晚高峰，因为这两个时间点餐、刷手机的人数会陡增，而其他时间段流量会下降比较多，如下图所示\n\n这意味着如果准备足以抗住高峰的机器，那在其他时间段大部分机器是空转的，或者说\nroi\n很低，因此往往在高峰的时候都需要扩容或降级。降级一般是指指降低请求数，按比例\ndrop 流量，但是 drop\n流量对总体效果肯定是有损的，因此也衍生出了算力优化这个研究方向，算力优化本质上就是做效果和机器成本的\ntrade-off, 或者说如何尽可能无损地降本\n本文主要介绍一些算力优化的常见手段，笔者将其总结为\ndrop、cache 和 dynamic\n三类方法；而如果把消耗的算力拆解，可以直观拆成 2 部分：请求量 ×\n请求消耗的算力，因此可以从这两部分出发去优化算力\n\ndrop：直接把流量 drop 掉，即直接减少 “请求量”\n\ncache：将之前的预估结果存到缓存中，每次预估不用经过实际机器的\ninference，即减少了 “请求消耗的算力”\n\ndynamic\n则是根据请求的价值，动态控制每条请求消耗的算力，这个方法也是减少了 “请求消耗的算力”，DCAF\n是这类方法的代表\n\n上面的几个方法都是偏流量维度的优化，还有一些方法是对模型 inference 的\n耗时进行优化的，主要方向是计算并行（硬件升级）、模型压缩（量化、蒸馏、结构调整等），本文就不详细展开了\n\ndrop\ndrop\n流量可以说是最原始和粗暴的降级手段，但是在流量突增等场景还是比较有效和立竿见影的手段，除了无差别地\ndrop 流量的粗暴方式，还可以进一步做得更精细：通过 rule-based 或\nmodel-based 手段来判断一条流量是否要 drop\nrule-based\n的方法往往需要定义具体业务中的无效请求，比如说这里的 “无效” 在推荐可能是一些用户无点击且停留时长短的请求，在广告可能是由于没有曝光导致的无扣费等。这样的话就可以梳理出一些基本的规则，比如说针对连续\nn 次出无效请求的用户，在下一次的请求可以直接 drop 掉\nmodel-based 的方法跟 ruled-based\n方法差不多，一般可建模成一个分类或回归的问题；如果是分类问题，需要明确定义二分类中的正例和负例 (即无效请求)；如果是回归问题，需要定义每条流量的价值（如推荐或广告最终排序时使用的预估分）\n这里值得注意的点是 为了数据集无偏或者说避免 feedback loop,\n需要从不生效 drop 策略的流量中随机划一部分来生成训练集（holdout\n数据集）\ncache\ncache\n也是常用的一种节省算力的手段，即把之前请求的预估结果缓存下来，然后相似请求过来后，把缓存的预估结果直接返回；这里有\n2 个关键问题\n(1) 缓存的粒度，一般粒度越细，预估结果越准确，但同时消耗的 kv\n存储会越大；常见的可以考虑的维度包括用户、设备、媒体位置、访问时段等，具体缓存的粒度可以从中选出若干个维度叉乘出来\n(2) 缓存的预估偏差，由于不是实时预估的，所以预估偏差往往会比实时预估的要高很多，最直观的解决思路就是在\ncache 结果直接再套一个 calibration module（如 Platt\nscaling）；这个问题其实也跟缓存的 TTL 比较相关，一般 TTL\n越大，预估偏差会越大，同时消耗资源会越少，所以保证缓存的预估偏差小对节省资源也是有较大意义的，因为这个时候的\nTTL 可以设得更长\n这里的预估偏差反映在后验上一般是高估，其实是因为被低估了的流量竞价会失败而没法曝光。这个现象跟很多\nab\n实验的 “小流量高估问题” 的原因是一样的，实际上并不只是高估，而是预估不准，只是看到的投后数据都是高估投出来的\n但这里两者预估不准的原因并不完全一致，前者更多是因为不实时带来的，后者（即小流量高估）本质上则是因为有\n2 个模型同时产生训练数据集，两个数据集的分布不太一致（feedback\nloop），可理解为有 2 个 domain，然后处于小流量 domain 的模型使用了大流量\ndomain\n的模型产生的数据集，总体被大流量的模型的数据主导看不到自身的高估（证据之一就是，如果把两个模型的数据严格隔离开，这个问题会大大缓解，但对大盘有损，因为小流量模型数据太少，这部分流量的变现效率就很低）；所以实际中往往会把一些能区分\ndomain 的特征（如 model\nname、rit 等）加入到模型里，也能缓解所谓的小流量高估问题\ndynamic\ndynamic\n可以说是研究得最多的方法，其思想也比较符合直觉：根据每条流量预估的价值，动态调整每条请求消耗的算力，如果价值越高，分配的算力越高，反之亦然\n基于这个思路，很自然能想到有 3\n个需要解决的问题：（1）流量价值的预估（2）算力的预估（3）分配策略\n针对第（1）个问题，在搜广推的业务场景中，会涉及到排序的过程，因此可以把排序分数近似作为流量的价值；对于推荐，这个价值的物理含义可能不是那么强，但对于广告这类业务场景，基于\necpm 的排序就是意味着这条流量能给平台带来的实际收入\n针对第（2）个问题，在召回、粗排、精排的经典三段漏斗中，可以把进入各个模块的候选数来衡量算力；值得注意的是：候选数和算力两者是正相关，但不是线性的，后面会详细展开这个问题\n针对第（3）个问题，对流量价值和算力进行量化后，可以很自然地用最优化的思路建模这个分配问题，最常见的模式是在算力约束下，最大化流量价值，变量则是每条流量的候选数\n分配策略的一个代表性工作是阿里在 2020 发表的 paper，DCAF: A Dynamic Computation\nAllocation Framework for Online Serving System，paper\n主要解决的是上面的第三个问题，下面会先讲 paper\n中的分配策略，然后再探讨下流量价值和算力建模的一些思路\nDCAF\n问题建模\npaper 很自然把算力分配建模成如下的问题\n\n上面相关符号含义如下\n\n\\(i\\): 请求的 index\n\n\\(j\\): action 的 index，这里的\naction 一般是指进入各个模块的候选数\n\n\\(x_{ij}\\): 是否采用第 i\n条请求中的第 j 个 action，取值为 0 或 1，每条请求只能采用一个\naction\n\n\\(q_j\\): 第 j 个 action\n消耗的算力资源，注意这里的假设是相同 action\n消耗的算力资源与请求无关\n\n\\(Q_{ij}\\): 第 i 个请求中的第 j 个\naction 带来的价值\n\n\\(C\\): 总体的算力限制\n\n在建模完这个问题后，paper 提到了这种建模方式面临的 2\n个重要问题，其实也是这类最优化范式通常都有的问题\n第一个是需要实时求解最优化问题，这个问题的原因在于我们通过这种方式建模时，默认是能拿到全部数据的，如果统计过去\nn 天的数据是可以做到的，但是在线上实时 serving 是无法做到这点的\n这个问题其实跟 bidding 场景下的最优化建模的问题一致，《Bid\nOptimization by Multivariable Control in Display\nAdvertising》阅读笔记 是一个例子，而 bidding\n的一般套路是先通过求解最优化问题求解出最优的出价公式的形式，然后线上通过实时调控的来决定公式里的未知变量，后面我们讨论这个问题的求解时，会发现这篇\npaper 里的解决的思路有些不一样，需要直接求解最终的解\n第二个是流量价值即 \\(Q_{ij}\\)\n计算的问题，paper\n里给出的思路是通过模型预估，且提出了模型需要足够轻量级，因为流量维度的预估\nqps 会比较大\n问题求解\n问题求解是通过拉格朗日对偶方法进行求解的，关于这个方法的原理可以参考这篇文章\n凸优化总结\n里面的拉格朗日对偶部分，\n根据理论可以写出上面的最优化问题的拉格朗日对偶问题如图下式子（3）所示\n\n与 bidding 中的推导不同，在 bidding\n中只需要获取最优出价公式而不需要具体的解，因此可以最变量直接求导数为\n0 推导出最优出价的公式，这部分详细过程可以参考《Bid\nOptimization by Multivariable Control in Display\nAdvertising》阅读笔记 中的另一种推导方法\n而在这里需要作出具体的决策或者说需要具体的解，即在请求\ni 中应该选择哪个 action，即需要让哪个 \\(x_{ij}\\) 的值为\n1，所以需要求解这个问题，paper 的推导过程如下\n\n下面是笔者对上面推导步骤的理解\n1）只有当 \\(-Q_{ij} + \\lambda q_j + \\mu_i\n\\ge 0\\)\n时，公式（3）中的式子才有下界（笔者不太理解这一点，因为这里的\n\\(x_{ij}\\) 取值是 0 或\n1，而不是任意整数，所以即使 \\(-Q_{ij} +\n\\lambda q_j + \\mu_i \\lt\n0\\)，也不会出现无下界 (负无穷) 的情况）\n2）有了 \\(-Q_{ij} + \\lambda q_j + \\mu_i \\ge\n0\\) 条件后，可以将 min 的约束取消掉，上图的公式\n（4）因为这个时候的取 min 一定会使得 \\(x_{ij}(-Q_{ij} + \\lambda q_j + \\mu_i)\\)\n这一项为 0，这个其实跟推导拉格朗日对偶的过程有点像，如下图所示（来源）\n\n3）取消掉 min 约束后，剩下的 max 问题的变量就是 \\(\\lambda\\) 和 \\(\\mu\\), 从约束条件 \\(-Q_{ij} + \\lambda q_j + \\mu_i \\ge 0\\)\n可以推出 \\(\\mu_i \\ge Q_{ij} - \\lambda\nq_j\\)，即遍历第 i 个请求下的所有 action 的 \\(Q_{ij} - \\lambda q_j\\) 的最大值，也应该比\n\\(\\mu_i\\) 小，又因为 max 问题跟 \\(\\mu_i\\) 是负相关的，所以 \\(\\mu_i\\) 的取值应该为公式（5）所示\n4）因此，对于第 \\(i\\)\n个请求，真正生效的约束条件是 \\(\\max_j (Q_{ij}\n- \\lambda q_j)\\), （这个约束条件是紧致的），只有这个时候 \\(x_{ij}\\) 的值才不是 0, 亦即最终的\naction（见公式（6） ）\n因此，从公式 （6）可知，线上 serving 时，还需要知道 \\(\\lambda\\) 的值（\\(Q_{ij}\\) 和 \\(q_j\\) 的建模方法下面一部分会讲）\n但是 \\(\\lambda\\)\n的解析解在这个问题中比较难求解，paper 在这里做了 2 个假设\n假设一：\\(Q_{ij}\\) 随着\n\\(j\\) 的增加而单调递增\n假设二：\\(Q_{ij}/q_j\\) 随着\n\\(j\\) 的增加而单调递增\n假设一的含义是随着候选数越多，能从这次价值得到的流量越大（笔者觉得这里还要加个约束，就是线上的资源能抗住的情况下，否则候选数越多可能会导致错误率增加，流量价值下降，paper\n后面提到的一个 MaxPower\n的概念就是在讲这个事情），假设二的含义是随着候选数的增加，边际收益是递减的，这个比较\nmake sense\n基于这两个假设，可以推导出总体的算力 \\(\\sum_{ij} x_{ij}q_j\\) 会随着 \\(\\lambda\\) 增大而单调递减 ,\n这部分可参考 paper 中的 4.2\n部分，笔者这里做一个更直观的解释，由上面推导的公式（5）可知，\\(\\max_j (Q_{ij} - \\lambda q_j) \\ge 0\\), 即\n\\(Q_{ij}/q_j \\ge \\lambda\\), 而\n\\(Q_{ij}/q_j\\)\n的物理意义可以理解为单位算力的收益，由假设 2\n可知，随着算力的增加或者说候选数的增加，算力的边际成本是下降的。因此，随着\n\\(\\lambda\\)\n的增加，意味着对算力成本的要求增加，即候选数需要减少（这其实也是 \\(\\lambda\\) 的一个物理含义）\n有了上面的 “总体的算力 \\(\\sum_{ij}\nx_{ij}q_j\\) 会随着 \\(\\lambda\\)\n增大而单调递减” 的结论后，可以通过二分法寻找一个 \\(\\lambda\\) 使得消耗的算力刚好接近总体的算力\nquota \\(C\\)，算法如下图所示，11\n行的符号应该是写反了\n\n系统设计\nDCAF 的总体框架如下图所示，左边部分主要是建模 \\(Q_{ij}\\), 右边部分则是在线 serving 系统\n\n左半部分比较好理解，这里着重讲下右半部分，其中右上角的 Information\nCollection and Monitoring\n部分，会实时搜集当前系统负载的各项数据，如错误率，超时率等，并通过 PID\n算法控制错误率和超时率，或者说控制系统当前能承载的上限避免整个系统被打崩，paper\n里也把这个称为\nMaxPower，可理解为根据系统的实时负载调整每次请求的候选数的上限\n这部分在实验设置里也能体现出来，即随着系统负载 qps 增加，MaxPower\n会自动下调，保证整体系统的稳定性，如下图所示\n\n\\(\\lambda\\)\n则是根据上面的二分搜索算法，利用采样一段时间的数据实时计算 \\(\\lambda\\) 的最优值，算法中的容量 \\(C\\) 则是根据 QPS\n做了兑换计算，如下是离线计算 \\(\\lambda\\) 的过程\n\n实验设计\n实验部分做了 2 个实验，离线和在线的\n离线实验主要是做了流量回放，\\(\\lambda\\) 还是按照上面的方法计算，算力\n\\(q_j\\) 表示请求的候选数量， \\(Q_{ij}\\) 表示算力 \\(q_j\\) 下候选里 top-k 个 ecpm 广告的和\n\n\nAction \\(j\\) controls the number of\nadvertisements that need to be evaluated by online CTR model in Ranking\nstage.\n\n\\(q_j\\) represents the\nadvertisement’s quota for requesting the online CTR model\n\n\\(Q_{ij}\\) is the sum of top-k ad’s\neCPM for request \\(i\\) conditioned on\naction \\(j\\) in Ranking stage which is\nequivalent to online performance closely. And \\(Q_{ij}\\) is estimated in the\nexperiment.\n\n\n离线实验结果如下，左纵轴的 ecpm\n是实际的流量价值，右纵轴是实际消耗的算力，从结果可知，离线回放能做到算力下降同时流量价值上涨，且随着\n\\(\\lambda\\) 值的增大，DCFA\n总体消耗的算力和流量价值都是下降的，这个其实跟前面推导 \\(\\lambda\\) 的过程是一致的，因为\n\\(\\lambda\\)\n的物理含义就是增加候选数的边际收益的一个门槛，随着候选数增加，边际收益是递减的，而小于\n\\(\\lambda\\)\n时就是要截断的候选数，也就是 \\(q_j\\)\n具体的值\n\n虽然离线实验显示打平算力 ecpm+3.7%, 打平 ecpm\n机器成本 - 49%，但毕竟是离线环境，比较稳定，选择的 action\n也不会影响环境，所以如果开线上实验，效果一般会打个折扣，paper\n里的在线实验也显示了这一点\n\n流量价值与算力建模\nDCAF 这篇 paper\n着重讲了分配策略，但是对流量价值和算力的建模讲得比较少，只提到了 \\(Q_{ij}\\) 是通过一个简单的线性模型来预估，而\n\\(q_j\\)\n则只是候选数，没有进行建模；但是这两个变量的建模对结果影响又是非常大的，尤其是\n\\(Q_{ij}\\)\n值得注意的一个点是，流量价值 \\(Q_{ij}\\) 和消耗算力 \\(q_j\\)\n都跟候选数正相关，且候选越多，越能接近流量的最大价值（错误率可控前提下），同时消耗的算力也会越大\n直接把候选数当做消耗的算力不是非常精确的做法，比如说所有候选都只需要抽一次特征，这个时间不会随着候选数增加而增加；而如果以候选数为横轴，算力为纵轴，大概率会画出一条\n\\(\\log(ax)+b\\) 形式的曲线（\\(a\\) 和 \\(b\\)\n是两个参数），即随着候选数增加，边际的算力成本是下降的，\\(Q_{ij}\\) 同理，所以可以把 \\(Q_{ij}\\) 和 \\(q_j\\) 都写成 \\(f (候选数)\\) 的函数形式\n\n美团在这篇文章中也描述了其对 DCAF 的一个改进，美团外卖广告智能算力的探索与实践，其中就包括了对这两部分的详细的建模，下面会大概讲一下美团的做法，并讨论一下其他可能的做法\n流量价值建模\n如果直接建模 \\(Q_{ij}\\)（比如说通过\nmulti-head 的方式，每个 head 代表一个 action），需要保证 \\(Q_{ij}\\) 随着 \\(j\\) 是单调递增的，可以通过在 loss 增加一项\nregularization 项来尽量保证预估值是递增的，如在 loss 中增加一项 \\(ReLU(pred_{j}-pred_{j+1})\\)\n来让预估值尽量保序，但这也不能完全保证预估是有序的\n针对这个问题，上面的美团的文章的做法是预估整条流量的价值，然后通过每个广告的预估分来估算不同档位的价值（即下图的档位价值预估），这样理论上能够计算所有可能的\naction，相比于 multi-head 的做法要更加灵活和轻量级\n\n而价值预估上，为了保证在线的效率，美团采用的是 “离线预估，在线查询” 的方法，即离线通过对特征分桶来划分流量，然后通过\nxgb 预估每个桶里的流量价值并写入外部 kv；在线时通过实时查询 kv\n和插值的方法来\nserving，保证任何流量都有一个预估值，最后通过档位价值预估的方法来得到不同\n\\(j\\) 下的 \\(Q_{ij}\\)\n除了这个思路，前面提到了如果以候选数为横轴，算力为纵轴，大概率会画出一条\n\\(\\log(ax)+b\\) 形式的曲线（\\(a\\) 和 \\(b\\)\n是两个参数），我们可以基于这个先验来做更简单的推理和求解，比如说最简单就是基于后验数据直接拟合出\n\\(a\\) 和 \\(b\\)\n这两个参数，但是需要考虑做在何种维度，比如说 user 维度、user ×\n广告位维度等，核心问题是要考虑做在何种维度，如果维度太细数据过于系数，可能需要考虑做到更大的维度；同时也可以考虑贝叶斯这一类方法\n算力建模\n美团的文章里算力的建模跟价值的建模很类似，都是离线预估、在线查询的\n离线先做特征分桶，然后在桶里建模条数和算力的关系，每个桶的建模采用了分段线性拟合，与上面的价值建模不同的点在于，这里没有插值的过程（就是默认所有流量都能找到对饮的桶？）\n而相比于 DCAF 原文采用条数来衡量算力，这里采用了 CPU\n的耗时来衡量算力，总体方法如下\n\n而另一种思路跟上面提到的利用 \\(\\log(ax)+b\\)\n形式作为先验比较相似，也是需要考虑在特定维度去拟合 a, b 这两个参数\n小结\n本文主要从流量维度介绍了一些算力优化的手段，可以将其分为 drop、cache\n和 dynamic 三类方法，如果我们把算力拆解为 “QPS× 平均请求消耗算力”，drop\n相当于是有丢弃一些低质请求，cache\n则是通过缓存降低平均请求消耗的算力，两个方法都比较直观\ndynamic\n也是在尝试在降低平均请求消耗的算力，或者更准确的说是重新分配算力，做到在打平业务效果减低机器资源，或者打平机器资源提升业务效果；DCAF\n是这类方法的一个典型工作，DCAF\n对问题的建模和求解比较值得学习，但是对流量价值和算力的建模讲得比较粗糙，美团技术团队的一篇文章对这部分做了较好的补充，本文也提出了建模这两个变量的一个思路；总体来说，这个方法在理论推导和业界落地上都证明了其有效性，值得学习。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"深度优先搜索和广度优先搜索","url":"/2015/12/03/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E5%92%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/","content":"在图论中图的遍历是非常常见的操作，两种图的遍历的经典方法：深度优先搜索和广度优先搜索。因为经常忘记其实现方法，这里特意写篇文章记录这两种方法的实现的关键点。可能会存在很多实现方法，这里只记录我知道而且觉得最好理解的方法。\n\n深度优先搜索\n　　深度优先搜索实现的一个关键的思想就是递归。\n　　关键代码如下：\nvoid dfs(int i){      cout&lt;&lt;i&lt;&lt;' '; //输出点i，代表访问这个点      visited[i]=1;      for(int j=0;j&lt;n;j++)          //访问与点i有连接且还没访问的点          if( edge[i][j] == 1 &amp;&amp; visited[j]==0 ){              dfs(j); //对访问的点再进行深搜          }      return ;  }  \n　　完整代码如下：\n#include &lt;iostream&gt;  using namespace std;int edge[1000][1000] = { 0 };  int visited[1000] = { 0 };  int nodes;  int edges;void dfs(int i){  \tcout &lt;&lt; i &lt;&lt; ' '; //输出点i，代表访问这个点  \t\tvisited[i] = 1;  \tfor (int j = 0; j&lt;nodes; j++)  \t\t//访问与点i有连接且还没访问的点  \t\tif (edge[i][j] == 1 &amp;&amp; visited[j] == 0){  \t\t\tdfs(j); //对访问的点再进行深搜  \t}  \treturn;  }int main(){  \tint s, d;  \tcin &gt;&gt; nodes &gt;&gt; edges;  \tfor (int i = 0; i&lt;edges; i++){  \t\tcin &gt;&gt; s &gt;&gt; d;  \t\tedge[s][d] = 1;  \t\tedge[d][s] = 1;  \t}  \tdfs(0);//从第0个点开始\treturn 0;  }  ```  输入:  ```  5 5  0 1  0 2  0 4  1 3  2 4  ```  输出：  ```  0 1 3 2 4  \n广度优先搜索\n　　广度优先搜索实现的一个关键点是通过队列来存储访问过的点，并以队列里面的点作为始点访问其周围的点。\n　　关键代码见下：\nvector&lt;int&gt; que; //存储已经访问过的点  int head=tail=0; //head表示当前所访问的点，tail表示最后一个点的后一个点  visited[0]=1;//从第0个点开始访问，访问过的点加入到队列中  que.push_back(0);  tail++；  while(head&lt;tail){ //当head和tail相等时表示所有点已经访问过了      for(int i=0;i&lt;nodes;i++){          int cur=que[head];//获取当前访问节点          //将与访问点距离为1且还没访问的点加入到队列中          if( edge[cur][i] == 1  &amp;&amp; visited[i]==0 ){              que.push_back(i);              visited[i]=1;              tail++;          }      }      //访问完当前访问点周围的点，再访问下一个点      head++;  }  ```  　　完整代码见下所示：  ```cpp  #include &lt;iostream&gt;  #include &lt;vector&gt;  using namespace std;  int main(){      int edge[100][100]={0};      int visited[100]={0};      int nodes,edges, s,d;      cin&gt;&gt;nodes&gt;&gt;edges;      for(int i=0;i&lt;edges;i++){          cin&gt;&gt;s&gt;&gt;d;          edge[s][d]=1;          edge[d][s]=1;      }vector&lt;int&gt; que; //存储已经访问过的点  int head=0,tail=0; //head表示当前所访问的点，tail表示最后一个点的后一个点  visited[0]=1;//从第0个点开始访问，访问过的点加入到队列中  que.push_back(0);  tail++;  while(head&lt;tail){ //当head和tail相等时表示所有点已经访问过了      for(int i=0;i&lt;nodes;i++){          int cur=que[head];          //将与访问点距离为1且还没访问的点加入到队列中          if( edge[cur][i] == 1  &amp;&amp; visited[i]==0 ){              que.push_back(i);              visited[i]=1;              tail++;          }      }      //访问完当前访问点周围的点，再访问下一个点      head++;  }for(int i=0;i&lt;que.size();i++){      cout&lt;&lt;que[i]&lt;&lt;' ';  }return 0;  }  \n输入:\n5 5  0 1  0 2  0 4  1 3  2 4  ```  输出：  ```  0 1 2 4 3  \n","categories":["杂"],"tags":["图","深度优先搜索","广度优先搜索"]},{"title":"深度学习在表情识别中的应用","url":"/2017/09/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%8E%A2%E7%B4%A2/","content":"本文的表情识别指的是给出一张图片，检测其中人脸的表情（如果含有人脸的话）。所有可能的表情种类往往事先约定好，粗分可以分为\npositive、negative、neutral\n三种，细分可以分为 neutral, angry,\nsurprise, disgust, fear,\nhappy, sad\n7 种或者更多种，这里的类别可根据具体采用的数据集进行调整，从机器学习的角度来说，这实际上就是一个多分类问题。\n本文主要讲述如何将深度学习应用在表情识别中，以及在图像分类中深度学习一些常用方法，如采用预训练的模型进行特征的提取，用数据集对预训练的模型进行\nFine-tunning，而这实际上又牵涉到了迁移学习。\n\n之所以采用深度学习的方法，是因为深度学习中的网络（尤其是 CNN）对图像具有较好的提取特征的能力，从而避免了人工提取特征的繁琐，人脸的人工特征包括常用的\n68 个 Facial\nlandmarks\n等其他的特征，而深度学习除了预测外，往往还扮演着特征工程的角色，从而省去了人工提取特征的步骤。下面首先讲述深度学习中常用的网络类型，然后讲述通过预训练的网络 (经过 ImageNet 进行预训练) 对图像提取特征，以及对预训练的网络采用自己的数据进行微调的\nFine-Tunning。\n数据集\n表情识别中常用的数据集有 CK+， MMI， JAFFE， KDEF等，这些数据有些是短视频，有些是图片序列（记录一个表情的若干张图片），有些则是单张表情图片。\n在训练时，需要根据实际的应用场景以及采用的模型的输入格式将这些数据集处理成相关格式，这里不在详细说明。\n网络类型\n假如采用深度学习中常用的网络层 cnn，rnn， fully-connect\n等层组合成网络，那么具有非常多种的选择，这些网络的性能需要在实际任务中检验，而经过实践发现，某些网络结构往往在图像分类上具有较好的结果，如 ImgeNet 比赛中提出的一些列模型：AlexNet，GoogleNet（Inception),\nVGG， ResNet 等。这些网络已经经过了 ImageNet\n这个数据集的考验，因此在图像分类问题中也常被采用。\n至于网络的结构，往往是先通过若干层 CNN\n进行图像特征的提取，然后通过全连接层进行非线性分类，这时的全连接层就类似与 MLP，只是还加入了\ndropout 等机制防止过拟合等，最后一层有几个分类就连接几个神经元，并且通过\nsoftmax 变换得到样本属于各个分类的概率分布。如下是 AlexNet\n的网络结构图\nAlexNet\n\n\nAlexNet\n\n关于 AlexNet 更详细的介绍可参考这篇文章。\nInception\n而 Inception 是 Google 研发的一个深度神经网络，经历了四个版本\n（也叫 GoogLeNet），各个版本及其对应的论文如下，各个版本\n[v1] Going Deeper with Convolutions, 6.67% test error,\nhttp://arxiv.org/abs/1409.4842 [v2] Batch Normalization: Accelerating\nDeep Network Training by Reducing Internal Covariate Shift, 4.8% test\nerror, http://arxiv.org/abs/1502.03167 [v3] Rethinking the Inception\nArchitecture for Computer Vision, 3.5% test error,\nhttp://arxiv.org/abs/1512.00567 [v4] Inception-v4, Inception-ResNet and\nthe Impact of Residual Connections on Learning, 3.08% test error,\nhttp://arxiv.org/abs/1602.07261\nInception 的结构与前面的 AlexNet 的大同小异，其核心是多了 Inception\n模块，而 Inception 模块的结构如下\n\n\nInception moule\n\nInception\n模块使得网络的当前层可以通过多种方式从前一层网络提取特征，如上图通过了三种不同大小的卷积层以及一个池化层，然后将这些特征进行\nconcate 送到下一层。\n如下是 Inception V3 的结构\n\n\nInception V3\n\nVGG\nVGG 并没有采用很新颖的结构，整个网络只是采用了 3X3 的卷积层以及 2X2\n的池化层，但是层数比较深，就是这么一个靠两种简单网络层堆叠起来的网络，却在\nImageNet 比赛上取得了非常好的结果。VGG\n主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能，越深的网络能够容纳更多数据的信息，对于更大的数据具有更好的效果，VGG 整个网络的结构如下\n\n\nVGG\n\nResNet\n前面 VGG\n提到了网络的深度（层数）起到了一个非常重要的作用，但是如果只是简单地将层堆叠在一起，增加网络的深度并不会起太大作用。这是由于梯度消失和爆炸（vanishing/exploding\ngradient）问题，深层的网络很难训练。因为梯度反向传播到前一层，重复相乘可能使梯度无穷小。结果就是，随着网络的层数更深，其性能趋于饱和，甚至开始迅速下降。\n而为了解决因深度增加而产生的性能下降问题， ResNet\n引入一个 “身份捷径连接”（identity shortcut\nconnection），直接跳过一层或多层，如下图所示：\n\n\nResNet\n\n提出\nReNet 的这篇论文指出，假设目标映射为 \\(H(x)\\)，这个模块并不是让 stacked layers\n去直接拟合这个目标映射，而是去拟合残差 \\(F(x)\n:= H(x)-x\\)，则拟合的 \\(H(x)\\)\n则变为了 \\(F(x) + x\\), 论文假设优化残差\n\\(F(x)\\) 比优化 H (x) 更容易。\n上面这段话基本翻译自提出 ResNet\n这篇论文，更直观的理解就是每一层不仅仅只是能从前一层获取信息了，而是还可以从更前面的几层获取，而\nResNet 最开始的只是建单地将更前面几层的 x 直接加到当前层的输出，也就是\n\\(F(x) + x\\)， 而 ResNet\n的若干变体则对这部分直接传递的 \\(x\\)\n进行了处理，如通过卷积层等操作，但是其核心思想还是跨层连接从而获得更多的信息。\n最开始提出的 ResNet 的结构如下\n\n\nResNet\n\nCNN-LSTM\n上面的模型均是对单张图像进行处理，但是还有一种模型是对连续图片\nsequence 进行处理的，由于连续图片 sequence 包含了时序信息，因此通过将\nCNN 与 RNN 进行结合对时序图片列进行预测。在表情识别中\nCNN-LSTM 是将 CNN 与 LSTM\n结合起来的一种模型，其基本结构如下，图片出自该论文\n\n\ncnn-lstm\n\n其思想就是首先通过 cnn 提取每张图片的特征，然后将这些带时序的特征传入\nLSTM 中，可以取每一个 LSTM 的输出进行平均后连接 softmax\n进行输出，也可以直接取最后一个 LSTM 的输出连接 softmax 作为输出。\n上面这些网络训练的时候均是通过 SGD 进行反向传播，某些会加入 momentum\n等其他改进。\n这些网络理解起来可能问题不大，但是如果要代码实现起来的话工作量并不小，好在已经有了若干框架实现了这些模型，在使用时直接调用即可，这里以最简单的框架\nKeras 为例说明，Kreas\n已经在这里列出了一些实现的网络，可以参考其文档直接进行调用。\n预训练模型提取特征\n上面提到的这些模型少则十几层，多则上百层，其参数数目也达到了百万级别，要训练这么庞大的一个网络，如果数据量不足，很容易会会导致整个网络过拟合（当然，如果训练 epoch 次数少，也会直接导致欠拟合）。\n而在实际中，像 ImageNet\n这种庞大的数据集很少，而且某些只是归少数大公司所有，假如个人或缺乏数据的小公司需要用到上面提到的网络时，那就是无米之炊了，因此在实际中使用时，往往不是从头开始训练一个很大的模型，而是采用下面提到的通过模型提取特征以及对模型进行\nFine-Tunning 的方法。\n深度学习中的网络的一个好处就是，经过大规模数据集训练过后的，网络具有了抽取图像特征的特性，而抽取出来的图像的特征，跟实际要处理任务没有关系，也就是说经过\nImageNet\n训练过后的网络，也可以用于表情识别中抽取人脸的特征，然后用这些特征再训练一个小一点的模型，如\nLogistics Regression，\nSVM 等，这时候的网络完全就是在扮演着一个自动特征工程的角色。\n这里 “通过网络提取的特征” 往往指的是网络最后的某个全连接层的输出值，具体采用哪一层取决于后续处理所需的特征维数。\nKeras 也提供了经过 ImageNet\n预训练的一些模型，通过这些模型抽取图像特征的样例代码如下\nfrom keras.applications.vgg19 import VGG19from keras.preprocessing import imagefrom imagenet_utils import preprocess_inputfrom keras.models import Modelbase_model = VGG19(weights='imagenet')model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)img_path = 'elephant.jpg'img = image.load_img(img_path, target_size=(224, 224))x = image.img_to_array(img)x = np.expand_dims(x, axis=0)x = preprocess_input(x)block4_pool_features = model.predict(x)\n在实际测试时，用 VGG16 抽取图像的特征后再经过带 L1 正则化的 Logistics\nRegression，再 CK + 上进行 10-fold cross validation， 得到的准确率约为 85%,\n也说明了这种方法的有效性。\n对预训练模型进行微调 (Fine-Tunning)\n上面通过预训练的网络提取特征的确有效果，但是这些经过预训练的网络基本都是在\nImageNet 数据集上进行训练的，而实际中的各种任务是千差万别的，光凭\nImageNet 是难以涵盖各个领域的需求的。\n因此很自然会想到在预训练的网络基础上，用涉及到的具体任务中的数据集再次训练这个网络，从而让这个网络能够学习到这个数据集内的信息，这种方法也称为\nFine-Tunning。\nKeras 的官方 blog 也写了一篇文章专门阐述这种方法，文章链接为\nhttps://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n在这篇文章中，并没有调整整个网络的参数，而是只调整了最后的几层卷积层和全连接层，文章称原因是越底层的卷积层所提取到的图像的特性越是有共性的特征，而越上层的卷积层提取的特征则越是跟具体的领域相关的，当然，到底要调整多少层，还取决于所拥有的数据量，另外，往往还会去掉网络最后的若干层，并根据实际的图像分类数目构建最后一层大小。\n通过 Keras 进行 Fine-Tunning 的样例代码如下\nfrom keras.applications.vgg19 import VGG19height, width, categoriees = 128, 128, 7model_vgg19_conv = VGG19(weights = 'imagenet', include_top = False)# just fine-tune the top five convulutional layersfor layer in model_vgg19_conv.layers[:-5]:     layer.trainable = False#Create your own input format (here 128x128X3)input = Input(shape=(height, width, 3),name = 'image_input')#Use the generated model output_vgg19_conv = model_vgg19_conv(input)#Add the fully-connected layers x = Flatten(name='flatten')(output_vgg19_conv)x = Dense(feature_dim, activation='relu', name='fc1')(x)# x = Dense(feature_dim, activation='relu', name='fc2')(x)x = Dense(categories, activation='softmax', name='predictions')(x)#Create your own model model = Model(inputs = input, outputs = x)\n最后讲的利用了预训练模型的两部分实际上可以归入到迁移学习的范畴了，原因是我们利用了模型在\nImageNet\n上学到的知识，迁移到了一个新的领域 (表情识别），同理，也可以将其推广至医学影像等领域，当然，迁移学习远不止这点内容，有兴趣的可以去查找相关资料，这里不在论述。\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"混排的那些事儿","url":"/2023/02/26/%E6%B7%B7%E6%8E%92%E9%82%A3%E4%BA%9B%E4%BA%8B/","content":"混排，往往是的推荐系统的最后一个环节，在这个阶段，自然内容（后面简称 item）需要与营销内容（后面简称\nad）进行混合，生成最终推送给用户的 list\n如果以 Long Term Value (LTV) 的视角来看，这是个在 LT 和 V 之间做\ntrade-off 的过程，ad 如果出得过多，必然会挤压 item\n的数量和位置，进而影响用户体验和留存即 LT，但相应的广告收入，或者说\nAverage revenue per user (ARPU)\n会提升，反之亦然\n所以业界往往的做法是定一个用户体验的约束，在这个约束下尽可能优化 ad\n的效率，即达到收入最大化，因此很自然可以把这个建模成一个最优化问题，LinkedIn\n在 2020 年的这篇 paper 就是这么做的，Ads Allocation\nin Feed via Constrained Optimization\n直观地看混排这个问题，有 2 个子问题需要解决\n（1）怎么计算每个 item 或 ad 在每个位置上的价值：因为 item 和 ad\n是各自排序的，目标不同，最终的值的量纲也不同，这么把两者的 scale\n拉到可比范围是一个需要讨论的问题\n（2）怎么分配能让最终 list 价值最大化：在 item 和 ad\n的价值确认后，怎么插入 item 和 ad 的位置，从而达到整个 list 的最大化\n上面提到的 LinkedIn 的 paper\n重点是在解决第二个问题，部分内容也涉及到第一个问题 ；本文会先讲一下这篇\npaper 的建模方法，然后讨论下计算 item 和 ad\n价值的一些思路，混排中一些其他需要注意的事项\n\n建模方案\npaper\n把问题建模成如下图最优化问题（单次请求的最优，目前不考虑请求间的优化）\n\n各符号含义如下\n\n\\(i\\): 一次请求中位置的 index\n\n\\(x_i\\): 是否在第 i 个位置插入\nad\n\n\\(j\\): 请求的 index\n\n\\(u_{i}^{o}\\): item 在第 i 个位置的\nengagement utility（可理解为内容本身的价值，item 和 ad 都有）\n\n\\(u_{i}^{a}\\): ad 在第 i 个位置的\nengagement utility\n\n\\(r_{i}\\): ad 在第 i 个位置的\nrevenue utility（可理解为商业价值，item 没有这个价值）\n\n\\(C\\): 全局 engagement utility\n的一个门槛，一种可能的方式是设置成可能最大的 engagement\nutility（没有广告） 的一个百分比\n\n通过对偶拉格朗日可以求解出如下的解，式子中的 \\(\\alpha\\)\n是上面第一个约束的拉格朗日乘数；这个变量的物理含义是一个 bid，paper\n称其为 “shadow bid”，作用是把 engagement utility 的 scale 变换至 revenue\nutility 的 scale；则最终在位置 i 插入 ad 或 item 的价值如下图表 1\n所示\n\n上面的最优化问题的约束只是总体 list 的 engagement utility\n要大于特定预制，但混排往往还有一些硬约束，在 paper 中提到的是：top slot 和\nmin gap\n，分别表示第一个广告的位置约束，两个广告最小间隔的约束；除了这两个约束，一些常见的约束还有\nshowtime gap 约束（出现广告的评率）、adload\n约束等 (广告出现比例的约束)\n\n这两个约束并没有直接体现在最优化的建模里，而是体现在最后的混排算法里，整个算法流程如下\n\n建模关键问题讨论\n上面的建模虽然比较直观，但涉及到的一些需要解决的关键问题，\nshadow bid \\(\\alpha\\) 的计算\npaper 提到的 shadow bid，在经济学上称为影子价格，物理含义是表示的是增加一个单位的资源所带来的边际收益，关于影子价格更多讨论可参考\n线性规划中的影子价格怎么理解？\n这里的影子价格 \\(\\alpha\\) 是一个关于\n\\(C\\) 的函数，paper\n提到获取这个值的三种方法，\n\n从上图可知，基本思路是\n（1）确定 \\(C\\)\n的值之后求解原问题，得到 \\(\\alpha\\)\n（2）通过在线 ab 实验来确定这个参数\n（3）离线回放（感觉这里跟第一个是重合的，因为求解原问题也是需要回放历史数据，只是用多长的历史数据，以及更新频率有多大）\n\\(u_{i}^{o}\\)、\\(u_{i}^{a}\\) 和 \\(r_{i}\\) 的计算\n关于这几个值的获取，paper 并没有提供明确的方法，只是提到了\n\\(u_{i}^{a}\\)、 \\(u_{i}^{o}\\) and \\(r_{i}\\) are drawn from the same\ndistribution as was the historical data\n但如果真的这么做，存在的问题必然是历史数据会很稀疏，容易出现新的 item\n或 ad\n没有数据，或者把统计历史数据的维度拉得更大，这样容易导致效果变差，数据没区分性；因此，最终还是需要往预估方向去做\n如果考虑实际的业务，\\(u_{i}^{o}\\) 和\n\\(r_{i}\\) 的值比较好获取，直接取原本\nitem 排序和 ad 排序中各自的分数即可，但 \\(u_{i}^{a}\\) 的值应该如何获取？（其实这里的\n\\(u_{i}^{o}\\) 和 \\(r_{i}\\)\n的值的获取还有个问题，就是怎么获取一个 item 或 ad 在所有位置的 \\(u\\) 或 \\(r\\)，这部分在下面的 position bias\n会讨论）\n如果让 ad 直接走推荐侧的模型，在物理意义上是 make sense\n的，但可能会存在 2 个问题（1）ad 的特征未必能跟 item\n的完全对齐（2）需要保证 \\(u_{i}^{a} \\lt\nu_{i}^{o}\\)\n这里第二个问题尤为重要，因为如果这个不成立，上面的最优化问题最终的解会是在满足\ntop slot 和 min gap\n的约束下，能出广告就出，这显然是不合理的，那如何做这一点的保证，笔者现在也没想到很好的方法；比如说在推荐模型做\nmulti-head，一个 head 预估 item，一个 head 预估 ad，通过 regularization\n尽量保证 item head 要大于 ad head，但也没法做严格的保证\nposition bias\n这个问题在上面的 \\(u_{i}^{o}\\)、\\(u_{i}^{a}\\) 和 \\(r_{i}\\) 的获取讨论中提到了\n如果在计算 \\(u_{i}^{o}\\)、\\(u_{i}^{a}\\) 和 \\(r_{i}\\)\n不考虑位置信息，那必然是有偏的，因为不同位置的 ctr、cvr 等天然不一致\npaper 里提到了一种方法，也是实际比较常用的：training\n阶段使用实际的 position，serving 阶段使用统一的\nposition，同时保留一张映射表，映射不同位置跟 serving 时使用的统一位置的\ndiscount，映射表可通过后验数据统计获取得到，最终预估值乘上这个 discount\n就能等到不同位置的预估值\n这种做法的缺点是这张映射表需要经常更新，所以更好的做法是把这张表做到模型里，让模型训练过程中就能学到这个变化，预估阶段同时预估所有的位置的\nscore\ngap effect\n对于推荐而言，往往序准确就可以了，但 ad\n因为涉及到实际扣费，会要求 ctr，cvr\n预估足够准确，才能避免扣费不准确，引起广告主的客诉等问题\n除了上面的 position bias 会影响 ctr 准确性，两个 ad 之间的 gap\n大小也会影响 ad 的 ctr 等，如下图所示，ad 的 gap 之间如果过小，ad\n的点击率会过低，但 item 不会出现这种情况，本质上还是因为 ad\n的密集度过高，即使前面提到了有 min-gap 这一类硬规则\n\npaper 提出的做法是给增加一个 gap 特征来捕捉这个信息，paper\n做了如下推导，最终生效的形式跟 position_discount 有点像，等价于在原来的\nctr 基础上乘上一个 gap_discount, 下图中的 \\(g\\) 是 Logit function, \\(y_{ij}\\) 表示是否发生点击（还可进一步把\n\\(\\beta\\)\n参数化，做成个性化的参数）\n\n而如果在上面的混排算法上加上 gap effect 的影响，会有如下的流程\n\nitem 与 ad\n价值度量的另一种思路\npaper 认为 item 价值是 engagement utility，ad 的价值是 engagement\nutility + revenue utility\n而上面也提到了获取 ad 的 engagement utility\n会比较难，因此可以也可以考虑使用另一种思路来度量 item 和 ad 的价值\n因为涉及到扣费，ad 的价值是比较好衡量的，一般采用的是 ecpm = bid ×\nctr ×\ncvr（为了讨论方便，此处省略 hidden_cost），因此很自然的一个想法是，能否为每个\nitem 也赋予一个 bid，这样也能在 item 侧算出一个类似 ecpm 的指标，与 ad\n侧能进行比较\n紧接着的问题是，item 侧的 bid 的含义是什么？ad 侧的 bid\n是有明确的物理含义的：广告主愿意为一个转化付的钱（还会叠加调价策略修改原始的广告主出价），但在\nitem 侧并没有广告主这一角色，由谁来出这个 bid 呢？\n最直观的方法就是基于 ad 和 item 各自预估值的量级的差异，拍一个 item\n侧的全局固定的 bid，但这样显然不是最优的，而且这个 bid\n的量级也需要及时监控和更新，因为随着迭代，两边的预估值的 sacle\n会发生变化\n而如果从另一个角度来看，ad\n更多是表达广告主的诉求，目标就是要更多的用户转化；item\n更多是平台的诉求，\n目标是要为平台带来更多的用户和留存时间；留存时间越长，也意味着可供平台变现的流量会更多，或者说用户指标其实也是跟平台长期收入挂钩的；因此可以基于大盘来测算用户指标与平台收入的关系，以\nstay_duration 为例，可以对 stay_duration 分桶，测算 stay_duration\n与平台长期收入指标的相关关系，建立一个函数 \\(f\\) 使得 \\(bid =\nf(stay_duration)\\), 当然这里的变量也不一定是\nstay_duration，也可以是考虑 dislike、active\n等各种信号，关键是要把用户在平台的留存、活跃等信息，与平台长期收入量化挂钩，然后基于这个收入倒推出一个\nitem 侧的 bid\n对于 ad 而言，如果与 item 混排时，把 ad 插在 items 的第 \\(k\\) 位带来的收益，不仅仅是广告本身的\necpm，还可进一步考虑由于广告插入给总体 items 带来的损失（VCG\n计费的思想），即混排时 ad 的价值也可以考虑成 “ad 带来收益 - item\n后移的损失”；这样的话如果插入的位置导致 item\n后移的损失较大，则总体的 ad 价值会下降，避免对 item 的挤压\n硬规则是否最优\n上面提到了混排中存在着各种硬规则，比如说广告出现的首位，广告之间的 min_gap，show_time\ngap 等，从技术角度来说，这样的离散的硬规则显然不是最优的，会极大限制算法的搜索空间；而从业务的角度，这种硬规则往往是业务发展初期拍定的，随着业务发展，对当前业务是否合理也是需要重新评估的\n举个例子，一个对 ad 敏感和一个对 ad\n不敏感的用户，使用同一套硬规则并不是最优的，因为同样的广告频率，在敏感用户那转化率会比较低，同时对用户的留存影响也会比较大（相对于不敏感的用户）；而如果给对\nad 不敏感的用户出更多的 ad，同时减少敏感用户的 ad，打平总体的 ad\n数，最终总体的 engagement utility 和 revenue utility\n理论上也是更优的\n具体的实现上，一般需要设置一套规则和计算方法，能够计算出硬规则中的所有可能情况下的\nutility，然后加入到 list 的总体价值中\n而这其实也涉及到推荐或广告系统里的优化的一个方向，就是个性化，一般大盘策略上对所有用户都相同的超参或策略，都会有个性化的空间\n当然在这个过程也要注意个性化往往也会有个限制，以混排为例，需要对不敏感的用户有体验保护，不能逮住这部分用户一个劲地薅\n小结\n本文从 LinkedIn 的一篇 paper\n展开，介绍了混排需要解决的问题以及一种建模方法，paper\n的建模不复杂，关键是建模中使用的各个 utility 的获取，paper\n在这部分并没有说得比较详细，可能也是因为这部分跟实际业务耦合比较紧密；因此，本文后面也讨论了一种\nitem 和 ad 价值可能的计算方式：为 item 计算一个 bid， 同时考虑 ad 插入对\nitem 的影响\n另外，paper 也提到了 position bias 、gap effect\n等在混排中常见的问题，本文针对 paper\n提出的方法和业界的一些做法做了讨论\n最后，也探讨了一个比较比较开放的问题，就是混排中涉及到的 hard\nrule，hard rule 一般是红线，但从技术视角来看肯定不是最优的，同时 hard\nrule 是否合理，以及如何从业务角度和技术角度来 soften the hard rule，拿到\nengagement utility 和 revenue utility\n两部分的收益，也许是个值得讨论的问题\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"炒股损失的不仅是钱，年轻人请远离股市","url":"/2016/01/17/%E7%82%92%E8%82%A1%E6%8D%9F%E5%A4%B1%E7%9A%84%E4%B8%8D%E4%BB%85%E6%98%AF%E9%92%B1-%E5%B9%B4%E8%BD%BB%E4%BA%BA%E8%AF%B7%E8%BF%9C%E7%A6%BB%E8%82%A1%E5%B8%82/","content":"转载，作者：李晓鹏，侵删\n这篇文章本来是该几年前写的，奉劝大家不要去玩股票。因为那个时候我的《中国崛起的经济学分析》这本书刚刚出版，里面用 “破坏性要素参与分配” 的理论来分析了中国经济。在写作过程中我发现这个理论也可以顺便用来解释股票市场，让大家看清楚股票市场的本质。但当时的大盘指数才 1980 点，我怕写出来很多人会被我 “忽悠”，把手里的股票 “割肉” 卖掉，回头会恨死我。所以就忍了。\n\n我的观点很简单：股票市场不是年轻人应该去的地方。对年轻人来说，玩股票就跟爱上赌博一样，是在浪费生命。年轻人最大的资本是自己，一旦把自己有限的积蓄投入到股市中去，就会被行情的波动死死的抓住，然后在里面虚度光阴：原本应该学英语的时间，却拿来研究波浪理论；原本计划去听一场学术讲座的，却跑到证券公司去被各种股票大师洗脑；原本可以把本职工作做得更好一些，却敷衍了事然后偷偷打开行情软件看股票；原本可以在自己喜欢并擅长的领域取得成就的，却跟成千上万的 “股民” 一样天天守在电脑前面想着一夜暴富，沦为庸碌之辈。\n总之，炒股，你损失的不仅是钱，最重要的是会耽误你自己最宝贵的财富 —— 个人才干的价值提升。而后者才是你可靠而长远的财富来源。\n我进入股市的时间是 2006 年 7 月，也就是我研究生毕业的时候。因为几个月前中国股市跌破了 1000 点，许小年吴敬琏等人炮轰中国股市还不如赌场，应该推倒重来。我却觉得被他们轰到 1000 点的中国股市的价值被严重低估，到处找人推销我的观点，但是没有人相信，所以就自己找人借了 10 万块钱去炒股。本来打算早点进去的，但是因为硕士论文的事情折腾了好久，答辩通过的第二天我就去银行开户了，这时大盘已经涨到了 1500 点了。\n总之，就是这么误打误撞，不经意间，竟然闯进了中国股市历史上最大的一轮牛市之中。那个时候挣钱真好挣啊，不管买什么股票都在涨。每隔几天就能撞见一次涨停。但是我并不满足于跟着大盘涨，还想赚的更多。就开始买书来看，把什么《股票作手回忆录》《波浪理论》《巴菲特致股东的信》…… 等等有关股票投资的 “名著” 都翻出来看了一遍。\n那个时候就有了一种幻想，觉得股票赚钱很容易，以后就可以靠这个吃饭了。也就懒得去找工作，此前还打算自己跟几个朋友一起创业的，那 10 万块钱既然被我用来炒股了，创业的事情也就不了了之。\n2007 年股市到 6000 点的时候，我大概赚了有 20 万。上证指数涨了 300%，我挣了 200%，没有跑过大盘，但是也很令人满意了。毕竟那是借来的 10 万块钱啊。我大概在 4000 点的时候，就已经把钱还给别人了，所以剩下的钱就都是纯利润。而且这一年的吃喝也都是从股市里出的。如果从这个时间点来看，我一年零三个月挣了 20 多万，对一个硕士毕业第一年的人来说是很不错了。\n但是当大盘指数从 6000 点开始下跌的时候，我这过去一年多挣的钱就开始一点一点往回吐了。这时候我发现，自己曾经非常得意的那种 “炒股技术”，没有一个是真正有用的，买什么股票都在跌。吐啊吐，吐啊吐，一直回吐到 3600 点，亏得还剩不到十万块钱了，终于受不了，斩仓出局，把剩下的钱全部转回到了银行卡上。退出股市。这个时候算下来，我接近两年的时间，实际上只挣了大约 10 万块钱。即使纯粹算一个收入帐，也变得非常不划算了。\n更要命的是，这个时候我再去找工作，就变得非常困难。因为同年毕业的同学们，都已经工作了一年多快两年了。一般很好的大公司大企业会给应届毕业生提供一些很好的岗位，一旦不是应届生，就会对工作经验有要求。我既不是应届毕业生，又没有足够的工作经验。连续给一些大企业投了一轮简历，没有收到一个回复。那个时候真的好惨啊，别说面试什么的了，连面试通知都没有一个。只有一些莫名其妙的小公司给了我面试机会，我跑过去一看，好多都是骗子公司，有做传销的，还有忽悠人炒外汇的…… 当时觉得自己前途一片渺茫，本来是重点大学的本科、硕士毕业，混了两年变成这个鸟样，哎，有何面目再见人呢？\n这个时候没有办法，才想起来重新回学校去读书。还好我考试的功力还是在的，经过几个月的准备，终于考上了博士。后来突然就一帆风顺起来了，去了剑桥、去了哈佛、去麦肯锡、出版《中国崛起的经济学分析》…… 瞬间整个人都变了，从一个到传销公司找工作的无业游民，变成了一个集各种高大上于一身的学者。\n这一段经历，让我刻骨铭心。我知道了什么叫做 “珍惜生命，远离股市”。\n人的一生，从 20 岁到 30 岁之间这一段时光 —— 如果你不是富二代或者官二代的话，是比较难熬的。包括像任正非、柳传志、马云、刘强东这些白手起家的大牛人，他们在这个年龄也是生活比较黯淡的。因为这一段时间，从学校这个与世无争的世外桃源走进社会了，要自食其力了，但是资历、经验、关系网络什么的都不够，付出和收获很不成比例。不管是创业还是工作，其实都很难。你的个人期许和社会对你的承认程度，往往有很大的差距。\n这段时期其实不是一个学习了很多年以后，开始收获的时期。应该是一个一半工作、一半学习的时期。就是说你即使去机关企业工作，你这种工作也带有学习的性质。所以机关企业并不会按照你的付出程度支付 “足够” 的报酬，因为你的工作能力各方面还很不成熟，他们同时也在为你提供一个学习进步的环境。严格来说只能叫做 “半工半读”。通过一段很长时期的 “半工半读” 之后，等你对于本职工作十分擅长了，人际管理的资源网络也比较健全了，才能度过这一段考验期，进入一个比较好的发展时期。\n如果不考虑家庭因素，同龄人之间在 20 岁到 30 岁这段时间并不会拉开很大的差距，都差不多，工资高点低点也就是那么一点，农民工和硕士毕业的收入差距不是很大。没有飞来横财的话，大家都过着一样平凡的日子，默默的为自己的理想奋斗着、努力着。但是过了 30 岁以后，差距就会拉大了，成功的人可能非常成功，变身土豪名流，而没有进步的人可能会原地踏步，还是原来那样的地位或收入。这种差距可以是天上地下的区别。\n正因为如此，年轻人如果把时间放到炒股票上去，每天被行情的波动折腾得对本职工作心不在焉，最大的损失不是钱，而是耽误自己能力素质的积累。对于那些一无所有的年轻人来说，他们最值钱的东西是自己的学识和才干。\n股票上挣的钱能够立竿见影的看到：行情最火爆的时候，一两个月就能翻一倍，刺激的很。但是时间长了你会发现，这个东西根本不挣钱、挣到的还会赔回去，得不偿失。\n在工作学习上所花的时间，短期内很难挣到什么钱，你做工作努力一点、还是敷衍一点，上班时间是在有空就看看本职工作相关的书籍，还是偷偷摸摸的打开电脑看股票，每个月都是那么一点工资收入，并不会增加。但是，长远来看，平时的一点一点的积累，最终会在关键时刻让你脱颖而出，并因此而受益终身。这种回报，比股票市场上能够挣到的钱，不知道要多出多少倍，不知道要可靠多少倍。\n这里边的利害计算，我们一定要想清楚。更何况，我们的理想，难道是一个每天坐在电脑前面看股票行情的人吗？\n我觉得，这不仅是 20-30 岁这个年龄段的年轻人的事情。任何年龄段的人，只要他还没有对自己的事业失去信心，还想着取得比现在更大的进步，那么就不应该去玩什么股票。股票，要么是专业人士玩，要么是退休了没事干的老爷爷老奶奶去玩，除此以外的其他人都不应该去玩。\n股票市场这种制度的设计，本来就不是给普通人赚钱的。我在《中国崛起的经济学分析》里面提出一个理论，就是：一个人，要想稳定的获得任何形式的收入，都要对应一种这个人所能掌握的资源。他要么掌握生产性要素进行创造获利，要么掌握破坏性要素进行破坏获利。\n比如，你精通计算机软件，那么就可以从编程中赚到工资；如果你精通企业管理和市场营销，你就可以成为一个成功的企业家，这些都是你利用自己的能力 —— 也就是你能掌握的生产性要素参与社会分工，获得的合理的报酬。\n还有一种相反的，就是你身强力壮，可以晚上找个偏僻的地方拦路抢劫，或者精通攀沿技术，晚上可以入室盗窃把别人的钱变成自己的…… 这些就是你个人掌握的破坏性资源，从别人手里掠夺财富来获得收入，这叫 “破坏性要素参与分配”。\n不管是生产性还是破坏性要素，你总得占有一样。如果你一样都不占有，纯粹就是去赌钱玩，那么最后一定是得到一个平局：一会儿输钱，一会儿赢钱，最后不输不赢。\n但是在真正的赌场里边，输钱的总是赌徒，赚钱的总是庄家。这是因为庄家掌握了 “破坏性要素”，就是可以通过操纵规则和作弊来掠夺赌徒的钱。\n那么到了股市里，这个不停波动的市场里面，我们可以问问自己，我们掌握了什么资源可以参与股票市场的财富分配呢？有生产性要素吗？没有。那些能够经营企业上市的企业家才有。有破坏性要素吗？还是没有，那些操作股市的庄家才有。那这些我们普通的年轻人，凭什么能赚到比把钱存在银行吃利息更多的钱呢？相信什么波浪理论、什么趋势线，你能玩的过那些金融、数学专业毕业的研究团队吗？\n把这个道理想清楚，我们就不难发现，我们去股市赚钱，最好的结果，就是不亏本，能够不被别人掠夺，挣点跟银行利息加社会最低工资标准差不多的收入就到头了，有时候看起来一夜暴富，其实很快又会吐回去去；但大部分结果是，被那些掌握了 “破坏性要素” 的人掠夺，浪费了时间还亏了本。\n不管是哪一种结果，我们的时间都耽误不起。有人会听信关于 “价值投资” 的谎言，说我就长期投资，学习巴菲特，放在那里不动，又能挣钱又不耽误本职工作。这种想法是同样行不通的：如果你只用很少的一点钱去炒股，比如收入的 10%，这点长期 “价值投资” 的收益其实对你没有影响，那么为什么要花那个时间呢？如果你投入大钱去炒股，股票的波动会影响你的生活，那么你虽然心里想着价值投资，但还是会变成一个赌博一样，每天除了关注股市行情以外什么事情也做不好。而这就会毁掉你的生活。不管是那一种，你都应该远离股市。所以，与其 “价值投资”，还不如把钱存银行，旱涝保收。\n总之，最好的选择，就是把我们的钱安安稳稳的放在那里，别去想什么理财之类的事情。在我们年轻的时候，能够掌握的那一点点资金，根本不值得浪费时间去 “理财”。最需要 “理” 的 “财”，是自己的知识和能力。\n如果我们真的要玩股票，学习巴菲特、索罗斯，那应该做的不是拿着自己的钱去玩，而是像他们一样，学一个金融或者数学的学位，然后进入金融行业工作，争取成为基金经理，然后拿着别人的钱去玩，赚了提成，亏了不管。这样，你就可以掌握 “生产性要素”—— 也就是运用各种专业知识和组织专业研究队伍来进行价值判断，真的为企业发展融资，促进企业成长；或者掌握 “破坏性要素”，就是具备影响股票走势的能力，玩死那些小散户。\n如果你不打算这么干，那么，请远离股市，最好是碰都不要去碰一下；如果你已经进入了股市，最好是马上清仓，把所有的钱转回到银行卡里面。然后专注于自己的梦想，把它一点一点变成现实。\n如果你还是对股市恋恋不舍，还做着一夜暴富的美梦，那么，你生活的价值，无非就是花一些时间在电脑屏幕上观察布朗运动罢了。\n现在大盘正在蠢蠢欲动，很有可能再现 2006 年、2007 年的盛况，又会有一大波还在校园里或者刚刚工作的年轻人会面对我当年面对的那种诱惑了。我写这篇文章，希望可以帮助一些看到它的人，不要再走一次我当年走过的弯路。\n","categories":["闲话几句"],"tags":["闲话几句","转载"]},{"title":"爬虫抓取代理 IP","url":"/2016/07/05/%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86IP%E7%9A%84%E8%8E%B7%E5%8F%96/","content":"由于某些网站对会对爬虫做限制，因此常常需要通过代理将爬虫的实际 IP 隐蔽起来，代理也有分类，如透明代理，高匿代理等。本文主要讲述如何获取代理 IP，并且如何存储和使用。\n\n某些网站会免费提供代理 IP，如下面的几个\n\nhttp://www.xicidaili.com\nhttp://www.kuaidaili.com\nhttps://proxy.peuland.com\n\n获取这些页面上的代理 IP 及端口也是通过爬虫抓取，下面以第一个网站 http://www.xicidaili.com为例，解释如何获取并存储这些代理 IP。一般的流程为：解析当前页面--&gt;存储当前页面的代理IP--&gt;跳转到下一页面，重复该流程即可。\n解析页面\n首先要解析页面，由于网页中显示代理 IP 时是在表格中显示的，因此只需要通过找出网页源码中相关的表格元素即可。下面是通过 python 中的 requests 和 bs4 获取页面 http://www.xicidaili.com/nt/上显示的 IP 及端口。\nimport requestsfrom bs4 import BeautifulSoupuser_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'referer = 'http://www.xicidaili.com/'headers = {'user-agent': user_agent, 'referer': referer}target = 'http://www.xicidaili.com/nt/'# 获取页面源码r = requests.get(target, headers = headers)# 解析页面源码soup = BeautifulSoup(r.text, 'lxml')for tr in soup.find_all('tr')[1:]:    tds = tr.find_all('td')    proxy = tds[1].text+':'+tds[2].text    print proxy\n输出如下： 36.235.1.189:3128219.141.225.149:80125.44.132.44:9999123.249.8.100:3128183.54.30.186:9999110.211.45.228:9000...........\n代理 IP 的存储\n上面代码获取的代理 IP 可以通过在代码一开始建立一个集合（set）来存储，这种情况适用于一次性使用这些代理 IP，当程序发生异常或正常退出后，这些存储在内存中的代理 IP 也会丢失。但是爬虫中使用代理 IP 的情况又是非常多的，所以有必要把这些 IP 存储起来，从而可以让程序多次利用。\n这里主要通过 redis数据库存储这些代理 IP，redis 是一个 NOSQL 数据库，具体使用参照官方文档，这里不做详细解释。\n下面是 ConnectRedis.py 文件，用于连接 redis import redisHOST = 'XXX.XXX.XXX.XXX' # redis所在主机IPPORT = 6379              # redis服务监听的端口PASSWORD = 'XXXXXX'      # 连接redis的密码DB = 0                   # IP存储的DB编号def get_connection():    r = redis.Redis(host = HOST, port = PORT, password = PASSWORD, db= DB)    return r\n下面是在上面的代码基础上将IP存储到redis中， import requestsfrom bs4 import BeautifulSoupfrom ConnectRedis import get_connection# 获取redis连接try:    conn = get_connection()except Exception:    print 'Error while connecting to redis'    returnuser_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'referer = 'http://www.xicidaili.com/'headers = {'user-agent': user_agent, 'referer': referer}target = 'http://www.xicidaili.com/nt/'# 获取页面源码r = requests.get(target, headers = headers)# 解析页面源码soup = BeautifulSoup(r.text, 'lxml')for tr in soup.find_all('tr')[1:]:    tds = tr.find_all('td')    proxy = tds[1].text+':'+tds[2].text    conn.sadd(\"ip_set\", proxy)    print '%s added to ip set'%proxy\n上面的 conn.sadd(\"ip_set\", proxy) 将代理 proxy 加入到 redis 的集合 \"ip_set\"，这个集合需要预先在 redis 中创建，否则会出错。\n页面跳转\n上面的代码获取的只是一个页面上显示的代理，显然这个数量不够，一般通过当前页面中的下一页的超链接可以跳转到下一页，但是我们测试的由于每页的的 url 都有规律，都是 http://www.xicidaili.com/nt/page_number, 其中的 page_number 表示当前在哪一页，省略时为第一页。因此，通过一个 for 循环嵌套上面的代码即可获取多个页面的代理。但是更一般的方法是通过在当前页面获取下一页的超链接而跳转到下一页。\n代理 IP 的使用\n当我们需要通过代理访问某一网站时，首先需要从 redis 中随机选出一个代理 ip，然后尝试通过代理 ip 是否能连到我们需要访问的目标网站，因为这些代理 IP 是公共使用的，所以往往也会被封的很快，假如通过代理无法访问目标网站，那么就要从数据库中删除这个代理 IP。反之即可通过此代理访问目标网站\n下面是实现上面所说流程的代码： import requestsfrom ConnectRedis import get_connection# 判断IP是否能访问目标网站def is_valid(url, ip):    proxy = {    'http': 'http://%s' %ip    }    user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'    headers = {'user-agent': user_agent}    try:        r = requests.get(url, headers = headers, proxies = proxy, timeout = 6)        return True    except Exception:        return Falseif __name__ == '__main__':    my_proxy, proxies, ip_set = None, None, 'amazon_ips'    conn = get_connection()    target = 'https://www.amazon.com/'        while not is_valid(target, my_proxy):            if my_proxy:                conn.srem(ip_set, my_proxy) #删除无效的代理IP            if proxies:                my_proxy = proxies.pop()            else:                proxies = conn.srandmember(ip_set, 5) #从redis中随机抽5个代理ip                my_proxy = proxies.pop()        print 'valid proxy %s' %my_proxy\nrequests.get(url, headers = headers, proxies = proxy, timeout = 6) 是通过代理去访问目标网站，超时时间设为 6s，也就是说在 6 秒内网站没有回应或返回错误信息就认为这个代理无效。\n除此之外，在爬取免费提供代理的网站上的代理 IP 的时候，爬取的速度不要太快，其中的一个原因是爬取太快有可能会被封，另外一个原因是如果每个人都无间隙地从这种网站上爬取，那么网站的负担会比较大，甚至有可能垮掉，因此采用一个可持续爬取的策略非常有必要，我爬取的时候是没爬完一个页面后让程序 sleep 大概 2 分钟，这样下来不会被封而且爬取的代理的量也足够使用。实际中可以根据自己使用代理的频率来进行调整。\n","categories":["python","爬虫"],"tags":["python","爬虫"]},{"title":"爬过第二座山","url":"/2024/10/03/%E7%88%AC%E8%BF%87%E7%AC%AC%E4%BA%8C%E5%BA%A7%E5%B1%B1/","content":"最近听了情感播客《面基》的两期关于中年的播客：《中年，人生的第二座山》和\n《中年之路上的四种觉醒》后，也看了提及的几本相关的书:《第二座山》、《中年之路》、《中年觉醒》以及一些研究报告；颇有感触，想写点东西来记录一下，于是有了这篇文章\n每次读到这类内容，都会有一种错觉，似乎听完后就可以合理化自己躺平，给自己不努力找一个借口；因为一命二运三风水、命运无常，在随机性面前的我们似乎无能为力（比较扎心的是，这的确是一个不争的事实）；但这并不意味着我们要躺平，原因在之前写的\n《做一个清醒的傻瓜》中也提到了：“在努力还没达到一定程度前，我们连面对随机性的机会都没有，或者说幸存者偏差也是有门槛的，当你的能力不足时，进入决赛圈的资格都没有”\n本文是写给自己的心理按摩，是为了让自己在 “尽人事” 后，能够更坦然地 “听天命”；是为了能在面对人生的第二座山的时候，能够更加从容；祝开卷有益～\n\n神奇的 35 岁\n35 岁在我国似乎是个神奇的节点，青山资本的这份报告《35 岁，中国式中年丨青山资本 2022 年度消费报告》，里面就提到了大量的事实表明，无论是在各种政策（如购房、深造、落户、创业等）、婚育、职场里，35\n岁都是一个重要的分水岭\n社会时钟的转折点\n35\n岁仿佛就是我国社会时钟的一个重要转折点，社会的政策、婚育观点、以及备受关注的职场等都会在这个时间节点迎来一个重要转折\n在买房、深造、落户、创业等各种政策里，几乎都提到了 “35 岁” 这个关键词\n\n嘉兴：购房时 35 周岁以下的全日制普通高校本科毕业生可使用 15 万元人才房票\n济宁：就业的硕士（不超 35 周岁）3 年内购首套房可发放购房补贴 6 万元\n长沙：35 周岁（含）以下的硕士首次购房一次性给予最高 5 万元购房补贴，35 岁以下博士首次购房的购房补贴由 6 万提高到 10 万\n公派留学，博士申请需为 “不超过 35 岁具有硕士学位”\n温州 “35 周岁以下中级工可先落户后择业”\n九江修水 “对 35 周岁以下全日制高校毕业生给予每人 1000 元落户奖，提供‘拎包入住’人才公寓”\n\n婚恋市场上也是这样，到了一定年龄尚未婚配的年轻人，难免被家庭和社会催婚。往往一开始年轻人需要和长辈激烈辩论，但在斗争了几年后会出现一个神奇的现象：突然有一天长辈不再唠叨催促了，朋友也不再热心介绍对象了，因为大家心里有一条年龄底线，过了这条线还不婚配的人，就默认为在婚介市场上已经失去了价值，打上了不必再劝的标签。催婚有 35 岁的心理线，催育更有 35 岁的警戒线。各类医学研究中都将 35 岁作为高龄产妇分界线。35 岁不生孩子，基本被视为丧失了怀孕生育的可能性，除非 “上科技”\n职场上这种现象则更为明显，互联网行业是近年职场里常被提及的吃 “青春饭” 的职业，但除了程序员头上悬挂着 “35 岁” 的这把达摩克利斯之剑，军人、主播、运动员，也面临类似困境，解放军作战部队在到达营长之前、非作战部队在到达连长之前，35 岁是最高服役年龄；欢聚时代一份主播年龄统计显示，35 岁以上的主播仅占 7%。运动员的退役年龄绝大部分也在 35 岁以前\n甚至在一些领域如公务员报考，35\n岁就是一个硬门槛，94 年颁布的《国家公务员录用暂行规定》要求报考者必须 “年龄为三十五岁以下”，直到\n2024 年，才把这个门槛放开至 40\n周岁以内（针对硕士、博士研究生）。但公务员招聘的示范效应，使得事业单位的考编、大学招聘教师也出现了 35 岁上限的要求，大部分央企、国企乃至部分民营企业也逐渐习惯性地将不超过 35 岁作为岗位录用的标准\n为什么偏偏是 35\n面对这个神奇的社会时钟转折点，我们不禁会问，为什么偏偏是 35\n创造 “中年危机” 一词的是美国精神分析学家艾略特・贾克，当时是 1965 年，而研究的研究出发点很简单：人的生命旅程过半时会意识到自己必然迎来死亡，生命有限、能力有限，因此便有危机感。在当时的人均寿命刚超过 70 岁，以及女性大量进入职场对男性造成一定的挤压，在七八十年代的美国媒体频发男性中年危机的文章，以及无数情绪共鸣下，造词宣告成功\n中国流行 “中年危机” 则是从 90 年代开始，最早的媒体记录是 1994 年出现的，之后情绪共鸣开道，媒体深入报道。报告提到一个诡异的现象：55 个国家的人在 46 岁时心态陷入最低谷，中国则提前了 10 年\n在不同国家的就业市场上，年龄收入曲线都呈现倒 U 型分布，倒 U 型顶部的年龄称为 “golden\nage (黄金年龄)” 美国和中国的 2000 年以前，“黄金年龄” 稳定在 50-55 岁左右。2000 年以后美国继续不变，但中国的黄金年龄不断下降，到 2005 年以后降至 35 岁，如下图所示，配图来自\n“GOLDEN AGES”:A TALE OF THE\nLABOR MARKETS IN CHINA AND THE UNITED STATES\n\n我们曾默认劳动者随着工作年龄更长，经验增加，理所应当要拿到更高的回报，这种想法其实扎根于技艺意识、匠人意识，思路的本质是生产非标化\n但是工业化就是要打破非标，因为这是整个社会提升生产效率、降低生产成本的重要途径；工业化意味着标准化生产流程、自动化流水线、固定化设备，劳动力此时如同 “螺丝钉”，被打上了工业化的关键设定 ——“可替代性”。流水线要能正常运转，最可靠的办法是消除每个环节的非标准性和不可替代性，在所有时代都如此；而这也许就是 “危机” 的开始，这两年\nAIGX\n的快速发展给各个行业带来的危机，跟第一次工业革命里的纺织工人面对蒸汽机时感受到的危机，并无二样\n当工业化进程逐渐发达，市场的必要性就突出了，用企业这种组织形式完成生产，加上用市场经济这种方式配置资源，成了最有效的发展方式；我国的工业化进程在 1994 年市场化改革的深入后加速推进，非国有企业快速增长，1996 年，人事部《国家不包分配大专以上毕业生择业暂行办法》宣布大学生包分配制度正式退出历史舞台，1998 年至 2000 年，国有企业共下岗职工 2137 万人，98\n年的大下岗，劳动力从生产设计的可替代化，变成了真正意义上可以随时替代掉的人，也让很多人意识到第一次真正意识到，没有铁饭碗，工作是可以丢掉的\n市场经济带来优胜劣汰，有下岗、有裁员，也就有了职场和就业的压力与危机。一言以蔽之，94 年之后劳动力要竞争上岗，用人单位需要选择性价比最高的劳动力，劳动力的职业发展有压力了\n在 “GOLDEN AGES”:A TALE OF THE LABOR MARKETS IN CHINA AND THE UNITED\nSTATES，将工资随着年龄变化的原因分解成三个部分：工作经验、时间效应（不同年份的平均工资水平变化）和队列效应（每个世代的劳动者能力差异）；研究显示美国不同年代的时间效应和队列效应很小，但在中国这两者影响巨大。如下图所示\n\n从图中清晰可见，大约从\n2005 年开始，35 岁开始工资随着年龄增长而下降。但美国不是这样，中国在 21 世纪以前也不是这样。从生理上来说，35 岁并不是身体和心理素质全面转衰的开始，全球范围内中年危机往往也是在四五十岁才逐渐显现；而中国近二十年的 “35 岁危机”，有着更独特的背景原因：产业结构、劳动者教育水平高速变化而带来的劳动力价值高点前移现象\n从 94\n年至现在，我国的产业结构持续保持着 “第一产业下降，第二产业缓慢增长，第三产业高速增长” 的态势，互联网、金融业、地产、租赁和商务服务、科技服务、交通物流业一直呈现增长趋势，其中尤其以互联网、金融为代表，逐渐成为最有前景最受关注的产业，这些逐渐变得更重要的新经济相关行业都有一个共同的特点：相关知识和技术更迭非常快\n在这样的产业发展中发挥作用需要依赖优秀的学习能力和开放的心态，并且越年轻的互联网、移动互联网以及人工智能时代的 “原住民” 越有前人难以匹敌的优势。因此，年轻人逐渐成为更受青睐的选择，有经验的劳动者的价值意义下降。从这个角度来看，“35 岁危机” 来自于拿着 25 岁学的东西去面对当下的市场\n对于学习新事物的开放性，英国科幻作家道格拉斯这样形容人性\n\n任何在我出生时已经有的科技，都是稀松平常世界本来秩序的一部分；任何在我 15-35 岁之间诞生的科技，都是会改变世界的革命性产物；任何在我 35 岁之后诞生的科技，都是违反自然规律要遭天谴的！\n\n因此，这样的产业结构的变化让新的主力产业对劳动力提出了更新的要求 —— 体力、心力、学习迭代能力，这些让年轻人成为了更合适的劳动力，也让我国的的人力资本高峰慢慢地从四五十往左移到了三十五\n人生的两座山\n第二座山这个概念，来自这本书《第二座山》\n\n我们每个人都在攀登人生的两座山。如果说第一座山是关乎构建自我和定义自我的，那么第二座山则是关乎摆脱自我和舍弃自我的。如果说第一座山是关乎获取的，那么第二座山则是关乎奉献的。如果第一座山是精英式的独自攀登，那第二座山则是置身于有需要的人之间，并与他们手携手同行\n\n第一座山\n对于绝大部分的人来讲，第一座山的攀登会在读书择校、毕业是找工作出现，正处于鲜衣怒马少年时的学生，如果你去问他们当前最大的目标是什么，基本都会给出清一色的答案，找个好学校或好工作。名校光环、体面的工作，似乎成了第一座山的山顶\n这当然无可厚非，因为那个就是绝大多数人所追求的阶段性目标和任务；对于绝大部分的毕业生包括笔者，在毕业的时候最大的问题是生存焦虑，彼时会像抓住救命稻草一样，抓住人生中的第一份工作，经历了一轮轮激烈的校招竞争后，然后开始在一个环境里勤勤恳恳地工作；至于自己该做什么、想成为什么样的人这些问题，会被压制在意识的水平线以下，暂时不重要，公司需要我成为什么样的人，我就成为什么样的人\n老钱在播客里是这么说的，“北京的群山中，公司让我爬哪座山我就爬哪座。世界说我是什么我就是什么，我要付房租，要有积蓄，要生活，要年底给家里面打一笔钱，我变成了一个公立的实用主义者。‘怎样才能成功’很快战胜了‘我为什么要这么做’，我成了自己的老板，也成了自己的员工，实现了韩炳哲口中的自我剥削”\n事实上，每一个人在攀爬第一座山时，都是在建立自己的社会身份的过程，也是走向自立的过程；离开父母、培养自己、完善自己，让自己有自力更生的能力，并且努力在这个世界上留下印记。此时大多数人需要的不是一个饱满的灵魂，而是拥有一套效率最大化的专业技能，成为一个精简的专业人士\n这个过程让我们有各种名牌可以依附：上过的名校、取得的头衔。阶段性地说，这些东西都比寻找自我更重要，第一座山和它所象征的成功学和精英制度，鼓励大家进入一种社会喜欢但也许我们个人并不喜欢的生活，一种并没有安全感但看起来又挺成功的状态；我们热衷于在各个参考系中寻找自己的位置和排名，经常拿自己和别人比较，被自己的幻想所困扰，也会去羡慕别人的辉煌和快乐\n有些人一直在爬第一座山，有些人已经在某种意义上登顶了，但是不可避免地，大家都会走向下山这条路。下山的路并不好走，走上坡辛苦，但下坡路走起来痛苦；但是再想想能走下去，而不是滚下去，不是坠下去，这又何尝不是一种幸福？而在谷底处，我们又能看到另一座更大的山，那是我们人生的第二座山\n第二座山\n山谷并不只意味着人生低谷，它更像是一种心态、能量、思维层面的匮乏力。我们在第一座山中获得的财富、名望和成就并不能让任何人远离山谷，它更像是人生的一段必经路。在山谷，我们不可避免会经历痛苦，它有很多形式，比如疾病、亲人的离世、意义感的丧失、失败、失业、重大亏损等等，攀登第一座山时所获得的各种名牌可能都会荡然无存。当然，痛苦也可能来得没那么有戏剧性，它可能是一种潜移默化的萎靡不振，对工作的麻木，对生活失去掌控感，被惯性主导\n人生的低谷固然不好过，但恰恰是在低谷中我们塑造了自己。因为痛苦和挫折，能够让我们看清平常生活中那些肤浅的一面，进一步的，我们能借此看到更深层次的自己，并慢慢发现我到底想要什么；这是脱离了 “等我\nXXX，就会 XXX” 的答案，因为外部条件并不会回答内心中 “a why to live\nfor”，而有了这个 why，才能去忍受生活丢给我们的各种\nfxxking；而这也是尼采口中的 “一个人知道自己为什么而活，就可以忍受任何一种生活”，而这就是我们第二座山的起始点\n每个人的第二座山都不尽相同，但可以肯定的是，爬第二座山的意义是我自己赋予的，而不是世界赋予的\n\n世界告诉我，要应该成为一个好的消费者，但也许我就是想过上很简单的低欲望的生活\n世界告诉我应该独立自主，但是我就是想沉溺于温暖的人际关系网中\n世界告诉我应该追求自由，但是我就是想要更多羁绊，对家人的责任和承诺\n世界告诉我应该攀登阶梯，追求成功，但也许我就是想为别人而活\n\n也许四十不惑，不光是没有困惑，也是不被诱惑，不被五光十色的世界诱惑，不被别人多彩多样生活诱惑，不被普世的价值观诱惑\n穿过了人生的低谷，我们就有足够的勇气去放弃部分以前的自己。某种程度而言，爬过第一座山，努力过，跋涉过，走过下坡路，才会有深刻体会。走过下坡路，才会意识到，原来第一座山并非我所愿，山外还有另一座更大的山，那才是我的追求，那是我人生的第二座山\n爬过第二座山\n如何找到并爬过第二座山，播客以及书里都给了一些方法论，这里我愿意将其总结为：四种觉醒、三个老板和\nN 条出路\n四种觉醒\n播客将《中年觉醒》书里的内容总结成了四种觉醒：分别是智慧、观念、心态和关系\n智慧\n在爬第一座山的时候，无论是否已经登顶，无论是否原因，我们都会不可避免地走下坡路，哪怕再功成名就的人，也难逃斜率向下的宿命\n《中年觉醒》这本书提到，统计数据发现科学家们常常在自己 30\n多岁时做出了伟大的发现；但是在 40 多岁、 50 多岁、 60\n多岁这几个时间段里面，有重大突破的可能性就急剧下降。一个人在 70\n岁时做出重大创新的概率，和 20 岁时差不多的都几乎为零，那原因是什么？\n一个解释是，我们大脑的前额叶皮质的性能会随着时间而发生变化，前额叶皮质是我们同年时期大脑发育最晚的一部分，但成年以后它又是最早开始衰退的习惯。它主要负责工作、记忆、执行能力，还有屏蔽与手头任务无关的信息。只有它足够强大，我们才能集中注意力去提高自己的核心技能 ;\n无论是处理法律案件、做手术，还是开公交车，但是人到中年，前额叶皮质的工作效率就是会降低，这意味着我们的快速分析和创新创造能力都会下降。其次它还意味着一些曾经很容易上手的技能，比如多任务处理也开始变得不再得心应手\n所以，职业下行必然常态，这对所有人都是公平的，而且他比你想象中要来得更快；几乎所有从事高技能型职业的人，在\n30 多岁到 50\n多岁之间的某个时间点都会开始进入下行通道，有可能是因为个人原因，也有可能是行业或者宏观因素的原因\n更可怕的地方，一个人在事业巅峰期所做出的成就越高，那么他在下行期的表现往往也就越差，这就是所谓的飞得越高就跌得越痛。一个人在中年时期的失落感和他之前取得的成绩以及对过往念念不忘的程度是高度正相关的；如果一个人本来就对自己没有什么期待（日常语境中我们经常说的 “没有上进心”），但是人家可能就是过得比你快乐；偏偏是那些人上人爱好者、奋斗和增长成瘾者们的下坡路，可能会极为痛苦，会面临着多重打击，播客里是这么说的\n\n第一重是我们会不自觉地渴望更大的成功，用更高的二阶导增速来避免意难平，但这不太可能，高增速的好时光往往就那么一段儿\n第二重打击是随着年龄的增长，我们真的很难维持现有的水平\n第三重打击就是当我们过分执着于更大的成功，同时伴随着职业技能下降的时候，很容易最终会陷入成瘾模式，比如变成工作狂，以牺牲自己的健康与配偶、孩子、朋友们的深层关系为代价，\n并且没有人会共情你\n\n那怎么办？觉醒这本书说在你面前有三道门\n\n1\n号门：你可以拒绝接受这一切，否认现实，不甘平凡、愤怒的、固执的对抗下行，但可能会陷入不断的沮丧和失望当中\n\n2\n号门：你可以耸耸肩，接受这一切，任命躺平，然后将人生的下坡路视为一场宿命般的不可避免的悲剧\n 3\n号门：你还是得接受现实，那些曾经令你青云直上，功成名就的职业技能确实都成了过眼云烟，一切都归于平淡了，但是你可以带着自己的好奇心重新找点新乐子，学点新事\n\n《时间游戏》里提到：终身学习并不是一个空洞的鸡汤式的口号，而是不管我们是否愿意，我们一生中总会经历几次集体游戏的切换。每次换游戏的根本原因是为了解决新的问题，为此新的游戏会鼓励一些新技能，鼓励的方式就是给予掌握这项技能的人更高的估值，让这些人的社会地位上升。所以学习新技能是为了适应新游戏，仅此而已\n因此，智慧觉醒意味着如果我们能顺势而为，借助某些方法，其实可以自然的变得更明智、更成熟。窍门在于理解并且利用那些年龄增长带来的新的优势，比如说当人的年纪变大，人的表达能力会变得越来越好，因为相比从前，自己的词典变得更厚了，思维方式也更成熟了，播客里将这归为了 “晶体智力”\n所谓的 “晶体智力”，就是那些以习得的经验为基础的认知能力；与之相对的是 “流体智力”，也就是我们常说的先天智力，我们的阅读、数学能力都与 “流体智力” 相关，但不幸的是到了\n30 多岁、 40\n多岁的时候，流体智力就开始衰退了，相反地晶体智力会随着年龄地增长而增长\n\n流体智力与晶体智力的变化，与人的人力资产曲线和金融资产曲线是非常相似的；关于人力资产曲线和金融资产曲线，在面基的《中产，一个阶层的自我修炼》里是这么说的\n\n人一生的财富大体分为两部分，其一是呈现倒 u\n型曲线的人力资本，其二是曲线起伏但最终会朝着东北方向推进的金融资产。所以度过中年在财务上的表达就是我们的人力资本曲线是向下的，但是在这个过程中，我们用保险兜住了下限，再用金融资产所释放出来的现金流去进一步的弥补劳动收入的减少\n\n如果你的事业完全依赖于流体智力，那确实容易更早地到达事业巅峰，然后进入下行通道；但如果你的工作是更依赖于晶体智力，事业巅峰就会晚一些，上破路也会久一些。说的更直白点，就是偏理工科的岗位更依赖流体智力，偏文科的岗位依赖大量知识储备以及应用能力的岗位更依赖晶体智力，所以如果能从偏流体智力的行业切换到偏晶体智力的行业，就是一次比较成功的跳船，或者说职场转型；比如说原来你是偏向创新和实践的，那之后就可以往指导和培训去转型\n但是我们不得不承认，不管是什么人生阶段，只要是转型期，就一定会面临着各种不确定性，这会让人本能地感到不安、自我怀疑，甚至恐惧\n观念\n这部分讨论如何温和地走下第一条曲线，如何摆脱渴望成功、渴望增长的瘾\n《中年觉醒》这本书聊到了一个引人深思的案例\n\n一位女士在华尔街工作，有钱有地位，看起来是人生赢家；但同时也有很多问题：婚姻失败，轻度酗酒，和孩子们处的还行，但是孩子们都上大学了，彼此离得很远，很少见面。她没有什么真正的朋友，长期的加班也导致了身体的透支。所以工作对她来说就意味着一切，这是一个巨大的难以摆脱的惯性，给他带来了几乎所有的意义感和成就感，当然还有那份丰厚的薪水；但问题是她的流体智力也处于下行期了，所以现在自己做的这份工作越来越难以应付，以前他是团队的主心骨，但是现在即便是年轻的组员都有点信不过她了\n她自己也意识到了这个问题，这样看起来问题似乎就很好解决了；因为都意识到了，那就着手挨个解决问题就好了。但令作者咋舌的是，这位女士面无表情地对作者说 “也许我更想要的就是成为人上人，而不是幸福。度假、和朋友家人一起待在一起，任何人都能做到，但不是每个人都能像我一样成就一番大事业”\n作者本想反驳他，但是转念一想，回顾自己的人生，大多数时候，作者的选择和他一模一样。这么多年，这位华尔街的成功人士精心打造了一个让旁人艳羡的角色，因为事业有成，勤劳敬业，财富增值，孩子们读了好大学的华尔街精英，她成功征服了自己的第一座山，但是现在她如坐针毡，工作给她带来的一切回报，边际效用都在降低\n\n作者在书里也提到了原因：就是她所打造出的那个人上人的角色并不是一个完整的人，这位女士和自己做了一笔交易，她用一个活生生的自我去交换了一个标签化、符号化的角色\n而更深层次的原因，则是当一个人评价自己的标准过于单一，比如只有工作业绩或者是财富数字；如果有一天这个唯一的参考系崩塌了，这对\nTA\n来说，无疑是一种毁灭性的打击；这点也许对那些工作狂的一种诅咒，终有一天他们会发现自己也会走入那个不那么尽如人意的下半场，然后痛苦的追忆往日荣光，对自己失望，和家人朋友的关系日益紧张\n我很喜欢这个例子，因为看过了太多的身边的人包括我自己，都有这样的问题；我们都非常擅长用简单的标签去物化别人，也会物化自己，我们可以将自己物化成人上人，用工作成功、世俗的认可和骄傲来定义自己\n但比较现实的问题是，人的 golden age 就只有那么一段（上面提及的倒 U\n型曲线），所以也难免会给自己洗脑 “老天爷赏了你这碗饭，就老老实实的，在能吃的时候多吃几口，因为很可能过了这个村，你就再也吃不上这口饭了”；而事实上，像互联网、金融这些行业，是中国为数不多依然同工不同酬的行业，过了这村就没这店了，现在都缩编，跳槽机会也不是那么多，所以多干一个月，我就多赚一个月的工资，真的要走，也要拿着大礼包再走\n因此，虽然喜欢书里的故事，但这个不是绝不是劝大家躺平或摆烂，这个作用，也许是当我们不得不告别的那天，能够更加坦然，对过往的收入、股票、地位等没有太多的执念和留恋；更重要的是，也许是在我们发现工作带来的边际效用在递减至一定程度时，我们可以尝试把工作的时间让渡给一些边际效用更高的事情，给自己多找一些参考系，比如说亲密关系、爱好等等\n给自己多找一些参考系很重要，因为如果只有一个评分，其实某种程度上我们又回归到了学生时代的分数，就是一切丢掉一分落后千人的逻辑里了，所以会有骄傲、恐惧、攀比、戒断反应等等这些泥坑\n对完美主义者（现在似乎不是那么好的一个词了）而言，骄傲和恐惧常常是伴生在一起，\n骄傲的人常常是恐惧的，他恐惧失败，那更可悲的是，一个充满了恐惧输不起的人，他也很难从自己获得的成就当中感到快乐。因为他们总是焦虑于在关键时刻掉链子，做事的动机也变成了害怕把事情搞砸\n在与他人做比较时所产生的优越感暂时给了完美主义者安全感，但是这种想法又转过来，让他们充满了恐惧，因为失败是不可被接受的。一旦失败，它一定伴随着巨大的自我否定，\n而且很多对成功和增长成瘾的人都承认过，他们经常认为他人之所得就是我之所失。所以当他们看到别人比自己更优秀的时候，会很搞笑的不由自主地认为自己是一个一败涂地的\nloser\n这一切的根本，还是上面说的评价体系过于单一的问题，而观念上的觉醒，就是需要重构自己的价值判断体系，从更多维度来度量自己的价值\n心态\n《中年之路》这本书基于荣格心理学的视角，逐步去回答这个问题：“除了我的过往以及我所扮演的角色，我究竟是谁？”\n书里提到，我们都带着过去经历的痕迹、在无意识的生活，过往的经历在我们心中形成了一个内在的小孩，而社会化成长的过程就是我们与这个内心的小孩逐渐感到疏远的过程\n所谓的中年危机，其实是内在小孩和后天人格之间越发不一致、越发分裂所产生的痛苦导致的。并且这种痛苦它不是坏事，因为痛代表了这个内在自我准备迎接新生的强烈要求\n在荣格的理论中，人有两个成年时期，第一个成年期大概是生理年龄在 12 ~\n40\n岁左右，在这个阶段我们的人格是临时的，它不过是一个对外界反应的集合，我们还不完全了解这个世界，我们在学习，或者说我们是在模仿身边大人的态度和行为；第二个成年期则是等生理年龄到了中年的阶段才会开启，因为那个时候才有了一点闲暇和空余时间，才有了一些力量，可以稍微琢磨琢磨那个终极的人生命题 “我究竟是谁”\n因此，《中年之路》这本书里，强调了中年危机的原因，不光是时代、宏观经济、家庭、财务责任等等外部因素导致的。哪怕没有这些因素，我们内在也会召唤中年危机，召唤我们让内心小孩这个临时人格走向真正的成年，从虚假的自我走向真实的自我，成为我自己\n这个过程，往往会伴随着压力和痛苦，这是一场自我身份的危机，因为除了社会角色和心理投射之外，我们并不知道自己是谁，这些内在问题会外化出来各种各样的形式，比如无聊反复更换工作 / 伴侣、滥用药物、各种成瘾、自我毁灭的想法、不忠、抑郁、焦虑，还有各种各样的强迫倾向等；荣格派认为这种痛苦的内在危机出现的目的就是为了让我们去纠正自己的方向，而荣格给出的建议是鼓励我们敢于展示自己的弱点和不足\n精英的意思是一个人拥有少数人才拥有的优势，普通人只能对此望洋兴叹，靠精英文凭和人设并不能让很多人与你产生共鸣，它只会吸引来一批慕强的人，而且你最好不要暴露出半点软弱错误以及道德瑕疵，因为这很不 “精英”。所以苦难、困境、软弱这些状态就不应该出现，所以他当然也不能与其他人分享，这些事具有私密性，它令人羞愧，难以启齿，所以很多人转而去找心理咨询师倾诉\n但事实上，人不可能是完美的，人因为有了弱点才变得真实，而一个人真实，他才可信，才会吸引过来那些对你也很真的人。所以，心态觉醒，意味着我们能坦然接受自己就是不如别人，就是有一堆臭毛病，能够诚实地面对自己的弱点，能够放过自己\n说到这里，我不禁想起《我们都是不明真相的群众》里方丈说的一句话：“在投资这件事上，发现自己傻 X 比证明自己牛 X 重要一万倍”；我想不仅仅是投资，在很多领域，承认自己的无知，向别人坦然示弱，而且不在乎他们的想法，能让自己活得更轻松一点；毕竟，人生就三万多天，装给谁看呢\n关系\nTED 的一个演讲 《What\nmakes a good life? Lessons from the longest study on\nhappiness》，是关于哈佛大学的一项成人发展的研究项目，也可以说是史上最漫长的有关于幸福人生的跟踪研究，因为这个项目它持续了将近\n80 年的时间，跟踪了 724 个研究对象\n研究对象分为两组，一组是当时哈佛的学生，在二战之前，他们在哈佛读书，其中很多人也上了战场。第二组对象人数更多，他们是来自波士顿贫民区的青少年，这些人普遍经历过大萧条年代，课题组年复一年的跟踪这\n724 个研究对象的工作、家庭生活、健康状况。这场耗时近 80\n年的研究，换了整整四代研究员\n最终得出了什么结论？研究成果有点出乎大家意料，它的最明确的、最重要的研究结论就是良好的人际关系能让我们的人生更加快乐、幸福和健康\n那什么叫良好的人际关系呢？课题组给了三个比较重要的点\n\n我们必须要保持社交关系，不能与社会绝缘，不能把自己封闭起来，因为孤独是有害的，它会让我们的中年阶段更加不快乐，也会让我们的大脑功能、健康程度下降的都很快。而那些与家人、朋友、邻居相处得很好的人，他们比那些离群所居的人更快乐，也更长寿\n\n即便你有社交关系，也并不意味着你就不孤独。因为人际关系的重点并不是你有多少朋友或者有没有伴侣，关键是你拥有的这些关系的质量如何。那些成天吵架、没有爱的婚姻对健康的危害比离婚还要大，而幸福的婚姻生活还能减轻衰老带来的痛苦，哪怕你身体出现了各种毛病，但依然会觉得自己的日子过得很幸福\n\n良好的人际关系不光能保护我们的身体，还能保护我们的大脑。上了年纪以后，如果依然对另一半信任有加，知道对方在关键时刻能指望的上，连记忆力都不容易衰退\n\n三个老板\n这里的三个老板，是《中年，人生的第二座山》中提到的，分别是身体健康、内心和良好关系\n身体健康\n身体健康的重要性无需赘言，这是所有 0 前面的的\n1；成功可能有一千种方式，但没有一种是应该要以损害自己身体为代价的\n尽量别让自己生大病，因为生大病受损的是你的人力资本，会打断的你后面整个的现金流的节奏；因为除了医疗本省要花很多钱，身体受损了之后，精力体力不如从前的话，那在关键的攒钱的这一二十年可能状态也不行；这一点结合下面这张家庭现金流表来看就很清晰了\n\n为了维持身体健康，老生常谈的就是运动、饮食、睡眠、情绪这几项了，这里不在赘述，推荐纵横四海的这一期播客\nEP41《超越百岁》：如何在 100 岁的时候，还能去滑雪？\n除此之外买保险也是一个选择（注意买的是保障性的消费险，不是投资险），来保护一下我自己这个的金融资产或者说人力资产\n照顾好内心\n这部分在前面的第二座山部分以及观念觉醒也着重描述了\n《男孩、鼹鼠、狐狸和马》中说到：“我们只能看到外在，但一切都发生在内在”。说 “一切” 当然有些绝对，但是人往往只关注外部世界，认为是外部世界导致了目前的境况，或者总希望在外部世界找到一个救世主或者解决问题的办法，却是很多人在解决人生困境的过程中，渐渐 “迷路” 的原因\n大多数人梦想中的财务自由的退休和孩子的样子以及自己满意的自己，是被社会规训的惯性的想法，很多时候并不切实际；对于很多外部的无法改变的客观事实，我们需要做好 “内外” 与 “外在” 的课题分离，更多向内求，或者说追问清楚自己当下正真想要的是什么；\n因为无论是\n“我们只能看到外在，但一切都发生在内在”，还是 “一个人知道自己为什么而活，就可以忍受任何一种生活”，都在告诉我们，看到自己的内在，才能知道自己要真正要去爬的第二座山，而不是被世界所塑造的第一座山\n良好的关系\n这里的良好关系跟前面的重合了，这里就不再赘述了；但是在两期播客里都强调了这一点，也足以见题重要也行\nN 条出路\n《35 岁，中国式中年丨青山资本 2022 年度消费报告》中，从宏观层面提出了解决中国式 “中年危机” 的新思路：制造业大国转向为服务业大国，而这里面也蕴藏着中年人的\nN 条出路\n由于加入 WTO、信息产业和更多科技产业逐步发展以后，我国产业结构从第二产业向第三产业转移是必然的趋势，这个过程必然会让一些人失业，但是让中年人失业的结构性变化，也在创造新的适合他们的就业。服务业尤其是新服务业会承载新的价值创造，承接中年劳动人群。社区的作用在生活中越来越明显，相关的生活服务需求也越来越需要成熟的产业来支持：维护、维修、零工、社工、月嫂、保姆、护理、保洁；也许有一天，每个人都可以有自己的管家、生活助理、爱好伴侣、运动指导\n事实上，当前已经有一类大规模的服务业，青山资本在一份报告里面把他们统称为平台工人，比如说网约车司机、货车司机、外卖员，还有达人、博主、号主、新媒体主理人、程序员这些都属于平台工人，因为这些人本质上还是要依附于平台，并没有自己的生产资料：接不到外卖订单的小电驴不是生产资料，接不到平台派单的小轿车只能在马路上空跑，没有平台导流的直播间不是生产资料，一个被平台封禁了的公众号只会沦为一日谈资，然后被世界彻底遗忘\n当下服务业的一个很重要的特点，就是会催生出千奇百怪的供需，无论你的需求是多么小众，多么细分，都能找到供给，这也正是经年正在快速形成的新服务业\n不少新服务业方向已经在逐渐崭露头角：老物件翻新、传统手工艺制作、手机装裱、宠物殡葬、娃娃改造、家庭整理等。社交媒体上的长短视频让很多不容易被看到的角落里的新业态和老匠人越来越火，不说做多大的生意，但支撑自己的生活收入完全没问题。除了这些尚在形成职业的新工种外，国家已明确认可的新服务业类型也在不断增多。国家职业分类大典在于 2022 年末再次发布新版，新增了 150 多个职业，其中不乏许多服务业新职业。比如会议接待服务师、健康照护师、社群健康助理员、森林园林康养师、民宿管家、研学旅行指导师、家庭教育指导师、在线学习服务师等。\n这些新服务业可以用 “反工业化” 来概括。追求效率的商品生产过程标准化，工具越来越发达，生产者越来越年轻；而另一类服务或产品，则追寻温度，关心情感连接，服务于人的社会性，消弭孤独感\n服务业一定是中年人的一条路，但每一个服务行业，哪怕它再普通，想做好都不容易，也都有用心者在绞尽脑汁的琢磨如何在那些细微处给出更多的\nAHA\n时刻。没有同理心的人一定不会在服务业里出类拔萃，因为没有同理心也不太可能发泄和尊重别人的需求。另一方面，对平台工人来说，平台的演化、算法的更迭、新技术的导入，都可以轻易改变一个平台工人的工作。当然，这些让中年人失业的结构性变化，可能也在创造新的适合他们的就业岗位\n小结\n人生的第二座山，也许是每个人的的生命旅途里都无法避免的一次挑战和机会；当我们从第一座山下来的时候，健康、收入、技能、社会地位等可能都会快速下滑，这是挑战；但同时也给我们一个机会，一个去看到更深层次的自己、慢慢发现我到底想要什么、找到自己的 “a\nwhy to live” 的机会\n绝大多数的人都不可能避免地会遇到这座山，只是由于我国近三十年产业结构、劳动者教育水平高速变化，让这座山开始前移到了\n35\n这个节点；每个人的第二座山都不尽相同，但可以肯定的是，爬第二座山的意义是我自己赋予的；同样的，如何爬过这座山，在方法上也是多种多样的，没有一个标准答案\n服务好我们的 “三个老板”：健康、内心和关系，别让工作成为自己唯一的价值评判标准，诚实地面对自己的弱点并接纳自己，带着自己的好奇心重新找点新乐子，也许能让我们更好地从\n“N 条出路” 中找到独属于自己的第二条曲线～\n\n文中的一些参考资料\n\n中年，人生的第二座山\n中年之路上的四种觉醒\n第二座山\n中年之路\n中年觉醒\n时间游戏\n“GOLDEN AGES”:A TALE OF\nTHE LABOR MARKETS IN CHINA AND THE UNITED STATES\n35 岁，中国式中年丨青山资本 2022 年度消费报告\n中产，一个阶层的自我修炼\n\n","categories":["闲话几句"],"tags":["闲话几句","读书"]},{"title":"物理 CPU、CPU 核数、逻辑 CPU、超线程","url":"/2016/01/06/%E7%89%A9%E7%90%86CPU%E3%80%81CPU%E6%A0%B8%E6%95%B0%E3%80%81%E9%80%BB%E8%BE%91CPU%E3%80%81%E8%B6%85%E7%BA%BF%E7%A8%8B/","content":"由于经常会混淆这几个概念，所以特意借该文比较详细地记录这几个概念的区别以及在 Linux 下如何查看这几个参数。\n\n基本概念\n\n物理 CPU：物理 CPU 就是插在主机上的真实的 CPU 硬件，在 Linux 下可以数不同的 physical id\n来确认主机的物理 CPU 个数。\n\n核心数：物理 CPU 下一层概念就是核心数，我们常常会听说多核处理器，其中的核指的就是核心数。在 Linux 下可以通过 cores 来确认主机的物理 CPU 的核心数。\n\n逻辑 CPU：核心数下一层的概念是逻辑 CPU，逻辑 CPU 跟超线程技术有联系，假如物理 CPU 不支持超线程的，那么逻辑 CPU 的数量等于核心数的数量；如果物理 CPU 支持超线程，那么逻辑 CPU 的数目是核心数数目的两倍。在 Linux 下可以通过\nprocessors 的数目来确认逻辑 CPU 的数量。\n\n超线程：超线程是英特尔开发出来的一项技术，使得单个处理器可以象两个逻辑处理器那样运行，这样单个处理器以并行执行线程。这里的单个处理器也可以理解为 CPU 的一个核心；这样便可以理解为什么开启了超线程技术后，逻辑 CPU 的数目是核心数的两倍了。\n\n在 Linxu 下查看物理 cpu、核心数、逻辑 CPU 和是否支持超线程\n关于 CPU 的一些信息可在 /proc/cpuinfo\n这个文件中查看，这个文件显示的内容类似于下图所示\n\n可以看到里面的内容是以 processor\n（也就是逻辑 CPU）为基本单元进行划分的，processor 下的\ncore id 表示这个逻辑 CPU 属于哪个核心，而 physical id 则表示这个核心或者说逻辑 CPU 属于哪个物理 CPU。了解这些信息，便可以方便地查看上面说到的那些参数。\n\n查看物理 CPU 数量\n物理 CPU 就是不同的 phycical id 的个数，可通过下面命令实现：\n\n\ncat /proc/cpuinfo | grep 'physical id' | uniq |wc -l  \nuniq 是为了去掉多个逻辑 CPU 属于同一个物理 CPU 的重复记录。\n\n查看核心数\n核心数就是不同 core id 的个数，可通过下面的命令实现\n\n\ncat /proc/cpuinfo | grep 'core id' | uniq |wc -l  \n原理同上\n\n查看逻辑 CPU 数目\n逻辑 CPU 就是 processor 的数目\n\n\ncat /proc/cpuinfo | grep 'processor' | wc -l  \n查看逻辑 CPU 时不需要去重\n\n查看是否支持超线程\n如果支持超线程就是说同一个 core 下会有两个 processors，这样可以简单地观察 /proc/cpuinfo 中的内容，如果两个的 processor 下的 core\nid 相同，那么说明支持超线程。\n\n还有另外一种方法是查看 siblings 和 cpu\ncores 的数值是否一致，评判方法如下\n&gt; 如果 \"siblings\" 和 \"cpu\ncores\" 一致，则说明不支持超线程，或者超线程未打开。\n如果 \"siblings\" 是 \"cpu\ncores\" 的两倍，则说明支持超线程，并且超线程已打开。\n另外，top 命令中看到的 CPU 数目是逻辑 CPU（输入 top 后再按 1）。\n","categories":["操作系统"],"tags":["操作系统","Linux"]},{"title":"程序的表示、转换与链接 - week1","url":"/2020/05/30/%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%81%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%93%BE%E6%8E%A5-week1/","content":"最近在 coursera 上发现一门不错的课程，程序的表示、转换与链接，内容是类似《深入理解计算机系统》这本书的，说来惭愧，虽然在上学时零星上过一些相关的课程，但是却没有系统地将这些内容串起来理解。本着算法工程师首先得是个工程的原则，觉得还是有必要去了解一下这块内容；而且课程内容讲得挺通俗的，值得一听。\n本文的内容主要是 week1\n的内容，较为宏观地介绍了如何从冯・诺依曼体系结构演进至现代计算机结构、程序执行的基本原理、微体系结构与指令集 (ISA) 等。由于课程\nPPT\n说得已经较为清晰了，这里大部分内容会直接截图（懒得再打字也是一个原因。。。）\n\n冯・诺依曼结构\n基本思想\n冯 - 诺依曼结构主要思想是存储程序的工作方式，即让计算机完成的任何的一项工作，事先要编写成程序，然后把这个程序以及程序处理的这个数据先要送到主存，然后启动运行，运行以后，计算机就可以自动的取出一条一条指令，并且取出来进行执行，就是取出指令，执行。然后再取下条指令再执行，这样按部就班的取出指令并执行\n按照上面的描述，计算机应该有如下的组成部分\n\n\nVon Neumann architecture\n\n更抽象一点的结构如下\n\n\nVon Neumann architecture1\n\n因此，冯 - 诺依曼结构主要思想是\n\n计算机由运算器，控制器，存储器，输入设备和输出设备五个基本部件组成\n各部件的基本功能是\n\n存储器不仅能存放数据，也能存放指令，形式上两者都是 0/1 序列\n控制器能自动取出指令来执行\n运算器能够进行加减乘除、与或非等运算\n操作人员可以通过输入设备和输出设备与主机进行通信\n\n内部以二进制表示指令和数据。每条指令由操作码和地址码组成，操作码指出操作类型，地址码指出操作数的地址\n采用 “存储程序” 的工作方式\n\n现代计算机的模型结构和工作原理\n现代计算机采用的基本就是前面提到的冯 - 诺依曼结构，更深入地看 CPU\n的基本组成如下图所示，\n\n\nVon Neumann architecture2\n\n各个部件的作用如下\n\nPC（program counter）: 程序计数器，是一个用于存储指令在 memory\n中的地址的寄存器，需要执行的指令先送到 PC，然后送到 MAR\nMAR (memory address register)：存储 memory\n中某些指令或数据的地址 (跟总线相连)，除了接收 PC 的指令地址，还可以接收\nGPR 的数据地址\n MDR (memory data register): 从 memory\n读出的指令或数据 (跟总线相连)，送给 IR 或 GPR（同理也可往里面写）\nIR（instruction register）：存储真实的指令，每条指令由 OP 和 ADDR\n组成，表示指令的具体操作和要操作的对象的地址，供控制器读取\n标志寄存器：存储运算的结果得到的符号是什么，有没有进位\n有没有溢出等等一些标志信息\n\n因此，cpu 从内存读取指令 / 数据的流程是：PC/GPR -&gt; MAR -&gt;\nmemory -&gt; MDR\n-&gt;IR/GPR(同时会有一些控制信号，即图中的红色虚线）\n下面是以做菜的例子更详细描述计算机是如何工作的\n\n\ncookVSprogram1\n\n\n\ncookVSprogram2\n\n从机器语言到高级编程语言\n\n机器语言（01 表示指令）：增减指令后需要重新对纸带打孔\n汇编语言（符号表示指令）:\n增减指令不影响，需要通过汇编程序转为机器语言\n高级语言：与平台无关，编译程序 (生成目标文件）和解释程序（不生成目标文件）\n\n因此，高级语言 (这里以 c 为例）需要执行如下步骤才能最终被计算机执行\n\n\ncompile&amp;link1\n\n各个步骤做的事情如下\n\n预编译（生成 .i 文件）：主要是处理 #\n开头的语句，如进行宏展开、将被 include\n的文件插入到对应的地方（递归执行）\n编译（生成 .s\n文件）：将代码编译成汇编代码，包括词法分析、语法分析、语义分析和生成汇编代码的优化；虽然不同语言都可通过\ngcc 来统一编译，但是 gcc\n对于对于不同的语言调用了不同的编译程序（如 c 是 cc1，c++ 是 cclplus，java 是 jc1）\n汇编（生成.o 文件）：将汇编代码逐条转换成机器指令（有查找表）\n链接（生成可执行文件）：静态链接与动态链接\n\n下面是一个简单的例子\n\n\ncompile&amp;link2\n\n上面的图中有几点值得注意\n\n汇编指令与机器指令是一一对应的\n任何程序最终都是通过执行若干条指令来完成\n指令集体系结构由计算机硬件决定\n\n计算机的层次结构\n计算机简单可分为软件和硬件两个层次，而在这两个层次内又可以做更细的划分，连接着两层的则是指令集 (ISA)\n如下图所示\n\n\nlevel\n\n作为软件和硬件的桥梁，ISA 具体做了些啥，简单来说，ISA\n规定了如何使用硬件；类似于一个协议，其细节如下图所示\n\n\nISA\n\n计算机组成也被称为微体系结构 (Microarchitecture)，微体系结构跟 ISA 的关系不是一一对应的，不同的\nISA 定义了不同的指令集，同一个 ISA 可对应于多个微体系结构\n\n\nMicroarchitecture v.s ISA\n\n小结\nweek1\n的内容比较宏观地介绍了冯・诺依曼结构的计算机系统层次结构、程序在计算机系统内运行的基本原理、高级语言如何转换为机器语言、ISA\n的概念等基本内容，课程总共有 12 周，后面的课程当于是对上述的相关部分内容进行展开。\n\n\ncontent\n\n","categories":["程序的表示、转换与链接"],"tags":["程序的表示、转换与链接"]},{"title":"程序的表示、转换与链接 - week10、11","url":"/2020/10/03/%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%81%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%93%BE%E6%8E%A5-week10/","content":"本文是 程序的表示、转换与链接\n中第 10、11\n周的内容，主要介绍了从源文件生成可执行文件的步骤 (预处理、编译、汇编、链接)，并详细描述了其中的链接这一步骤中的两大过程：符号解析与重定位，并对比了链接输入的可重定位目标文件和输出的可执行目标文件的差别；对了解文件的从编译到执行原理有一定帮助，可配合\n《链接、装载与库》\n阅读笔记 一起阅读。\n\n通过对源文件的预处理、编译，汇编等转换\n可以得到一个个模块对应的机器代码，也就是目标文件；\n但是目标还不能直接执行，还需要将这些目标文件链接起来才能得到一个可执行文件\n链接过程用到的目标文件主要有：\n\n编译、汇编后得到的目标文件，称为可重定位目标文件\n\n链接后生成的目标文件，称为可执行目标文件，\n\n共享库文件\n\n下面以 Linux 平台所用的 elf 格式 为例，来讲解这些目标文件的相关内容\n程序转换过程\n以 c 语言为例，从源码到可执行文件的典型过程如下图所示\n\n\n转换过程\n\n在 Linux 下，通过 gcc 命令能完成上面各个步骤，而实际上， gcc\n命令实际上是由具体程序 (如 ccp，cc1,\nas，ld 等) 包装的命令，用户实际上是通过 gcc 命令来使用具体的预处理程序\nccp，编译程序 cc1 ，汇编程序 sa 和链接程序 ld 等。\n以 c 语言为例，各个步骤的 input/output 及其主要做的事情如下所示\n预处理\n主要处理以 #\n开头的代码，具体处理包括宏展开、条件预编译等\ninput: 源代码文件（文本文件，.c 文件）\noutput：预处理后的代码文件（文本文件，.i 文件）\n预处理主要做的事情有\n\n删除 #define 并展开所定义的宏\n\n处理所有条件预编译指令，如\n#if、#ifdef、#endif 等\n\n插入头文件到 #include 处，可用递归方式进行处理\n\n删除所有注释\n\n保留行号和文件名标识，以便编译时编译器产生调试用的行号信息\n\n保留所有 #pragma 指定的编译指令 (编译器需要使用)\n\n下面是一个预编译例子，其实就是把头文件嵌入到源文件中\n\n\npreCompiled\n\n编译\n对预处理后的文件进行词法分析、语法分析、语义分析并优化，生成汇编代码文件；这个步骤通过编译程序 (就是编译器）完成的\ninput: 预处理后的代码文件（文本文件，.i 文件）\noutput：汇编代码文件（文本文件，.s 文件）\n这部分详细内容就不展开了，详细可参考这一系列的文章 LLVM 概述 —— 基础架构\n汇编\n对编译后的文件通过查表操作（因为机器指令和汇编指令是一一对应的）生成机器指令序列；这个步骤通过汇编程序 (就是汇编器）完成的\ninput：汇编代码文件（文本文件，.s 文件）\noutput：目标文件 / 可重定位文件（二进制文件，.o 文件）\n链接\ninput: 多个可重定位文件\noutput: 可执行文件\n后面会详细介绍链接的主要内容\n链接的由来与本质\n链接的由来\n程序往往会被分成多个模块，在进行模块间会进行引用 (reference) 操作，因此在运行时需要确定被引用的符号的地址，而链接可以认为是在生成可执行文件时确定程序中引用的符号的地址\n如下是通过两个目标文件生成一个可执行文件的简单例子\n\n\nlink example\n\n上图中的四个步骤从概念上又可分为符号解析和重定位两大步骤，符号解析是就是在合并之前先确定引用和定义之间的关系。\n重定位则是代码和代码\n合并、数据和数据合并，合并以后它就在一个地址空间里面，\n这个地址空间实际上我们称为虚拟地址空间。\n虚拟地址可以认为是从 0\n开始连续增长的，在虚拟地址空间中，能得到各个符号的地址。\n再把这个地址填到引用的地方，那么就得到了每一条指令真实的 01\n序列。每个进程都有自己独立的虚拟地址空间，那最终虚拟地址空间怎么跟物理内存挂钩呢，实际上这个是由操作系统进行内存管理与分配时确定的，\n详细可参考 Linux\n虚拟内存和物理内存的理解\n链接的好处是\n1.\n模块化编程：程序可分为多个源程序文件多人协助开发、可构建共享函数库等\n2. 效率高：重新编译时只需要编译那些被修改过的源代码\n链接的本质\n链接的本质实际上就是在合并不同目标文件中相同的节，这部分跟之前写的《链接、装载与库》\n阅读笔记 (1)- 基本概念与静态链接 内容比较相似，这里不再赘述\n\n\nmerge\n\n存储在磁盘中的可执行文件中有一个比较重要的组成部分：程序头表，该部分存储着各个节到虚拟地址空间的映射关系 ,\n这部分后面会详细描述\n目标文件\n目标文件可分为三大类：可重定位目标文件、可执行目标文件 和\n共享的目标文件，这三类文件的特点如下\n\n可重定位目标文件（.o 文件）\n\n其代码和数据可和其他重定位文件合并成可执行文件\n\n每个 .o 文件由对应的 .c 文件组成\n\n每个 .o 文件的代码和数据地址都从 0 开始\n\n\n可执行目标文件（Linux 的 .out 文件，Windows 下的\n.exe 文件）\n\n包含的代码和数据可直接被复制到内存并被执行\n\n代码和数据的地址为虚拟地址空间中的地址\n\n\n共享的目标文件 (Linux 下的 .so 文件，Windows 下的\n.dll 文件)\n\n用于动态链接\n\n在装入和运行时 (前面的都是在链接阶段就用到了目标文件) 被转入到内存并自动被链接\n\n\n在 Linux 下最常见的目标文件格式就是 ELF\n格式，其实就是将代码和数据放到不同的节 (section) 中，如下 ELF\n文件的基本格式和一个简单的例子\n\n\nelf file\n\n值得注意的是，目标文件与可执行文件的格式非常相似，基本都是 ELF\n文件格式，因此可以从两种视角来看 ELF\n文件，即链接视角和执行视角，两者的简单对比如下，\n后面会详细分析这两种视角下的 ELF 文件的异同\n\n\nelf2view\n\n可重定位文件\n可重定位文件主要由 ELF 头、各种类型的节和节头表 (Section header\ntable) 组成，各部分的含义如下如所示\n\n\nreload file\n\n\n\nreload file1\n\n为什么要将未初始化的变量 (.bss 节) 与已初始化的变量 (.data 节) 分开？\n主要是为了节省磁盘空间，因为.data 节中存放的是具体的初始值，需要占磁盘空间；而\n.bss 节中无需存放初始值，只要说明 .bss\n中的每个变量将来在执行时占用几个字节即可（通过节头表来说明应该为 .bss\n节预留多大的空间）\n除了上面提到的各个节，ELF 头和节头表也是 ELF\n文件的两个重要组成部分\nELF\n头位于一个 ELF 文件的最开始的地方，里面包含了一些文件结构的说明信息，如 ELF\n魔数、版本、小端 / 大端、操作系统平台、目标文件的类型、机器结构类型、程序执行的入口地址、节头表的起始地址和长度等；且根据操作系统主要分两种结构，一种是 32 位系统对应的结构，一种是\n64 位系统对应的结构\n如下是通过 readelf 读取 ELF 头信息的一个例子\n\n\nELF Header\n\n除了 ELF 头，节头表是 ELF\n可重定位目标文件中最终要的部分，节头表主要描述了各个节的节名及其在文件中的偏移、大小、访问属性、对齐方式等，如下是一个简单的例子\n\n\nSectionHeaderTable\n\n从上面的 Off 和 Size\n可知，有四个节会分配存储空间：.text、.data、.bss、.rodata\n可执行目标文件\n可执行目标文件与可重定位目标文件很像，两者的不同点在于\n（1）在 ELF 头里面，有一个字段 e_entry，表示程序的执行入口，e_entry\n在可\n重定位文件当中是 0，因为可重定位文件只是用来链接的；而在可执行目标文件当中，这个字段给出这个程序执行的时候第一条指令的地址\n（2）可执行目标文件多了一个程序头表，也称为段头表（segment header\ntable）；\n类似可重定位目标文件中的节头表，描述的是各个段的一些信息，而可执行目标文件中的一个段 (segment) 是由可重定位目标文件中的多个有相同访问属性的节 (section) 组成的 ;\n如下图是可执行文件中的各个段及组成其的各个节\n（3）多了一个 .init 节，用于定义 _init\n函数，该函数用来进行可执行目标文件开始执行是的初始化工作\n\n程序头表 / 段头表主要是用来说明可执行目标文件中各个段的一些属性，如各个段在可执行文件中的位移、大小、在虚拟地址空间中的位置、对齐方式、访问属性等，如下是其定义和一个例子\n\n\nSegmentHeaderTable\n\n符号解析\n前面提到，链接主要分为符号解析和重定位两大步骤，符号解析的目的是把符号的引用和符号的定义关联起来；每个定义符号在代码段和数据段都分配了存储空间，而在引用符号和定义符号建立关联以后，重定位时就可以把引用符号的地址重定位成相关联的定义符号的地址 (虚拟地址空间中的地址)\n每个可重定位目标模块的符号存都放在各自的符号表 (在\n.symtab 节)，这些符号可分为三种\n\n全局符号 (Global\nsymbols)：由当前模块定义且能被其他模块引用的符号 (不带 static)\n\n外部符号 (External symbols)：由其他模块定义且被当前模块应用的\n\n局部符号 (Local symbols)：仅有当前模块定义和引用的本地符号\n\n值得注意的是，这里的局部符号 (Local\nsymbol) 不是指程序当中的局部变量，局部变量是分配在栈中\n的临时性的变量，链接器是不关心这种局部变量的，而局部符号则是分配在静态\n数据区，且在整个模块里面都可以使用的。 如下是一个例子：局部变量\ntemp 分配在栈中，不会在外部过程被调用，因此不是符号定义\n\n\nsymbolType\n\n符号强弱性\n当一个符号在多个地方有定义的时候，最终解析的时候只能有一个确定的定义；因此有如下规则：\n\n强符号不能多次定义\n\n若一个符号被定义为一次强符号和多次弱符号，则以强符号为准 (即对弱符号的引用被解析为其强定义符号)\n\n若有多个弱符号的定义，则任选其中一个 (gcc -fno-common 链接时会有\nwarning)\n\n那强符号和弱符号的定义又是什么呢？全局符号又可分为强符号和弱符号，区分两者特点如下\n\n所有的函数名和已初始化的全局变量名都是强符号\n\n未初始化的全局变量名是弱符号\n\n如下是一些例子\n下面的例子中由于 x 被重定义了多次，因此链接时会报错\n\n\n多重定义\n\n静态库文件\n静态库文件实际上是把若干可重定位文件打包好的一个文件，如 C\n自带的标准库 libc.a\n就是一个静态库文件，下面是一个例子，描述了如何定义与使用静态库文件\n\n\nstatic library\n\n符号解析过程描述如下所示\n\n\nparse symbol\n\n此外，值得注意的是，上面的链接中如果调换了 mylib.a 和 main.o\n的位置会报错，原因是先扫描 mylib.a 时，由于找不到 mylib.a\n中定义的函数被调用的地方，因此 mylib.a 中的所有 .o\n文件都会被丢弃 , 然后扫描 main.o 文件时，myfunc1 无法解析。\n因此，使用静态库时链接器对外部引用的符号的解析算法如下\n\n\nlink process\n\n小结\n这一周主要讲了从源文件生成可执行文件的步骤，并详细描述了其中的链接这一步骤的过程，包括使用链接的优点，链接如何将可重定位目标文件组合成可执行目标文件，以及这两类文件的差别，两类文件都是\nELF\n格式，但是可执行目标文件中多包含了一些程序执行的入口信息和初始化信息（因为可重定位目标文件是无法执行了），且两类文件中都有一个表头用来描述\nsection/segment\n的一些属性，在可重定位目标文件中是节头表，而在可执行目标文件中是段头表。\n","categories":["程序的表示、转换与链接"],"tags":["程序的表示、转换与链接"]},{"title":"程序的表示、转换与链接 - week2","url":"/2020/06/06/%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%81%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%93%BE%E6%8E%A5-week2/","content":"本文的内容主要是 程序的表示、转换与链接\n这门课第二周的内容，主要介绍了浮点数和整数在机器内如何编码和存储 (大端和小端) 的，了解这些细节后，能够更好地理解代码中进行数值计算和比较时出现的违反直觉的结果，同时也能避免出现这样的问题。\n\n数据在机器中都是 01\n编码的，而在程序中常用的数值类型是整数和浮点数，下面会描述下两者在计算机中的表示方式，且会重点描述浮点数部分\n整数的表示\n整数主要分为带符号（signed） 和无符号\n（unsigned）的，需要注意的是无论是否带符号，整数在内存中都通过补码来表示，正数和 0 的补码就是该数字本身。负数的补码则是将其对应正数按位取反再加 1\n对于 C 语言，需要注意的是，若同时带有无符号和带符号整数，C\n编译器会将带符号整数强制转为无符号数，比如说对于如下关系表达式，某些结果并不符合直觉就是由这个原因引起的\n下图中的后面带 U\n的数字表示这是个无符号的整数，反之就是有符号的，其中标红的就是结果不符合直觉的例子\n\n\nsignedVSunsigned\n\n上面那三个不符合直觉的例子原因分析如下\n\n第三个例子由于右边的 0U 是个无符号数，因此左边的 -1\n也被解析成一个无符号数，而其补码是 32 个 1，被解析成无符号整数后就是\n\\(2^{32}-1\\)\n\n第五个例子由于同理，右边的结果 -2147483648 的补码是首位的 1 加上 31\n个 0，被解析成无符号数后就是 \\(2^{31}\\)\n\n第六个例子中， 2147483648U 的补码同样也是是首位的 1 加上 31\n个 0，但是因为前面加了一个 (int)\n后被转换成一个带符号整数，则在解释的时候变成了 \\(-2^{31}\\)\n\n同理，在 printf 时通过 %u 和 %d\n分别将整数解析成无符号和带符号的，因此也会出现如下结果\n\n\n此处输入链接的描述\n\n浮点数表示\n在介绍浮点数之前，先介绍一些移码的含义，因为在浮点数的表示中用到了这个概念\n\n\nExcessNotion\n\n整数的通过 01\n表示很好理解，那小数中的小数点该如何在计算机中表示？采用的基本思想就是用科学计数法来表示小数，然后将科学计数法中不同部分 (包括正负、尾数部分和指数部分三大部分) 分段存储在\n01 序列中，如下是详细的描述\n\n\nfloat\n\n上面提到了任何实数都能通过科学计数法表示成 \\(X=(-1)^s × M × R^E\\)，\n而在计算机中都是二进制表示的，因此 R 固定为 2\n而 M 的取值则可以有很多种方式了，如让 M 的取值小于 1\n且小数点后第一位为 1, 则对于十进制的 12.0，写成二进制是 1100.0，相当于\n\\(0.11×2^4\\)，则可以得出 s=0，M=0.11，R=2，E=4；\n且由于 M\n的小数点后第一位总是 1，可以将其省略掉，则对于浮点数有以下的表示方式\n\n\nfloatpoint\n\n而在通用的 IEEE754 标准下，定点小数 M 是一个值在 1-2\n之间的数；比如说对于十进制的 12.0，写成二进制是 1100.0，相当于\n\\(1.1×2^3\\)，则可以得出 s=0，M=1.1，R=2，E=3；而对于十进制的 - 12.0，相当于 - 1.1×2^3 吗，则\ns=1，M=1.1，R=2，E=3，因此 IEEE745 的标准跟上面的很类似，具体如下\n\n\nIEEE754\n\n如下是通过 IEEE754 标准来存储和解析浮点数的两个简单的例子\n\n\n机器数 -&gt; 真值\n\n\n\n真值 -&gt; 机器数\n\n上面的 IEEE754 标准中，阶码是全 0 和全 1\n表示的是特殊值，而这可以分为以下几种情况\n\n\n\n-\n 尾数 M 全 0\n 尾数 M 非全 0\n\n\n\n\n 阶码 E 全 0\n0\n 非规格化数 / Denorms\n\n\n 阶码 E 全 1\n 无穷大\n非数 / Nan (Not a number)\n\n\n\nNan 表示的是不合法的数值如 \\(\\sqrt{-4.0}\\)、\\(0/0\\)、\\(+\\infty+(-\\infty)\\) 等都是非数\n而 Denorms 则是被用来标识前面通过 IEEE754 标准中最小正数到 0\n之间的这段距离\n\n\ndenoms\n\n从上面的描述中可知，并非每个浮点数都可以用 01\n精确表示，对于那些无法找到精确表示的浮点数，只能进行舍入来进行近似，如下图所示是一个例子\n\n\nfloat sheru\n\n大端与小端\n大端与小端其实就是数据的字节在地址中是如何排列的：大端指的是 MSB\n所在的地址是数的地址，小端指的是 LSB 所在的地址是数的地址\n\n小结\nweek2\n的内容很多细节，但是比较重点的就是整数和浮点数是如何在计算中表示和存储的，其中整数都是以补码来表示的，同时要注意同一个补码会根据上下文被解析成\nsigned 或\nunsigned，而这会导致一些违反直觉的情况出现；而浮点数通过科学计数法写成了三个主要部分：正负号、尾数和指数，并在计算机中分三段分别存储这些数，解决了无法直接表示小数点的问题，单精度和双精度的表示凡是是一致的，只是各个段的长度不一，需要注意的是，并非每个浮点数（这个集合是无穷大）都可以用\n01\n精确表示，对于那些无法找到精确表示的浮点数，只能进行舍入来进行近似，因此在浮点数进行比较时要慎重使用\n==\n","categories":["程序的表示、转换与链接"],"tags":["程序的表示、转换与链接"]},{"title":"程序的表示、转换与链接 - week7","url":"/2020/07/04/%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%81%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%93%BE%E6%8E%A5-week7/","content":"本文是 程序的表示、转换与链接\n中第 7 周的内容，主要介绍了 C\n语言程序中过程调用、也就是函数调用对应的机器级表示，\n包括如何传递参数，如何将控制转移到被调用过程，\n寄存器使用约定，递归函数的实现等等。\n通过了解这些内容，能够更清楚机器执行的详细过程，同时也能更清楚函数调用过程中栈空间是如何变化的；课程选用的指令系统是前面介绍过的 IA-32 指令系统。\n\n过程调用概述\n如下是一个简单的例子，通过这个例子主要阐述 3 个问题\n\n过程调用的机器级代码是什么\n\n参数怎么传递到 add 函数中\n\nadd 的结果是如何返回给调用过程的 \n\nint add(int x, int y) {      return x+y;  }int main() {      int t1=125;      int t2=80;      int sum=add(t1, t2);      return sum;  }  \n关于第一个问题和第三个问题，当 main 中调用\nadd 函数时，实际上是通过 call 指令实现的，call\n指令会把返回地址，也就是 call 指令下一条指令的地址放到栈里面，这样的话在\nadd 函数最后执行 return\n指令的时候，可以从栈里面取出这个返回地址使得它能够正确的返回到 main 执行。\n关于第二个问题，参数是通过栈传递的，即在调用的时候会把参数压栈，如下所示是进程的虚拟地址 (也叫) 空间分布的情况，其中栈是从高地址往低地址增长的，指向栈顶的指针也叫\nESP 指针\n\n\nexec2virtualaddress\n\n如下图所示是过程调用的机器级表示，调用的函数执行的是 P\n过程，被调用的函数执行的是 Q 过程；P\n过程做的事情主要是把参数和返回地址压栈，Q\n过程做的则是保存现场、执行被调用函数，恢复现场并返回调用函数；其中现场指的是共享的通用寄存器，因为是共享的，因此被\nQ 使用时需要先保存 P 之前在上面的值，后面再恢复\n\n\n过程调用机器执行过程\n\n前面提到因为过程 P 和 Q\n共享通用寄存器，因此需要保存和恢复现场，实际上这只是针对部分的寄存器，在\nIA-32 中的具体约定如下：\n\nEAX、EDX 和 ECX\n是调用者保存的寄存器 (必要时)，被调用者不需要先保存再使用\n\nEBX、ESI 和 EDI\n是被调用者保存的寄存器 (必要时)，被调用者需要先保存在使用\n\n因此，为了减少准备和结束阶段的开销，每个过程应该优先使用\nEAX、EDX 和 ECX 这几个寄存器\n由于 IA-32 共有 8 个寄存器，剩下的两个 EBP 和 ESP\n是帧指寄存器和栈指寄存器，分别用来指向当前栈帧的底部和顶部\n因此，过程 P 调用过程 Q 时栈和栈帧的变化如下图所示\n\n\nstackAndStackFrame\n\n上面简单介绍了过程调用的一些基本操作，下图则是基于上面的例子更详细地描述其对应的汇编指令和栈的变化\n\n\nprocedure example\n\ncaller\n这个函数对应的指令序列如上图所示，指令对应的基本操作在图中已经描述的比较清楚了，在这里着重讲指令序列中前三条指令，前三条指令是准备阶段，\n做的事情是保存 P 的现场，并且形成新的栈帧，这三条指令的执行过程如下所示\n第一条 pushl 指令：把老的 EBP 的值压栈，在这条指令执行完后，栈顶指针\nESP 也会自动指向这个地方\n第二条 movl 指令：把当前的 ESP 赋给了个 EBP,\n相当于是栈底的形成\n第三条 subl 指令：会把 ESP 减去 24 (相当于用了 24 个字节来存储在 caller\n中的变量、参数等)\n而在被调用的 add 函数中最开始的两条指令其实也是上面的 pushl 和\nmovl，起作用便是形成栈底\n最后的 leave 指令实际上是退栈，其操作是把 EBP\n的内容送到 ESP，相当于是把栈里面所有的空间释放掉了\n此外，汇编指令中一些符号含义如下\n\n前加 $ 符号的表示这是一个立即数，如第四条指令的\n$125\n\n-8(%ebp) 表示 EBP 的地址减去 8\n个字节后所在地址里存的值\n\n(%ecx) 表示 ECX 寄存器存的地址对应的值\n\n因此，一个 C 过程的大致结构如下\n\n\nsummary\n\n过程调用的参数传递\n这里讲了一个比较经典的例子，就是按地址传参和按值传参带来的会带来不同结果，其中的原因就是这两种方式最终对应的机器指令是不一样的\n\n\nswap compare\n\n下面是上面分别按地址传递和按照参数传递对应的汇编指令，从中可知，两者的一些关键区别有\n\n按地址传递时被压栈的是参数的地址而不是参数本身 (使用 leal\n指令)，按参数传递时被压栈的是参数本身（使用 movl 指令）\n\n按地址传递时会通过寄存器 ebx 和 ecx\n暂时存储两个参数的值，然后把参数赋值给存储着这两个参数地址的寄存器 eax\n和 ebx 中，一共用了 4 个寄存器；而按值传递参数时，则只用了 2\n个寄存器，只是对入口的参数进行交换\n\n\n\n swap address\n\n\n\nswap val\n\n递归过程调用\n下图是一个简单的递归过程对应的机器指令，假定这个过程是通过 P 调用的\n\n\nrecursive compile code\n\n从上图中可知\n\n在递归过程中，栈帧是一个个累积叠加生成的\n\n在递归过程当中用到了被调用者保存的寄存器\nebx，起作用是保护调用过程的现场\n\nebp 加 8 即 8(%ebp) 是第一个参数的地址\n\neax 中存储着返回值\n\ncmpl 和 jle 指令是用来表示上面的条件比较部分的，如果满足条件就会跳到\nL2\n执行退栈等清理工作；如果不满足，则会递归地执行入栈、生成新的栈帧的操作等\n\naddl 指令会在递归返回后一次执行累加操作得到最终结果\n\n因此，递归过程的总体执行流程如下\n\n\nrecursive\n\n递归调用中有很多额外的开销，这种额外的开销体现在\n\n空间：每递归调用一次都会形成一个新的栈帧，如果递归深度很大，栈帧的个数也变大，导致占用的栈的空间越来越多，最终引起栈溢出\n\n时间：每递归调用一次，都需要额外执行一些指令用于生成新的栈帧、进行参数压栈等等\n而这些准备阶段以及恢复阶段的额外的指令的执行都需要花时间\n\n因此，正常情况下如果能够不用递归，尽量不要用递归，因为它在空间上面和时间上面都会增加很多额外的开销\n选择结构的机器级指令\nif-else, 用了条件转移指令 jbe 和无条件转移指令 jmp\n\n\nif-else\n\nswitch-case 语句有一张跳转表，即下图中的 L8，\n其跳转到的目标实际上是通过基址加比例变址加位移量的方式来找到的。这个跳转表实际上是在在后面讲到的目标文件中 (可重定位目标文件或者是可执行目标文件)。\n在这些文件当中都有相应的一个段，叫只读数据节，rodata。\n\n\nswithc compilation\n\n循环结构的机器级指令\n循环结构的逻辑及其机器级表示如下图所示\n\n\ncycle structure\n\n下面是 for\n循环语句的机器代码及其分析，其他循环结构的机器指令结构类似，从中可知，实现同样功能，循环比起递归要节省时间和空间，因此优先使用循环方式去实现\n\n\nfor compilation\n\n小结\n这一章主要介绍了 C\n语言中常见的过程调用、递归调用、选择语句、循环语句等对应的机器级指令和具体的执行过程，通过了解这些语句的具体执行过程，能够更清楚计算机在执行时栈内存是如何分配的，以及为什么通过循环来实现相同功能时比递归更节省资源\n","categories":["程序的表示、转换与链接"],"tags":["程序的表示、转换与链接"]},{"title":"程序的表示、转换与链接 - week5、6","url":"/2020/06/14/%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%81%E8%BD%AC%E6%8D%A2%E4%B8%8E%E9%93%BE%E6%8E%A5-week5%E3%80%816/","content":"本文是 程序的表示、转换与链接\n中第 5、6\n周的内容，主要介绍了程序和指令的关系，目标文件的基本格式，并详细地介绍了\nIA-32\n体系下的指令系统，包括各种指令的类型、指令执行的基本流程，通过以上内容可以对计算机内部如何执行程序有一个感性的认识。\n\n目前所用的主流计算机基本上都是基于 intel 架构的，因此本文主要基于\nIA-32 指令系统介绍\n程序与指令\n在介绍 IA-32 指令系统之前，需要了解如何将高级语言程序转换为用机器指令表示的机器代码\n程序的表示、转换与链接 - week1\n中介绍了计算机的硬件基本组成，基于这个硬件上面\n采用的是一种存储程序的工作方式：即所有要通过计算机完成的任务事先要编写成程序，然后将程序装入到存储器里面，一旦要执行这个程序，计算机就可以把\n程序当中的指令一条一条取到这个 CPU 里面自动的来完成\n从计算机的执行角度来看，程序可认为是由指令和数据组成的，它们都是存放在存储器里面的，\n指令和数据都是用二进制的 01 进行编码的，在指令执行过程当中，指令和数据从存储器要取到 CPU，然后存放在 CPU 的寄存器里面，两者的关系如下图所示\n\n\ninstructionAndData\n\n上面说的指令指的是机器指令，在一些其他地方往往也能看到微指令与伪指令的概念，三者简单对比如下\n\n指令:\n也叫机器指令，在硬件和软件交界面上的，表示计算机的一个完整的基本动作\n\n微指令:\n硬件范畴，一条机器指令的功能是若干条微指令组成的序列来实现的\n\n伪指令：软件范畴，由若干个机器指令构成的一个指令序列\n\n机器指令和汇编指令是一一对应的，由硬件决定\n目标代码\n这一小节的内容基本在 《链接、装载与库》\n阅读笔记 (1)- 基本概念与静态链接\n有描述，因此这里只做简单的记录，下图描述了编译的过程，即如何从源文件生成目标文件\n\n\ncompile\n\n上图中的 .c 表示 C 语言的源文件、.i\n表示预编译文件、.s 表示汇编文件、.o\n表示目标文件 (也叫可重定位目标文件)\n从上图可知，原始的汇编文件中的指令与反汇编得到的汇编指令在形式上有一点差别，主要体现在\n(1) 编译得到的汇编指令的助记符当中都带长度后缀，而反汇编的都没有\n(2) 编译得到的汇编指令用的是十进制，反汇编用的则是十六进制\n此外的 test.o\n里面的起始地址总是从零开始，因为它还没有链接，还没生成可执行文件，所以它的地址是不确定的，需要被链接生成可执行目标文件，链接的过程也可参考上面的文章，从下图中可知，可执行目标文件中的地址不再是从\n0 开始的，而是一个确切的地址\n\n\nobjectFilesCompared\n\n这个确切的地址是什么地址呢？实际上是一个虚拟地址，它是在一个虚拟地址空间里面的，其实就是进程的虚拟地址空间，如下图所示\n\n\nobject file mapping\n\nIntel 处理器与 IA-32\n指令系统\nx86 实际上是 Intel 开发的一类处理器体系结构的一个泛称，比如说\n8086、80286 是 16 位的； 而 386、486 这些是 32 位的\n因为这些架构后面都带有 86，所以就称为 x86，Intel\n处理器中涉及的产品如下\n\n\nIntelProcessor\n\nIntel 把 32 位的 x86 架构的名称 x86-32 改为 IA -32，IA 实际上是 Intel\nArchitecture 的缩写\n因此 IA-32 就是 Intel 体系结构的 32 位机器的一个泛称\nIA-32 体系结构规定了寄存器的个数、\n各自指令功能以及寄存器的宽度、存储空间等；整体体系结构大致如下图所示\n\n\nIA-32\n\n从上图可知，IA-32 的体系结构的一些特点如下\n\n8 个通用寄存器，编号就是 0 到 7\n\n2 个专用寄存器：标志寄存器 EFLAGS 和指令计数器 (PC), 叫 EIP\n\n6 个段寄存器（间接给出段基址）\n\n可寻址的空间是 4GB，也就是 寻址空间是从 0 一直到 0xffffffff\n\n随着体系结构从 8 位的机器，逐渐扩展到 16 位、32 位的机器，寄存器的宽度也在不断地扩展，如下图所示是\n\n\nIA32-Register\n\n后面的 80 位的寄存器和 120 位的寄存器在后面的内容中会有介绍。\n寻址方式\n什么叫寻址方式？寻址方式是根据指令当中给出来的信息得到操作数或者操作数地址的方式\n根据操作数可能的地址，寻址方式也有多种\n\n指令中：立即寻址\n\n寄存器中：寄存器寻址\n\n存储单元 (属于存储器操作数，按字节编址)：其他寻址方式（多种）\n\n详细如下图所示\n\n\nsearchAdd\n\n下面是高级语言中的一个寻址例子，描述了声明变量时，内存的分配情况以及访问变量时的寻址方法（以\nWindows 为例）\n\n\nCSearchAddress\n\n前面提到，寻址指的是从如何从机器指令中获取找到操作数的地址，那 IA-32\n中的指令是怎么组织然后坐到这一点的呢？\n下面是关于这个指令的一张详细的图，比较复杂，这里不展开讨论了\n\n\nMachineCode\n\nIA-32 常用指令类型\n传送指令\nIA-32 当中的指令类型有很多种，最基本的就是传送指令，就是从一个地方传到另外一个地方。传送指令又可分为以下几种\n\n数据传送指令，数据传送指令一般是用 MOV 指令来实现的；\n\n地址传送指令，用于加载有效地址\n\n输入输出指令，用于输入 / 输出端口和通用寄存器之间进行数据的交换\n\n标志传送指令，将标志寄存器的值入 / 出栈\n\n如下图中的 LEA (Load Effective Address)\n指令就是用来加载有效地址的，leal 做的就是把 edx\n的值 (相当于是基址值) 加上 eax 的值 (相当于是变址值),\n基址加变址，这样两个寄存的内容加起来，这样就是一个有效地址\n\n\nTransform Instruction\n\n如下如所示是一个程序通过反汇编得到的指令，标红的是就是数据传送指令；实际执行时会从上到下依次执行\n\n\nprogram2instruction\n\n下面以上图中第一条指令即 55 这条指令介绍其执行过程，\n这条指令可大概分为 3\n个步骤，下面其描述过程，可配合后面的图进一步理解\n\n取指令，取指令的过程主要分为下面 3\n步（1）控制器把指令地址（即 80483d4）写入指令计数器即 PC，在 IA-32 中 PC\n为\nEIP，然后 MAR 把指令的地址送到地址线（2）时控制器会发出一个读命令，把控制信号送到控制线上面去，告诉存储器说，我要去读\n读这个单元的内容（3）存储器接收到读命令和地址以后，开始进行读\n读操作并把地址里面的内容读到数据线\n\n指令译码，数据线上的指令（即 5589e5）读过来，读到 MDR\n里面去以后，再把它送到 IR\n即指令寄存器里面去译码，而指令寄存器里面的高位，就是 op 字段，会送到控制器里面进行译码；\n译码以后，计算机就知道这条指令的功能是把\nesp (栈指针，用于指向栈的栈顶) 内容减 4 送到\nesp（即入栈，因为栈地址是从高到低增长的）; 然后把\nebp (帧指针，指向当前活动记录的底部) 的内容送到 esp\n所指向的那个内存单元\n\n指令执行，这一步执行的其实就是根据步骤 2 译码出来的过程，对 esp 减 4\n是在 ALU 中完成的（就是从 bfff0000 变为 beeefffc），然后把 ebp 的内容 (即\nbfff0020) 写到 beeefffc 中，原理也跟取指令的过程差不多，MAR 把地址\nbeeefffc 送到地址线上，同时把写的数据（即 ebp 的内容）写入 MDR 中， MDR\n通过数据线把内容传输给存储器；这样当控制器发一个写信号到存储器时，存储器会将数据线的内容写入到存储器中，最后则是将\nEIP 加 1，继续执行下一条指令\n\n下图是上面步骤 1~2 步骤的执行过程和结果\n\n\nstep1-2\n\n下图是上面步骤 3 步骤的执行过程和结果\n\n\nstep3\n\n定点运算指令\n常用的定点运算指令包括下面这些，可以有个大概的认识，这里就不详细展开讲了\n\n\nfixed num\n\n逻辑运算指令\n常用的逻辑运算指令包括下面这些，同样地可以有个大概的认识，不详细展开讲了\n\n\nlogic instruction\n\n条件转移指令\n指令的执行有两种顺序，一种是按顺序执行，还有一种是跳转到一个转移目标指令处执行，也称为条件转移指令\n条件转移指令也可以根据是否要满足条件来跳转分为以下几种\n\n\njump instruction\n\n\n 无条件转移指令，比如说 jump 指令就是无条件地转移到 DST 所指出的那个目标指令处执行\n\n条件转移指令，这个指令属于分支指令，可能是按顺序执行，也可能要跳转到转移目标指令处执行；所以在这个指令当中有一些是表示条件的\n助记符，如上面的 cc 就是一个条件码，是根据标志\n也就是条件标志来判断是否满足条件 如果满足条件就转移到目标指令 DST\n处执行，否则就按顺序执行\n\n条件设置指令， 跟上面的条件转移指令类似的\n在指令助记符当中有条件码，如果满足这个条件就把目标寄存器 DST\n里面置 1，否则是 0\n\n调用和返回指令，就是在函数调用或者过程调用的时候使用的指令，调用指令就是\ncall 指令 CALL，返回指令就是 return 指令\nRET。这两个指令会通过调用函数的地址关联起来，其实就是我们常听到的函数调用时入栈和出栈的操作\n\n中断指令它也是一种跳转指令，\n实际上是从用户态跳转到了内核态\n详细的信息在后面的章节中介绍\n\n在这些条件转移或者条件设置指令\n当中有一个非常重要的寄存器就是标志寄存器，如下图所示，这些标志信息\n实际上是用来进行判断条件是否满足的一些信息\n\n\nflag register\n\n在 C 语言里面规定，在一个表达式当中\n只要有一个变量是无符号数变量，unsigned 的，\n那么所有的运算都按无符号数进行运算。\n所以这边比较的结果按无符号数来比。\n小结\nIA-32 是典型的 CISC (复杂指令集计算机) 风格 ISA，前几章提到 ISA\n是一种规约，规定了软件如何使用硬件，包括指令的集合、寄存器的类型和数量、指令可接受的操作数类型\n而 IA-32 复杂的点在于：指令可能会很长\n有些字段有可无，操作码和指令是不定长\n且每个字段的含义还要有另外一个字段来解释\n其特点包括\n\n有 8 个通用的寄存器， 并且可以扩展，从 8 位扩展到 16 位，到 32 位\n\n有 2 个专用寄存器：EIP（PC）、标志寄存器（EFLAGS）\n\n有 6\n个段寄存器是间接的给出段址（存放的是一个指针，根据这个指针到另外一个地方去取，取出来的就是段基址）\n\n寻址空间是 4 个 GB (存储单元的地址是 32 位的)\n\n寻址方式有多种\n\n立即寻址\n\n寄存器寻址\n\n存储器寻址\n\n相对寻址\n\n\n指令和操作码是变长的\n\n同时介绍了 IA-32\n常用指令类型包括传送指令、运算指令、逻辑指令、条件转移指令等，并且介绍了一些执行结果不符合源程序的例子，其原因往往是类型的默认装换导致了最终生成的指令不同，这一点可以作为程序排查时的一个思路。\n","categories":["程序的表示、转换与链接"],"tags":["程序的表示、转换与链接"]},{"title":"维特比算法","url":"/2017/03/02/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/","content":"维特比算法（Viterbi\nalgorithm）是在一个用途非常广的算法，本科学通信的时候已经听过这个算法，最近在看\nHMM（Hidden\nMarkov model）\n的时候也看到了这个算法。于是决定研究一下这个算法的原理及其具体实现，如果了解动态规划的同学应该很容易了解维特比算法，因为维特比算法的核心就是动态规划。\n\n对于 HMM\n而言，其中一个重要的任务就是要找出最有可能产生其观测序列的隐含序列。一般来说，HMM 问题可由下面五个元素描述\n观测序列（observations）：实际观测到的现象序列隐含状态（states）：所有的可能的隐含状态初始概率（start_probability）：每个隐含状态的初始概率转移概率（transition_probability）：从一个隐含状态转移到另一个隐含状态的概率发射概率（emission_probability）：某种隐含状态产生某种观测现象的概率\n下面以维基百科上的具体例子来说明\n&gt; 想象一个乡村诊所。村民有着非常理想化的特性，要么健康要么发烧。他们只有问诊所的医生的才能知道是否发烧。\n聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。\n假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种 “健康” 和 “发烧”，但医生不能直接观察到，这意味着状态对他是 “隐含” 的。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。\n整个系统为一个隐马尔可夫模型 (HMM)。\n医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。\n换句话说，医生知道隐马尔可夫模型的参数。则这些上面提到的五个元素表示如下\nstates = ('Healthy', 'Fever')observations = ('normal', 'cold', 'dizzy')start_probability = {'Healthy': 0.6, 'Fever': 0.4}transition_probability = {   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},   'Fever' : {'Healthy': 0.4, 'Fever': 0.6},   }emission_probability = {   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},   }\n其对应的状态转移图如下所示\n\n\n状态转移图\n\n现在的问题是假设病人连续三天看医生，医生发现第一天他感觉正常，第二天感觉冷，第三天感觉头晕。\n于是医生产生了一个问题：怎样的健康状态序列最能够解释这些观察结果。维特比算法解答了这个问题。\n首先直观地看这个问题，在 HMM 中，一个观测现象后面的对应的各个状态都有一个概率值，我们只需要选择概率值最大的那个状态即可，但是这个概率值是跟前面一个状态有关的（马尔科夫假设），因此不能独立考虑每个观测现象。\n为了从时间复杂度方面进行比较，现在将问题一般化：假设观测序列的长度为\nm，隐含状态个数为\nn。则有下面的隐含状态转移图（下图为了便于表示，将只画出 n = 3\n的图）。\n\n\nViterbi 算法的状态图\n\n假如采用穷举法，穷举出所有可能的状态序列再比较他们的概率值，则时间复杂度是\n\\(O(n^m)\\),\n显然这样的时间复杂度是无法接受的，而通过维特比算法能把时间复杂度降到\n\\(O(m*n^2)\\)\n从动态规划的问题去考虑这个问题，根据上图的定义，记\nlast_state\n为上一个观测现象对应的各个隐含状态的概率，curr_state\n为现在的观测现象对应的各个隐含状态的概率。则求解 curr_state 实际上只依赖于 last_state。而他们的依赖关系可通过下面的\npython 代码表示出来\nfor cs in states:    curr_state[cs] = max(last_state[ls] *                          transition_probability[ls][cs] *                                      emission_probability[cs][observation]                          for ls in states) \n计算过程利用了转移概率 transition_probability 和发射概率\nemission_probability，选出那个最有可能产生当前状态\ncs 的上一状态 ls。\n除了上面的计算，同时要为每个隐含状态维护一个路径\npath， path[s] 表示到达状态 s\n前的最优状态序列。通过前面的计算选出那个最有可能产生当前状态\ncs 的上一状态 ls 后，往 path[cs]\n中插入 ls\n。则依照这种方法遍历完所有的观测序列后，只需要选择\ncurr_state 中概率值最大的那个 state\n作为最终的隐含状态，同时从 path 中取出 path[state]\n作为该最终隐含状态前面的状态序列。\n从上面的分析可知，观测序列只需要遍历一遍，时间复杂度为 \\(O(m)\\)，而每次要计算当前各个状态最可能的前一状态，时间复杂度为\n\\(O(n^2)\\), 因此总体的时间复杂度为 \\(O(m*n^2)\\).\n假如在 NLP 中应用\nHMM，则将词序列看做是观测到的现象，而词性、标签等信息看做是隐含状态，那么就可以通过维特比算法求解其隐含状态序列，而这也是\nHMM\n在分词，词性标注，命名实体识别中的应用。其关键往往是找出上面提到的初始概率（start_probability）、转移概率（transition_probability）、发射概率（emission_probability）。\n而在通信领域中，假如将收到的编码信息看作是观测序列，对应的解码信息为隐含状态，那么通过维特比算法也能够找出概率最大的解码信息。\n需要注意的是维特比算法适用于多步骤多选择的最优问题，类似于下面的网络，《数学之美》中将其叫做 “篱笆网络 (Lattice)”。每一步都有多个选择，并且保留了前面一步各个选择的最优解，通过回溯的方法找到最优选择路径。\n\n\n篱笆网络\n\n这里要强调的是 viterbi 算法可以用于解决 HMM\n问题，但是也可以用于解决其他符合上面描述的问题。\n最后，上文中的完整的代码见这里\n\n参考： 维特比算法\n","categories":["NLP"],"tags":["动态规划","NLP"]},{"title":"聊聊管理","url":"/2022/09/25/%E8%81%8A%E8%81%8A%E7%AE%A1%E7%90%86/","content":"管理，似乎是职场人无法回避的一个话题，因为随着组织的庞大，由于沟通、分工、人员参差不齐等问题，人效不可避免地下降，管理就是在缓解人员数量增加而带来的边际收益递减问题。而无论是管理者还是被管理者，笔者觉得都有必要去了解一下管理的相关内容，才能更好地扮演自己在职场中的角色\n在这个话题下，有着不同的派系和理论，每一种看起来都讳莫如深；但是回到管理的本质，就是如何高效地组织一群人去完成一个任务，并且是长期可持续的\n虽入职场时间不长，也有一些谈不上正式的管理经历，同时有幸从接触到的多位直属\nleader\n身上、从与一些前辈的谈话中，都获得了不少的启发和指导，自己也在这个问题上做了一些简单的调研，一直想对这部分做个总结，于是就有了这篇文章\n文章主要是笔者的一些经历以及对管理的简单认知，样本必然是有偏的，未必在所有场景下都适用，文章可能会比较发散，祝开卷有益\n\n身边的那些管理者\n在笔者当前有限的职业经历中，有几位给笔者留下较为深刻印象的的老板，下面会简单讲一讲他们的故事，以及笔者从中学到的一些东西\n能力是出众的，态度是谦和的\n一般从 IC (Individual Contributor)\n到管理者的角色的转变，都是做了一些比较成功的项目；而这也意味着管理者的业务能力一般是会比较强的，从笔者的经历来看，也的确如此\n在笔者接触到的大老板里，都有这样的特性：他们的能力是出众的，态度是谦和的（这里的能力更多的是指业务能力）\n在这一点上，Z 老板给笔者留下的印象最深，也是笔者尤为佩服的一点，Z\n老板是那种工作了好多年，但是仍然保留着对技术与业务的热情和好奇心的老板，就是那种如同刚毕业的年轻人对新鲜的职场的热情与好奇心\nZ\n老板是笔者毕业进厂时就认识的老板，当年刚毕业的时候觉得这种状态很正常，但是工作了几年后，似乎在经历着职场的\nBurnout (关于 Burnout，永不停止的列车)\n时，才意识到这一点的可贵之处，尤其是 Z\n老板的经历如同做过山车一般，即使在所谓的底部，也是依然保留着这种热情不变，respect~\n也许也是因为这一点，Z\n老板能够一直保持这对业务的敏感性，能够很好地判断出业务上各类事情的优先级，也能在各种讨论会中提出很多\nIC 也没有考虑到问题\n此外，笔者接触过的老板，对人的态度都是很谦和的，或者说是比较 “真” 的；这一点\nZ\n老板留下的印象也是尤为深刻，当你有问题或者说需要做出职场上的选择去找他时，他会站在你的角度，考虑你总体的利益，给你很多中肯的建议，哪怕这些建议对他来说并不是利益最大化的；换位思考的确是一种技巧，但是如果能够知行合一且长期地身体力行，在笔者看来就是一种难能可贵的 “真”\n这一点在 D 老板身上也是如此，我跟 D\n老板是两条线的，没有直接或间接的汇报关系，但是今年面临一些选择去找他时，他还是很客观地给我提供了非常多的有效信息和建议，这里也实名感谢匿名的\nD 老板，respect~\n而或许保持一种 “真”，在漫长的职业生涯中，才会有真正愿意长期追随你的人；因为员工也不傻，当你是真的站在他的利益角度上为他考虑问题，考虑如何培养他、为他争取应有的利益时，他也是会怀着感激之心愿意跟随着你去打仗的\n而假装强大，只会吸引那些来依附你的人，而不是能跟你一起打仗的人，这不是威信，这是一种负担。坦诚会让管理变得没那么累，对下属坦诚，也让下属对自己坦诚\n见证着 Z 老板经历过的起起落落，如果从功利的角度或者说业务的 scope\n大小，现在 Z 老板似乎在底部，但是笔者是很相信均值回归的，相信 Z\n老板肯定会回归到他应属 scope 那一天～\n边界需要清晰，才会有 owner\n意识\n一些厂常提到的不设边界，但同时又要有 owner\n意识的理念，似乎跟上面是很矛盾的，但两者解析的角度不太一样，不设边界这个理念笔者觉得对个人更适用：即个人不应该局限于本质工作，而是去多了解其他人、其他团队工作，更好地融会贯通，同时需要对自己手上的工作有\nowner 意识\n而笔者这里提的 “边界需要清晰，才会有 owner\n意识” 则是在团队角度去考虑的：即需要把团队工作清楚划分到具体的人上，才能让每个人都有\nowner 意识。这一点，笔者在过去一年里感悟尤为深刻\n这里不得不介绍 J 老板，J\n老板也是上面说的那种能力出众、态度谦和的老板，而令我印象深刻的是 J\n老板可以说是手把手教了我很多管理上的知识\n在 J\n老板正式接收我所在的小团队前，团队内的分工可以说是惨不忍睹，每个人同时参与着\n4、5 个项目，但是每个项目又没有明确的负责人，导致的后果就是每个方向的\nroadmap 都不明确、每个方向下关键的 action\n也没有厘清楚，人效很低，团队里的很多同学的士气也比较低落（后来跟 J\n老板的沟通中才发现，这是由于团队里较多初入职场的员工且缺少成就感）\n而 J\n老板过来后，做的第一件是就是把这个小团队的方向划分清楚成几个重点的业务方向，并且为每个方向指定一个较为资深的同学作为\nowner，每个 owner 下有几个新同学（PS,\n现在回头看，这个策略也许并不普适，比如说面对着都是资深的同学；但也许是当时的团队组织中最好的策略）\n作为其中一个方向的 owner，我也依葫芦画瓢，把这个方向的\nroadmap、重点的子方向梳理并划分清楚，分配到具体的新同学上，同时作为陪练的角色，指导每位同学去解决各个子方向上遇到的卡点问题等\n在这个过程中，遇到的另一个有意思的问题，就是在何种情况下需要 “get\nhands\ndirty”，一开始看到很多事情，笔者都会忍不住想上手去做，因为的确觉得短期内会让事情推进更快；但现在回看，这样的弊端也会很明显，一是下面的同学得不到锻炼，二是这其实是一个 “老黄牛” 类型的伪中层（这部分会在下面介绍），会把自己搞得很累，团队也得不到发展；而\nJ\n老板在这个过程中跟我说过、现在仍然印象深刻的一个观点是：要长期培养有战斗力的团队，需要让每个成员成长，让每个成员有成就感\n这个过程往往会存在一个事和人的权衡，或者说业务产出和培养成员的权衡，业务产出是能比较好去量化的，但是培养成员的产出就不太好去衡量。最理想的状态下把团队成员在进步，同时把业务完成得更好，但这往往需要更多时间与上级的耐心。在实际中，需要考虑的更实际的问题是：短期内需要保证业务产出到何种程度，长期又应该如何建设好团队，是一个以业务产出为约束，最大化团队长期发展的优化问题，这个问题没有标准答案，只能说是不同业务、发展规模、团队规模下可能的最优解都是不同的\n经过半年这样的安排，总体的业务收入增长了 1.5\n倍，同时每位同学都有相应产出，部分同学也在 one one\n反馈中与笔者透露过这种安排让他更明确了自己的任务，并在完成了这个项目的一些\nmilestone 后，有信心迎接更大的挑战了，而在这之前，这位同学差点就要被 pip\n了。这样的结果，印证了 J\n老板的方法的正确性，也从某个角度说明了管理对于挖掘个人潜能的重要性\n虽然跟着 J\n老板的时间并不长，但是从他身上学习到了很多，无论是管理还是技术，也是非常感谢\nJ 老板几乎是手把手教会了我这些管理的基本知识～\n明确团队定位，组建有凝聚力的团队\n业务的迅速发展不免会导致组织的频繁调整，而在这个过程也让我有机会看到了\nG 老板到一个新团队的 landing 的过程\nG 老板首先以项目维度，与 - 1、-2 组织了多次会议，来 review\n当前组里进行的绝大多数项目，同时判断其合理性，笔者觉得这个过程挺好的，首先让\nG 老板能同时了解团队已有的项目和人员情况\n而判断项目合理性的一个重要依据是当前团队定位，即我们当前团队做这个事情是否合理，如果这个事情已经有其他团队在做，我们参与到其中是否符合团队长期的定位，是否有一些事情是团队应该做但是没有做的？这种方式笔者理解是站在整个大团队的视角去考虑最优的解决问题的方法，需要较好的业务理解能力、扛得住一些短期的压力以及魄力；同时也让笔者第一次意识到，想象中的所谓组与组中间的 “卷”、“抢地盘” 等现象，其实并不会发生在所有的公司和团队里～respect\n通过这种方式，G 老板最终 close 了一些跟其他团队 overlap\n比较严重且定位不太符合团队的项目，同时通过与团队配合比较紧密的销售团队的沟通，也新立项了一些项目。重新调整后，组里做的事情，虽然看起来不是并不是那么高大上，但是更符合了团队的定位；虽有被调整的同学体感可能会有些不舒服，但如果\nYY 一下，站在 G 老板的 + 1\n的角度，相信对这次的调整应该也是满意的，毕竟站在 G 老板的 + 1\n并不会希望看到团队下的所有同学都去卷同一个方向比如说\nranking，而是需要尽可能保持各司其职，避免一些 “脏活累活” 没人干\nG 老板做的第二件事，就是组建组里的核心团队 (比较常见的就是 XX-core\n团队)，主要是 G 老板的 -1 以及一些核心项目里的 -2\n同学。令笔者印象比较深刻的有几件事，一是 G\n老板愿意主动授人以渔，会给他的 -1\n赠送过一些管理上的相关书籍，并且非常真诚地说了这本书对他的其他很大；二是还会跟\n-2 的一些同学定期\none-one，聊业务、团队发展、职业发展等问题（说实话这是笔者第一次跟 +2\n的老板的定期 one\none）；三是对组里的核心团队，会定期进行面试等培训，统一面试的标准（这个在招聘还是挺重要的，如果不对齐标准，面试里就很容易出现类似\nranking 中的召回与精排不一致等问题），同时亲力亲为组织核心团队的一些\ntb\n通过这一系列的动作，至少在笔者的体感上，团队是更有凝聚力了，因为对于比较大的团队，上面的老板肯定无法亲自去了解每个人、把控每个项目的方向；而\nG\n老板的做法就是在大面上明确了整个团队的定位，同时把一小部分的同学组建为核心团队，把一些理念传递给这些同学，让这些同学去把控各个子方向；避免了放养式的管理导致团队方向不明确，在兼顾了手下同学的体感，愿意放权；或许这就是管理所希望达到的目标吧\n因为业务调整等原因，与 G\n老板的汇报线只持续了大半年；但是在这大半年里，因为跟 G\n老板坐得比较近，从 G 老板身上看到了一个高阶的管理者的 landing\n过程，也看到了成熟的谈吐和处理问题的方式，更从与他的 one one\n中学习到了很多，回想这段经历，还是非常感谢 G 老板的\n坚持做长期正确的事情\n坚持做长期正确的事情，是一句政治正确、但在实际执行中往往会因为各种约束而变形的话\n在这点一上，给笔者留下比较深刻印象的是 H 老板；H\n老板不算非常高阶的管理者，但是还是较好地践行了这一点，且最终证明了其价值的；这一点需要很强的业务判断能力，以及能顶住压力来拍板的魄力\n之前 H\n老板接手的一个项目，老大不太看好，曾负责过的相关同学也不愿意继续去做，但\nH\n老板从业务角度出发，觉得这个方向还是有价值的，于是还是坚持投入了人力去做了这个事情，最终产品还是做起来了\n后来跟 H 老板沟通过程中，H\n老板告诉了我他坚持做这件事情的原因，或者说怎么去判断要做的这个产品是否有价值（主要是广告主侧的产品）；H\n老板给出的答案是能否真的能够给客户带来价值（非常官方但是正确的一句话），如提升了投放效率、或者提高了\nroi（即使短期的的消耗是掉的，但长期来看随着 roi\n的提升客户会增加预算，也正对应了他之前做的一个业务）等；而这样的结论了其实是真正把广告主当合作伙伴，并站在他们的角度去考虑他们的真实利益，才能有的一些思考\n另外，H\n老板给我留下的另一个比较印象深刻的点是敢拍板，也看过隔壁的某个团队老大在很多事情上都唯唯诺诺，一直不敢拍板或给结论，之前不理解，知道看到这个故事才懂：为什么不靠谱的管理者能稳如泰山,\n因为敢拍板意味着对自己的决定有充分的认知和信心，因为如果真的出了问题需要扛得住\n\n往上爬需要领导的信任和至少看起来好看的成绩；但是要想不掉下来，关键是不犯错、学会免责，让错误决策都归因不到自己头上去\n为什么有时候，我会遇到某些管理者，碰到重大决策时一定先开会。会上他把本该由他做出的决策丢给大家。这个时候态度很谦卑：“我也左右为难，需要大家建议。”\n然后不管你给他什么建议，他都说出好多问题。为的其实就是把这些问题留在会议纪要里。为的也是这个重大决策一旦失败，是 “大家一起做的，不是我一个人做的”，可以不用担责。\n这样带来的好处是他个人的，他就像个泥鳅，你觉得他有问题、想找到到底是什么问题时，发现他滑不留手。\n这样带来的问题是什么呢？如果 10 个人一起讨论，必须要所有人都达成一致才能决策。那最后，大概率决策的质量是这 10 个人中最蠢的人所能理解的决策的质量。为什么很多影响我们生活的决策，看起来总是让人忍不住吐槽。因为参与决策的人们需要免责。\n如果你在工作中，遇到这样的管理者 —— 越是重大决策，越要开会讨论，而且在会上总在唱反调，就是不做出自己的选择。\n就要小心了，这是个为了自保，不惜坑害业务和其他团队的人。\n\n当然，做长期正确的事情，哪怕短期有损失，这样肯定会有压力，这个时候往往需要跟老板的信任感，而信任感的建立，往往是在一起打过几次胜战\n打造好团队这个产品\n如果我们把团队也当做一个产品，那作为管理者需要考虑的问题，就是如何设计好这个产品。\n设计好这个产品，需要清楚产品演进的过程，需要产品的执剑人在清楚意识到自己在这个过程需要扮演的角色，以及需要执行的动作\n衡量这个产品的效果，则是整个团队是否能打，你的团队成员（用户）是否了解且认可这个产品的，当一个新的成员加入你的团队的时候，他是否清楚整个团队的职责，他负责的任务以及考核的目标等。\n从 IC 到管理者\n一般 IC 被提拔为管理者，都是因为其较为出众的业务能力，而从 IC\n到管理者的过程往往过程如下\n\n短期要为了结果服务，不能让结果掉链子，这个过程往往是\nIC 来主导，然后几个新人跟着来做\n\n中期慢慢的是新人做 IC 看、新人说 IC\n听，慢慢的要从一个尖兵般的特种部队的队长，变成一个真正是通过你的队员都变成合格的特种队员\n\n最后才是你彻底形成了一个管理者，更多的是理清一些事务制度、流程机制等等。然后你管的部分你减少了，因为他们都成熟了，然后也代表你变成一个成熟的管理者，能够\ncover 更多的人了。就再给你来 10\n个人，你这套东西你也知道有节奏有理念，给你时间你都能盯得起来\n\n而作为管理者，尤其是中后期阶段的管理者，需要确认自己比下属 “闲”：先让下属忙起来，确保业务正常 (图生存)；管理者要闲下来思考解决更大、更长远的问题 (谋发展)；因为作为一个球队的队长，最主要的职责不是进球，而是带领团队进球，需要分辨哪些是必须自己做的事\n套用一个比喻：如果你原来是一条鱼，成为管理者后你需要做的不是成为一条大鱼；而是要蜕变成鸟，得飞起来，否则，按照原来的方式，活多了会累死你。所以成为管理者，不是从做一件事，到多做一件事，而是完全换一种方式去做事；\n招人\n成为管理者，常常需要肩负起的任务是招人\n首先需要清楚，为什么要招人？一定是从业务出发，即业务现在因为要做哪些事情，所以需要有什么样的团队的成员来一起做\n既然招什么样的人，取决于这个公司做什么样的业务，那就不一定说有经验的比年轻的更好，有的时候也会有所谓的资源陷阱或者经验陷阱，这就涉及到了招聘时的年龄限制的问题\n虽然整个社会都有所谓的 35 岁焦虑或 35\n岁危机，但这里面最重要的是心态是否足够年轻，就是这个候选人是否有足够的热情和好奇心；只要是持续迭代，持续成长的人，年龄会显得不那么重要，当然前提是能力要匹配你的岗位\n另外，在团队要扩张之前，一定要想明白怎么用最小的资源去验证这件事情是不是正确的。人越多一定是信息的流通变慢，而且会有信息的扭曲。这就好比在产品设计之初通过\nmvp 版本去初步验证这个事情的收益和可能的局限\n除了招人，还要培养人（这里的人更多是指团队下属），需要记住一点是需要影响人，而不是控制人，管理者需要有成就他人的心态，即你不再是为自己负责，而是为整个团队，为团队里的每个同学，一定要打从心里希望看到自己的同学能够成长，能够晋升；因为管理是通过别人拿结果，需要记住功劳都是团队里实打实干活的同学的\n围绕着事而不是人\n管理者一定要知道：要围绕着事情，而不是围绕着人\n永远不要想 “他很资深，他是公司的老员工，所以我今天当他的 leader\n的话会发生什么问题”；也不要担心谁不服我，谁比我资深很多，我管不管得住他；总体就是不要戴有色眼镜去看自己，也不要戴有色眼镜去看别人\n这个时候要想的是，我们要完成一件事情的话，每个人的分工是什么，它的产出是什么；当这些都能讲得明白的时候，这个时候就有了标准，就能够去做客观的绩效的考核\n如何围绕着事情开展的方法论因人而异，笔者这里提供一个听到过的较好的说法：复杂的结果看过程，标准的过程看结果，短期的结果看策略，长期的发展看团队\n\n复杂的结果看过程：无法预测的结果，更多精力应该关注过程，过程的纠偏到位才有可能确保复杂的结果拿到\n\n标准的过程看结果：标准或者说比较常规的事情，基本可预测，主要看执行力\n\n短期的结果看策略：短期的事情往往取决策略是否得当，策略适合，拿到的结果可能效率更高、质量更好\n\n长期的发展看团队：组织、事业的长期发展，一定要依靠组织能力，或者说培养一个有战斗力的团队\n\n另外，当你要去 push\n一个人的时候，本质上就是大家对目标还有分工上面没有讨论清楚，这个时候要做的事情不是去\npush、而要拉回来再讨论清楚我们的目标是什么，我们的分工是什么，当然前提是这个员工基本还算靠谱，而不是真的赖着不干活的老白兔\n而从心态上，需要承认的事实是，一个良性发展的团队，肯定会有下属在特定的专业能力上比自己要强；而我们都是用人所长，整个公司范围内大家都是用人所长，所以不要想着什么事情自己都要做到最好，而是要想着这个人的这个能力，放在哪个岗位上能发挥出他最大的价值，当然识别长处这一点也很考验管理者的识人能力和业务能力\n做个好的翻译\n管理相当于不需要你亲自做很多很多事了，但是你得有办法让更多的人把这件事做得更好更大。从这个角度来看，管理就是在做翻译的工作，即把上级的任务翻译成需要下达的任务\n而管理者的老板或者公司压下来的那个东西，是不能不经你翻译直接再传递给员工；因为如果你的工作只做中转站，那你一定会发现你的工作是没有价值的，不要原封不动地传话，需要带上自己的思考和判断\n比如说一些事情如果传递下来，本身对于团队来说没有积极正向作用，可能你作为一个上级，你的价值就在于你需要把它过滤掉，就是不要往下再去传递\n\n信息传递要清晰\n\n有时候如果下达命令时如果对方 “不听”，或者说疑惑为什么员工会做得这么慢；先问自己是否说清楚，很多时候是因为你还是把自己的认知强加在员工身上，没把上下文或优先级同步到位，本质的问题还是信息的传递之间出了问题\n《新晋管理者的避坑指南》里的提到了很常见的一种现象：今天你要把一个工作交给你的团队成员的时候，你交代的够不够清楚他对这件事情的理解认知。有时候你问他懂不懂，你会发现\n99.99%\n的员工一定会回答，尤其是中国的员工一定会回答你。懂了，但是他下一步的动作你如果仔细观察之后，他会回去，然后问旁边的同学，那个老板交给我一个工作，我怎么做？他不敢去告诉你，他没有听懂。\n很多管理者需要学的一个技能是你怎么再三确认他对你交代给他的工作或任务理解到位，给下属安排任务时，不要只说 “要做什么”，还得说 “为什么要做”“什么时候做完”“要做到的结果”\n为了确保有效沟通，你还需要再做两个动作：\n\n告诉员工为什么要做这个事情，目标是什么，这比告诉他怎么做更重要\n\n信息来回确认三回，每次你讲完，最好让员工复述一遍。“你能复述一下你是怎么理解的吗”“你能再复述一遍吗”，“这个任务给到你了，你接下来会怎么做？” 不断修正信息，来回确认他想的跟你想的尽可能接近\n\n\n沟通\n\n管理中沟通的重要性不言而喻，一些沟通的技巧和方法\n（1）不要恶意假设对方，不要揣测\n当你对事情不理解的时候，带着好奇多问一句：“你是怎么想的？这么做你的考虑是什么？”\n（2）坦诚清晰\n说话方式有很多种，总体要坦诚清晰，因为不管是正向的还是负向的，对于员工来讲，他能够比较好的接受这个事情，而这个比较清晰的接收到了你的反馈，是好过于你支支吾吾啥都不讲的；但是也要注意表达，不让别人误解，曲解自己的意思\n（3）把批评变成动力\n高效的管理是激发一个人的善意，而不是一个人的阻抗；从提问开始转变，“你为什么总是\nXX” 到 “你怎么样才能避免 XX”\n（4）包容开放\n沟通是互动的，包容、开放地接受他的反馈，不要情绪一上来一激动，就给别人塞很多东西，但凡觉得他有个地方做的不好了，就非常激动的直接告诉他这个事情你做的不对，叭叭讲一大堆，然后不给他机会说话（能力强的\nIC 的这个问题越为为严重）\n\n跨部门合作的翻译\n\n除了上下级的翻译，跨部门的合作其实也是在翻译\n在上面的播客中，Zara\n提到的很有意思的一点是把我们想要实现的目标翻译成对方的目标，让人帮你的最高境界就是让他觉得你在帮他。所以如果要跟跨部门合作，Zara\n会先去看他的 OKR 然后看他的 OKR\n里有哪些是我这件事情可能能帮他实现的，然后就翻译成他的目标\n调动积极性\n虽然员工在工作中的积极性往往取决于其自身的目标、追求和欲望的强烈程度，但是一个管理者的重要任务之一也是激励和激发，激励员工的能动性，激发员工的创造力\n\n要激励而不是洗脑\n\n虽然说激励很重要，如果你这个做的太过了，就又会让人觉得在洗脑，怎么找到一个平衡？\n可能最重要的是要倾听下属的声音，然后针对下属的想法和诉求，尽量满足他的诉求，设置较好的激励；很多管理者一上来很容易变成纯单向的输出，就是给下属讲各种自己的愿景、价值观等等，但是忘了去倾听下属的声音，包括下属的真实想法，他实际的诉求等\n当然，这种双向的沟通需要信任：在还没有信任没有建立之前，一个员工跟你讲他心里的话是很困难的，也不要去做这样的预期；而建立信任则是一个比较长的过程了，可能是一块打过仗，或者管理者的办事作风、人格魅力等能打动员工\n\n耐心\n\n对于一些初入职场的新人员工，需要更多的耐心，让他们去慢慢地去成长。不能说一上来就以过高的标准去要求他，因为你要意识到其实你也是这么一路走过来的，不能说你做到今天，然后你要求他在一个很短时间内达到你那样的一个水准，这是不科学的，这也是不现实的\n要给他信心，告诉他做错了不要紧，做错了之后可以总结、复盘。这样的调整之后，你会发现他也就越做越有信心，而且也愿意去跟你交流；他的自信建立起来了之后，这件事情就越做越顺，越做越好\n同时要避免新人员工的结果不达预期就自己动手去做，还是要坚持让你的团队的同学去做事情，不要去插手把他推到一边说放着我来，这个会特别大的去影响到他的积极性，以及对他长远的跟你的配合会有很大的影响\n\n识人\n\n人不是千篇一律的，管理者千万不要幻想我用一套标准对所有的人都适用\n有些人就是以情绪为导向，就是你今天给了他正反馈，他就开心的像一只小鸟一样满屋子飞，然后就好好工作；有些人是不吃你那一套鼓励的，他不吃鸡汤的，他需要的是你要用逻辑征服我；有些人就是不愿意听你讲逻辑，他是那种行动派，就说不要叨叨了，站起来就去干吧\n而在公司的两大类业务（创新性业务和成熟业务）中，往往需要两类不同的人才\n（1）公司在面临寻找第二曲线，要去找一些探索型业务的时候，需要找一批很愿意探索创新东西，创造新的东西，同时韧性特别强，不怕失败的人；因为探索型业务就意味着叫胜率低赔率高，一旦做成了很棒，但是做不成是大概率，就是派出去的敢死小分队一样\n（2）对于成熟期的业务，需要另外一拨人：逻辑严谨，很少犯错，对风险很厌恶，对安全感有要求。这部分人他的整个思维是相对比较缜密，能保证你现在成熟期的业务少犯错，不出现短板，导致业务崩掉\n另外，还着重提到了一种人群:\n推土机，即专业能力是很强的，然后自信心也很强。正是因为他自信心和专业能力很强，就很自我；这种人适合单兵作战缺，管理上我们叫保护性和限制性使用，避免它影响现有团队的一些文化；因为能力越强，水准越高，其实破坏力也越大，比如说阿里也出现过的例子，两位科学家来了，一人带个上百人的团队，然后花多少亿吵两年什么结果没有，俩人互相不对付，一起走了留下烂摊子\n\n从员工的追求出发\n\n这部分本质上就是要换位思考，即从员工的角度去推进一些事情，这里有个很好的例子，\n从这个故事我们也能看到，管理者应该是一个通过 “帮助团队每个人实现个人目标”，从而实现管理者自己目标的角色。\n\n我为啥要指出他身上的问题呢？是因为希望他改掉问题。\n那他为啥会改掉问题呢？会因为我指出了他就改掉吗？肯定不会呀！他要改掉问题，只会是为他自己！\n于是，第二天，我试了一下用新的谈话策略，和一个小朋友聊问题。分成四步走：\n1、“XX，你未来希望自己在职业巅峰时在做什么呀？”\n他说：“我想当一个创业者！”\n2、“那你觉得，可以让你成为一个成功创业者的最大优势是啥？”\n他说：“我觉得我能坚持正确的事儿！只要一件事儿，我想明白了，就会坚决执行。”\n3、“我觉得，你离成功创业者此刻还有个差距，你看这回这个项目这里、这里都有考虑不全的地方。如果要想成功创业，你得比现在细致才行。”\n他说：“对哎，我是觉得我还应该更细致。”\n4、“不过我很认可你的优点。细致这个问题你已经意识到了，我相信你会发挥你的优点，把它解决掉的！”\n从此，我再向团队指出问题的时候，对方常常都很开心，也会在谈话后很快行动起来。团队的氛围也在快速改善。\n其实就是想清楚了一件事：\n每个人是不会仅仅为了别人的要求就改变自己的。能让他行动起来的，只会因为他自己的目标，尝试将需要他改变的地方跟他的目标联系起来；如果没有目标，那就是个混子，不适合呆在面对挑战的位置\n\n关于伪中层\n这里之所以提到这个话题，是因为听到这期播客 《汇报要彩排，遇事先甩锅，工作中遇到伪中层怎么办》特别有感触，同时在每个人职业生涯过程中不可避免会遇到这一类的伪中层，要思考的是如何识别这一类伪中层，更重要的是同时如何避免称为这一类伪中层\n这里主要摘自听了播客的一些笔记，也有一些个人思考\n\n中层应该具备的的三层能力\n\n（1）向上的战略理解能力\n（2）业务的精专，尤其是技术这一类岗位\n（3）管理能力：能不能发挥好手下员工的能动性\n\n三种类型的伪中层：传话筒型、欺上瞒下型、老黄牛型\n\n（1）传话筒：没有经过自己的翻译和拆解，只会机械地传话\n（2）欺上瞒下：只会汇报和甩锅\n（3）老黄牛：不懂授权，手下员工得不到发挥，没有成就感\n\n如何应对伪中层\n\n（1）高层视角：大老板走到一线 (包括访谈一线客户、与一线员工约谈、看一线的数据)，不接受单一的中层的二手信息；需要降低大老板获取一线信息的成本\n（2）中层视角：把信息 (问题和功劳) 尽可能快地传递到老板处，因为往往谁先说，谁就有先发优势，后面说的在大老板看来都是在狡辩；优雅地邀功；不要替老板做判断，客观地陈述数据的变化和具体过程，体现自己对业务的思考和洞察，老板自然会\nget 到这是你做出来的一个成绩\n（3）员工视角：老板是伪中层，如果能汇报且为你争取利益没问题，但往往伪中层都会把利益归到自己身上；与伪中层相处：1）不独食且有能力类型：要让伪中层觉得你是他的自己人，为他解决问题然后你分得相应的利益；2）独食或无能力类型，想办法干掉或者跑路（如果是新人，可能只能跑路，但工作时间长了，可以想办法干掉），干掉关键打破信息壁垒，但是要提防伪中层的 + 1，也是一个伪中层（反映时要关系到伪中层 + 1\n的实际利益，比如说可能会影响公司的股价或期权价格）\n\n怎么在面试识别伪中层（面试时反问的一些技巧）\n\n（1）公司目标如何制定、如何考核的，既当裁判又当运动员容易滋养伪中层【合理的范式应该是从上到下定目标，有独立的裁判】\n（2）高层是否会有 all hands 或者 -1、-2 的 one one\n（3）反问面试官做过的最有意义的或者印象深刻成果，如果是真实参与的会滔滔不绝跟你说\n（4）问面试官当前团队的员工有哪几个来源；伪中层一般会带着自己的嫡系，形成一个稳固的小圈子，这种情况小心被招进去当做背低绩效的替罪羊【合理的范式应该是老板手下有多个来源，存在有能力的老员工，能够信服这个老板】\n最后，着重讲一些老黄牛类型的中层的问题，因为笔者也曾迈入过相似的误区；当你不懂得授权的，严格地控制所有的动作，最后会发生几个问题，第一个是他自己会越来越累，他整个人会变成团队的瓶颈；第二个是团队的所有人会越来越失去主动性，大家没有什么发挥的空间，最后主动性会流失。第三个就是团队的所有同学也会失去成就感，越优秀的人走得越早，然后越早会离开他。最后只有那些就是希望自己有一个安稳工作，不要承担责任的小白兔才会留下来。\n套用一句比较形象的话就是救火容易救出成就感，每天无数人找管理者救火，管理者会觉得自己好像是英雄、整个公司非自己不可；\n这也是人的弱点。下面的例子也许不适用所有的业务，但是做法值得参考，主要想表达的观点是成功的管理者一定是能将紧急的事情借机制分担，保证自己最主要精力得以分配在重要事务上的人 ,\n\n每天我的团队要找我很多次 —— 很多时候是看方案，很多方案，我得看一眼，团队才能往下推进。有时候是出了问题，得赶快查问题。还有的时候，是推进遇到了阻力，得我帮团队找到其他部门，把资源谈下来\n我也要找团队很多次。有很多事儿，我心里挂着，是真不放心。时不时想起来就找团队问一嘴：做了没有？效果如何？\n但是，这些事儿，就不会发生在他与团队之间。\n1、\n他在自己的团队内，搞了个小规模的数据分析的团队。\n不过他的数据分析，可不是仅仅支持 AB 测试。他让这个团队，量化了自己的评价标准。\n于是，再有团队拿着方案，找他看行不行的时候，他都说：\n你去找那个数据分析的负责人，他掌握着我的评价标准，他帮你测算一下，他说行，那就行。于是，团队需要找他过的方案，大大减少。\n2、\n他还搞了个项目经理。\n团队有遇到资源问题，自己搞不定？也先不用找他。先找项目经理！\n公司有哪些资源、怎么协调，项目经理门儿清。\n真遇到项目经理搞不定，需要他说句话的。\n项目经理会帮他把微信说啥编好，他一键转发。\n于是，团队需要找他协调资源的情况，也基本消失。\n3、\n项目经理还负责催进度。凡是他在意的事儿，他都让项目经理管理起来，周期性催活儿。于是，他需要找团队问东问西的情况也大大减少。\n那什么事儿他必须亲自干呢 —— 团队在方法上遇到了困难，实在找不到办法，他和团队一起讨论。\n他花了大量时间面试和接触更好的候选人，他还会听大量的用户访谈、每天自己看很多数据。\n相比之下，我们大多数管理者，每天都被不得不做的紧急事务催得团团转。紧急的事儿消耗精力，但是重要的事儿才会产出关键业绩。\n\n文化与战略\n文化与战略，似乎是一个很大又很虚的词，但是在听了《十点读书林少：一线员工可以为公司战略做些什么？》\n后，发现从高层视角去审视管理这件事情，文化和战略对一个公司的长期发展也是不可缺少的手段\n\n关于文化\n\n随着企业变大、人变多后，大家认知体系必然会不同频，必须要有一个同频的目标去做决策的一个标准，这个时候就凸显了企业文化的重要性，而文化就是帮战略、帮组织纠偏，它是很好的一个纠偏剂；套用林少的话是这么说的\n\n首先文化确实蛮虚的，但我个人觉得其实从一家公司的底层来看，文化它是一个底座，它可能没有那么显性，但是它对这个公司的长远的一个指导是有非常重要的一个作用的。\n战略、组织、文化这三件事情其实是一个公司成功的比较重要的三个要素。那战略这个事情咱们比如说每年甚至每个季度都要开战略会，为什么？因为市场在变化，所以你的战略可能是会变化的，而且很有可能你的战略是会制定错误，可能会跑偏。组织其实它也是跟着战略变化的。那有可能我们这段时间我们这个战略出现了偏差，那我们大的这个组织它也会有一些变化，那组织里面也会有不同的同学加入。那可能有些同学非常合适，但有些同学他可能就不合适，可能也会又走偏的时候。\n那什么是文化呢？我觉得其实文化就是帮战略纠偏，帮组织纠偏，它是很好的一个纠偏剂。就从长远的发展来看，当我们有一些公司发展碰到瓶颈，公司走了弯路，那这个时候我们靠什么？就靠你的文化里面的使命，还是回到原点，到底我们做这件事情意义是什么？然后到底我们长远我们要去向何方？然后我们从起点走向愿景的这条过程中，我们的行为准则是怎样的？我们是跑着去，走着去，还是抄着小路去？还是说我们可能通过一些不常规的道路去？我觉得这些都是文化可能在起到指导的作用。\n我们企业文化其实比较核心的是三个部分，第一个是使命，第二个愿景，第三个价值观\n\n\n关于战略\n\n战略应该是简单的，应该是你一说出来大家觉得他是容易理解的，跟公司的使命是极度契合的，然后跟我们自己的能力也是非常的契合的；战略不太需要多条，也不太需要说你去经常换方向，反而是你围绕着自己主赛道，然后好好的去进行战略的一些比较简单的升级\n战略的制定往往分不同的阶段，早期更多就是靠直觉拍脑袋，就是老板看到有一个机会，还是一个无人地带；直觉这里会有机会，所以自己就往前冲了，可能大部分会失败，但少数会成功。战略直觉是挺重要的，就是一个\nCEO\n他每天都在接触这个公司的所有的信息，然后他也在跟很多外部的一些市场去打交道，就是这每天的信息其实是会让\nCEO 去产生一些战略直觉的。有些 CEO\n其实是有战略直觉天赋的，会是一家公司一个 CEO\n非常核心的竞争力。但仅有这个不够，因为即便如此，一个人依然是有盲区的。那这个时候就需要多方的视角，需要有高管团队帮忙来从不同的维度去做战略的复盘，做战略的探寻，结合外部的顾问，结合\nCEO\n的直觉，再结合整个管理团队的多维度的战略的探寻，是会制定出相对比较有效的战略方向。\n而战略直觉依赖的的信息来源，播客里提到了三个（1）跟更多的同事去交流，不仅仅高管，跟更多的中层甚至是基层的员工去交流，一线的员工其实是离炮火最近的；可以听听他们的想法，他们对公司的看法，然后听听他们对市场的看法，他们对用户的看法（2）每天要有大量的输入，这些输入可以是来自于你所关注的一些账号，有公众号，有抖音账号（3）保持足够频率的跟外部的同行交流，他们可以是你的密友，也可以是你新认识的一些创业者，也可以是你的投资人，从他们这边也可以获取到非常高质量的一些信息跟建议\n有了战略信息，如何向员工传递？需要靠机制一层层落实；比如说每个季度制定一个战略\nOKR 分享会，向全员表达（类似字节的 all\nhands）；把战略贴在会议室墙上，每个人都很聪明，当他们的信息足够，当公司的信息差越来越少之后，那这个时候其实战略的理解、推行他就会相对比较容易得多。很多时候战略落地不了，可能是信息的传达不够\n\n管理需要工具化\n\n除了文化和战略这些看起来比较大而虚的内容，播客还提到比较实在的一点，就是管理需要工具化\n管理很多其实是非标的，因为每个人的管理方式不太一样，每个人他自己的管理风格也不太一样，他的性格也不太一样。那这个时候，一个新员工加入试点，他怎么更好地去理解我们的沟通方式、协调方式跟会议方式？\n如果能够将非标的管理工具化，通过更多的文档、通过 OKR、通过绩效把它工具化。那这个时说一个新员工进入公司，他通过飞书等工具，他能够接触到的这些资料他就能够很快地\nget\n到这家公司是怎么通过文档交流的，开会的互动的，然后通过周报到底是怎么去协调的\n小结\n管理这个词看起来似乎高深莫测，整篇文章看下来，的确也有很多的技巧和方法论，但回归到本质，还是如何通过高效的方式组织一群人去完成任务，并且是长期可持续的\n正如没有一种通用的技术能适用于所有的业务，也没有一种通用的管理方式使用所有的团队，因为这跟业务类型、业务发展阶段、团队规模、团队成员的特点都有关；而回到最开头说的像打造一个产品一样打造一个团队，我们要做的就是随着业务等发展，不断地调整团队的这个产品的形态\n往往我们会被传达这样的一个理念：当上了管理者似乎就能一劳永逸，避免 35\n岁危机，虽没实证过，但相信也不会是真的，只是一个妄念。而随着业务的发展不断调整、进化自己，以更年轻和开放的心态去迎接当前的挑战，有管理工作委任时能够扛得起，没有也能做个能打的\nIC，可能才是 “一劳永逸” 的答案；毕竟真正当上管理者的人是少的，而当上管理者可能也需要在合适的时机踩中合适的机会，有运气与努力成分；好好生活，快乐工作，才是每个普通人应该去努力追求的，与君共勉。\n\n\n十点读书林少：一线员工可以为公司战略做些什么？\n\n新晋管理者的避坑指南\n\n做一个受欢迎的管理者\n\n如何管理比自己年龄大的人\n\n汇报要彩排，遇事先甩锅，工作中遇到伪中层怎么办\n\n为什么晋升总是轮不到你？可能是因为忽视了这项能力\n\n职场晋升 101\n\n","categories":["闲话几句"],"tags":["闲话几句","管理"]},{"title":"计算广告笔记 (1)-- 广告的基本知识","url":"/2017/04/20/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(1)--%E5%B9%BF%E5%91%8A%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/","content":"本系列文章是刘鹏老师的计算广告学中的一些记录。本文是第一章的相关笔记：广告的基本知识，主要介绍广告的定义与模型，目前在线广告的特点、技术架构与市场形态，可以说简要地概括了整个课程的内容。\n\n什么是广告？\n广告有商业上的诉求和目的，并不是简单的技术的堆砌。\n广告的定义\n\n广告是由已确定的出资人通过各种媒介进行的有关产品 (商品、服务和观点) 的，通常是有偿的、有组织的、综合的、劝服性的非人员的信息传播活动。\n\n重点：\n广告的主体：出资人 (sponsor) 即广告主 (advertiser)，媒介 (medium)，受众 (audience)\n广告的本质功能：是借助某种有广泛受众的媒介的力量，完成较低成本的用户接触 (reach)\n品牌广告 (Brand\nAwareness) 与效果广告 (Direct Response)\n品牌广告：创造独特良好的品牌或产品形象，\n目的在于提升较长时期内的离线转化率\n\n\n品牌广告\n\n效果广告：有短期内明确用户转化行为诉求的广告。用户转化行为例如：购买，\n注册，投票，捐款等。大部分互联网广告都是这种类型。\n\n\n效果广告\n\n广告的有效性模型\n三个大阶段，同时可分为 6 个小阶段\n\n\n广告有效性模型\n\n这里值得注意的几点有\n1. 广告的天然属性（如广告的位置）很重要，远远强于技术带来的效果\n2. 引起用户关注时需要遵循一定的原则：以上列出了三点\n3. 解释阶段包括用户对广告的理解和认可两方面；理解阶段\n4. 保持（retention）主要指品牌在用户中树立起其形象，但是点击率往往不会高\n下面一些广告策略的效果，这些广告策略都是有利有弊的，下面的\n+ 表示对该项有正面效果， -\n表示对该项有负面效果。\n\n\n广告策略的效果\n\n在线广告\n在线广告是与传统的线下广告对比而言的，下面主要介绍在线广告的特点、目前的市场、核心计算问题。\n在线广告的特点\n当前的在线广告行业的有以下特点\n1）\n技术和计算导向。原因是数字媒体的特点使在线广告可以进行精细的受众定向，而技术使得广告决策和交易朝着计算驱动的方向发展\n2）\n可衡量性。指的是可以用量化的方式来衡量广告的效果，广告的点击是效果的直接收集途径\n3）\n标准化。指的是有行业制定了相关的规则来指导广告市场。如下是一些美国广告行业相关的机构。\n\n\niab\n\n\nInteractive Advertising Bureau\n\n 在线广告供给方的行业协会，推动数字化市场营销行业的发展\n制定市场效果衡量标准和在线广告创意的标准\n会员: Google, Yahoo, Microsoft, Facebook 等\n\n\n\n\n 4a\n\n\nAmerican Association of Advertising Agencies\n\n 主要的协议是关于广告代理费用的收取约定 (17.65%)，以避免恶意竞争\n主要集中在创意和客户服务，在线业务是一部分\n会员：Ogilvy &amp; Mather, JWT,\nMcCann 等，Dentsu 等非 4A 会员的大公司但也被列为 4A 公司\n\n\n\n\n ana\n\n\nAssociation of National Advertisers\n\n 主要代表广告需求方的利益 (也有媒体和代理会员)\n 会员：AT&amp;T, P&amp;G, NBA 等\n\n\n在线广告市场\n在线广告市场发展情况如下所示，主要分为了三大部分：需求方，供给方和连接两者的平台，需求方指的是需要投放广告的广告主，供给方指的是提供广告位的媒体。\n\n\n在线广告市场\n\n从媒体也就是 supply 端来看，其变现的手段有以下三种\n\n\n在线广告变现手段\n\n方式一：将广告位托管给广告网络 (Ad net1)\n方式二：将广告位对接到广告交易平台 (Adx), 以实时竞价的方式变现\n方式三：将广告位托管给 SSP（supply side platform),\n通过 SSP 可以对接多个广告网络和 DSP，按照动态分配的逻辑选择变现最高的需求方。\n上面简单的说明了媒体方通过广告位进行变现的方式，涉及到多个概念，可以参考刘鹏的《计算广告》中的第六章内容。\n在线广告核心计算问题\n在线广告的核心计算问题是 ROI（Return On\nInvestment，投资回收率）其定义如下\n\nFind the best match between a given user u, in a given context c, and\na suitable ad a.\\[\\begin{align}\\max\\sum_{i=1}^{T} ROI(a_i, u_i,\nc_i)\\]\\begin{align}\n\n上面的 \\(T\\) 表示广告共展示 \\(T\\) 次，ROI 主要有两部分构成:\nInvestment 和\nReturn，一般来说主要优化的目的在于 Return，\n其计算公式如下\n\\[\\begin{align} Return = \\sum_{i=1}^{T}\n\\mu(a_i, u_i,c_i)v(a_i,u_i) = \\sum_{i=1}^{T} e(a_i, u_i,c_i)\n\\end{align}\\]\n其中\n\\(\\mu(a_i, u_i,c_i)\\) 表示点击率\n\\(a_i, u_i,c_i\\)\n分别表示广告，用户和广告上下文 \\(v(a_i,u_i)\\) 表示点击价值 \\(e(a_i, u_i,c_i)\\) 表示 eCPM（expected\nCPM，预期每次展示能够带来的价值）\n而对上式的不同的分解对应不同的市场形态：\n\nCPM(Cost per\nmille) 市场:\n按照千次展示结算。是需求方与供应方约定好千次展示的计费。在这种方式下，点击率和点击价值都需要需求方预估。\nCPC(Cost per\nclick) 市场:\n按照点击结算，最早产生于搜索广告。在这种方式下，点击率估计交给供给方，点击价值的估计交给需求方，并通过点击出价的方式向市场通知自己的估价。\nCPA(Cost per\naction)/CPS(Cost per\nSale)/ROI 市场:\n按照转化行为数、销售订单数和投入产出比来结算。这三个都是按照转化付费的一些变种。在这种方式下，点击率和点击价值都需要供给方预估。\n\nCPA/CPS/ROI 市场中需要注意广告主可能会有作弊行为：\n如隐瞒订单，卖高价物品（品牌得到了展示，但是转化率低，不用向平台付费）\n优化 ROI\n的问题可从以下两个角度来考虑，每个角度都有其重点关注的点，下面简单列出\n从优化角度来看，主要的关注点在于\n\n特征提取：受众定向\n微观优化：CTR 预测\n宏观优化：竞价市场机制\n受限优化：在线分配\n强化学习：探索与利用\n个性化重定向：推荐技术\n\n从系统角度来看，主要的关注点在于\n\n候选查询：实时索引\n特征存储：No-sql 技术\n离线学习：Hadoop\n 在线学习：流计算\n交易市场：实时竞价\n\n在线广告计算的主要挑战有\n\n大规模\n(Scale)：百万量级的页面，十亿量级的用户，需要被分析处理；\n高并发在线投放系统 (例: Rightmedia 每天处理百亿次广告交易); Latency\n的严格要求 (例: ad exchange 要求竞价在 100ms 内返回)\n 动态性\n(Dynamics)：用户的关注和购物兴趣非常快速地变化\n丰富的查询信息 (Rich query)：\n需要把用户和上下文中多样的信号一起用于检索广告候选\n探索与发现 (Explore &amp;\nexploit)：用户反馈数据局限于在以往投放中出现过的 \\((a, u, c)\\)\n组合，需要主动探索未观察到的领域，以提高模型正确性\n\n搜索、广告与推荐\n搜索，广告和推荐可以说是联系紧密同时又有各自特点的三个领域。\n\n\n比较\n\n比起搜索，广告不需要爬虫，索引数也比较少。\n推荐不等于个性化，个性化是推荐的一个准则，其他准则还包括新鲜性，多样性等。\n广告与推荐系统：文字广告点击率高于图片广告点击率，但是推荐系统刚好相反\n推荐与广告的一个重要区别在于 Downstream\n优化，推荐出来的物品还可顺带其他的推荐物品，优化的目的是一系列用户可能会点击的物品；而广告的推送只是要优化用户对这个广告的点击率。\n在线广告系统结构\n下面的在线广告系统结构图，需要注意这并非实际设计图，只是概念性的结构图\n\n\n在线广告系统\n\n从上图可以看到，整个系统可以分为四大部分\n\n高并发的投送系统 (Ad Server)：在线部分，根据\n\\(u,c\\) 决定出 \\(a\\), 特点是高并发\n受众定向平台：离线部分，分布式机器学习，用于预估点击率等信息，常用的是\nHadoop 平台\n数据高速公路：收集线上日志文件等供其他部分使用\n流式计算平台：重点在于实时性，比 Hadoop\n要快，包括反作弊，计价，实时索引 (广告的加入和删除) 等任务\n\n将上面的架构图各部分做更细致的划分时，可以得到如下的划分图\n\n\n在线广告系统更细致的分类\n\n\n Ad Serveing: 主要指 Ad Server\n接受两种请求，一种来源于用户（USer），另外一种是广告交易市场发过来的（RTBS）\nAd retrival：找出与页面和用户相关的广告\n Ad ranking：有多个广告满足要求时，根据某种指标 (如 eCPM)\n来排序，选出最符合要求的广告\n Streaming Computing：流式计算平台\n Data highway：把线上数据传到 Hadoop 平台或流式计算平台\n Session log generation:\n搜集用户的浏览、搜索的行为整理成一份标准日志，提供给其他的系统\n Customized audience\nsegmentation：受众的定制化，不由平台固定受众分类，而是由广告主选择具体的受众类型，因为业务的需求是各式各样的\n Page attribute system：爬取有广告展示的页面，用于广告的\nretrieval\nAudience\ntargeting：受众定向，根据用户及其浏览的上下文决定出推送哪个广告\n Ad management system: 供广告主投放广告的平台\n\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算广告笔记 (2)-- 合约广告系统","url":"/2017/04/25/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(2)--%E5%90%88%E7%BA%A6%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/","content":"本系列文章是刘鹏老师的计算广告学中的一些记录。本文是第二章：合约广告系统，主要介绍了广告系统中一些常用的开源工具，合约广告系统的概念以及在线分配问题的建模与求解。\n\n常用广告系统开源工具\n广告系统中常用的工具有以下这些\n\n\n常用广告系统开源工具\n\n总体可分为两类工具：离线与在线\n离线\n\nHBASE（列存储的 NOSQL 数据库，类似的有 BigTable，HyperTable，Cassandra）\nPig: 一种脚本语言\n Elephant-bird：将二进制文件转为 pig 可处理的文本文件\n Hive: 一种脚本语言\n mahout: 分布式机器学习\n\n在线\n\nZooKeeper/Chubby: 分布式环境下解决一致性问题\n Avro/Thrift: 分布式环境下跨语言的通信问题\n S4/Storm: 流式计算平台\n Chuhwa/Scribe/Flume: data highway,\n分布式日志收集工具，并送到其他平台\n\n合约广告系统\n合约广告有两个重要组成部分：广告位合约和展示量合约。\n广告位合约：实际上是将线下的模式搬到了线上，指的是广告主与媒体约定在某一时间段内，在某些广告位上固定投放该广告主的广告，相应的结算方式是\nCPT (Cost Per\nTime)（也就是按照展示时间结算），这种模式下售卖给广告主的是广告位。\n展示量合约：展示量合约广告指的是约定某种受众条件下的展示量，然后按照事先约定好的单位展示量价格来结算，这种结算方式是\nCPM (Cost Per\nMille)（也就是每一千次展示的付费）。这种方式也叫担保式投放 (Guarantee\nDelivery,\nGD)，意思就是先广告主担保其提出的广告展示量会被满足。这种模式下售卖给广告主的是广告位 + 人群。\n广告位合约\n广告位合约对供给方和需求方的技术的要求都不高。\n供给方即媒体往往会使用一种在合同确定以后自动的执行合同的广告管理工具：广告排期系统。广告排期系统能够帮助媒体自动执行多个合同的排期，可以将广告素材直接插入页面，且对于图片等静态资源，会放到\nCDN 上进行加速。\n需求方即广告主往往会通过代理商（agency）进行媒介采买（也就是广告位采买），代理商帮助广告主策划和执行排期，而对于广告的质和量，是根据代理公司人员对媒体广告位的历史经验以及对广告主业务的了解通过人工优化的方式来满足。这样的代理公司的代表有我们前面一讲提到的\n4A 公司。\n展示量合约\n媒体从前面的广告位售卖变为按 CPM\n的售卖，初衷是为了在受众定向的基础上提高单位流量的变现能力，可是面向的任然是原来的品牌广告主。广告主按广告位采买时，比较容易预估到自己能够拿到的流量，可是按照人群定向的方式采买，流量有诸多不确定的因素，因此，需求方希望在合约中加入对量的保证，才能放心购买。而假如约定的量未完成，则需要向广告商补偿。\n因此，这种方式卖的不仅仅是广告位，而是人群。\n后面半句我们很容易理解，而前面半句话的指的是在 CPM\n这种结算方式下，任然没有摆脱广告位这一标的物，原因是无法将多个差别很大的广告位打包成统一售卖标的（这样的话每个广告主都会抢着要那些曝光率更高的广告位）；因此实际中的展示量合约往往是以一些曝光量很大的广告位为基础，再切分人群售卖，最典型的例子就是视频网站的铁片位置或者门户网站首页的广告位。\n展示量合约这种模式的出现实际上已经反映了互联网广告计算驱动的本质：分析得到用户和上下文的属，并且由服务器端根据这些属性以及广告库情况动态决定广告候选。这一商业模式的出现，需要有一系列技术手段的支持，这些技术手段主要包括受众定向、流量预测、在线分配等。下面主要介绍流量预测和在线分配，受众定向会在下一讲中详细讲解。\n流量预测\n流量预测 (traffic forecasting)\n简单来说就是预测某个标签的人群访问某个站点的量。流量预测其目的有多种，典型的有售前指导，在线流量分配，出价指导，前面两个是合约广告中的内容，而后面一个是竞价广告中的内容。\n1）售前指导指的是在展示合约广告系统中，由于要约定曝光总数，事先尽可能准确地预测个人群标签的流量非常重要。因为如果流量低估，会出现资源售卖量不足的情形；而如果流量严重高估，则会出现一部分合约不能达成的状况。\n2）在线流量分配指的是在展示量合约广告系统中，由于合约之间在人群的选择上会有很多的交集，因此一次的曝光往往会满足多个合约的要求，这时候就需要在多个合约之间进行分配，目的是达到整体满足所有合约的目的。这也是下面要详细探讨的在线流量分配的问题。\n3）出价指导是竞价广告中的内容，在竞价广告中，没有了量的保证，广告主往往需要根据自己预计的出价先了解一下可能获得多少流量，以判断自己的出价是否合理。与前面在合约广告中的应用不同，这里还多了出价这一因素。\n综上，广告里一般的流量预测问题，可以描述为对流量 \\(t(u,b)\\) 这个函数的估计，其中 \\(u\\) 是给定的人群标签或这些标签的组合，而\n\\(b\\)\n是具体的出价。在展示量合约中，由于没有竞价，可以看成是 \\(b \\rightarrow \\infty\\) 的特例。\n在线分配\n在合约广告系统中主要讨论在线分配（Online\nAllocation）问题，在线分配问题指的是在通过对每一次广告展示进行实时在线决策，从而达到在满足某些量的约束的前提下，优化广告产品整体收益的过程。\n在线分配是广告中比较关键的算法框架之一，适用于许多量约束下的效果优化问题，而这实际上是广告业务非常本质的需求。\n问题建模\n在线匹配可看作是一个二部图匹配问题，二部指的是代表广告库存的供给节点（集合记为\n\\(I\\)）和代表广告合约的需求节点（集合记为\n\\(A\\)）。如下图所示，上边为集合 \\(A\\), 下边为集合 \\(I\\),\n如果供给节点的受众标签能够满足某个需求节点的要求时，就在相应的两个节点之间建立一条连接边，所有边的连接记为集合\n\\(E\\)。\n\n\n在线分配\n\n在线分配技术并不仅仅适用于展示量合约中的担保投放（GD）问题，还适用于\nAdWords 问题，展示广告问题等。下面主要介绍 GD 问题和 AdWords 问题。\nGD 问题\n前面已经提到了 GD 的概念，在这里如果不考虑合约 \\(a\\)\n未完成的惩罚，收益一定是常数。那么 GD 的优化问题可以表示为\n\\[\\begin{align}\n&amp;\\max \\quad C\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t. &amp;\\sum_{a \\in \\Gamma(i)} x_{ia} \\le 1 &amp;\\forall i \\in I\\\\\\\n&amp;\\sum_{i \\in \\Gamma(a)} s_i x_{ia} \\ge d_a &amp;\\forall a \\in A\\\\\\\n&amp;x_{ia} \\ge 0 &amp;\\forall (i,a) \\in E\n\\end{array}\n\\end{align}\\]\n上面各式的符号含义如下\n\\(C\\) 是一个常数，指的是总收益 \\(I\\)、\\(A\\)、\\(E\\)\n上面已经提到，分别表示供给点集合，需求点集合，边的集合 \\(\\Gamma(i)\\) 表示与供给节点 \\(i\\) 连接的所有需求节点的集合 \\(x_{i,a}\\) 表示供给节点 \\(i\\) 分配给需求节点 \\(a\\) 的流量的比例 \\(\\Gamma(a)\\) 表示所有与需求节点 \\(a\\) 连接的供给节点 \\(i\\) 的集合 \\(s_i\\) 表示供给节点 \\(i\\) 的总流量 \\(d_a\\) 表示需求节点 \\(a\\) 的展示量需求\nAdwords 问题\nAdWorks\n问题，也被称为有预算约束的出价问题，是竞价广告领域内的问题。简单来说，这个问题讨论的是在按照\nCPC\n方式结算的广告环境下，给定广告主的预算，整体化市场营收问题。需要注意的是，竞价广告中已经没有了量的约束，广告主给的约束是其预算费用。因此可以将这个问题表示为如下的优化问题\n\\[\\begin{align}\n&amp;\\max_{(i,a) \\in E} \\quad q_{ia} s_i x_{ia}\\\\\\\n&amp;\\begin{array}\\\\\\\ns.t. &amp;\\sum_{a \\in \\Gamma(i)} x_{ia} \\le 1  &amp; \\forall i \\in I\\\\\\\n&amp;\\sum_{i \\in \\Gamma(a)} q_{ia}s_i x_{ia} \\le d_a &amp; \\forall a \\in\nA\\\\\\\n&amp;x_{i,a} \\ge 0 &amp;\\forall (i,a) \\in E\n\\end{array}\n\\end{align}\\]\n上式大部分的符号跟 GD 问题相同，不同的地方主要是以下两个符号 \\(q_{ia}\\) 表示需求节点 (广告主) \\(a\\) 对供给节点（某个人群标签） \\(i\\) 的出价 \\(d_a\\) 表示则表示广告主 \\(a\\) 的总预算\n问题求解\n上面的两个最优化问题均是线性规划问题，未知量是 \\(s_i\\)（供给节点 \\(i\\) 的流量） 和 \\(x_{ia}\\)（供给节点 \\(i\\) 分配给需求节点 \\(a\\) 的流量的比例）。但是对于 \\(s_i\\)\n，常常利用历史流量去估计它的值，因此上面的优化问题变成了仅仅需要求解\n\\(x_{ia}\\)\n的问题。下面解决这个问题的几种思路\n直接求解\n对于这类线性规划问题，可以通过内点法或单纯形法直接进行求解，但是在大型的广告合约系统中，供给节点和需求节点的数目都很大，因此边\n\\(|E|\\)\n的数目也会非常大（百万级以上), 这样会使得对应的分配问题变得过于复杂而无法直接有效求解。令\n\\(n\\)\n为变量的个数，则内点的时间复杂度为 \\(n\\) 的多项式级别，单纯形法的时间复杂度为\n\\(O(n^2)\\), 这样直接求解的解参数正比于\n\\(|E|\\)\n的数量，规模有可能过于庞大，无法进行实时的在线分配。因此有必要探索更新效率更高的的在线分配方案。\n对偶求解\n通过拉格朗日对偶可将原问题装化为对偶问题，但是对偶问题的变量数目仍然正比于约束的数目（供给约束和需求约束），前者的变量的量级为十万甚至百万千万，但后者的量级在数千级别。\n为了减少所需求解的变量，这篇文献 Optimal Online\nAssignment with Forecasts\n提出了一个方法：只保留需求约束对应的对偶变量，然后通过数学变换恢复出供给约束对应的对偶变量和分配率。具体的算法过程可参考上面提到的文献。\n上面的方法在求解对偶问题时代价仍然比较高，因此在文献 SHALE: an efficient\nalgorithm for allocation of guaranteed display advertising 提出了\nSHALE\n算法，优化了求解对偶变量的步骤，采用了原始对偶方法迭代进行求解，求解出对偶变量后，通过数学变换恢复出供给约束对应的对偶变量和分配率跟上面的方法一致。\n启发式分配方案 HWM\n上面根据历史流量数据来求解的分配方案原理上可行，但是在实际的工程应用中仍然显得有些复杂，比如离线仍然要消耗大量的时间求解对偶解。因此，人们希望实现一种快速算法，保持前述方法的紧凑分配的特性，效果上也能够近似最优。前述方法中通过合同节点的对偶变量即可恢复最优解，受其讨论启发，可以发现，只要大体确定好每个合同在分配中的相对优先级以及分配时得到某次展示的概率，就可以构造出一种直觉上可行的在线分配方案。\n文献 Ad serving\nusing a compact allocation plan 提出的 HWM（High Water MArk）\n算法便是这样一种方案，虽然在数学上并不完全严谨，但是由于根据历史数据来指定的分配方案本身就具有相当程度的近似，因此其实际效果也不错，而工程上的便利性则是这个算法的一大优点。\nHWM 分配算法有两个关键点：\n1）根据历史流量确定每个广告合约资源的稀缺程度，通过可满足各合约的供给节点总流量的升序排列进而得到分配优先级\n2）根据优先级确定每个广告合约的分配比例\n具体过程可参考文献内容\n合约广告系统主要模块\n在前面提到的广告提供架构图中，合约广告系统主要表示为以下模块\n\n\n合约广告系统\n\n上面是竞价广告系统中截取出来的，但是截取出来的部分在概念上与合约广告系统相差不大；主要由以下几个部分构成\nAd retrieval：搜索页面内容相关的广告 Ad Ranking：计算\nCTR，根据 CTR 排序 Yield management（Allocation）：广告分配的问题\n流量预测模块：跟 Allocation 打交道，在上图中没有画出来 Billing 和\nAnti-spam：实时计算部分，用于计价和防作弊，任何广告系统都有\nHadoop 介绍\n上面简单介绍了一些计算广告系统中常用的开源工具，下面主要介绍 Hadoop\n这个使用最为广告的分布式系统。\nHadoop 源于 Lucene 项目一部分，2006 年成为子项目，\n后来成为 Apache 顶级项目\nHadoop 最为重要的两个部件： 1. HDFS：一个高可靠性，\n高效率的分布式文件系统 2. MapReduce: 一个海量数据处理的编程框架\nHDFS\nHDFS 的架构如下： \nHDFS 中主要有 Namenode 和 Datanodes 两种角色，其中 Namenode 存储的的是\nmetadata，其包含的信息是组成文件的各个 block 存储在哪个 Datanode 上，而\nDataNodes\n是真正存储文件数据的地方，并且为了达到高可用的效果，文件的一个 block 会以多个 replication 的方式存在多个 Datanode 上。\n当 client 访问 HDFS 上的文件时，会先访问 NameNode，得到 文件所在的\nDataNode 的后再去对文件进行具体操作。\nMapReduce\nMapReduce\n是一个分布式计算框架，整个过程包括一个 Map 过程和一个 Reduce 过程。相比于 MPI,Map\n过程处理之间的独立性使得整个系统的可靠性大为提高；并且分布式操作和容错机制由系统实现，\n应用级编程非常简单。\nMapReduce 的计算流程非常类似于简单的 Unix pipe\nPipe: cat input   | grep  |        sort    | uniq -c   &gt; outputM/R:   Input      |  map  | shuffle &amp; sort |   reduce | output\nMapReduce 中进程间通信的时间只能是在 map 和 reduce 间的 shuffle &amp; sort\n过程，该过程主要是将经过 map 操作后 key\n相同的那些记录聚合到一起，已进行后面的 reduce 操作。其过程如下图示：\n\n\nMapReduce\n\nMapReduce 与 分布式机器学习\n在分布式机器学习中常用的统计模型有两种：指数族分布和指数族混合分布\n指数族分布 (Exponential\nFamily) 是条件概率服从某种形式的，常见的很多分布（如高斯分布，泊松分布，多项式分布等）通过变换都能够转化为这种形式，也就是常见的分布很多都是指数族分布。\n这种分布的一个好处是通过最大似然 (Maximum likelihood,\nML) 估计可以通过充分统计量 (sufficient\nstatistics) 链接到数据，或者说得到数据的规律。这里的充分统计量 (sufficient\nstatistics) 是指数族分布中的一个组成部分，一般来说就是模型的参数（如高斯分布的均值和方差）。\n而当单个的指数族分布无法刻画数据的分布的时候，就要考虑多个指数族分布混合在一起的分布，也就是指数族混合分布；常见的指数族混合分布有 Mixture\nof Gaussians, Hidden Markov Models, Probabilistic Latent Semantic\nAnalysis (PLSI) 等。\n在指数族混合分布中， ML 估计一般通过 EM 算法迭代得到。每个迭代中，\n我们使用上一个迭代的统计量更新模型。\n而一般的 ML 和 EM 算法能够通过 MapReduce 过程较好地刻画\n\n\nMapReduce 与 EM\n\n对于 ML 过程只需要一个 mapper 和 一个 reducer 即可，上面从 reducer\n经过 model 返回到 mapper 的过程代表 EM 的迭代过程。\n这样刻画的好处使得在 mapper 中仅仅生成比较紧凑的统计量，\n其大小正比于模型参数量，\n与数据量无关；同时这样的流程可以抽象出来，\n而具体的模型算法只需要关注统计量计算和更新两个函数。\n但是对于需要迭代的算法，MapReduce 需要与 HDFS\n进行多次交互从而导致性能不佳，而这也是 Spark\n等框架致力于解决的问题之一。\nMapReduce 的多种实现方式\nMapReduce 提供了多样的编程接口，除了上面介绍的直接通过 Java\n写 MapReduce 程序外；通过 Streaming 可以利用标准输入输出模拟以上\npipeline；而通过 Pig 只需关注数据逻辑，无须考虑 M/R 实现\nHadoop 的 Streaming 模拟 Pipe 方式执行 Map/Reduce Job,\n并利用标准输入 / 输出调度数据；开发者可以使用任何编程语言实现 map 和 reduce 过程，\n只需要从标准输入读入数据，并将处理结果打印到标准输出.\n其限制是只支持文本格式数据，数据缺省配置为每行为一个 Record,\nKey 和 value 之间用 \\t 分隔，如生成大量文本上的字典可通过下面的 linux 命令模拟 map 操作和 reduce 操作\nmap：   awk  '{for (i=1; i &lt;=NF; i ++){print $i}}'reduce: uniq\nPig 通过类 SQL 操作在 Hadoop 上进行数据处理，如下是一段 pig 代码实例：\nUsers = load ‘users’ as (name, age); Fltrd = filter Users by     age &gt;= 18 and age &lt;= 25; Pages = load ‘pages’ as (user, url); Jnd = join Fltrd by name, Pages by user; Grpd = group Jnd by url; Smmd = foreach Grpd generate group,     COUNT(Jnd) as clicks; Srtd = order Smmd by clicks desc; Top5 = limit Srtd 5; store Top5 into ‘top5sites’;\nPig 解释器会进行整体规划以减少总的 map/reduce 次数，而如果需要通过 Java\n来写 MapReduce 程序的话，会非常冗长，如下所示是实现相同功能的 MR\n代码\n\n\nnaive java code\n\npig 常用的一些语句如下所示\n\n\npig 常用语句\n\nHadoop 上还有一个工作流引擎：Oozie，用于连接多个 Map/reduce Job,\n完成复杂的数据处理，处理各 Job 以及数据之间的依赖关系（可以依赖的条件：数据，时间，其他 Job 等);\nOozie 使用 hPDL (一种 XML 流程语言)\n来定义 DAG 工作流。这里需要注意的是工作流引擎在线上环境中非常重要，原因是当执行任务多了以后，任务间的关系依赖关系不能出错，否则会带来各种意想不到的后果。\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算广告笔记 (4)-- 竞价广告系统","url":"/2017/05/07/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(4)--%E7%AB%9E%E4%BB%B7%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/","content":"本文是刘鹏老师的计算广告学中的一些笔记。本文是第四章:\n竞价广告系统。主要介绍竞价系统理论，广告网络的概念，以及点击率预测设计到的一些技术。\n\n合约广告系统两个核心问题：在线分配和受众定向。\n竞价广告系统是广告系统发展的里程碑。从搜索引擎的关键词\n的竞价延伸到展示广告，标签精细化的必然发展。\n不保量，但是保质，量由 demand 端的 agency 保证。\n竞价系统理论\n竞价广告系统可以被描述成一个位置拍卖问题 (Position\nauctions)，该问题描述如下\n\n该问题要解决的是将广告对象 $ a = , 2, … A $ 排放到位置 $ s = , 2, …,\nS $，这里的位置主要针对搜索广告而言\n假设对象 \\(a\\) 的出价 (bid) 为 \\(b_a\\) , 而其对位置 \\(s\\) 的计价为 \\(u_{as} = v_ax_s ,(x_1&gt;x_2\n&gt;…&gt;x_S)\\)， \\(v_a\\)\n视为点击价值，\\(x_s\\)\n视为点击率，该模型可近似描述广告系统竞价问题 (对显示广告，可以认为 S =\n1，但是与搜索广告的不同点在于搜索广告可以一个都不出，而显示广告至少要出一个)\n 可以将问题描述成一个对称纳什均衡 (Symmetric Nash equilibrium)\n其目标是 $(v_s – p_s) x_s &gt;= (v_s – p_t) x_t $, 其中 \\(p_t = b_{s+1}\\), 这里的 \\(p_s\\)\n表示根据广告的位置实际的收费情况，\\(b_{s+1}\\) 在位置 \\(s+1\\)\n的广告的出价；寻找收入最大化且稳定的纳什均衡状态是竞价系统设计的关键\n\n上面的问题建模比较复杂，需要较多的数学推导，这里主要关注上面问题建模求解以后得到的结论：就是定价机制该如何设置。这里讲的定价机制有两种：VCG (Vickrey–Clarke–Groves) 机制和广义第二高价 (Generalized\nsecond pricing) 机制。\n从理论上来讲，VCG 机制是最优的，VCG\n认为某对象的收费应等于给他人带来的价值损害，而且其有一个很好的性质：整体市场是 truth-telling 的，也就是每个广告主只需要根据自己真实想法出价，避免广告主间的博弈。\n但是实际的定价机制是广义第二高价，就是每个位置收取的费用是其下一个位置的出价加 1（比下一个位置高即可）即\n\\[\\begin{align} p_s = b_{s+1} + 1\n\\end{align}\\]\n这种做法看似会降低收益，但是实际上与 VCG 机制相比，会收取广告主更多的费用。关于原因，视频讲解时给了一个例子\n&gt; 假如说现在第一高价的广告是 5 块，第二高价的是 3 块。\n&gt; 假如采用第一高价的机制，那么出了第一高价的广告主就会设法调低自己的价格，比如说调到 3 块 1 毛，因为这时候仍然可以拿到第一位且支出更少，而如果再来一个人要第一的广告位，那么他可能就会调到 3 块 2；\n&gt; 而采用第二高价的时候，出了第一高价的广告主不会设法调低自己的价格，因为收取的是第二高价的价格，而此时如果来了一个想要第一的广告位的广告主，那么他出的价格必须要在 5 元以上了\n广义第二高价的整体市场不是 truth-telling 的，从上面的例子也可看到，商家之间会存在着博弈，但是由于其简单易行，为在线广告系统广泛采用，而 VCG 则用得较少\n广告网络 (Ad Network)\n维基百科对广告网络的定义如下： &gt;Connects advertisers to web sites\nthat want to host advertisements\n在竞价广告系统中，其主要的特征有：\n\n竞价系统 (Auction system)\n 淡化广告位概念：卖的是人群，而不是媒体，媒体已经变成了一个载体\n最合适的计价方式为 CPC（Cost Per Click）:\n非实时竞价方式，广告网络估计 CTR，广告主估计每个广告的价值\n不足：不易支持定制化用户划分：广告主只能购买广告网络指定的关键词\n\n广告网络的系统架构示意如下\n\n\n广告网络架构\n\n该架构的工作原理大致如下： 1. User\n开始浏览某个页面时，Ad retrieval 根据 User 的\nUser Attributes 和页面的 Page Attributes 从\nAd Index\n中找到相关的广告 (如果有广告主将广告新加进来时，也要通过\nReal-time indexing 将广告加入 Ad Index 中) 2.\n对相关广告根据 eCPM 进行排序，由于点击的 value\n已经由广告主决定，这时只需要预估点击率即可。 3.\n进行排序后需要将排序的结果及其用户后续的点击行为记录到日志中，从而便于改进点击率预估模型，同时通过流式计算平台进行反作弊监测以及对广告主收费（如果用户有点击行为的话）。\n下面要介绍以上广告网络中提到的一些技术，\n广告检索 (Ad Index)\n规模较大的时候才用。检索是一种搜索技术，这里主要介绍两个重点：布尔表达式检索和长 Query 情况下的相关性检索。\n布尔表达式检索\n将每篇文档看做一个布尔表达式，同时将每个查询也看做一个布尔表达式。\n如下面是一个表示文档的布尔表达式\n\n\nboolean\n\n只有当文档满足查询的布尔表达式条件时，才能出这个广告，为了加快这个检索过程，布尔表达式检索通过建立两层索引来实现加速。其涉及到的一些概念和具体方法如下 (摘自 PPT）\n\n\nb1\n\n\n\nb2\n\n\n\nb3\n\n长 Query\n情况下的相关性检索指的是当查询的布尔表达式中的条件较多时，如果采用传统的搜索引擎的搜索方法，会导致计算量非常大，这里提供的思路是：在查找候选文档的过程中做一个近似的评估，切掉那些理论上不需要再考虑的文档，只对进候选的文档进行相关性计算，比 Top-N 最小堆最小值大的插入\n采用的具体算法是 WAND 算法，其细节如下（摘自 PPT）\n\n\nWAND\n\n一致性问题\nZookeeper\nZookeeper\n是在基于消息传递通信模型的分布式环境下解决一致性问题的基础服务\n用层次式 Namespace 维护同步需要的状态空间\n\n\nzk1\n\n保证实现特性：Sequential Consistency, Atomicity, Single System Image,\nReliability,\nTimeliness，较复杂的同步模式需要利用 API 编程实现。Zookeeper 的实现利用了 Paxos 算法。\nPaxos 算法概念\n目的是解决分布式环境下，怎么分布式地决策某些状态使得所有机器都处于一致性。\n节点角色： P (roposer): (提出提案 (n, value)),\nA (cceptor), L (earner)\n三个约束： 1. value 只有在被提出后才能被批准 2.\n在一次算法的执行实例中，只批准一个 value 3.\nlearners 只能获得被批准的 value\n准备阶段 1. P 选择某提案编号 n 并将\nprepare 请求发给 A 中的某多数派； 2.\nA 收到消息后，若 n 大于它已经回复的所有消息，则将自己上次接受的提案回复给 P，并承诺不再回复小于 n 的提案；\n批准阶段： 1.\n当 P 收到了多数 A 回复后，进入批准阶段。它要向回复请求的 A 发送 accept\n请求，包括编号 n 和根据约束决定的 value 2.\n在不违背向其他 P 的承诺的前提下，A 收到请求后即接受。\n点击率预测\n从广告检索出相关的广告后，需要根据 eCPM\n对相关广告进行排序，由于出价由广告主决定，因此实际中需要估算的往往是广告的点击率。\n基于统计的模型\n\\[\\begin{align}  u(a,u,c) = p(click|a,u,c)\n\\end{align}\\]\n在这个问题上，Regression 比 Ranking 合适一些，因为广告的实际排序是根据 eCPM，而 eCPM 由点击率和出价相乘决定，因此需要尽可能准确估计 CTR，而不仅仅是各候选的 CTR 排序正确\n冷启动问题：指的是新的广告非常多，这种情况下利用广告层级结构 (creative,\nsolution, campaign, advertiser)，以及广告标签对新广告点击率做估计\n捕获点击率的动态特性，两种方案： 动态特征:\n快速调整特征 在线学习：快速调整模型\n逻辑回归\n逻辑回归是工程中常用的点击率预估方法。\n动态特征\n在线广告的三个维度 \\((u,a,c)\\)\n上均有不同的特征，可以通过组合这些特征构造高纬特征\n\n\n三个维度上的特征\n\n上面的组合特征均为静态特征，如果在这些组合的静态特征上加上这个特征的历史数据就变成了动态特征，和某个静态特征为 “年龄为 25~35 且为男性”，如果这个特征的取值为在某段时间的下单量而不是单纯的 0 或 1，那么这个特征就是一个动态特征。因此动态特征即在标签组合维度上聚合点击反馈统计作为 CTR 预测的特征。\n优势 1. 工程架构扩展性强，变 features 不变\nmodel (与在线学习相比) 2. 对新 \\((a, u,\nc)\\) 组合有较强 back-off 能力\n缺点 1. 在线特征的存储量大，更新要求高\n组合维度举例:\n\ncookie(u) and creative(a)\ngender(u) and topic(c)\nlocation(u) and advertiser(a)\nCategory(a) and category(u)\ncookie(u)\ncreative(a)\ngender(u)\n\n优化方法\nL-BFGS\n基于梯度的方法在工程上的收敛性不好，因为工程上的问题总是病态的。用二阶的方法，一般用\nQuasi-Newton 方法。\nBFGS (Broyden, Fletcher, Goldfarb, and Shanno)\n是 Quasi-Newton 方法的一种，\n思路为用函数值和特征的变化量来近似 Hession 矩阵，以保证正定性，并减少计算量。\nBFGS 方法 Hession 计算公式如下 (空间复杂度为 \\(O(n^2)\\) )：\n\n\nBFGS\n\nL (imited memory)-BFGS 是为了解决 BFGS 的空间复杂度问题。将 nxn\n的 Hession 阵用下图方式加以近似 (\\(B_k\\)\n为 Hession 近似)\n\n\nHessian 近似\n\n这样的方法将空间复杂度降为 \\(O(nk)\\), 在特征量大时比 BFGS 实用\n可以非常容易地用 map/reduce\n实现分布式求解，这种方法也适用于梯度法：mapper\n求部分数据上的梯度，reducer 求和并更新参数\nADMM\nAlternating Direction Method of Multipliers 的形式\n\\[\\begin{align}\\min f(x)+g(z)\\\\\\\ns.t. Ax + Bz = c\\end{align}\\]\nAugmented Lagrangian 及迭代解法如下\n\n\nADMM\n\n上面的迭代方法也可以用下面的迭代公式描述，其效果是一致的，但是下面的描述更加简便，而且在实际中也更常用\n\n\nADMM 更一般表示方法\n\nADMM 这种迭代的解法能够很容易地通过 MapReduce 模式迭代进行求解。\n下面介绍逻辑回归的 ADMM 分布式解法，这种方法将将样本划分为 N 份，每个\nmapper 负责一份，其描述的最优化问题如下\n\\[\\begin{align}\\min \\sum_{i=1}^{N}\\sigma\n(w_ix_i)+r(z)\\\\\\\ns.t. w - z = 0\\end{align}\\]\n分布式的迭代解法入下:\n\\[\\begin{align}w_i^{k+1} \\leftarrow \\arg\n\\min_{w_i} (\\sigma(w_ix_i) + \\frac{\\rho}{2}||w_i - z^k +\n\\mu_i^k||_2^2)\\\\\\\nz^{k+1} \\leftarrow \\arg \\min_{z} (r(z) + \\frac{N\\rho}{2}||z^k -\n\\overline w^{k+1}+ \\overline \\mu_k||_2^2)\\\\\\\n\\mu_i^{k+1} \\leftarrow \\mu_i^k + w_i^{k+1} - z^{k+1}\n\\end{align}\\]\n探索与利用 (Explore and\nexploit， E&amp;E)\n这一问题主要是为长尾的 \\((a, u, c)\\)\n组合创造合适的展示机会以积累统计量，从而更准确地估计其 CTR\n原因是真实的环境中，数据总是长尾的，总体集没法通过采样获得，实际上大批广告主的广告是没有机会展示，为了让更多的广告主的广告能够得到恰当的展示，需要做一些探索（即不选择当前出价最高的广告，而是选择一些符合要求的长尾广告），但是最终的目的仍是提升整体的广告收入，即需要严格控制探索的量和有效性\n基本方法思路 1. 通常描述为 Multi-arm Bandit (MAB)\n问题：有限个 arms (或称收益提供者) \\(a\\), 每个有确定有限的期望收益 \\(E(r_{t,a})\\) 2.\n在每个时刻 t, 我们必须从 arms 中选择一个，最终目标是优化整体收益 基本方法为\n\\(\\epsilon\\)–greedy: 将 \\(\\epsilon\\) 比例的小部分流量用于随机探索\n上面的方法应用在广告中的主要挑战有：\n\n海量的组合空间需要被探索\n各个 arm 的期望收益是动态变化的\n\n因此提出了以下两个思路， UCB 和 Contextual Bandit\nUCB\n在时刻 t，通过以往的观测值以及某种概率模型，计算每个 arm 的期望收益的 upper\nconfidence bound (UCB)，并选择 UCB 最大的 arm。\n实际上是将每个 arm 的收入看作一个分布，选择所有分布中可能达到最大的那个 arm 作为最终的选择。\n我们不可能一直选择非最优的 arm, 原因是我们选择的此 arm 次数越多，\n其 UCB 就越接近于其期望收益\n具体 UCB 策略有以下两种：\n\nβ-UCB 策略：依一个很大的概率，我们选择非最优 arms 的次数存在着一个上界，\n该上界与总的选择次数无关\n UCB-tuned 策略：我们已选择的次数越多，\n就越可以自信地抛弃不太有前途 (但仍有可能最优) 的 arm.\n\nContextual Bandit\n解决 arm 数目过多问题，降维，映射到另外的特征空间\n\n对每次展示，每个 arm (广告) \\(a\\)\n有一个对应的特征矢量 \\(x(u,a)\\)\n 用此特征矢量代替 arm 本身进行 Bandit 决策\n\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算广告笔记 (5)-- 搜索广告与广告网络 Demand 技术","url":"/2017/05/10/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(5)--%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A%E4%B8%8E%E5%B9%BF%E5%91%8A%E7%BD%91%E7%BB%9CDemand%E6%8A%80%E6%9C%AF/","content":"本文是刘鹏老师的计算广告学中的一些笔记。本文是第五章:\n搜索广告与广告网络 Demand 技术。主要介绍搜索广告中的几个典型问题以及广告网络中 demand 端需要用到的技术\n\n搜索广告的特点\n搜索广告与显示广告不同的特点在于\n\n用户定向标签 \\(f(u)\\):\n远远弱于上下文影响（query），一般可以忽略\n Session 内的短时用户搜索行为作用很重要\n上下文定向标签 \\(f(c)\\):\n关键词\n\n搜索广告是一种典型的位置竞价模式，如下是搜索广告中常见的三种位置\n\n\n搜索广告中的位置\n\n根据上图，可知搜索广告中的位置一般分为北，南，东三个广告区块，根据各位置的 reference\nctr 决定，各位置在竞价系统中的位次 reference ctr\n可以通过 ε 流量较准确测定出\n搜索广告的典型问题\n搜索广告中需要考虑的几个典型问题如下\n\n查询词扩展 (Query Expansion)\n 用户相关的搜索广告决策\n短时用户行为反馈\n\n查询词扩展 (Query\nExpansion)\n目的是 supply\n端为了赚取更多的利润，同时拓展了广告主的竞价范围，常见的思路有以下三种\n（1）基于推荐的方法：挖掘 (session,\nquery) 矩阵找到相关 query, 可类比 (user,\nitem) 矩阵，这种方法利用的是搜索数据\n（2）基于语义的方法：用 topic\nmodel 或概念化的方法中找语义相关 query，这种方法利用的是其他文档数据\n（3）基于收益的方法：根据实际 eCPM 统计得到变现能力最好的相关 query，这种方法利用的是广告数据\n用户相关的搜索广告决策\n首先需要明确结果个性化对于搜索广告作用有限，原因是上下文信息 (c) 太强，\n个人兴趣基本可以忽略；同时搜索页上的结果需要保证主题上某种一致性\n但是广告展示条数是可以深度个性化的，因为约一半的用户无法明确区分广告与搜索结果，在平均广告条数的约束下，可以对每个用户的广告条数进行个性化，以最大化营收。因此这又一个约束优化问题！\n另外可以根据同一 session\n内的行为调整广告结果，如在第一页没点的广告是否要放到第二页。\n短时用户行为反馈\n短时用户行为的定义如下，狭义来说是用户在一个 session 内的行为，广义来说是\n用户在短时间 (一般为一到两天) 内的行为\n通过短时用户行为反馈，可以实现： （1）短时受众定向:\n根据短时行为为用户打上的标签 （2）短时点击反馈:\n根据短时广告交互计算的动态特征\n而短时用户行为计算需要准实时 (分钟级) 对用户行为进行加工，不适合在 Hadoop 上进行\n可以利用流式计算 (stream computing) 平台 ,\n如 S4（雅虎开源的一个流式计算平台）, Storm 等\n流式计算平台\n前面提到了流式计算平台，下面以 storm 为例简单讲述\nStorm\n是一个大规模实时数据处理框架，能够自动完成数据分发和可靠性管理，开发者只需要关注处理逻辑，数据流基本在网络和内存进行（极端情况下会有磁盘调度）\nStorm 计算逻辑类似 Map/Reduce,\n区别在调度数据而非调度计算，其拓扑及任务分配如下（spout\n是输入，根据输入的 key 分发到不同的 Bolt 上处理，最后将结果组成）\n\n\nstorm\n\n广告网络 demand 端技术\n广告购买平台 (Trading Desk) 是 demand 端的一种产品，其目的是\n&gt;Allows advertisers buy audience across publishers and ad\nnetworks\n其关键特征有\n\n连接到不同媒体和广告网络，为广告商提供 universal marketplace\n 非实时竞价 campaign 的 ROI 优化能力\n经常由代理公司孵化出来\n\nROI 优化能力\nROI 优化目标是给定总预算，在多广告网络中采买并优化 ROI\nROI 优化其中若干关键问题有\n（1）在合适的流量 segment 上投放广告；如 SEM 中的选词、显示广告网络中的标签组合选择等\n（2）在每个投放上合理地出价以优化 ROI；与实时竞价不同，采买方无法控制每次展示的出价 (因此一般采用每次点击固定价格的策略)，但是因为\n\\(u, c\\)\n的取值未知，需要在各流量分割上估计其分布并合理出价\n（3）对每个 segment 的量以及 Market price 进行预估，以完成整体的优化\n在这个领域有代表性的公司有\nEfficientFrontier，这个公司的核心业务是为搜索广告主提供大量关键词情形下的\nROI\n优化服务，并收取固定比例的提成；广告主只需要提供预算、关键词、受众类型等信息即可，EfficientFrontier\n会通过计算为其提供最优方案\n其核心技术为 Portfolio\nOptimization，原是金融领域内的一个优化算法，目前正在向显示广告领域扩张，需要注意的是除了算法以外，长时间数据积累也很重要\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算广告 (6)-- 广告交易市场 (Ad Exchange)","url":"/2017/05/14/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(6)--%E5%B9%BF%E5%91%8A%E4%BA%A4%E6%98%93%E5%B8%82%E5%9C%BA(Ad%20Exchange)/","content":"本文是刘鹏老师的计算广告学中的一些笔记。本文是第六章:\n广告交易市场 (Ad\nExchange)。主要介绍目前最常见的实时竞价的广告交易模式，分别介绍了这一模式下的供给方平台 (SSP)\n和 需求方平台 (DSP), 并且重点介绍了 DSP， 因为在这一交易模式下，DSP\n负责了绝大多数的关键任务。\n\n广告交易平台 (Adx)\n广告交易平台的关键特征是用实时竞价 (RTB) 方式连接广告和\n(上下文，用户)，按照展示上的竞价收取广告主费用\n实时竞价指的是事先不对广告做 retrieval，而是在需要展示的时候向下游的\nDSP 展示目前的 context，让各个 DSP\n通过竞价的方式获取这次展示的机会。其流程如下\n\n\nRTB 过程\n\n从上图可知，RTB 是一个多方参与的过程，而从实现上可以分为两个阶段\n1）cookie mapping：目的是将 supply 网站和 demand\n端的用户对应起来；实际中通过 Adx（代表 supply） 与 DSP（代表 demand）\n来进行直接的 mapping，后面会提到这方面的具体技术 2）ad\ncall：指有广告展示机会到来，这时 Adx 通知各个 DSP 进行竞价，DSP\n根据自身需求出价，出价最高者获得投放机会。\n这种模式的出现是由于广告主需要的定制化受众定向在原来的广告网络上无法得到满足，举例来说：京东商城需要对其用户进行重定向投放广告，但是在广告网络上是采买不到这种流量的，因为哪些用户是京东的只有京东自己知道，因此需要让\ndemand 端对流量进行选择才能达到深度定制化的要求。\n这种主要技术难点在于 1） Adx 与 DSP 之间的用户身份同步，一般通过 cookie\nmapping 实现 2） DSP 数量较多时的服务和带宽成本优化\n但是存在着的潜在问题是： 1）\n存在浏览数据的泄露风险。指的是某些 DSP 目的并不是获取广告，而是获取\nAdx\n展示的用户浏览数据；即每次出价都很低，保证自己不会赢得广告的展示机会，但是却能获得了\nAdx 提供的用户 cookie\n及其访问的 url，为其日后进行受众定向积累数据基础。\n2） 多一次 round trip，对 latency\n有较大影响。在之前的广告体系（合约广告，竞价广告）中，supply\n端只需要先广告网络进行请求，广告网络根据其 retrieval\n以及相关广告主的出价返回相关的广告即可，但是在实时竞价中，Adx 还需要等待\nDSP 的出价以及通知获得展示机会的 DSP\n进行广告的投放，这样无疑会增加投放的 latency 。\n除了实时竞价，广告交易平台另外一个重要的特点是按展示计费而不是按照点击收费，其商业逻辑在于将点击率和点击价值的预估全放到\ndemand 端。\nAdx 的系统架构图\nAd Exchange\n的系统架构很简单，原因是其不需要存储广告之类的信息，只需要将广告展示机会告诉给每个\nDSP ，然后进行竞价拍卖选出出价最高的那个\nDSP。其难点在于系统的吞吐量大且对 latency\n的要求严格。其系统架构图如下\n\n\nAd Exchange 系统架构\n\n其流程如下 1）某个 User 访问某个网站，网站通知 Adx 目前有一次展示机会\n2）Adx 通过 RTBD 接口向各个 DSP 询价，从而获取\nDSP 提供的各个广告（相当于图中的 Ad retrieval） 3）Adx\n通过出价进行排序，并选择出价最高的广告展示给用户（相当于图中的 Ad\nranking） 4）Adx 在日志中记录该广告的展示记录\ncookie mapping 的技术\ncookie mapping 需要弄清楚三个问题：谁发起 cookie mapping\n的请求？在哪里发起的请求？谁存 mapping 表？\n下面以两个例子说明，第一个例子是 DSP 与 Adx 间的 cookie\nmapping，第二个例子是 DMP 与媒体的 cookie mapping。\n下面是 DSP 与 Adx 的 cookie mapping 的过程\n\n\nDSP 与 Adx\n\n这个例子中 cookie mapping 由 DSP 发起，在有 DSP\n代码的广告主网站发起，mapping 表存在 DSP。其具体流程为\n1）DSP 在广告主网站中嵌入 JS 代码，有用户访问时，选择性加载一个 DSP\n域名下的 iframe 2）DSP 根据其记录判断该用户是否需要 cookie\nmapping，返回包含多个 beacon 的动态 HTML，这里的多个 beacon\n目的是为了与多个 Adx 交换 cookie。 3）通过某个 beacon\n向对应的 Adx 发送 cookie 映射请求，并带有 Adx\n标识（xid），DSP 标识（did）和 DAP cookie（dck） 三个参数。\n4）Adx 通过 302 重定向向 DSP 返回 Adx\n标识（xid）及其域名下的 cookie（xck) 5) DSP 返回一个 1x1 的\nbeacon 并记录下 Adx 方的 cookie（xcd） 与己方的 cookie（dck）\n的对应关系。\n下面是媒体与 DMP 的 cookie mapping 的过程\n\n\nDMP 与媒体\n\n这个例子中 cookie mapping 由媒体方发起，在媒体的页面上发起，并且\ncookie mapping 的表由 DMP 保存。其过程如下\n\n用户访问媒体网站时，媒体网站向媒体的 cookie\n映射服务请求一段负责此功能的 JS 代码 2）媒体的 cookie 映射服务返回该段\nJS 代码 3）该 JS 代码判断是否需要映射，如果需要，则先 DMP\n发送映射请求，并传送两个参数：媒体的标识（mid）以及媒体方的\ncookie（mck） 4）DMP 返回一个 1x1 的 beacon，并记录下媒体方\ncookie (mck) 和己方的 cookie (dck) 对应关系。\n\n供应方平台（supply side\nplatform）\nSSP 完全代表着媒体方的利益\n广告市场中，媒体变现流量方式一般有三种\n\n合约广告，与广告主签订合约进行投放（CPM 结算）\n竞价广告，将广告位托管给广告网络，广告网络根据人群售卖给广告主（CPC\n结算）\n实时竞价（按展示结算）\n\n而 SSP\n应该能够为媒体提供上面的所有变现方式，通过组合以上方式达到收益最大化 (称为收益管理，Yield\nOptimizer)\n综合以上可知，SSP 的关键特征是\n\n提供媒体端的用户划分和售卖能力\n可以灵活接入多种变现方式\n收益管理\n\n整个行业有代表性的公式有： AdMeld，Rubicon，Pubmatic\n需求方平台（Demand Side\nPlatform）\n将决策交到 demand 端，\n维基上对 DSP\n的定义如下\n\nA demand-side platform (DSP) is a system that allows buyers of\ndigital advertising inventory to manage multiple ad exchange and\ndata exchange accounts through one interface.\n\nDSP 为广告主提供的便利之处在于，广告主主需要通过 DSP\n的一个接口就可以获取各式各样的流量，从而进行定制化广告投放。\nDSP 的关键特征如下\n\n定制化用户划分\n跨媒体流量采购\n通过 ROI 估计来支持 RTB\n\nDSP 的代表公司有 InviteMedia 和 MediaMath\nDSP 的架构\nDSP 的架构示意图如下\n\n\nDSP\n\n复杂部分在于计算部分，就是要估计 eCPM = CTR *\nclickValue，CTR 表示点击率， clickValue\n表示点击价值，后面会详细介绍，在这里 DSP 需要同时计算 CTR 和\nclickValue，复杂度自然大大提升。\nDSP 流量预测\n流量预测指的是给定一组受众定向标签组合以及一个 eCPM\n的阈值，估算在将来某个时间段内符合这些受众标签组合的条件、且市场价在该\neCPM 阈值以下的广告展示量。\nDSP 也需要预测流量以决定采买策略，因为 DSP 与广告主是 CPM\n结算的，DSP 只要将市场中符合广告主的那部分低价的流量买下来才能获取更大的利润。\n但是由于 DSP\n无法拿到所有的流量情况，因此无法像攻击方那样通过历史流量那样进行流量预测，这个问题目前没有一个公认的较好的解决方法\nDSP 点击价值预估\n点击价值指的是上面提到 eCPM = CTR * clickValue 中的\nclickValue。一般衡量该点击的价值的指标是点击后的转化率\n这个问题的挑战有\n1）训练数据非常稀疏（最终得到的训练数据的比例是\n点击率*转化率）\n2）价值的预估与广告主类型强烈相关的行为模式（比如说游戏领域与电商领域不能相同的方法预估）\n这个问题目前也没有很好的解决方法，但是有以下两点原则供参考\n1）模型估计的时候，用较大的 bias 换较小的\nvariance，以达到稳定估计的目的\n2）充分利用广告商类型的层级结构以及转化流程上的特征\nDSP 重定向（retargeting）\n什么是重定向呢？假如有一个电商网站，每天有几千人访问。当然，这几千人里面大部分当时并没有买东西就离开了。而重定向就是说这些人到了别的网站，就在该网站的广告位上投放该电商的广告，这样一来，用户点击广告和进而下单购买的比例都会相当高。\n如下是一个重定向的例子\n\n\nretargeting\n\n重定向的分类\n1. 网站重定向。就是上面提到的方法\n2. 搜索重定向。根据用户在搜索引擎上与广告主相关的搜索行为\n3. 个性化重定向。对用户购买流程的追踪和推荐，即根据用户在广告主网站上关注的具体产品和购买阶段，推送商品粒度的广告。对于广告主而言，可以视为一个站外推荐引擎。\n上面提到了推荐算法，这里对推荐算法做一个简单的介绍，推荐算法可以分为两大类：基于协同过滤的算法和基于内容算法\n基于协同过滤的算法又可分为两种\n1）内存方法（非参数方法）：neighbor-based methods，User-based/Item-based\ntop-N 2）模型方法（参数方法）:matrix factorization, bayesian belief\nnets\n基于内容的推荐算法指的是从 user 和 item\n提取出相同的特征向量，或者说将两者映射到相同的向量空间中再比较两者的相似性。\n推荐算法的本质是对 user-item\n这一系数矩阵的参数化或非参数化的描述，而推荐算法选择的关键也是探索合适的\nbias 与 variance 的平衡，以适应问题的数据的稀疏性。\n重定向的典型代表公司是： Magnetic 和 Criteo\n新客推荐（look-alike）\n上面的重定向针对的是广告主已经有的客户，但是对于中小电商，仅仅对老用户定向营销远远不够；而对于某些类型的广告商如银行，大多数用户无法通过重定向渠道捕获。这时候就产生了新客推荐（look-alike）的需求。\n新客推荐指的是由广告商提供一部分种子用户，DSP\n通过网络行为的相似性为其找到潜在用户。需要注意的是尽量利用非 demand\n端的数据，避免在广告主之间倒卖用户。\n广告流量交易方式\n目前为止介绍了多种广告流量的交易形式，可以说交易的方式发展趋势为：合约广告 -&gt; 竞价广告 -&gt; 实时竞价广告，对应着下图从右到左的顺序。而发展到竞价广告之后，更加强调了\nsupply 端和 demand 端的分工的专业化。\n\n\n广告流量交易方式\n\n优先销售有两种模式\n1）CPT 结算，即传统广告的销售方式，技术要求低 2）GD：CPM 结算 +\n人群定向\n程序交易包括竞价广告（Ad\nnetwork）和实时竞价广告（Adx），其对应的 supply 端和 demand\n端负责的任务也不同，但是其目的均是如何将自身的资源或需求分发到多个点（网络）上从而获取最大的利益。\n1）demad：network optimization + RTBS 2）supply：portfolio selection\n+ RTBS\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算机网络课程总结 --BGP 协议","url":"/2016/12/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--BGP%E5%8D%8F%E8%AE%AE/","content":"路由器工作在 IP 层，其作用是根据 IP 地址将数据包传输到正确的目的地，因此路由器必须要知道网络的 “地图” 才能正确投递，而这个网络的 “地图” 就是存储在路由表中的路由规则，简称为路由。\n根据获得路由的方式可以将其分为静态路由和动态路由，静态路由就是管理员静态配置的路由，动态路由则是路由器通过算法动态地学习和调整而得到的路由。而常说的路由协议就是指这些动态路由的学习算法，根据其作用域的不同，又可分为内部网管协议 (IGP) 和边界网络协议 (BGP)；内部网络协议包括 RIP，OSPF 等，边界网关协议则包括 BGP 等。\n本文主要讲述 BGP 相关的一些知识。\n\n当网络过大的时候，也会导致路由表过大而难以维护，这时候采用分治的方法，将一个大网络划分为若干个小网络，这些小网络称为自治系统 (AS)，BGP 的诞生就是用于自治系统间的通信。其在网络中的位置如下\n\n\nBGP 在网络中的作用\n\n那么是不是 AS 之间的通信都要使用 BGP？答案并不是，只有当两 AS 间存在多条路径，需要做路由策略和选择才需要 BGP；如果 AS 只有一个出口或者所有出口指向一个 ISP 的时候，是不需要 BGP 的。\n基本概念\nBGP 允许基于策略（policy-based）的路由选择，策略与政治、安全和经济等因素相关，由 AS 的网络管理者确定，也就是说人为影响的因素较大。\n从上面可知，每个划分后的小网络称为 AS，每个 AS 都有自己独特的 AS 号码 (ASN),ASN 的原来使用 16 位表示。但是由于和和 IP 地址一样，ASN 同样面临分配告罄的危机，自 2006 年 12 月 1 日起，原为 16 位 (1-65535)\n的 ASN 扩展为 32 位空间。\nAS 根据其位置的不同，也被分为为不同类型的 AS。\n\n\nAS 类型\n\n一般来说，AS 号码只是在一家 ISP 与至少两家 ISP 做对等互联，交换路由的时候才需要用到，也就是说一个国家的 AS 号码的数量实际上是跟大中型 ISP 的数量有关。\nBGP 是运行在 TCP 协议上的，与其他路由协议对比如下\n\n\nBGP 与其他路由协议所在位置对比\n\nBGP\n是一种距离矢量路由协议，但避免了环路；其避免环路的策略是不仅仅记录路径代价，还记录下全路径信息。如下图所示\n\nBGP 有两种邻居，其中运行在同一个 AS 内的 BGP 邻居称为 IBGP (Interior\nBGP), 不同 AS 间的邻居称为 EBGP (Exterior\nBGP)，注意无论是 IBGP 或者 EBGP，上面都必须运行着 BGP 协议，也就是说与 BGP 路由器直连的内部路由器不一定是它的 IBGP，如下图所示\n\n注意：BGP 邻居不是自动发现的，而是手动配置的，原因有以下两点：\n1）可以与对端设备用任何 IP 地址建立邻居，而不限于某个固定的接口 IP。这样，当两台\n设备采用环回地址而非直连地址建立 BGP 邻居时，即使主链路中断了，也可以切换到备份链路\n上，保持邻居不断。这种稳定性正是 BGP 作为大型网络路由承载的必要特质。\n2）可以跨越多台设备建立邻居。当一个 AS 有多个设备运行 BGP\n建立域内全连接时，不必每台设备物理直连，只要用 IGP 保证建立邻居的地址可达，即可建立全网连接，减少不必要的链路建设。\nBGP 报文\nBGP 报文类型有以下四种：\n\nOpen 报文：打招呼，“你好，交个朋友吧”（协商参数）\nKeepalive 报文：我还活着，别不理我（30 秒钟交换一次）\nUpdate 报文：有新闻（链路的变化）\nNotification 报文：我不跟你玩了（异常情况的通报，终止连接）\n\nBGP 的工作机制也可以通过这四种报文描述：\n1）通过 TCP 建立 BGP 连接时，发送 OPEN 报文\n2）连接建立后，如果有路由器需要发送路由或路由发生变化时，发送 UPDATE 报文\n3）稳定后，周期发送 KEEPALIVE 报文，维持连接有效性\n4）当本地 BGP 运行中发生错误时，发送 NOTIFICATION 报文通告 BGP 对端\n路由注入及通告\n首先要明确一点，BGP 路由器的路由注入和通告都是为了修改 BGP 路由表。\n当路由器之间建立 BGP 邻居之后，就可以相互交换 BGP 路由。一台运行了 BGP 协议的路由器，会将 BGP 得到的路由与普通路由分开存放，所以 BGP 路由器会同时拥有两张路由表。\n一张是存放普通路由的路由表，被称为 IGP 路由表，就时平时我们使用命令 show ip route 看到的路由表，IGP 路由表的路由信息只能从 IGP 协议和手工配置获得，并且只能传递给 IGP 协议；另外一张就是运行 BGP 之后创建的路由表，称为 BGP 路由表，需要通过命令 show ip bgp 才能查看，BGP 路由表的路由信息只能传递给 BGP 协议，如果两台 BGP 邻居的 BGP 路由表为空，就不会有任何路由传递。\n在初始状态下，BGP 的路由表为空，没有任何路由，要让 BGP 传递相应的路由，只能先将该路由注入 BGP 路由表，之后才能在 BGP 邻居之间传递。注入的方式有多种，如\n1）动态注入：将 IGP (如 OSPF) 发现的路由纯动态地注入到 BGP 路由表中，这种方式配置简单，但操控性差，可能不稳定。具体过程及配置如下图所示：\n\n2）半动态路由注入：通过 IGP 协议 (如 OSPF) 学习到的路由，再通过\nnetwork 发布到 BGP 中。具体过程及配置如下图所示： \n3）静态路由注入：工配置的静态路由，再由 network 发布到 BGP 中。具体过程及配置如下图所示：\n\n在 BGP 路由表注入路由后，BGP 路由器之间会将这些路由在 BGP 路由器间进行通告。通告要遵守以下规则\n\nBGP 路由器只把自己使用的路由通告给相邻体\n BGP\n路由器从 EBGP 获得的路由会向它的所有 BGP 相邻体通告（包括 EBGP 和 IBGP）\nBGP 路由器从 IBGP 获得的路由不会向它的 IBGP 相邻体通告（避免内部产生环路）\nBGP\n路由器从 IBGP 获得的路由是否通告给它的 EBGP 相邻体要依 IGP 和 BGP 同步的情况而定\n\n对于最后一条，只有当 IGP 与 BGP 同步时（也就是该路由可以通过 IGP\n获得），才能通告，反之不通告，这样做的目的是为了避免路由黑洞。\n路径属性\n在默认情况下，到达同一目的地，BGP 只走单条路径，并不会在多条路径之间执行负载均衡。对于 IGP 路由协议，当有多条路径可以到达同一目的地时，则根据最小 metric 值来选择最优路径，而\nBGP\n存在多条路径到达同一目的地时，对于最优路径的选择，BGP 并不会以 metric 值大小为依据，BGP 对于最优路径的选择，需要靠比较路由条目中的 Path\nAttributes，即路径属性，只有在比较多条路由的属性之后，才能决定选择哪条为最优路径。\nBGP 的路径属性可以划分为以下四类：\n\n公认强制 （Well-Known\nMandatory）: 所有的路由中都需要写入公认强制属性\n公认自选 （Well-Known\nDiscretionary）：能够理解和支持即可，不一定要写入路由\n可选可传递 （Optional Transitive）：不一定要理解或支持\n可选不可传递（Optional\nNontransitive）：只有特定的 BGP 路由器才能理解和传递\n\n对于任何一台运行 BGP 的路由器，都必须支持公认强制属性，并且在将路由信息发给其它 BGP 邻居时，必须在路由中写入公认强制属性，这些属性是被强制写入路由中的，一条不带公认强制属性的路由被 BGP 路由器被视为无效而被丢弃，一个不支持公认强制属性的 BGP，是不正常的，不合法的 BGP。BGP 路由必须携带的公认强制属性有三个：Origin，Next_Hop，AS-path。\norigin 属性\norigin 属性为起源属性，描述路由是以何种方式注入到 BGP 路由表中的，主要有以下两种情况\n1）以 network 命令注入到 BGP 路由表中，origin 属性为 IGP\n2）以 redistribute 命令注入到 BGP 路由表中，origin 属性为\nIncomplete 其中，IGP 优先级比 Incomplete 的要高。\nAS Path 属性\n描述了该路由经过的 AS 组成的路径，AS 路径中不能算上自己的 AS，从离自己最近的 AS 开始，以目的网络的 AS 结束。下图为\nAS5 到 AS1 的路由的 AS Path 属性\n\n\nas path 属性\n\n借助路由的 AS Path 属性，可以避免环路，具体操作就是收到一条 AS\nPath 属性中含有自己 AS 的路由的时候丢弃该路由。\n在选路的时候，优先选 AS PATH 最短的那条，如果 AS PATH 距离相等，则优选本 AS 内到出口路由器最短的那根，如果还相等，则选择 Router_ID（发送路由的路由器）最小的那根\n但是要注意，这种选择并不总是明智的，如下图所示\n\n\nas path 选路并非最优\n\nNext Hop 属性\n指示下一个 AS 的路由器入口的网段，同一个 AS 内 Next\nhop 的值不变，如下图所示\n\n\nnext hop 属性\n\nlocal pref 属性\n可选的属性，用于引导流量，local pref 的缺省值是 100，如下图所示\n\n\nlocal pref 属性\n\nMED (Multi Exit\nDistiguisher) 属性\n当 AS 有多个出口的时候，告诉上游的 AS 如何选最优的路，MED 值越小，优先级越高\n\nBGP 选路的策略为\n\n\nBGP 选路策略\n\n其他概念\nBGP 过滤 BGP\n拥有强大的过滤功能，可以按照以下规则进行过滤：\n\n可按照路由的 IP 地址过滤\n可依照路由经过的 AS-Path 过滤\n可以依照路由的属性过滤\n可以依照路由到来的接口过滤\n\nBGP 聚合\n由于 BGP 路由器的路由表庞大，往往超过 10 万条，通过 BGP 聚合 (BGP 支持 CIDR) 解决这个问题，如下图 AS100 先将内部的路由聚合再通告给 AS200\n\n\nBGP 聚合\n\nBGP 联盟和反射\n从前面的描述可知，从 IBGP 收到的路由不会通告给其他的 IBGP（避免环路），所以 AS 内部的 IBGP 必须全连接。但是 IBGP 相邻体过多，逻辑全链接不现实，实际中通过 BGP 联盟和反射解决这个问题。\nBGP 联盟就是将大的 AS 分割成小的 AS，从而减少全连接的数目。\n\n\nBGP 联盟\n\nBGP 反射是通过将网络内的路由器划分为客户机，非客户机以及路由反射器的角色，从而减少 IBGP 间的连接，如下图所示：\n\n\nBGP 反射\n\nBGP 反射中需要遵循以下规则\n\n来自客户机的路由通告给其它的客户机和非客户机\n来自非客户机的路由只通告给它的客户机\n来自 EBGP 的路由向所有相邻体通告\n\nBGP 衰减\nBGP 衰减是为了处理不稳定的路由（如路由频繁更新），避免影响整个互联网络的稳定运行\n&nbsp;\n路由抑制可以阻止公布不稳定的路由，它为每条路由分配一个动态的度量数字用来反映稳定程度，当一条路由出现摆动，就给他分配一个惩罚值，摆动得越多，惩罚值越大。当一段时间不摆动，惩罚值降低，在一个半衰期后，降到原来的一半。如果惩罚值超过抑制上限，该路由就被抑制，只有当一个半衰期后惩罚值降低到重新使用界限时，才重新使用。\nBGP 配置\n通常在路由器配置 BGP 需要开启 BGP 进程，指定 AS 号码，指定邻居 (EBGP 或 IBGP) 并注入 BGP 路由，如下是一个简单的例子\n//开启BGP进程，指定AS号Router(config)#router bgp as-number  //注入BGP路由Router(config-router)#network network-number [mask network-mask] //指定EBGP或IBGP(as-number 决定)Router(config-router)#neighbor ip-address remote-as as-number 下图是一个简单的例子：\n\n\nBGP 配置\n\n除此以外，还有一些检查 BGP 工作情况的命令如下所示\n\n\n检查 BGP 配置\n\n","categories":["杂"],"tags":["计算机网络"]},{"title":"计算广告笔记 (3)-- 受众定向","url":"/2017/04/28/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(3)--%E5%8F%97%E4%BC%97%E5%AE%9A%E5%90%91/","content":"这一讲主要介绍了受众定向的概念和若干种定向的方法。\n受众定向概念\n受众定向是目前广告系统的核心部分，要做的就是根据人群划分进行广告售卖和优化。\n受众定向可以认为是为\nAUC（Ad，User，Context）打标签的过程，上下文标签可以认为是即时受众标签，如下所示是 AUC 上的标签类别\n\n\n\nAUC 上的标签系统\n\n标签的两大主要作用 1.\n建立面向广告主的流量售卖体系，注意这些特征需要有具体的意义才能展现给广告主，一般通过事先定义而不是通过文本聚类方法获取\n2. 为各估计模块 (如 CTR 预测) 提供原始特征\n下图的定向方式从右到左效果逐渐变好，从上到下表示不同阶段的定向方式；\n\n\n定向方式\n\n上图中的 \\(f(u)\\), \\(f(c)\\), \\(f(a,u)\\) 含义如下：\n\\(f(u)\\)：用户标签，即根据用户历史行为给用户打上的标签\n\\(f(c)\\)：上下文标签，即根据用户当前的访问行为得到的即时标签\n\\(f(a,u)\\)：定制化标签，也是一种用户标签，但是是针对某一广告主而言\n除了上面提到的三种标签还有一中标签 \\(f(a)\\)\n表示广告标签，便于与上下文标签或用户标签做匹配。广告标签常常有两种选择\n1）直接将广告投放中的广告主、广告计划、广告组、关键词等直接作为标签\n2）用人工的方式归类\n除此之外，各种定向方式中的数字表示该方式有利于某个个阶段需要遵循的原则，各个阶段及其需要遵循原则如下所示\n\n\n各阶段主要原则\n\n各种定向方式含义如下所示：\n重定向：如果用户曾经访问过广告主的网站，就会给用户推该广告主的广告。（品牌广告较为常用）\n上下文：用户目前浏览的网页内容相关的广告\n行为：根据用户的历史行为进行推荐\n网站 / 频道：根据网站的属性投放与该网站内容相关的某一领域的广告（如汽车广告）\nHyper-local：将定位做得更细（如定位到某条街道的小饭馆，一般在移动广告较容易实现）\nLook-alike：一般来说重定向的量比较少，该方式可以找到与广告主提供的种子用户相似的用户进行广告投放\n人口属性：人口，性别，教育水平，收入水平等，主要给广告商看，效果不是很好\n地域：主要给广告商看，效果不是特别好\n下面介绍一下 audience targeting 在业界的一种商业模式\nAudience Science 是一个做 audience\ntargeting 的第三方发公司，其核心业务有两个：\n\n主要提供面向 publisher（如 NewYork\nTimes）的数据加工服务，从 publisher 提供的数据中提取出用户标签供 publisher\n使用\n直接运营 ad\nnetwork，并帮助广告主进行 campaign 管理和优化；该过程中会通过上面提取出来的用户标签优化效果，同时使用标签创造的营收按照一定比例跟 publisher 分成\n\n行为定向\n根据用户的历史行为给用户打上标签 (上面提到的 \\(f(u)\\))，下面是九种重要原始行为 (按信息强度排序，往往强度越大，量越少)\n\nTransaction, 就是用户购买行为\n Pre-transaction, 用户购买前的一些行为，如商品浏览、加入购物车等\n Paid search click, 搜索广告中的点击行为\n Ad click, 广告的点击行为\n Search click, 搜索引擎上的点击行为\n Search, 搜索引擎上的搜索行为\n Share, 分享行为，如微博等\n Page View,\n浏览页面的行为，注意这个页面不是上面搜索出来的页面，而是用户在某个站点中只能看到的页面（如贴吧），量大但是效果一般\n Ad view，看某个广告的次数，但是带来的效果往往是负面的\n\n行为定向计算的一种方式如下 (\\(t^{(i)}(u)\\) 表示用户 \\(u\\) 在标签 \\(i\\) 上的强度)\n\n\n定向行为计算\n\n上面的方式比较简单，但是在海量数据中首先要做的是 shallow 的挖掘。\n行为定向的还有其他一些问题：如 session log 和 long-term 行为定向\nSession log\nsession log 关系到怎么从日志文件中提取出所需数据，有以下两点建议\n\n将各种行为日志整理成以用户 ID 为 key 的形式，完成作弊和无效行为标注，作为各数据处理模块的输入源\n可以将 targeting 变成局部计算，大大方便整个流程\n\nLong-term 行为定向\nLong-term\n行为定向指的是如何从用户的长期行为中提取出用户的标签，常用的有两种方法：滑动窗口方式和时间衰减方式。\n滑动窗口方式：直接将前面 \\(T\\)\n天的行为标签进行相加。下面的公式中 \\(f\\) 为 long-term 标签，下标为日期\n\\[\\begin{align} f_d^{(i)}(u) =\n\\sum_{j=0}^T t_{d-j}^{(i)}(u) \\end{align}\\]\n\n\n滑动窗口方式\n\n时间衰减方式：按照时间衰减方式，对前 n 天的标签进行相加，时间越长，权重越小。这种方式空间复杂度低，仅需昨天的\n\\(f\\) 和今天的 \\(t\\)\n\\[\\begin{align} f_d^{(i)}(u) =\nt_{d}^{(i)}(u) + \\alpha f_{d-1}^{(i)}(u) \\end{align}\\]\n\n\n指数衰减方式\n\n实际中上面两种方法的效果差异不大，但是时间衰减方式的计算代价较小。\\(T\\) 的选择与实际商品相关，如汽车的 \\(T\\) 往往较大，而运动鞋的 \\(T\\)\n往往不大，目前这个值主要是根据经验来设。\n如何评判为用户打的标签？\n受众定向评测可以借助 Reach/CTR 曲线，该曲线如下所示，横轴表示 reach 到的用户，纵轴表示广告点击率\n\n\nReach/CTR 曲线\n\n上图从左到右阈值设置逐渐变小，设为 0 时，可以 reach 到所有人，但是这个时候就相当于无定向投放了\n注意拐点以及该曲线是否服从一个递减的趋势；如果不是递减，可能标签没意义，而拐点可以知道设置在哪一个值能够将有更强的购买倾向的人群与其他人群分开。\n上下文定向\n上下文定向指的是根据用户的访问内容给用户打标签 (上面提到的 \\(f(c)\\))，这样的定向中有一些根据广告请求中的参数经过简单运算就可以得到，如地域定向，频道 / URL\n定向、操作系统定向等。另外一类则是根据上下文页面的一些特征标签，如关键词、主题、分类等进行定向，下面重点讨论这种上下文定向方式。\n上下文定向（打标签）主要有以下几种思路：\n1）用规则将页面归类到一些频道或主题分类，如将 auto.sohu.com\n归类到 “汽车” 的分类中 2）提取页面的关键词\n3）提取页面的入链锚文本中的关键词，这需要一个全网的爬虫作支持\n4）提取网页流量来源中的搜索关键词，这种方法除了页面内容，也需要页面访问的日志数据作支持\n5）用主题模型将页面内容映射到语义空间的一组主题上\n半在线抓取系统\n确定了对上下文页面打标签的方法后，在在线广告投放时，页面标签系统需要某个查询\nurl\n返回其对应的标签。在广告系统中，可以通过半在线 (Near-line) 上下文定向系统实现这个事情，如下就是一个\nNear-line 上下文定向系统\n\n\nnear-line 上下文定向系统\n\n该系统用一个缓存 (Redis) 来保存每个 URL\n对应的标签，当在线广告请求到达的时候，执行如下操作\n\n如果请求的上下文 URL 在缓存中存在，直接返回其对应的标签\n如果 URL\n在缓存中不存在，为了广告请求能够及时得到处理，立刻返回空的标签集合，同时向后台的抓取队列中加入此\nURL 进行抓取和存储\n考虑到页面的内容会不定期更新，可以设置缓存合适的 TTL 以自动更新 URL\n对应的标签\n\n步骤 2\n中能够返回空标签的原因是在广告系统中，某一次展示时标签的缺失带来的影响是可以忍受的，因为对于广告系统而言，这部分只是起到一个锦上添花的作用。\n主题模型\n除了直接提取页面内容的关键词作为页面的特征以外，还可以通过主题模型（Topic\nmodels）这一类模型对文本进行聚类得到文本的主题分布情况。\n主题模型分两大类：有监督和无监督的。有监督指的是预先定义好主题的集合，用监督学习的方法将文档映射到这一集合的元素上；无监督指的是不预先定义主题集合，而是控制主题的总个数或聚类程度，用非监督的方法自动学习出主题集合以及文档到这些主题的映射函数。\n广告中的主题挖掘有两种用途\n\n用于广告效果优化的特征提取\n用于售卖给广告主的标签体系\n\n对于第一种用途，\n用有监督或非监督的方法都可以；对于第二种用途，应该优先考虑采用监督学习的方法，因为这样可以预先定义好对广告主有意义而且可解释的标签体系。下面先介绍非监督方法，再介绍监督方法。\n非监督方法\n非监督的主题模型的发展经历了 LSA -&gt; PLSA -&gt;\nLDA 的过程，下面简单介绍这三种模型。\nLSA (Latent Semantic Analysis) 有时也叫 LSI (Latent Semantic\nIndexing)，这种方法实际上是将 SVD 分解应用到了 “文本 - 单词”\n矩阵中，即\n\\[\\begin{align} X \\approx U \\Sigma V^T\n\\end{align}\\]\n\\(X\\)\n矩阵中的值有多种选择：0-1，出现次数，TF-IDF 值。则 \\(U\\)\n矩阵的一行对应的就是一篇文本在各个主题上的分布， \\(V\\)\n矩阵每行对应的就是一个单词在各个主题上的分布，而选择奇异值的个数则决定了隐含主题的个数，也就是代表文本或词语的向量的维度。通过比较向量间的余弦相似性，便可比较文本或单词间的相似性。\n这样的方法虽然直观，但是有几个问题，一是分解后矩阵 \\(U\\)、\\(V\\)\n中可能存在着负值，二是这些数值在概率上没有意义。为了解决这些问题，便提出了 PLSA。\nPLSA (Probabilistic Latent Semantic\nAnalysis) 可以说是概率化了的 LSA，但是采用的方法与 LSA 完全不同，PLSA\n没有涉及到 SVD，而是采用混合模型的做法。\nPLSA\n方法假设文本包含多个主题，这些主题服从多项式分布，而每个主题下的有多个词，这些词也服从多项式分布。假设有\nM 篇文档，每篇文档有 N 个词，则生成这 M\n篇文档的过程通过有向图模型表示如下\n\n\nPLSA\n\n在有向图模型中，灰色的点表示能够观察到的点，其他白色的点表示需要求解的点，而最后需要求解的点是没有入度的点，其他的有入度和出度的点会被积分积掉，框及其框内符号表示框里面的内容重复若干次\n而上图中 \\(d\\) 表示文本，\\(c\\) 表示主题， \\(w\\) 表示词语，且文本 \\(d\\) 中生成词语 \\(w_i\\) 的概率是\n\\[\\begin{align} P(w_i|d) = \\sum_c P(c|d)\nP(w_i|c) \\end{align}\\]\n其中 \\(P(c|d)、\nP(w|c)\\) 均服从多项式分布\n则整篇文本生成的概率为\n\\[\\begin{align} P(d) = \\prod_i\nP(w_i|d)  \\end{align}\\]\n这个模型跟混合高斯模型（mixture of\nGaussian）非常相似，都是融合了多个指数族分布的模型，这一类模型也可以称为混合模型，而求解这一类的问题的方法便是\nEM 算法，这里不详细展开。除此之外，假如将上面 PLSA\n中文本主题服从的多项式分布改为伽马分布，将主题下的词语服从的多项式分布改为泊松分布，那么\nPLSA 就变为了 GaP (Gamma-Poisson) 模型。\nLDA (Latent Dirichlet Allocation) 则是在 PLSA\n的基础上为其两个多项式分布加上了贝叶斯先验，先验选为 Dirichelet\n分布，原因是更多是数学上的便利性，因为 Dirichelet 是 multinational\n的共轭先验，容易求解。\n\n\nLDA\n\n上图中 \\(\\alpha\\), \\(\\beta\\) 表示参数为 \\(\\alpha\\), \\(\\beta\\)\n的狄利赫里分布，这两个分布分别是文本的主题概率分布和主题下词语的概率分布的先验分布。文本的主题概率分布的先验分布为\n\\[\\begin{align}  \\theta_i \\sim\nDir(\\alpha),~i=1, 2...M  \\end{align}\\]\n\\(\\theta_i\\) 表示第 \\(i\\) 篇文档的主题分布，而 \\(\\theta_{i,k}（k=1,2...K）\\) 表示第 \\(i\\) 篇文档中包含第 \\(k\\) 个主题的概率\n而主题下的词语的概率分布的先验分布为\n\\[\\begin{align}  \\phi _k \\sim\nDir(\\beta),~k=1,2...K  \\end{align}\\]\n\\(\\phi_k\\) 表示第 \\(k\\) 个主题下的词语分布，而 \\(\\phi_{k,j}（j=1,2...V）\\) 表示第 \\(k\\) 个主题中包含第 \\(j\\) 个词的概率，\\(V\\) 为语料库的词表的大小\n确认先验分布后，文档中的主题分布以及主题下的词语分布均服从多项式分布，与\nPLSA 相同，求解 LDA 得思路是先求解出其最终的联合概率分布，然后通过 Gibbs\nSampling 收敛到该概率。\n经验贝叶斯 (Empirical\nBayes)\n如下图模型，如何确定 hyperparameter \\(\\eta\\)?\n\n\n经验贝叶斯\n\n用 Empirical Bayes 估计的解为：\\[\\widehat\n\\eta = arg \\max_{\\eta} \\int\n\\prod_{k=1}^{K}p(D_k|\\theta_k)p(\\theta_k|\\eta)d\\theta_k\\]\n当 \\(p(x|\\theta)\\)\n为指数族分布，\\(p(\\theta|\\eta)\\)\n为其共轭先验时，可用 EM 求解，其中 E-step 为 Bayesian inference 过程，由 \\(\\eta^{old}\\) 得到后验参数 \\(\\widetilde \\eta_k^{old}\\) ,\n而 M-step 为:\\[(\\theta,\nln[g(\\theta)])_{\\eta^{new}} = \\frac{1}{K}\\sum_{k=1}^{K}(\\theta,\nln(g(\\theta)))_{\\widetilde \\eta_k^{old}}\\]\n从经验贝叶斯看 LDA\nLDA 可以视为 PLSI 的经验贝叶斯版本，由于 PLSI 不是指数族分布，而是其混合分布，因此其贝叶斯版本不能使用前面的 EM 算法。工程上常用的求解方法有两种：Deterministic\ninference 和 Probabilistic inference\nDeterministic inference：\n可用变分近似，假设 z 和 θ 的后验分布独立迭代求解过程与 EM 非常相似，称为 VBEM，但是在大多数问题上无法保证收敛到局部最优\nProbabilistic inference:\n可用 Gibbs-sampling (Markov-chain Monte-Carlo, MCMC\n的一种)，以概率 1 收敛到局部最优值；还有一种方法是 Collapsed\nGibbs-sampling\nTopic model 的并行化\n\nEM 及 VBEM 的并行化较为简单\n\nE-step (mapper): 可以方便地并行计算\n M-step (reducer): 累加 E-step 各部分统计量后更新模型\n将更新后的模型分发到新的 E-step 各个计算服务器上\n\n AD-LDA: Gibbs Sampling 的并行化\n\nMapper: 在部分 data 上分别进行 Gibbs sampling\nReducer: 全局 Update\n\n\n\\[\\begin{align} n_{i,j} \\leftarrow\nn_{i,j}+\\sum_p(n_{i,j,p} - n_{i,j}),~n_{i,j,p}\\leftarrow n_{i,j}\n\\end{align}\\]\n文档的 Topic\nmodel 抽取可以认为是一个大量 (而非海量) 数据运算，采用类 MPI 架构的分布式计算架构 (例如 spark) 会比\nMapReduce 效率更高\n虽然 LDA 能够聚类，但是 supervisord learning 对标签体系更有意义。\n数据加工和交易\n精准的广告业务是什么？下面的图将数据的加工过程类比于石油的加工和提炼的过程，在这个过程中，实际上与媒体的关系已经不大了。\n\n\n精准广告业务\n\n精准广告业务的若干值得探讨的观点\n\n越精准的广告，给市场带来的价值越大\n媒体利益与广告主利益是相博弈的关系\n精准投放加上大数据可以显著提高营收\n人群覆盖率较低的数据来源是不需要的（长尾？）\n不同的广告产品应该采用不同的投放机\n\n有价值的数据\n下面列出了一些在广告系统中有价值的数据\n\n用户标识\n\n除上下文和地域外各种定向的基础，需要长期积累和不断建设\n可以通过多家第三方 ID 绑定不断优化\n\n用户行为\n\n业界公认有效行为数据（按有效性排序）\n交易，预交易，搜索广告点击，广告点击，搜索，搜索点击，网页浏览，分享，广告浏览\n需去除网络热点话题带来的偏差\n越靠近 demand 的行为对转化越有贡献\n越主动的行为越有效\n\n广告商 (Demand) 数据\n\n简单的 cookie 植入可以用于 retargeting。\n对接广告商种子人群可以做 look-alike，提高覆盖率。\n\n用户属性和精确地理位置\n\n非媒体广告网络很难获取，需通过第三方数据对接。\n移动互联和 HTML5 为获得地理位置提供了便利性。\n\n社交网络\n\n朋友关系为用户兴趣和属性的平滑提供了机会\n实名社交网络的人口属性信息相对准确\n\n\n数据管理平台 (Data\nManagement Platform)\n\n目的:\n\n为网站提供数据加工和对外交易能力 (如 Audience Science)\n 加工跨媒体用户标签，在交易市场中售卖\n是否应直接从事广告交易存在争议\n\n关键特征:\n\n定制化用户划分\n统一的对外数据接口：demand 端提供给 supply 端\n\n代表:\n\nBluekai, AudienceScience\n\n\nDMP 的系统架构示意图如下所示\n\n\nDMP 架构\n\nDMP 主要是 Data highway 部分，主要完成两个工作： 1. 挖掘出各个用户的标签\n2. 利用挖掘出来的用户的标签，售卖或使用\n这里介绍一个 Data Highway 的工具：Scribe\n\n大规模分布式日志收集系统，可以准实时收集大量日志到\nHDFS，利用 Thrift 实现底层服务\n类似工具: Flume, Chukwa\n\n\n\nscribe\n\n这个工具在 Facebook 经过实践，验证了其面对大规模数据时的可靠性。\n下面介绍一下 audience targeting 在业界的一种商业模式\n上面提到了 Bluekai 这个公司，其核心业务主要有以下两个：\n\n为中小网站主提供数据加工和变现的方式\n通过汇聚众多中小网站用户资料和行为数据，加工成受众定向标签，通过 Data\nexchange 对外售卖\n\nBluekai\n提供大量细分类别、开放体系上的标签，如 “对宝洁洗发水感兴趣的人”，“想去日本旅游的人”；靠数据出售变现，并与提供数据的网站主分成，但是并不直接运营广告业务；对于设计用户隐私的问题，用户可以看到自己的资料被谁使用，也可以选择 “捐给慈善机构”\n","categories":["计算广告"],"tags":["计算广告","机器学习"]},{"title":"计算机网络课程总结 --IPV6","url":"/2016/12/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--IPV6/","content":"本文主要讲述 IPV6 的一些基础知识，包括 IPV6\n的术语，地址，一些基本协议以及从 IPV4 过渡到 IPV6 的的一些技术等。\n\nIPV6 的出现最主要的原因是因为 IPV4 存在着一些不足，主要有\n\n地址不足\n端到端模式无法实施 (NAT)\nQos 和性能问题\n配置复杂\n安全问题\n路由表的膨胀\n移动性支持不足\n\n其中最主要的就是地址不足。\n在介绍 IPV6 的具体知识前，首先了解 IPV6 中的一些基本术语\n\n\nIPV6 基本属于\n\n从上图可知，IPV6 中有以下术语\n\n领节点：不跨越网段的两台主机\n局域网段：交换机某个端口下的网络\n链路：路由器某个端口下的网络\n子网 (site)：一个机构管辖的所有网络（不同于 IPV4 中的子网）\n\nIPV6 地址\nIPV4 地址\n讲述 IPV6 地址前，首先回顾一下 IPV4 地址，IPV4 的地址长度为 32 位，通过点分十进制表示，点分十进制的规则如下\n\n将 32 位 IP 地址分为 4 个 8 位组\n每个 8 位组之间用圆点 “.” 分隔\n每个 8 位组转化为对应的十进制数\n\n每个地址被划分为网路地址和主机地址两部分。地址可分为 ABCD 四类，每一类的地址范围如下图所示\n\n\nIPV4 地址\n\nIPV6 地址表示\nIPV6\n地址长度是 128 位，通过冒分 16 进制表示，冒分 16 进制将 128 位划分为 8 组，组与组之间通过冒号隔开，每一组 16 位，用四个 16 进制数表示，如下所示\n\n\nIPV6 地址\n\n但是这样表示出来的地址往往在 8 各组中有很多组是全 0 的，为了书写的简便，定义了以下的规则：\n1）忽略前导 0，\n2）忽略全 0，用双冒号代替，注意一个地址最多只能有一个双冒号\n如下是运用了这两条规则的一个简单例子\n\n\nIPV6 简便书写的规则\n\n除此之外，IPV6 中通过 /XX 表示地址前缀，替代了 IPV4 中子网掩码的概念。\nIPV6 中的地址根据其作用域可以分为单播地址 (Unicast\nAddress)，组播地址 (Multicast Address) 和任播地址 (Anycast\nAddress) 三大类。IPV6 中没有广播地址的概念。\n单播地址\nIPV6 单播地址用于唯一标识支持 IPV6 的设备上的接口，与 IPV4 类似，源\nIPV6 地址必须是单播地址。在 IPV6 中，单播地址分为六类\n\n链路本地地址 (Link Local Address)\n 环回地址 (Loopback Address)\n 未指定地址 (Unspecified Address)\n 站点本地地址 (Site Local Address)\nIPV4 兼容地址 (IPV4 Compatible Address)\n 可聚合全球单播地址 (Aggregate Global Unicast Address)\n\n链路本地地址\nIPV4 也有链路本地地址 (169.254.0.0/16), 但是在 IPV4 中，该地址的主要被用于地址自动配置：当主机不能从 DHCP 服务器处获得 IP 地址时，它会使用链路本地地址作为自己的地址。\n但是在 IPV6 中，链路本地地址用于与同一链路（同一个路由器端口下的网络）中的其他设备通信，而且也只能用于同一链路中，因此，路由器不会转发具有本地链路源地址或目的地址的数据包。需要注意的是，每个支持\nIPV6\n的网络接口均需要有链路本地地址。有了链路本地地址，就可以与与同一子网中的其他支持\nIPV6\n的设备通信，包括与默认网关（路由器）通信。所以假如只需要在局域网内通信，可以不需要全局单播地址。\n支持 IPV6 的主机在启动时自动创建 IPV6\n链路本地地址。链路本地地址格式为 FE80::/64 ，如下图所示\n\n\n链路本地地址\n\n链路本地地址前 64 位为 FE80:0:0:0，后 64 位为 EUI-64 地址，EUI-64 地址通过 mac 地址变换过来，变换规则如下所示\n\n\nEUI-64 地址变换过程\n\n注意上图中前 24 位 mac 地址中用 u\n标记的位表示要对原来的位取反。如下为一个简单的例子 \n链路本地地址的用途包括：\n1）主机使用本地路由器的链路本地地址作为默认网关 IPV6\n地址。 2）路由器使用链路本地地址交换动态路由协议消息。\n3）转发 IPV6\n数据包时，路由器的路由表使用链路本地地址确定下一跳路由器。\n环回地址\n与 IPV4 的 127.0.0.1 类似，在 IPV6 中对应为::1/128 或简单的\n::1。\n通过环回地址启用 ping 命令，从而测试本地主机的 IP\n协议是否正确安装。\n未指定地址\n未指定地址是全 0 地址，使用压缩格式表示为 ::/128 或\n:: 。\n未指定地址不能分配给接口，仅可作为 IPV6\n数据包的源地址。\n未指定地址的作用在于当设备尚无永久 IPV6\n地址时或数据包的源地址与目的地址不相关时，使用未指定地址用作源地址。\n站点本地地址\n站点本地地址是 IPV6 的私网地址，就像 IPV4 中的私网保留地址（10.0.0.0/8,\n172.16.0.0/12,192.168.0.0/16）一样。站点本地地址的前缀范围为\nFC00::/10。\n最初的 IPV6\n规范定义了用于特定用途的本地站点地址，但是因为站点本地地址规范中有很多不明之处，因此，IETF\n已经弃用本地站点地址，开始倾向于唯一本地地址。\nIPV4 兼容地址\n兼容 IPV4 的 IPV6 地址，用于过渡时期在 IPV4 网络上建立自动隧道，以传输 IPV6 数据包。其中高 96 位设为 0，后面的 32 位的 IPV4 地址，即地址为::IPV4.\n可聚合全球单播地址\n可聚合全球单播地址是由 IANA 分配的可在全球路由的公网 IP，地址由格式前缀\n001\n标识，设计目标是聚合或汇总该地址以便产生有效的路由基础结构\n\n\n可聚合全球单播地址\n\n从上图可知，ISP 商分配的前缀位为前 48 位，Site 部分则是由 ISP 下一级的组织机构用于划分子网，最后是 64 位的接口 ID。\n接口 ID 的产生方式有三种 1. EUI-64 (前面讲述过，通过 mac 生成) 2. 随机生成\n3. 手工配置\n组播地址\nIPV6 中没有广播的概念，IPV6 中通过组播代替广播。\nIPV6 中组播地址前八位均为 1，因此组播地址总是以 ff 开头，具体的组播地址格式如下\n\n\nIPV6 组播格式\n\n其中\n\nFlags: 用来表示 permanent (0000) 或 transient (0001) 组播组\n Scope: 表示组播组的范围 &gt;0：预留 1：节点本地范围\n2：链路本地范围 5：站点本地范围\n Group ID: 组播组 ID\n\n一些众所周知的组播地址如下图所示\n\n\n众所周知的组播地址\n\n除此之外还有有一种特殊的组播地址：Solicited-node 节点地址（被请求节点组播地址），主要用于重复地址检测和获取邻居节点的链路层地址。\n地址构成为\n前 104 位：FF02::1:FF/104（64 个零）\n后 24 位：单播地址的后 24 位\n凡是单播地址后 24 位相同的接口会自动加入相应的请求节点组播组，如下为一个实例\n\n\n请求组播组地址实例\n\n关于组播更详细的信息可参考这篇文章。\n任播地址\n在 IP 网络上通过一个任播地址标识一组提供特定服务的主机，同时服务访问方并不关心提供服务的具体是哪一台主机 (比如 DNS 或者镜像服务)，访问该地址的报文可以被 IP 网络路由到这一组目标中的任何一台主机上，一般来说，目标地址为任播地址的数据报将发送给最近的一个服务主机。\n综合上面提到的地址，可以知道一个接口上可以具有的 IPV6 地址如下所示\n\n\n\n地址类型\n具体地址\n\n\n\n\n链路本地地址\n FE80::/10\n\n\n 环回地址\n::1/128\n\n\n 所有节点组播地址\n FF01::1,FF02::1\n\n\n 可聚合全球单播地址\n 2000::/3\n\n\n 被请求节点组播地址\n FF02::1:FF00:/104\n\n\n 主机所属组播地址\n FF00::/8\n\n\n\nIPV6 地址配置\nIPV6 中地址配置方式有三种 1. 手工配置 2. 无状态地址自动配置（ND 协议）\n3. 有状态地址自动配置（DHCPv6）\n手动配置一般不常用，各系统均有自己的配置方式，这里不详细展开。下面主要讨论自动配置的两种方式。\n无状态地址自动配置（ND 协议）\n无状态地址自动配置指的是无须任何配置即可和外界通信，达到了真正的即插即用。\n无状态地址自动配置通过 ND (Neighbor\nDiscovery, 邻居发现) 协议实现，主要包括如何自动获取地址以及实现重复地址检测 (DAD)\nRS 和 RA\nND 协议中有两种重要的消息：RS 和 RA。其中，RS (Router\nSolicitation) 由主机发出，作用是促使路由器发送 RA (Router\nadvertisment) 消息，RA 消息中包含了路由器前缀等信息，主机会利用路由器的前缀加上自己的 EUI-64 地址作为自己的 IPV6 地址。这两种消息都是以 ICMPv6 报文的形式出现，也是 5 种 ND 协议消息中的两种。具体过程如下图所示\n\n\nRS 和 RA 消息\n\n注意 RS 消息的目的地址是 FF02::2, 也就是主机先整个链路的路由器请求 RA 消息，RA 消息的目的地址是 FF01::1，也就是路由器向整个链路的主机发送 RS 消息。\n因此为了避免 RS 泛滥，节点启动时最多只能发送 3 个 RS，而路由器也会主动周期性地发送 RA（默认值 200 秒）。主机在收到路由器的 RA 之后，自动设置默认路由器，建立默认路由器列表、前缀列表及其它参数。\n需要注意的是，自动配置的 IPV6 地址在系统中有一个生存周期，跟优先时间和有效时间有关，对应着以下 4 种状态\n\n\nIPV6 地址生存时间\n\n在使用的时候需要遵循以下规则\n1. 在 Preferred Lifetime 周期内的前缀生成的地址，任何上层应用都可不受限制地使用\n2. 在超过 Preferred Lifetime\n但未超过 Valid Lifetime 周期内的前缀生成的地址，正在使用该地址的上层应用可继续使用，但任何新的上层应用不能使用这个地址\n3. 在超过 Valid Lifetime 周期内的前缀构造的地址，任何上层应用都不能使用该地址\n一个链路本地地址的优先时间和有效时间是无限的，即永不超时！\nNS 和 NA\n在自动配置地址后需要检测该地址在链路上是否与其他地址重复 (链路上的主机的 mac 地址可能重复，从而 EUI-64 重复)，该过程称为 DAD (Duplicate\nAddress\nDetection), 所有的 IPV6 单播地址，不管是自动配置还是手动配置，都必须要通过 DAD。\n结合上面地址的生存周期，一个地址在分配给一个接口之后且通过重复地址检测之前称为 tentative 地址，即试验地址。\nDAD 机制是 ND 协议的一部分，因此通过 ND 协议中的 NS/NA 两种消息实现。DAD 的基本流程:\n1. 节点组播发送 NS (Neighbor Solicitation) 消息 2. 如果收到 NA (Neighbor\nAdvertisment) 消息，就证明地址重复 3.\n如果尝试若干次发送请求，都没有收到邻居通告，即可启用该地址\n过程如下所示\n\n\nNS 和 NA\n\n注意上面的 NS 消息的目的地址是被请求节点的组播地址，就是单播地址最后 24 位相同的主机都会加入的一个组播。而 NA 消息的目的地址是 FF02::1, 原因是 NS 消息中的源地址是未指定地址，发出 NA 的主机并不知道 NS 是由那一台主机发出的。\n当其他主机收到 NS 后，会有以下两种情形\n1）NS 接收者如果发现其中的目标地址对它而言是 tentative 的，则主动放弃使用这个地址；\n2）NS 接收者如果发现其中的目标地址是一个它正在使用的地址，则发送 NA 消息，请求发起者\n将放弃使用这个试验地址\n结合上面的 RS 和 RA，4 中 ND 消息交互入下\n\n\n四种 ND 消息交互\n\n前缀重新编制\n前缀重新编制允许网络从以前的前缀平稳地过渡到新的前缀，提供对用户透明的网络重新编址能力。\n在前缀重新编址时，路由器会继续通告当前前缀，只是优先时间和有效时间被减小到接近 0，同时，路由器开始通告新的前缀，这样，链路中至少有两个前缀共存。\n节点收到优先时间和有效时间被减小到接近 0 的 RA 时，会发现当前前缀的生命周期较短，停止使用；同时开始用新的前缀配置接口，并进行 DAD，通过后，获得新的地址使用。\n在转换期间，节点有两个单播地址使用，旧的地址基于旧的前缀，用以维持以前已经建立的连接；新的地址，基于新的前缀，用来建立新的连接。当旧的前缀的有效时间递减为 0 时，旧的前缀完全废止，此时，RA 中只包含新的前缀\n有状态地址自动配置（DHCPv6）\n有状态的自动配置依赖于 DHCPv6 实现，DHCPv6 是 DHCP 的升级版本。\n但是既然有了无状态自动配置，为什么还需要 DHCPv6 呢？主要有以下几个原因\n\n需要动态指定 DNS 服务时\n当不希望 MAC 地址成为 IPV6 地址的一部分时 (安全性）\n当需要良好的扩展性时\n\n在讲述 DHCPv6 前，先介绍一下原始的 DHCP 协议\nDHCP 的过程非常自然，主要包括下面三步\n1）DHCP 客户发送广播请求\n2）DHCP 服务器单播应答\n3）DHCP 客户接收应答，获取 IP 等信息\n具体过程使用四种 package 来实现这个过程\n1）DHCP 客户机在本地发送 DHCP DISCOVER 广播包；\n2）DHCP 服务器单播发送携带租约信息的 DHCP OFFER 包；\n3）DHCP 客户机确认租约信息并发送 DHCP REQUEST 广播包；\n4）DHCP 服务器单播送回 DHCP ACK 確认完成 IP 地址租用\nDHCP 有三种地址分配机制：\n1）自动分配方式－由 DHCP 分配一个永久的 IP\n2）手动分配方式－网络管理员预先安排分配，由 DHCP 转达\n3）动态分配方式－由 DHCP 分配具有租约期的 IP\n相比于 DHCP，DHCPv6 有了以下的改变 1.\n使用 UDP 来交换报文，端口 546/547（V4：67/68） 2.\n使用本地链路地址或其它机制获得的地址来发送和接收 DHCPv6 报文 3.\n没有了广播，客户机只需发送给保留的链路范围组播地址（FF02::1:2,all dhcp\nrelay agents and servers ） 4.\nDHCP 中使用的四种 package 在 DHCPv6 中依次变为 DHCP Solicit,\nDHCP Advertis, DHCP Request,\nDHCP Relay.\n其过程如下所示： \n当客户端已经记录了地址和其他配置信息，只需要 DNS server、NTP\nserver 等信息的时候，可以通过 DHCPv6 快速配置来快速获得所需地址。这个过程只需要两个消息的交互，过程如下所示：\n\nIPV6 报文\nIPV6 的报文主要由三部分组成 1.\n基本头（固定 40 字节，v4 不固定，为 20~60 字节范围内） 2.\n拓展头（可选，0~n 字节，v4 中没有） 3. 有效负载（即上层传输数据）\n基本头 (与 v4 对比)\nv4 与 v6 的报文对比如下所示\n\n\nv4 与 v6 报文对比\n\n且 v4 的报文的具体字段如下所示\n\n\nv4 报文具体字段\n\nv6 的报文的具体字段如下所示 \nv6 在 v4 字段的基础上有删除，也有修正的项，其中\n修正的项有\n\n服务类型→业务等级\n TTL→跳数限制\n数据总长度→净荷长度 (因为头部固定长度为 40 个字节)\n 地址 32 位→128 位\n协议→下一个头 (next header, 也是指示具体的协议的，不要被名称误导)\n\n删除的项有\n\n报头长\n标志和分段偏移量 (与分片相关)\n 报头校验和\n\n增加的项为\n\n流标记\n\n下图是一个抓取到的 ICMPv6 的具体 package \n拓展头\n除此之外，IPV6 将一些 IP 层的可选功能实现在上层封装和 IPV6 基本头部之后的扩展头部中，主要的扩展报头有：\n\n逐跳选项\n路由报头\n分段报头\n认证报头\n封装安全有效载荷报头\n目标选项\n\n每一种扩展报头其实也有自己特定的协议号，例如：路由报头为 43，AH 报头为 51。上图中抓到的 ICMPv6 包的协议号为 58 (0x3a 转为 10 进制)，其他一些常见的协议号如下所示\n\n\n\n协议号\n含义\n\n\n\n\n 0\n 逐跳扩展头\n\n\n 1\nICMPv4\n\n\n6\nTCP\n\n\n17\nUDP\n\n\n43\n 路由扩展头 (使数据分组经过指定的中间节点)\n\n\n58\nICMPv6\n\n\n89\nOSPF\n\n\n\n每一个基本报头和扩展报头的 NextHeader 字段标识后面紧接的内容，如下图所示\n\n\n拓展报头\n\nICMPv6 协议\nICMPv6 协议与回声请求、抑制消息、重定向、参数错误等功能相关，相关的命令为 ping、traceroute\nICMP 报文格式为 Type+Code+CheckSum, 其报文的格式以及在整个分组的位置如下所示\n\n\nICMPv6 分组格式和位置\n\nICMPv6 报文类型可分为两种\n(1) 差错报文 (Type=0~127)：通告 IPV6 分组传输中出现的错误。如目标不可达、数据包超长、超时、参数问题\n(2) 信息报文 (Type=128~255)：提供诊断和附加的主机功能。如回声请求 (Type=128) 和应答 (Type=129)，ND 协议等\nICMPv6 的三个实际应用为\n\nping\ntracert (HopLim 与 v4 中的 TTL 意思相同) &gt; 第一个请求：HopLim=1\n第一跳路由器收到，发送超时 (HopLim=0) 消息 得到第一跳路由器的信息\n第二个请求：HopLim=2 第二跳路由器收到，发送超时消息\n得到第二跳路由器的信息\n注意：HopLim 的最大值为 30，且为了让每一个请求都返回超时信息，通常设置 ICMPv6 报文的端口不可达。\nPMTU 发现：通过试探的方式发现路径允许的最大的 MTU, 如下为一个简单的过程。\n\n1) 源机向目的机发送 MTU=1500 字节的 IPV6 数据包\n2) 路由器 B 向源发送超长消息，指定 MTU=1400 字节\n3) 源机向目的机发送 MTU=1400 字节的 IPV6 数据包\n4) 路由器 C 向源发送超长消息，指定 MTU=1300 字节\n5) 源机向目的机发送 MTU=1300 字节的 IPV6 数据包\n6) 此后，该路径的 MTU 都使用 1300 字节\n\n\nPMTU 发现\n\nIPV6 路由\n在讲述 IPV6 的路由之前先回顾 IPV4 的路由，IPV4 的路由可以分为两大类：同一网络的路由和不同网络的路由。\n同一网络间主机的通信主要依赖于 ARP 协议，根据目标 IP 查询其对应的 mac 地址，然后两者便可通信，中间可以不经过路由器。\n不同网络间的通信则需要借助路由器，其过程如下所示 \n而对于 IPV6 的路由也可分为两种情况：\non-link：源机和目的机在同一链路的数据转发\noff-link：源机和目的机不在同链路的数据转发\n通过地址前缀判断源和目的是否在同一链路。\non-link\n处于同一链路的两条主机要通信就要知道对方的 mac 地址，在 IPV4 中通过 ARP 实现，ARP 是通过广播实现的，但是 IPV6 中并没有广播的概念。在 IPV6 中，通过 ND 协议来完成这个地址解析的工作。\nND 协议在我们介绍无状态地址自动配置的时候已经介绍过，但是除了无状态地址自动配置外，ND 协议还被用于地址解析和路由重定向。\nND 协议共有五种报文，五种报文都是以 ICMPv6 报文的形式出现，如下图所示\n\n\nNS 五种报文\n\n其地址解析过程如下：\n1）首先查找邻居缓存表（IPV6 nc），没有则进行地址解析\n（类似于查找 ARP 表）\n2）源主机发送组播 NS 报文，该报文的目的地址为目标 IPV6 地址所对应的被请求节点组播地址（Solicited-node），在其中也包含了自己的链路层地址\n3）目标主机收到 NS 报文后，就会了解到发送主机的 IPV6 地址和相应链路层地址；同时由于目标主机正在使用报文内的目标地址，所以会目标主机向源主机单播发一个邻接点公告报文（NA），该报文中包含自己的链路层地址。\n这里需要注意的是最后的 NA 报文是单播的，而在无状态地址自动配置中 NA 报文是组播，原因在于无状态地址自动配置的时候发送 NS 的主机还没有有效的地址，而这里的主机已经有了，只是要找到另外一台主机的 mac 地址而已。\n下图便是上面提的地址解析过程 \noff-link\n当源和目的不在同一链路的时候，需要考虑两个问题\n1. 主机发给哪个路由器？（主机 - 路由器）\n2. 路由器发给哪个路由器？（路由器 - 路由器）\n对于第一个问题，支持 IPV6 的主机有一个数据结构 DestinationCache，要发送数据到某个目的地址的时候，首先查询这个数据结构，如果查不到，就查路由表，让后将查到的信息记录在这个数据结构中。\n如果查询到的目的地址是 on-link 的，将目的地址本身加入 DC 表的 nexthop 域；如果目的地址是 off-link 的，将路由表中的下一跳加入 DC 的 nexthop 域。\n在这个过程中，会涉及到重定向的问题，重定向的作用其实就是给主机发送更好的路由。下图为一个简单的例子\n\n\n路由重定向\n\n当 PC1 要与 PC2 通信时，首先会先向 RT1\n查询，RT1 查询后发现 RT2 可以直接提供这个路由，于是 RT1 告诉 PC1 以后如果要与 PC2 通信直接找 RT2 就好了，效率会更高，这就是路由重定向。\n实际中，RT1 发现报文的出口和入口相同或者源地址跟报文下一跳同属一个网段，则发出重定向报文，重定向报文就是 ND 报文中的最后一种报文 (其他四种是 RS，RA，NS，NA)\n对于问题 2，也就是路由器与路由器之间的传输，就需要依靠路由表了。路由表中的路由根据是否需要人工配置而分为静态路由和动态路由，其中静态路由需要人工配置，动态路由则通过协议学习，在 IPV6 中的动态路由协议主要有 RIPng，OSPFv3 和 BGP4+。\nRIPng 保留了 RIP 的主要特点\n\n距离矢量采用跳数，16 跳为不可达\n工作机制不变；\n仍然采用水平分割、毒性逆转、触发更新等技术减少路由环的发生\n\n主要改变的地方：\n\n组播代替广播：主机不再受骚扰\n下一跳信息由单独的 R Table Entry 表示 （RTE）\n安全考虑：不单独设置验证，由 IPV6 本身保证\n只用于 IP 网络：不再支持其他网络协议\n\n同样，OSPFv3 保留了 OSPFv2 的主要工作机理\n\n采用链路状态数据库\n与邻接路由器同步\n DR 选举、SPF 算法、area 区域支持\n\n主要改变的地方\n\n地址信息从 LSA 中移除；（LSU 载荷中包含地址信息）\nRouterID 仍然采用 32 位，但不再跟地址有关\n重新定义了 LSA（如增加了 link-LSA、Intra-Area-Prefix-LAS 等，即 LSDB 的内容发生了变化）\n不再支持认证\n\n过渡技术\n从 IPV4 到 IPV6 的过渡被认为要经过三个阶段\n\n\nv4 到 v6 的过渡\n\n过渡的技术共分为三类\n\n双栈：网络设备上运行 IPV6/IPV4 双协议栈\n隧道：IPV6 网络上承载 IPV4 分组，或相反\n翻译 / 转换：地址、分组、端口的转换\n\n双栈\n一般基础设施设备，如路由器、交换机、公用服务器等，需要运行和支持双栈，非基础设置则可运行单协议或双协议。\n隧道\n根据创建方式可以大致分为两类 1）手动隧道：事前配置\n2）自动隧道：创建和拆除都依赖当前网络条件\n根据实际的网络环境又可以分为两类隧道\n\nIPV6 分组通过 IPV4 网络的隧道\n IPV4 分组通过 IPV6 网络的隧道\n\nIPV6 分组通过 IPV4 网络时，IPV6 分组作为数据部分搭载到 IPV4 分组中，在这种情形下，IPV4 分组头部的 protocol=41，如下图所示。\n\n\nv6 over v4\n\n根据 IPV4 网络位置的不同，在不同的位置建立\n\n\n隧道 1\n\n\n\n 隧道 2\n\n\n\n 隧道 3\n\n\n\n 隧道 4\n\nIPV4 分组通过 IPV6 网络的情况跟上面一样。\n翻译 / 转换\n翻译 / 转换就是从 IPV4 转换到 IPV6，或反过来，不仅发生在网络层，还有传输层和应用层。如下图所示是一个翻译 / 转换的例子。\n\n\n翻译 / 转换\n\n注意：当双栈和隧道都无法使用的时候，才使用翻译 / 转换技术；适用纯 IPV4 节点和纯 IPV6 节点间的通信。\n","categories":["杂"],"tags":["计算机网络"]},{"title":"计算机网络课程总结 -- 组播基础","url":"/2016/12/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--%E7%BB%84%E6%92%AD%E5%9F%BA%E7%A1%80/","content":"组播是介于单播和广播间的一种通信方式，单播是单台源机和单台目的机的通信，广播是单台源机和网络中所有其他主机的通信。而组播则是单台源机和网络中部分主机的通信。本文主要介绍组播中的一些基本概念。\n\n基本概念\nIP 组播是指在 IP 网络中将数据包以尽力传送（best-effort）的形式发送到网络中的某个确定节点子集，这个子集称为一个组播组。\n组播传输时，源主机只发送一份数据，这份数据的目的地址为组播组地址。\n组播组中的所有成员都可接收到同样的数据拷贝（通过路由器进行复制分发），并且只有组播组内的主机（成员，目标主机）可以接收该数据。\n\n从上图可知，由于每个分支只发送一份报文，所以网络规模（如用户数量）的增大不会额外增加网络的负担。\n故组播的优势为 1）降低了骨干上的网络流量\n2）降低了应用服务器的负担\n但是组播也存在着以下缺点： 1）传送不可靠！（尽力投递 /best effort）\n2）组播报文的复制开销使得路由器的资源消耗增加！\n3）可控可管性差，用户管理困难，存在安全问题（用户可随意加入某个组，无须密码）\n## 组播地址 组播组用 D 类 IP 地址标识，以 1110 开头。组播范围为\n224.0.0.0~239.255.255.255。\n各个地址范围段及其含义如下\n\n\n\n\n\n\n\n地址范围\n含义\n\n\n\n\n224.0.0.0～224.0.0.255\n预留的组播地址（永久组地址），地址 224.0.0.0 保留不做分配，其它地址供路由协议使用\n\n\n224.0.1.0～224.0.1.255\n公用组播地址，可以用于 Internet\n\n\n224.0.2.0～238.255.255.255\n用户可用的组播地址（临时组地址），全网范围内有效\n\n\n239.0.0.0～239.255.255.255\n本地管理组播地址，也称私人组播地址空间，仅在特定的本地范围内有效\n\n\n\n几个常用的组播地址及其含义如下所示\n\n\n\n地址\n含义\n\n\n\n\n224.0.0.1\n子网上所有主机 (包括路由器)\n\n\n224.0.0.2\n子网上所有路由器\n\n\n224.0.0.5\n所有 ospf 路由器\n\n\n224.2.0.0-224.2.255.255\n多媒体会议呼叫\n\n\n\n除了 IP 地址，组播中的 mac 地址也有特殊的规定，一个组播 mac 地址通过映射对应一个组播 ip 地址，其映射规则为：将 IP 组播地址的低 23 位代替以太网地址 01.00.5e.00.00.00(16 进制) 中的低 23 位。，也就是说组播的 mac 地址一定会以 01:00:5e 开头。\n这种映射方式可能会带来不同的组播 IP 地址映射成相同的 mac\n地址，给定一个组播 IP 地址，还有几个组播 IP 地址映射成的 mac 地址与其相同？\n除去低位相同的 23 位以及高位的 1110, 还剩下 5 位，因此还有\n2^5-1=31\n个地址会与给定的组播 IP 地址映射成相同的 mac 地址，也就是说有 32:1 的组播 IP 地址的 mac 地址重叠。\n组播转发树\n组播转发树指的是数据从源主机到接受者的传输路径。组播转发树主要有两类：有源树 (SPT) 和共享树 (RPT)。\n有源树\n有源树中源主机构成了源树的根，数据传输的路径构成了有源树的分支，有源树又称为生成树或最短路径树 (沿着最短路径传输)。下图中箭头便是有源树\n\n共享树\n共享树是各个组播组共享的传输路径，如下图所示\n\n有源树和共享树均是无回路的树，且均在分支处复制数据包，但是有源树能够提供一个最优的路径，而共享树路径可能不是最优的，相应的代价是有源树会占用较多的内存。\n组播路由\n组播路由和单播路由是相反的，单播路由关心数据报文要到哪里去，组播路由关心数据报文从哪里来。\n组播路由使用 “反向路径转发” 机制 (RPF, Reverse Path\nForwarding)，它是 IP 组播转发过程的基础，几乎所有的 IP 组播路由协议都利用 RPF 作为决定是否转发或丢弃从某个接口上接收到的组播信息包。\nRPF 的具体机制是路由器收到组播数据报文后，只有确认这个数据报文是从自己到源的出接口（单播）上到来的，才进行转发，否则丢弃报文。实际上就是在路由器中查询到组播报文源地址的路由，假如该路由的出口就是组播报文的入口，RPF 检查成功，转发数据包，否则丢弃数据包。\n如下为 RPF 的具体例子 RPF 检查失败 \nRPF 检查成功 \nTTL 阈值\nIP 组播包被路由器转发的时候，IP 头中的 TTL 值要减 1；路由器在传输的时候可以在每个接口设置一个 TTL 阈值，只有数据包的 TTL 值大于或等于接口的 TTL 阈值时，路由器才能在出接口转发该数据包。这个机制的目的是为了限制组播的范围。如下图所示\n\n组播协议\n组播协议分为主机与路由器之间的组成员关系协议和路由器与路由器之间的组播路由协议。\n组成员管理协议（主机 - 路由器）\nIGMP（internet group management protocol）是 IP\n协议簇中负责 IP 组播组成员管理的协议，用来在\nIP 主机和与其直接相邻的组播路由器之间建立、维护组播组成员关系，所有参与组播的主机必须支持 IGMP 协议。\n注意，IGMP 不包括\n组播路由器之间的组成员关系信息的传播与维护，这部分工作由各组播路由协议完成。\nIGMP 作为 TCP/IP 第三层的协议，被封装在 IP 数据包中进行传输。其报文格式如下所示\n\nIGMP 协议有三个版本，各个版本的功能和区别如下：\n\nIGMP\nv1：提供成员关系查询 / 成员关系报告两个基本功能。\nIGMP\nv2：增加了查询器选择 / 特定组查询 / 离开组消息及最大响应时间字段等扩展功能。\nIGMP\nv3：增加了对特定 (源，组) 的加入离开的支持，以提供对组播应用更好的支持\n\nIGMP v1\nIGMP v1 的报文格式如下所示\n\n上面的报文中，类型字段有两种取值： 1：成员关系查询 (路由器发出)\n2：成员关系报告 (主机发出)\n组地址只有在成员关系报告时才填入，注意这个地址不是发送 IGMP 数据包时用于寻址的地址，用于寻址的地址在 IP 报头中。\n查询报文是周期性发送的，默认为 60s 一次，过程如下所示\n\n上图出现了报告抑制的情况，这个机制主要是为了使得当前子网中对于每个组只需有一个成员响应成员关系查询，在上图中 H1 和 H2 属于同一组的，因此只需要一个成员响应成员报告。\n报告抑制的步骤如下 1) 主机收到 IGMP\n成员关系查询后，主机对已经加入的每个组播组启动一个倒数计时器，计时器初始化为一个给定时间（在 IGMPv1 中为固定值 10S）范围内的随机值。\n2) 当计时器计时值为 0 时，主机发送成员关系报告至与该计时器相关的组播组，以通知路由器本地网中有处于活动状态组播组接收者\n3) 主机在它的倒数计时器达到 0 之前收到其他主机发送的某一成员关系报告，那么它就停止与这个组播组相关的倒数计时器的计时，这样就抑制了主机到这一组播组的成员关系报告\n加入组播组：主机并不等待来自路由器的下一次成员关系查询，可主动向要加入的组播组发送成员关系报告表示加入\n离开组播组：IGMPv1 中没有定义离开机制，主机在任何时候可以默默地离开\n为了避免这种默默离开的机制导致路由器给一个空的组播组发送数据包，路由器设置组播组关联定时器（一般为 3 倍查询周期，3min），超时无组成员报告，则停止转发。\nIGMP v2\nIGMP v2 与 IGMP v1 的最大区别为： 1）支持特定组查询\n2）通过离开消息允许主机主动汇报他们的离开\nIGMP v2 的报文如下所示：\n\n上面的报文类型有以下四种：\n\n0x11：成员关系查询（与 IGMP v1 兼容）\n0x12：IGMPv1 成员关系报告\n 0x16：IGMPv2 成员关系报告\n 0x17：离开组（组播地址段为目标组播组地址）\n\n而成员关系查询消息又分为两类\n1）普通查询（组播地址段为零）\n2）特定组查询：直接对单个组查询（组播地址段为正在查询的组播组地址）\n在 IGMP\nv2 中组成员的离开并不是默默离开了，而是会主动发送离开的消息，然后路由器会进行特定组的查询已确认这个离开的主机是不是这个组播组最后的一个成员，有响应报文说明还有其他成员，否则这个成员就是最后的成员。\n上面的成员查询报文由查询路由器发出。在 IGMP v1\n中没有正式的选举规定，它依赖于路由协议，IGMP v2\n协议声明了正式的查询路由器选举过程：\n1）多访问网络上的每个路由器假设自己为查询器并发出查询\n2）IP 地址低（接口）的路由器被选为查询器\n3）非查询路由器设置定时器，当超时没有收到查询器的周期查询，认为查询器出事了，重新选举\n当 IGMP v1 和 IGMP v2\n混杂在一个子网的时候，两种协议的交互需要遵循某种规则，这些规则都是因为\nIGMP v1 并不能识别 IGMP v2 的报文。如\nIGMPv2 主机与 IGMPv1 路由器交互时，有以下规则：\n1）IGMPv1 路由器把 v2 报告看作无效的信息并且忽略它\n2）当 V1 路由器作为查询路由器时，V2 的主机必须发送 V1 成员报告。\nIGMPv2 路由器与 IGMPv1 主机交互时，有以下规则：\n1) V2 路由器的查询可被 V1 的主机所处理，只是忽略第二个八位组的信息，就是忽略特定组的查询，全认为是普通查询。\n2）v2 路由器必须忽略离开报告，否则后果很严重！因为 v2 路由器收到离开报文后会发出的特定查询，而特定组查询并不被 v1 主机理会，此时假如剩下的全是 IGMP\nv1 的主机，IGMP\nv2 的路由器收不到响应报文，会认为组播组没有成员从而不再发送数据到这个组播组。\n基于上面的原因，同一网段上的所有路由器必须运行同一版本的 IGMP 协议。缺省为 V2。但是假如网段上存在其它 IGMP\nv1 路由器，所有的路由器必须手工配置为运行 IGMP v1。\nIGMP v3\nIGMP v3 与 IGMP\nv2 的最大区别是允许主机只收到组播组内某个特定信源的传输，如下图所示\n\n三个版本的 IGMP 协议比较如下\n\n\n\n协议版本\n IGMP v1\nIGMP v2\nIGMPv3\n\n\n\n\n 查询器选举\n依靠上层路由协议\n自己选举\n自己选举\n\n\n离开方式\n默默离开\n主动发出离开报文\n主动发出离开报文\n\n\n指定组查询\n无\n有\n有\n\n\n接收组内指定源\n无\n无\n有\n\n\n\n路由协议（路由器 - 路由器）\n组播路由协议的类型主要有两种：密集模式（Dense-mode）和稀疏模式（Sparse-mode）\n\n密集模式（Dense-mode） 1）使用 “推”（Push）模型\n2）组播数据在整个网络的泛滥（Flood） 3）下游不想接收的话则剪枝（Prune）\n4）泛滥、剪枝、泛滥、剪枝… 周而复始 (通常 3 分钟折腾一次)\n 稀疏模式（Sparse-mode） 1）使用 “拉”（Pull）模型\n2）组播数据只发送到有需要的地方 3）有显式的加入（Join）过程\n\n目前主要有 4 种具体的组播路由协议 DVMRP，MOSPF，PIM-DM，PIM-SM。\nDVMRP 是第一个组播路由协议，一个较为古老，具有实验性质的协议，现已经不常使用。属于密集模式协议。\nDVMRP\n基于基于距离矢量，类似于 RIP，最大不能超过 32 跳，不支持共享树，不适合于大规模的网络。\nMOSPF 是对 OSPF 单播路由协议的扩展，在 OSPF 链路状态通告中包含组播信息，以此构建组播分发树。MOSOF 与单播路由协议相关，仅在 OSPF 网络内运行，适合在单路由域中使用。不支持共享树，且支持的厂家较少，市场鲜有使用。\n目前最常用的组播协议是 PIM（Protocol Independent\nMulticast，协议无关组播）。PIM 有以下特点：\n1）独立于单播协议，也就是支持所有的单播协议 2）扩散和剪枝机制\n3）无类\nPIM 又分为 PIM-DIM 和 PIM-SM 两种模式，对应于密集型和稀疏型的 PIM。\nPIM-DM\n该协议用 PUSH 方式，将组播流量周期性扩散到网络中所有设备，建立和维护 SPT (short\npath tree) （假设所有主机都需要接收组播数据）。\n主要步骤为以下三个： 1）周期性扩散 (泛洪，Flood)：为每个路由器创建 (S,G)\n2）剪枝 (Prune)：除去不需要组播数据的路径\n3）嫁接 (Graft)：迅速得到数据，而不用到下一周期\n泛洪和剪枝的过程如下所示 \n \n满足以下任一条件即可发送剪枝消息 1）信息到达 PIM-DM\n路由器的非 RPF 点对点接口；\n2）PIM-DM 路由器没有下游邻居，且所有叶网络上没有组成员；\n3）PIM-DM 路由器接口上所有的下游邻居已经通过了剪枝表决\n嫁接的目的是为了能够迅速得到数据，从而不用等到下一次的泛洪。过程如下图所示\n \n\nPIM-DM 中还有一种断言（Assert）机制。目的是为了避免出现组播流量重复和多份，如下图所示，BCD 都会收到重复的数据\n\n为了避免这种情况，断言机制过程如下\n1）当路由器从其组播 “出接口列表”(oiflist) 中的某个接口收到与其发送的组播数据相同的数据\n2）路由器发送 “PIM Assert” 消息 3）计算 distance 和\nmetric 值，谁到源的路由最优谁获胜；如果 distance 和\nmetric 相等，IP 地址大的获胜，输的就停止转发 (剪枝接口)\n对于上图运行断言机制后，假如 C 获胜，那么情况如下 \n原因是 B 经 Assert 断言成了 loser 之后，将自己的 loser 接口设为剪枝状态，并向 winner\nC 发送剪枝消息，D 的 RPF 接口也会收到该剪枝消息，发出 join 消息，否决。\nPIM-DM 的优点为 1）易于配置 2）实现机制简单（泛滥剪枝）\n缺点为 1）泛滥剪枝过程不够高效 2）复杂的 Assert 机制\n3）控制和数据平面混合：导致网络内部的所有路由器上都有 (S,\nG)，可能会导致非确定性的拓扑行为 4）不支持共享树\nPIM-DM 适用于 1）小规模的网络 2）组播源和接收者比较靠近\n3）源少，接收者多 4）数据流大且稳定\nPIM-SM\nPIM-SM 协议假设没有主机需要接收组播数据，除非它们明确地发出了请求。\n稀疏组播的特点为 1）组成员所在的网络数相对少 2）组成员跨越的区域太大\n3）带宽资源还没有富裕到可以忽略 DM 模式带来的消耗\n在 PIM-SM 中有个重要的概念：汇聚点，RP (Rendezvous\nPoint)，发送者和接收者在 RP 处进行汇聚，表现为\n1）发送者的第一跳路由器把发送者注册到 RP 上（报个到，挂个号）\n2）接收者的 DR（直连网络上的负责人）为接收者加入到共享树 (树根在 RP)\n接收者加入或离开组播组的行为表现为\n1) 加入: 接收者发送加入消息，逐跳上行到 RP，沿途路由器记录组播转发状态；\n2) 离开: 接收者不想要组播数据时，发送剪枝消息，逐跳上行到 RP，沿途路由器更新它的转发状态\n从上面接收者离开或加入的行为可以看出 SM 跟 DM 本质的差别：路由器转发状态通过组播消息的抵达而建立或更新。\n上面的过程中有几个关键问题， （1）如何知道 RP (RP 发现)？\n（2）如何让源组播数据到达 RP？\n（3）能否在接收者和源之间建立一个转发树，分担 RP 的负担？\n针对问题（1），采用自举路由器机制 (BSR) 来选出 RP，通常通过人工配置，将一组路由器配置为候选自举路由器 (C-BSR)，另一组路由器配置为候选汇集点（C-RP），通常建议这两组路由器是同样的路由器。C-RP\n会定期把候选汇集点通告消息（C-RP-Advs）以单址的形式发送到 C-BSR；汇集点通告消息是一种 PIM 消息，它包括通告\nC-RP 的地址、可选的组播组地址和一个掩码长度域（说明组的前缀）；C-BSR\n收集这些通告消息并产生相应的自举报文，自举报文也是一种 PIM 消息，它包括 C-RP 和相应的组前缀并由自举路由器以一跳一跳的形式发送到所有普通路由器。普通路由器通过接收自举报文便可知道 C-RP 的地址。\n针对问题（2），采用了源注册的机制，过程如下；\n1）源的 DR (执行注册的源第一跳路由器) 将组播数据封装进一个注册消息，单播到 RP；\n2）RP 打开注册消息，将组播数据在 RPT 上转发，发送 (S,G) 加入消息，沿途建立 (S,G) 状态\n3）当 RP 察觉到从源到 RP 的 SPT 树已经建立，RP 发送 “注册停止” 消息给源\n上面的过程图示如下所示；\n\n\n\n\n需要注意的是当源的 SPT 建立起来后，源的 DR 不会马上停止注册，而是等待收到 RP 的注册停止消息后才会停止，这时候空注册消息和沿着 SPT 的组播数据流并存。\n在通过组播传输数据的时候，数据的传输方向为\n源→SPT→RP→RPT，如下图所示\n\n从上面的传播路径可知，RP 可能会成为瓶颈，针对这个问题，也就是问题（3），提出了 SPT 切换的方法，其过程如下\n\n\n\n\n\n\n上面中\nSPT 切换的条件为：最后一跳路由器（和接收者直连的路由器）一旦发现某个特定的组播源的数据量超出了某个界限 (阈值)，马上向组播源发送（S，G）Join 消息。\n共享树剪枝的条件为：最后一跳路由器根据自己的状态表中的（*，G）和（S，G）的入接口情况来判断是否发送剪枝消息（剪共享树），触发条件是：在（S，G）的入接口上收到了相符合的组播数据\n（源树已经建立）\nPIM-SM 对于稀疏和密集应用都很高效，其优势为\n\n数据流仅沿 “加入” 的分支向下发送\n可以根据流量等条件动态地切换到源树\n与具体的单播路由协议无关\n域间组播路由的基础 (和 MBGP、MSDP 共同结合使用可以完成跨域的组播)\n\nPIM-SM 适用于 1）大规模的企业网络 2）接收者稀少\n3）几乎是任何网络的优选方案（目前 PIM-SM 占主流）\n源特定组播 (SSM)\n源特定组播（SSM：Source Specific\nMulticast）是一种区别于传统组播的新的业务模型，它使用组播组地址和组播源地址同时来标识一个组播会话，而不是像传统的组播服务那样只使用组播组地址来标识一个组播会话，由于源地址的加入，组地址在不同源地址之间可以重用。\nSSM 保留了传统 PIM-SM 模式中的主机显示加入组播组的高效性，但是没有\nPIM-SM 模式中的共享树和\nRP 的概念，SSM 直接建立由 (S,G) 标识的一个有源树，\n在 SSM\n中，主机主动发起对指定 (S,G) 的加入，由最后一跳路由器直接向源发送（S,G）加入消息。\nipv6 组播\nipv6 的组播与 ipv4 的组播非常类似，这里做简单介绍。\nipv6 的组播地址规定前 8 位均为 1, 也就是以 ff 开头，其他部分含义如下所示：\n\n\nFlags: 用来表示 permanent 或 transient 组播组\n Scope: 表示组播组的范围 &gt;0：预留 1：节点本地范围\n2：链路本地范围 5：站点本地范围\n Group ID: 组播组 ID\n\nipv6 中一些众所周知的组播地址以及与 ipv4 的对应关系如下：\n\nipv6 组播中的 mac 地址也是通过映射来的，映射规则为 33:33:+IPv6组播地址的后32位。同样也存在 mac 重复问题。\nipv6 的组播协议也分为组管理协议和路由协议。组成员管理协议为 MLD（Multicast\nlistener\nDiscovery，侦听发现协议）,MLD 几乎全盘继承了 IGMPv2 和 IGMPv3，更名为 MLDv1 和 MLDv2，用在路由器和 ip 主机之间。而路由协议依然是 PIM。\n","categories":["杂"],"tags":["计算机网络"]},{"title":"详解 Java 的 TCP 网络编程","url":"/2016/02/08/%E8%AF%A6%E8%A7%A3Java%E7%9A%84TCP%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","content":"网络编程简介\n网络通讯的方式有 TCP 和 UDP 两种，其中 TCP 方式的网络通讯是指在通讯的过程中保持连接，有点类似于打电话，只需要拨打一次号码 (建立一次网络连接)，就可以多次通话 (多次传输数据)。这样方式在实际的网络编程中，由于传输可靠，类似于打电话，如果甲给乙打电话，乙说没有听清楚让甲重复一遍，直到乙听清楚为止，实际的网络传输也是这样，如果发送的一方发送的数据接收方觉得有问题，则网络底层会自动要求发送方重发，直到接收方收到为止。\n\n在 Java 语言中，对于 TCP 方式的网络编程提供了良好的支持，在实际实现时，以 java.net.Socket 类代表客户端连接，以 java.net.ServerSocket 类代表服务器端连接。在进行网络编程时，底层网络通讯的细节已经实现了比较高的封装，所以在程序员实际编程时，只需要指定 IP 地址和端口号码就可以建立连接了。正是由于这种高度的封装，一方面简化了 Java 语言网络编程的难度，另外也使得使用 Java 语言进行网络编程时无法深入到网络的底层，所以使用 Java 语言进行网络底层系统编程很困难，具体点说，Java 语言无法实现底层的网络嗅探以及获得 IP 包结构等信息。但是由于 Java 语言的网络编程比较简单，所以还是获得了广泛的使用。\n在使用 TCP 方式进行网络编程时，需要按照前面介绍的网络编程的步骤进行，下面分别介绍一下在 Java 语言中客户端和服务器端的实现步骤。\n客户端\n在网络编程中客户端需要做的事情包括：建立连接 -&gt; 发送并接受数据 -&gt; 关闭连接。\n建立连接\n在客户端网络编程中，首先需要建立连接，在 Java\nAPI 中以 java.net.Socket 类的对象代表网络连接，所以建立客户端网络连接，也就是创建 Socket 类型的对象，该对象代表网络连接，示例如下：\nSocket socket1 = new Socket(“192.168.1.103”,10000);  Socket socket2 = new Socket(“www.sohu.com”,80);  ```  上面的代码中，**socket1实现的是连接到IP地址是192.168.1.103的计算机的10000号端口，而socket2实现的是连接到域名是www.sohu.com的计算机的80号端口，至于底层网络如何实现建立连接，对于程序员来说是完全透明的。**如果建立连接时，本机网络不通，或服务器端程序未开启，则会抛出异常。### 传输数据  连接一旦建立，则完成了客户端编程的第一步，紧接着的步骤就是按照“请求-响应”模型进行网络数据交换，在Java语言中，数据传输功能由Java IO实现，也就是说**需要发送的数据写入连接对象的输出流中，在发送完成以后从输入流中读取数据**即可。示例代码如下：```java  OutputStream os = socket1.getOutputStream();   //获得输出流  InputStream is = socket1.getInputStream();     //获得输入流  \n上面的代码中，分别从 socket1 这个连接对象获得了输出流和输入流对象，在整个网络编程中，后续的数据交换就变成了 IO 操作，也就是遵循 “请求 - 响应” 模型的规定，先向输出流中写入数据，这些数据会被系统发送出去，然后在从输入流中读取服务器端的反馈信息，这样就完成了一次数据交换过程，当然这个数据交换过程可以多次进行。\n这里获得的只是最基本的输出流和输入流对象，还可以根据前面学习到的 IO 知识，使用流的嵌套将这些获得到的基本流对象转换成需要的装饰流对象，从而方便数据的操作。\n关闭连接\n最后当数据交换完成以后，关闭网络连接，释放网络连接占用的系统端口和内存等资源，完成网络操作，示例代码如下：\nsocket1.close();  ```  ### 客户端简单案例  这就是最基本的网络编程功能介绍。下面是一个简单的网络客户端程序示例，该程序的作用是向服务器端发送一个字符串“Hello”，并将服务器端的反馈显示到控制台，数据交换只进行一次，当数据交换进行完成以后关闭网络连接，程序结束。实现的代码如下：```java  import java.io.*;  import java.net.*;  /**   * 简单的Socket客户端   * 功能为：发送字符串“Hello”到服务器端，并打印出服务器端的反馈   */  public class SimpleSocketClient {           public static void main(String[] args) {                     Socket socket = null;                     InputStream is = null;                     OutputStream os = null;                     //服务器端IP地址                     String serverIP = \"127.0.0.1\";                     //服务器端端口号                     int port = 10000;                     //发送内容                     String data = \"Hello\";                     try {                              //建立连接                              socket = new Socket(serverIP,port);                              //发送数据                              os = socket.getOutputStream();                              os.write(data.getBytes());                              //接收数据                              is = socket.getInputStream();                              byte[] b = new byte[1024];                              int n = is.read(b);                              //输出反馈数据                              System.out.println(\"服务器反馈：\" + new String(b,0,n));                     } catch (Exception e) {                              e.printStackTrace(); //打印异常信息                     }finally{                              try {                                       //关闭流和连接                                       is.close();                                       os.close();                                       socket.close();                              } catch (Exception e2) {}                     }           }  }  \n在该示例代码中建立了一个连接到 IP 地址为 127.0.0.1，端口号码为 10000 的 TCP 类型的网络连接，然后获得连接的输出流对象，将需要发送的字符串 “Hello” 转换为 byte 数组写入到输出流中，由系统自动完成将输出流中的数据发送出去，如果需要强制发送，可以调用输出流对象中的 flush 方法实现。在数据发送出去以后，从连接对象的输入流中读取服务器端的反馈信息，读取时可以使用 IO 中的各种读取方法进行读取，这里使用最简单的方法进行读取，从输入流中读取到的内容就是服务器端的反馈，并将读取到的内容在客户端的控制台进行输出，最后依次关闭打开的流对象和网络连接对象。\n这是一个简单的功能示例，在该示例中演示了 TCP 类型的网络客户端基本方法的使用，该代码只起演示目的，还无法达到实用的级别。\n服务器端\n介绍完一个简单的客户端编程的示例，下面接着介绍一下 TCP 类型的服务器端的编写。\n服务器端需要完成的事情包括：监听端口 -&gt; 获得客户端的连接\n-&gt;\n连接后传送数据 -&gt; 关闭连接。\n监听端口\n在服务器端程序编程中，由于服务器端实现的是被动等待连接，所以服务器端编程的第一个步骤是监听端口，也就是监听是否有客户端连接到达。实现服务器端监听的代码为：\nServerSocket ss = new ServerSocket(10000);  \n该代码实现的功能是监听当前计算机的 10000 号端口，如果在执行该代码时，10000 号端口已经被别的程序占用，那么将抛出异常。否则将实现监听。\n获得客户端的连接\n服务器端编程的第二个步骤是获得连接。该步骤的作用是当有客户端连接到达时，建立一个和客户端连接对应的 Socket 连接对象，从而释放客户端连接对于服务器端端口的占用。实现功能就像公司的前台一样，当一个客户到达公司时，会告诉前台我找某某某，然后前台就通知某某某，\n然后就可以继续接待其它客户了。通过获得连接，使得客户端的连接在服务器端获得了保持，另外使得服务器端的端口释放出来，可以继续等待其它的客户端连接。\n实现获得连接的代码是：\nSocket socket = ss.accept();  \n该代码实现的功能是获得当前连接到服务器端的客户端连接。需要说明的是 accept 和前面 IO 部分介绍的 read 方法一样，都是一个阻塞方法，也就是当无连接时，该方法将阻塞程序的执行，直到连接到达时才执行该行代码。另外获得的连接会在服务器端的该端口注册，这样以后就可以通过在服务器端的注册信息直接通信，而注册以后服务器端的端口就被释放出来，又可以继续接受其它的连接了。\n连接获得以后，后续的编程就和客户端的网络编程类似了，这里获得的 Socket 类型的连接就和客户端的网络连接一样了，只是服务器端需要首先读取发送过来的数据，然后进行逻辑处理以后再发送给客户端，也就是交换数据的顺序和客户端交换数据的步骤刚好相反。这部分的内容和客户端很类似，所以就不重复了，如果还不熟悉，可以参看下面的示例代码。\n关闭连接\n最后，在服务器端通信完成以后，关闭服务器端连接。实现的代码为：\nss.close();  \n简单例子\n这就是基本的 TCP 类型的服务器端编程步骤。下面以一个简单的 echo 服务实现为例子，介绍综合使用示例。echo 的意思就是 “回声”，echo 服务器端实现的功能就是将客户端发送的内容再原封不动的反馈给客户端。实现的代码如下：\nimport java.io.*;  import java.net.*;  /**   * echo服务器   * 功能：将客户端发送的内容反馈给客户端   */  public class SimpleSocketServer {           public static void main(String[] args) {                     ServerSocket serverSocket = null;                     Socket socket = null;                     OutputStream os = null;                     InputStream is = null;                     //监听端口号                     int port = 10000;                     try {                              //建立连接                              serverSocket = new ServerSocket(port);                              //获得连接                              socket = serverSocket.accept();                              //接收客户端发送内容                              is = socket.getInputStream();                              byte[] b = new byte[1024];                              int n = is.read(b);                              //输出                              System.out.println(\"客户端发送内容为：\" + new String(b,0,n));                              //向客户端发送反馈内容                              os = socket.getOutputStream();                              os.write(b, 0, n);                     } catch (Exception e) {                              e.printStackTrace();                     }finally{                              try{                                       //关闭流和连接                                       os.close();                                       is.close();                                       socket.close();                                       serverSocket.close();                              }catch(Exception e){}                     }           }  }  \n在该示例代码中建立了一个监听当前计算机 10000 号端口的服务器端 Socket 连接，然后获得客户端发送过来的连接，如果有连接到达时，读取连接中发送过来的内容，并将发送的内容在控制台进行输出，输出完成以后将客户端发送的内容再反馈给客户端。最后关闭流和连接对象，结束程序。\n这样，就以一个很简单的示例演示了 TCP 类型的网络编程在 Java 语言中的基本实现，这个示例只是演示了网络编程的基本步骤以及各个功能方法的基本使用，只是为网络编程打下了一个基础，下面将就几个问题来深入介绍网络编程深层次的一些知识。\n深入介绍网络编程\n为了一步一步的掌握网络编程，下面再研究网络编程中的两个基本问题，通过解决这两个问题将对网络编程的认识深入一层。\n如何复用 Socket 连接？\n在前面的示例中，客户端中建立了一次连接，只发送一次数据就关闭了，这就相当于拨打电话时，电话打通了只对话一次就关闭了，其实更加常用的应该是拨通一次电话以后多次对话，这就是复用客户端连接。\n那么如何实现建立一次连接，进行多次数据交换呢？其实很简单，建立连接以后，将数据交换的逻辑写到一个循环中就可以了。这样只要循环不结束则连接就不会被关闭。按照这种思路，可以改造一下上面的代码，让该程序可以在建立连接一次以后，发送三次数据，当然这里的次数也可以是多次，示例代码如下：\nimport java.io.*;  import java.net.*;  /**   * 复用连接的Socket客户端   * 功能为：发送字符串“Hello”到服务器端，并打印出服务器端的反馈   */  public class MulSocketClient {           public static void main(String[] args) {                     Socket socket = null;                     InputStream is = null;                     OutputStream os = null;                     //服务器端IP地址                     String serverIP = \"127.0.0.1\";                     //服务器端端口号                     int port = 10000;                     //发送内容                     String data[] ={\"First\",\"Second\",\"Third\"};                     try {                              //建立连接                              socket = new Socket(serverIP,port);                              //初始化流                              os = socket.getOutputStream();                              is = socket.getInputStream();                              byte[] b = new byte[1024];                              for(int i = 0;i &lt; data.length;i++){                                       //发送数据                                       os.write(data[i].getBytes());                                       //接收数据                                       int n = is.read(b);                                       //输出反馈数据                                     System.out.println(\"服务器反馈：\" + new String(b,0,n));                              }                     } catch (Exception e) {                              e.printStackTrace(); //打印异常信息                     }finally{                              try {                                       //关闭流和连接                                       is.close();                                       os.close();                                       socket.close();                              } catch (Exception e2) {}                     }           }  }  \n该示例程序和前面的代码相比，将数据交换部分的逻辑写在一个 for 循环的内容，这样就可以建立一次连接，依次将 data 数组中的数据按照顺序发送给服务器端了。\n如果还是使用前面示例代码中的服务器端程序运行该程序，则该程序的结果是：\njava.net.SocketException: Software caused connection abort: recv failed  at java.net.SocketInputStream.socketRead0(Native Method)  at java.net.SocketInputStream.read(SocketInputStream.java:129)  at java.net.SocketInputStream.read(SocketInputStream.java:90)  at tcp.MulSocketClient.main(MulSocketClient.java:30)服务器反馈：First  ```  显然，客户端在实际运行时出现了异常，出现异常的原因是什么呢？如果仔细阅读前面的代码，应该还记得前面示例代码中的服务器端是对话一次数据以后就关闭了连接**，如果服务器端程序关闭了，客户端继续发送数据肯定会出现异常，这就是出现该问题的原因**。  按照客户端实现的逻辑，也可以复用服务器端的连接，实现的原理也是将服务器端的数据交换逻辑写在循环中即可，按照该种思路改造以后的服务器端代码为：```java  import java.io.*;  import java.net.*;  /**   * 复用连接的echo服务器   * 功能：将客户端发送的内容反馈给客户端   */  public class MulSocketServer {           public static void main(String[] args) {                     ServerSocket serverSocket = null;                     Socket socket = null;                     OutputStream os = null;                     InputStream is = null;                     //监听端口号                     int port = 10000;                     try {                              //建立连接                              serverSocket = new ServerSocket(port);                              System.out.println(\"服务器已启动：\");                              //获得连接                              socket = serverSocket.accept();                              //初始化流                              is = socket.getInputStream();                              os = socket.getOutputStream();                              byte[] b = new byte[1024];                              for(int i = 0;i &lt; 3;i++){                                       int n = is.read(b);                                       //输出                                       System.out.println(\"客户端发送内容为：\" + new String(b,0,n));                                       //向客户端发送反馈内容                                       os.write(b, 0, n);                              }                     } catch (Exception e) {                              e.printStackTrace();                     }finally{                              try{                                       //关闭流和连接                                       os.close();                                       is.close();                                       socket.close();                                       serverSocket.close();                              }catch(Exception e){}                     }           }  }  \n在该示例代码中，也将数据发送和接收的逻辑写在了一个 for 循环内部，只是在实现时硬性的将循环次数规定成了 3 次，这样代码虽然比较简单，但是通用性比较差。\n以该服务器端代码实现为基础运行前面的客户端程序时，客户端的输出为：\n服务器反馈：First  服务器反馈：Second  服务器反馈：Third  \n服务器端程序的输出结果为：\n服务器已启动：  客户端发送内容为：First  客户端发送内容为：Second  客户端发送内容为：Third  ```  在该程序中，比较明显的体现出了“请求-响应”模型，也就是在客户端发起连接以后，首先发送字符串“First”给服务器端，服务器端输出客户端发送的内容“First”，然后将客户端发送的内容再反馈给客户端，这样客户端也输出服务器反馈“First”，这样就完成了客户端和服务器端的一次对话，紧接着客户端发送“Second”给服务器端，服务端输出“Second”，然后将“Second”再反馈给客户端，客户端再输出“Second”，从而完成第二次会话，第三次会话的过程和这个一样。在这个过程中，每次都是客户端程序首先发送数据给服务器端，服务器接收数据以后，将结果反馈给客户端，客户端接收到服务器端的反馈，从而完成一次通讯过程。在该示例中，虽然解决了多次发送的问题，但是客户端和服务器端的次数控制还不够灵活，如果客户端的次数不固定怎么办呢？是否可以使用某个特殊的字符串，例如quit，表示客户端退出呢,这就涉及到网络协议的内容了，会在后续的网络应用示例部分详细介绍。下面开始介绍另外一个网络编程的突出问题。### 如何使服务器端支持多个客户端同时工作？前面介绍的服务器端程序，只是实现了概念上的服务器端，离实际的服务器端程序结构距离还很遥远，**如果需要让服务器端能够实际使用，那么最需要解决的问题就是——如何支持多个客户端同时工作**。**一个服务器端一般都需要同时为多个客户端提供通讯，如果需要同时支持多个客户端，则必须使用前面介绍的线程的概念。简单来说，也就是当服务器端接收到一个连接时，启动一个专门的线程处理和该客户端的通讯。**按照这个思路改写的服务端示例程序将由两个部分组成，MulThreadSocketServer类实现服务器端控制，实现接收客户端连接，然后开启专门的逻辑线程处理该连接，LogicThread类实现对于一个客户端连接的逻辑处理，将处理的逻辑放置在该类的run方法中。该示例的代码实现为：```java  import java.net.ServerSocket;  import java.net.Socket;  /**   * 支持多客户端的服务器端实现   */  public class MulThreadSocketServer {           public static void main(String[] args) {                     ServerSocket serverSocket = null;                     Socket socket = null;                     //监听端口号                     int port = 10000;                     try {                              //建立连接                              serverSocket = new ServerSocket(port);                              System.out.println(\"服务器已启动：\");                              while(true){                                       //获得连接                                       socket = serverSocket.accept();                                       //启动线程                                       new LogicThread(socket);                              }                     } catch (Exception e) {                              e.printStackTrace();                     }finally{                              try{                                       //关闭连接                                       serverSocket.close();                              }catch(Exception e){}                     }           }  }  \n在该示例代码中，实现了一个 while 形式的死循环，由于 accept 方法是阻塞方法，所以当客户端连接未到达时，将阻塞该程序的执行，当客户端到达时接收该连接，并启动一个新的 LogicThread 线程处理该连接，然后按照循环的执行流程，继续等待下一个客户端连接。这样当任何一个客户端连接到达时，都开启一个专门的线程处理，通过多个线程支持多个客户端同时处理。\n下面再看一下 LogicThread 线程类的源代码实现：\nimport java.io.*;  import java.net.*;  /**   * 服务器端逻辑线程   */  public class LogicThread extends Thread {           Socket socket;           InputStream is;           OutputStream os;           public LogicThread(Socket socket){                     this.socket = socket;                     start(); //启动线程           }         public void run(){                     byte[] b = new byte[1024];                     try{                              //初始化流                              os = socket.getOutputStream();                              is = socket.getInputStream();                              for(int i = 0;i &lt; 3;i++){                                       //读取数据                                       int n = is.read(b);                                       //逻辑处理                                       byte[] response = logic(b,0,n);                                       //反馈数据                                       os.write(response);                              }                     }catch(Exception e){                              e.printStackTrace();                     }finally{                              close();                     }           }         /**            * 关闭流和连接            */           private void close(){                     try{                              //关闭流和连接                              os.close();                              is.close();                              socket.close();                     }catch(Exception e){}           }         /**            * 逻辑处理方法,实现echo逻辑            * @param b 客户端发送数据缓冲区            * @param off 起始下标            * @param len 有效数据长度            * @return            */           private byte[] logic(byte[] b,int off,int len){                     byte[] response = new byte[len];                     //将有效数据拷贝到数组response中                     System.arraycopy(b, 0, response, 0, len);                     return response;           }  }  \n在该示例代码中，每次使用一个连接对象构造该线程，该连接对象就是该线程需要处理的连接，在线程构造完成以后，该线程就被启动起来了，然后在 run 方法内部对客户端连接进行处理，数据交换的逻辑和前面的示例代码一致，只是这里将接收到客户端发送过来的数据并进行处理的逻辑封装成了 logic 方法，按照前面介绍的 IO 编程的内容，客户端发送过来的内容存储在数组 b 的起始下标为 0，长度为 n 个中，这些数据是客户端发送过来的有效数据，将有效的数据传递给 logic 方法，logic 方法实现的是 echo 服务的逻辑，也就是将客户端发送的有效数据形成以后新的 response 数组，并作为返回值反馈。\n在线程中将 logic 方法的返回值反馈给客户端，这样就完成了服务器端的逻辑处理模拟，其他的实现和前面的介绍类似，这里就不在重复了。\n这里的示例还只是基础的服务器端实现，在实际的服务器端实现中，由于硬件和端口数的限制，所以不能无限制的创建线程对象，而且频繁的创建线程对象效率也比较低，所以程序中都实现了线程池来提高程序的执行效率。\n这里简单介绍一下线程池的概念，线程池 (Thread\npool) 是池技术的一种，就是在程序启动时首先把需要个数的线程对象创建好，例如创建 5000 个线程对象，然后当客户端连接到达时从池中取出一个已经创建完成的线程对象使用即可。当客户端连接关闭以后，将该线程对象重新放入到线程池中供其它的客户端重复使用，这样可以提高程序的执行速度，优化程序对于内存的占用等。\n\n转载，作者不详，侵删\n","categories":["Java","网络编程"],"tags":["转载","Java","计算机网络"]},{"title":"计算机网络课程总结 --RIP 与 OSPF","url":"/2016/12/27/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--RIP%E4%B8%8EOSPF/","content":"本文主要讲述内部网关协议中两个著名的协议 RIP 和 OSPF。\n\n路由器的基本功能是路由 (Routing) 和转发 (Forwarding)。其中路由指的是通过路由选择协议将路由信息注入到路由表中，转发指的是依据路由表和分组携带的信息将分组从入口转到正确的出口的过程。\n分组转发的技术有以下几种\n\nSource routing（源路由）：分组携带路径\n Table of virtual\ncircuits（虚电路）：穿越网络建立链接及状态，使用链接转发分组\n Table of global addresses\n(IP，数据报交换)：路由器维持到目的地的下一条，分组需要携带目的地地址\n\n本文主要涉及到的是第三种。\n静态路由\n配置命令\nIp route prefix mask {address|interface} [distance]\n使用地址 (address) 和使用接口 (interface) 的差别\n\n使用接口，路由器知道从哪里转发出去，更高效\n使用地址需要第二次查找，以确定转发接口\n\n距离 (distance) 的特定\n\n直连网络的管理距离为 0\n 静态路由的管理距离为 1\n 值越小，优先级越高\n到达某个网络出现多条路由的情况下，可以依据管理距离进行选择\n\n还可以通过管理距离配置备份路由（也称为浮动静态路由），如下图所示\n\n\n备份路由\n\n动态路由\n动态路由就是通过协议动态地学习到路由信息，据其作用域的不同，又可分为内部网管协议 (IGP) 和边界网络协议 (BGP)；内部网关协议有 RIP，OSPF，IGRP，EIGRP 等，其在网络中的位置入下\n\n\n路由协议在协议层中的位置\n\n这里主要讲述内部网络协议中的 RIP 和 OSPF。\nRIP\nRIP 是典型 DV (Distance Vector, 距离矢量) 协议，其工作原理如下：\n\n每个路由器维护两个向量 \\(D_i\\) 和 \\(S_i\\) 来表示该点到网上所有节点的路径距离及其下一个节点\n相邻路由器之间交换路径信息\n各节点根据路径信息更新路由表\n\n向量 \\(D_i\\) 和 \\(S_i\\) 的含义如下: \\(d_{i1}\\)：从节点 i 到节点 1 的时延向量 \\(S_{i1}\\)：从节点 i 到节点 1 的一条最小时延路径上的下一个节点\nn ：网络中的节点数\n\n\n距离矢量\n\nRIP 使用跳数作为路由选择的度量，当到达目的网络的跳数超过 15 跳，数据包将被丢掉。\nRIP 路由更新广播默认周期为 30 秒。\n下图是 RIP 的一个简单例子\n\n\nRIP 工作原理\n\nRIP 在 ipv4 中的协议版本有 RIPv1 和 RIPv2, 其中 RIPv1\n只支持传输自然网段的地址 (也就是 ABC 三类网络)，而 RIPv2\n修复了这个缺陷，增加了一个子网掩码的段，支持所有长度的子网掩码，也支持\nCIDR 和 VLSM。\n路由环路\nRIP 协议可能会导致路由环路，下图为一个路由环路的例子，图中初始状态是正常状态，但是当 40.0.0.0 的网络挂掉后，C\n不会修改到网络 40.0.0.0 的距离不可达，而是相信 B 传过来的关于 40.0.0.0\n网路的距离为 1 的消息，从而导致错误信息不断在网络中传输，每个路由器到\n40.0.0.0 的距离不断增大，直到距离超过 15 才将网络 40.0.0.0\n标记为不可达。\n\n\n路由环路\n\n虽然上面的例子中距离增大到 16 的时候会将网络 40.0.0.0\n标记为不可达，但是这样的话收敛的速度会非常慢。为了提高收敛的速度，常常会有以下方法\n\n水平分割（Split Horizon）\n毒性逆转（Poison Reverse）\n抑制定时器（Hold-Down Timers）\n触发更新（Triggered Updates）\n\n水平分割（Split Horizon）\n分析上面的例子中路径环产生的原因，就是 B 向 C 提供了一条过时的、错误的路由信息。\n但是分析可知，B 必须经由 C 方可到达网络 40.0.0.0，所以 B 不可能向 C 提供任何有价值的路由信息。因此可以修改 B 对 C 提供的路由，禁止 B 向 C 提供关于此信宿的路由信息，具体操作就是 B 告诉 C 一条在正常情况下不真实的消息：网络 40.0.0.0 不可达（距离为无穷大，实际中记为 16 即可）。\n通过水平分割可以使得上面的情况的收敛速度加快，如下所示\n\n\n水平分割\n\n毒性逆转（Poison Reverse）\n分析上面的例子，当网络 40.0.0.0\n挂掉的时候，路由器 C 并没有采取任何措施，而是利用了 B 的信息更新。\n而毒性逆转的方法就是指当 C\n发现网络 40.0.0.0 发生故障时，主动将到达信宿的距离改为无穷大（实际中改为 16 即可），收敛过程如下图所示\n\n\n毒性逆转收敛\n\n抑制定时器（Hold-Down Timers）\n当 C 发现网络 40.0.0.0 发生故障时，启动抑制计时器。\n在抑制计时期间内，有三种可能 1.\n如果网络状态转变，即 down→up，则关闭计时器，保留原有路由信息\n2.\n如果收到来自 B 的关于信宿的路由信息，且路径比原有路径短，则关闭计时器，更新路由信息\n3. 如果无上述两种情况发生，计时器到时，更新路由为信宿不可达。\n触发更新（Triggered Updates）\n当 C 发现网络 40.0.0.0 发生故障时，不等下一刷新周期到来，立刻更改路由为 “信宿不可达”，引起全网的连锁反映，迅速刷新\n类似于 RIP 这种 DV 类协议的优点是算法简单，但是缺点有：\n\n交换的路径信息量大\n路径信息传播慢，使得路径信息可能不一致。\n收敛速度慢，存在无穷计算问题。\n不适合大型网络\n\nRIPng\nRIPng 是 IPV6 中使用的 RIP 协议，保留了原来 RIP\n协议的一些特性，也增加了一些新的特点。\nRIPng 有以下特点\n\nUDP 端口号：使用 521 端口收、发报文（RIPv2 用 520 端口）\n组播地址：FF02::9 作为链路本地范围的路由器组播地址\n源地址：使用链路本地地址 FE80::/10 作为源地址发送 RIPng 路由信息报文\n下一跳地址：使用 128 位的 IPv6 地址\n\n’ RIPng 和 RIPv2 的报文对比如下\n其中 RIPng 的一个报文种可包含多条路由信息，而 RIPv2 一个报文只含有一条路由信息。\nRIPng 的报文格式为 \nRIPv2 的报文格式为\n\n\nRIPv2 报文\n\nRIPng 中有 Request 和 Response 两种报文，其作用分别如下\nRequest 报文：当路由器启动或更新时，发该类报文（组播），用于请求其他路由器的路由信息\nResponse 报文：对 Request 的回应，通常包含全部的路由信息。除了收到 Request 的时候会发送，路由器还会周期性地发送。\n配置\nRIP 的配置的主要工作就是启动 RIP 进程并声明路由器所连接的网络。如下图所示\n\n\nRIP 配置\n\n默认是 RIPv1，如果要配置 RIPv2, 需要 version 2 的命令，如下图所示\n\n\nRIPv2 配置\n\nRIPng 的配置过程入下 \nOSPF\nOSPF 时典型的 LS (Link\nState, 链路状态) 协议，其思想是通过邻居的 LSA (Link-state\nadvertisement,\n链路转态通告) 构建整个网络的拓扑并构建最小生成树，然后通过 dijkstra\n计算最短路径，并根据最短路径修改路由表。\nOSPF 有以下特点\n\n无路由自环\n支持 VLSM、CIDR\n 使用带宽作为度量值（108/BW）\n收敛速度快\n通过分区实现高效的网络管理\n支持帧中继，X2.5, Ethernet, ppp 等网络\n可以在大型网络中使用\n\nOSPF 协议的一些基本概念如下；\n\n协议号 = 89：IP 头中代表 OSPF 报文的协议号是 89\nRouterID：一个 32 位的无符号整数，是一台路由器的唯一标识，在整个自治系统内唯一\n路由器间的关系：邻居 (Neighbor)、邻接 (Adjacent)、未知 (Unknown)\nTTL=1：通常 OSPF 报文不转发，只被传递一条，即在 IP 报头的 TTL 值被设为 1，\nDR (Designated Router) 和 BDR (Backup Designated Router):\nDR 是由所有路由器选举出来的，目的是减少路由器间同步的次数，所有路由器只需要和选举出来的 DR 进行同步即可，原理如下图所示，BDR 是 DR 的备份路由器。\n\n\n\n选举 DR 的必要性\n\n这里需要注意的是，DR 是路由器选出来的，选举规则根据其 priority 值的大小，若 priority 相同则比较 router\nid；DR 一旦当选，除非路由器故障，否则不会更换；DR 选出的同时，也选出 BDR，DR 故障后，由 BDR 接替 DR 成为新的 DR\n报文类型\nOSPF 的分组有五种，分别如下所示\n\n\n\n\n\n\n\nOSPF 报文类型\n作用\n\n\n\n\n Type = 1, Hello 报文\n建立和维护连接，报文中包含自己的 RouterID 和区域 ID\n\n\nType = 2，DD (数据库描述) 报文\n描述一个 OSPF 路由器的链路状态数据库内容，传输 LSA 摘要\n\n\n Type = 3，LSR (链路状态请求) 报文\n请求对方发送自己没有的 LSA\n\n\nType = 4，LSU (链路状态更新) 报文\n LSR 的应答，可回传多条 LSA\n\n\nType = 3，LSAck (链路状态确认) 报文\n确认收到 LSA\n\n\n\n建立毗邻关系\nOSPF 运行的步骤为 1. 建立路由器毗邻关系 2. 选举 DR 和 BDR 3. 发现路由 4.\n选择最佳路由 5. 维护路由信息\nOSPF 中，一个路由器的的状态可能为\n\nDown\nInit（初始）\nTwo-way（双向）\nExStart（准启动）\nExchange（交换）\nLoading（加载）\nFull adjacency（全毗邻）\n\n结合路由器的状态，OSPF 建立毗邻关系的步骤如下\n\n\nOSPF 建立毗邻关系\n\n1）RT1，RT2 在某个接口激活了 OSPF 后，都会开始在这个接口上去发组播的 Hello 报文，目的是发现 OSPF 邻居，此时双方都处于 Down 状态。\n2）当 RT2 收到 RT1 发来的 Hello 包（Neighbors Seen\n为空），此时 RT2 的状态变为 init，然后将 RT1 的 Router-ID 存储放在 Hello 报文中 (Neighbors\nSeen =\nRT1) 发送出去，当 RT1 收到这个 hello 报文并从中找到自己的 Router-ID，RT1 会认为与 RT2 已经完成了双边关系的建立，此时 RT1 的状态变为 Two-way，而 RT1 会发送 Neighbors\nSeen = RT2 的 hello 包让 RT2 的状态也变为 Two-way。\n3）接下来 RT1 和 RT2 会进入 ExStart 状态并开始进行 Master、Slave 的协商。协商 M/S 的目的是为了决定在后续的 LSA 交互中，谁来决定 DD（Database\nDescription）报文的序列号（Sequence\nNumber），而 Router-ID 大的那个 OSPF 路由器的接口将会成为 Master（注意这里的 Master 不是 DR）. 协商过程通过 DD 报文实现，有三个关键的字段：I、M、MS，其含义如下\n\n\n\n\n\n\n\n字段\n含义\n\n\n\n\n I(Init)\n 如果是第一个 DD 报文则置 1，其它的均置 0\n\n\nM(More)\n 如果是最后一个 DD 报文则置 0，否则均置 1\n\n\nM/S\n 设置进行 DD 报文双方的主从关系，如果本端是 Master 角色，则置 1，否则置 0\n\n\nSequence Number\n 指定所发送的 DD 报文序列号。主从双方利用序列号来确保 DD 报文传输的可靠性和完整性\n\n\n\n4）确认了 M/S 关系后，两个路由器就进入了\nExchange 状态，\n主路由器首先开始和从路由器共享链路状态信息。如果将链路状态数据库比喻成一本书，那么 DD 报文相当于这本书的目录，通过 DD 报文，可以发现自己所没有的信息。\n5）当所有的 DD 报文传输完后，假如从路由器通过 DD 报文发现了自己所没有的信息后，会发送 LSR 报文给主路由器，随后主路由器会发送 LSU 报文给从路由器。从路由器将该信息合并到它的本地链路状态数据库中。从路由器会回应一个 LSAck 包给主路由器。此时两者处于 loading 状态。\n6）两者的链路数据库一致，达到了 full 状态。\n其状态转移图如下所示，图中的稳态有三种 (Down,Two-way,Full)\n\n其他概念\n克服路由自环\nOSPF 能够克服路由自环的原因有以下几个\n\n每一条 LSA 都标记了生成者（用生成该 LSA 的路由器的 RouterID 标记），其他路由器只负责传输，这样不会在传输的过程中发生对该信息的改变和错误理解。\n路由计算的算法是 SPF，计算的结果是一棵树，路由是树上的叶子节点，从根节点到叶子节点是单向不可回复的路径。\n区域则通过规定骨干区域避免\n\n大型网络中存在的问题及对策\n在大型网络中，OSPF 存在着以下问题\n\n链路状态数据库 (LSDB) 非常庞大，占用大量存储空间\n计算最小生成树耗时增加，CPU 负担很重，一点变化都会引发从头重新计算\n网络拓扑结构经常发生变化，网络经常处于 “动荡” 之中\n\n针对该问题，常用的解决方法是对 OSPF 划分区域\n\n\nospf 划分区域\n\nOSPF 路由器的类型\n根据位置不同，OSPF 路由器可以被被划分为不同类型\n\n内部路由器 --- 路由器所有接口都在一个区\n主干路由器 --- 所有接口都在主干区域的路由器\n区域边界路由器 (ABR) --- 路由器接口分属不同区域 (Area)\n 自治域边界路由器 (ASBR) ---\n路由器至少有一个接口不属于本自治域 (AS).\n\n\n\n 路由器类型\n\nOSPFv3\nOSPFv3 保留了 OSPFv2 的基本机制\n\n网络类型和接口类型\n邻居发现和邻接（毗邻）建立机制\n接口状态机和邻居状态机\n基于 LSDB 计算路由\n LSA 老化更新机制\n泛洪机制 (Flooding mechanism)\n 共五种协议报文: Hello, DD, LSR, LSU, LSAck\n\n但是 OSPFv3 在 OSPFv2 的基础上做出的变化为：\n1）基于链路运行\n在 OSPFv2\n中，协议的运行是基于子网的，邻居之间形成邻接关系的条件之一就是两端的 IP 地址属于同一网段而且掩码相同。\n而在 OSPFv3\n中，协议基于链路运行，与具体的 IPv6 地址、前缀分离开来，即使同一链路上的不同节点具有不同的 IPv6 地址时，协议也可以正常运行。\n2）取消了编址语义\n在 OSPFv2 中，协议分组和 LSA 中的许多字段都是来自于网络上的某个 IP 地址，或掩码，或某个 IP 子网号。严重依赖 IPv4。\n在 OSPFv3 中，取消了上述编址性语义，而只保留协议运行必须的核心内容。ID 依然保留 32 位，但只是一个编号，不再包含地址信息。\n3）链路本地地址的使用\n在 OSPFv2 中，每一个运行 OSPF 的接口都必须有一个全局的 IPv4 地址，协议的运行和路由的计算都依赖于它。\n在 IPv6 中，每个接口都会分配本地链路地址（link-local address），OSPFv3\n使用了这个本地链路地址作为协议分组发送的源地址（虚连接除外），而且使用它作为路由的下一跳。\n这样可以节省大量的全局地址，同时可以说协议的运行独立于 IPv6，可以方便的扩展用于多种协议的路由\n4）使用专门的 LSA 来发布路由前缀信息\n新增加了 Intra-Area-Prefix-LSA，用于传递区域内路由前缀\n新增加了 Link-LSA，用于传递链路范围内的 IPv6 前缀。\n5）明确的 LSA 泛滥范围\n泛滥的范围分为：本地链路范围（Link-local scope），区域范围（Area\nscope），AS 范围（AS scope）\n6）提供了对多实例的支持\n在 OSPFv2 中，不同的实例必须运行在不同的链路上；在 OSPFv3 中，明确的提供了对多实例的支持，同一链路也可以运行多个 OSPF 实例了，而且互相独立运行不会影响。\n配置\n单区域配置\n\n\nOSPF 单区域配置\n\n配置 Routert-ID\nrouter-id 是一个可选的配置，其获取方式依次为 1）手动配置\n2）使用环回地址作为 router ID\n3）如果没有，选择路由器的最高逻辑地址作为 routerID\n注意：IOS 的某些早期版本无法识别 router-id\n命令；因此，为这些路由器设置路由器 ID 的最佳方法是使用环回接口\n\n\n配置 router-id\n\n配置优先级\n优先级的取值范围为 0~255，优先级为 0 的路由器不能被选举为 DR\n\n\n优先级配置\n\n配置计时器\n广播型 OSPF 网络，缺省 hello 包间隔为 10 秒，down 机判断间隔为 40 秒\n非广播型 OSPF 网络，缺省 hello 包间隔为 30 秒，down 机判断间隔为 120 秒\n实际配置的例子如下\nRouter（config-if）＃ip ospf hello-interval 5Router（config-if）＃ip ospf dead-interval 10\n","categories":["杂"],"tags":["计算机网络"]},{"title":"并行算法的设计","url":"/2016/06/04/%E8%AE%BE%E8%AE%A1%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95/","content":"本文主要讲述并行算法设计的一些注意事项。\n\n设计并行算法首先需要要确认问题是否可以被并行化，最典型的情况是在处理一个任务队列时，队列中的前后任务是独立的，这时候就可以通过并行化让多个进程或线程同时处理其中的多个任务。关键点在于大任务可以被分解为若干个互相独立的小任务。\n设计并行算法可以从以下四方面着手 1）分治 (divide and conquer)\n2）数据分解 (data decomposition) 3）管道分解任务 (decomposing tasks with\npipeline) 4）任务分配 (processing and mapping)\n分治 (divide and conquer)\n分治法是一个非常常用的思想，通过将一个问题分解成规模更小的子问题，然后并行解决子问题，再合并子问题的结果即可。如快速排序、归并排序都是分治法的经典例子。下图是归并排序的一个例子\n\n数据分解 (data decomposition)\n数据分解指当要处理的数据的最小单元间相互独立时，可通过并行化来实现。如将一个 2X2 的矩阵中的每个元素都乘上 4 时，如果采取串行化需要按顺序将每个元素逐一乘上 4，但是采取并行化时可以同时将每个元素乘上 4。如下所示，图中的 worker 指进程或线程。\n\n上面的例子比较简单，仅仅是为了说明数据分解的概念，在实际应用中，往往还需要考虑数据量与 worker 数量的不对称性问题，并且在将各自的结果合并时中需要考虑不同 worker 间的通信问题。\n管道分解任务 (decomposing\ntasks with pipeline)\n管道（pipeline），也可以称为流水线技术，是类似于工厂生产的流水线的一种设计方式。如下图所示就是流水线的一个例子：\n\n上图首先将一个大任务分成了四个小任务，然后每个 worker 分别处理其中的一个任务，每个任务的输出是下一个任务的输入。\n假设每个小任务消耗的时间是 t，那么当完成 1 个大任务的时候串行和并行方式消耗的时间均是 4t。但是当完成 k 个大任务时，串行化消耗的时间是 4t*k, 但是并行化需要的时间是 4t+(k-1)*t, 当 k 很大时，并行化节省的时间就相当可观了。\n任务分配 (processing and\nmapping)\n上面提到的三种方法均是将大任务分解，然后通过并行完成小任务来实现并行化。但是在将任务分解后，需要注意的是如何分配任务给各个\nworker，使得每个 worker 的负载均衡，从而达到最优的效果。\n在这个问题主要是要区分独立的任务和需要交换数据（通信）的任务。独立的任务可以分配给不同的 worker 去完成，因为这些任务不需要通信的成本；而将需要经常进行通信的任务让单独一个 worker 完成，考虑到网络通信的开销，这样能够提高性能。\n总结\n上面主要提到了并行化算法的几个关键点，包括将大任务进行分解的几种方法（分治、数据分解、管道）以及将分解后的任务分配给 worker 时的注意事项。要注意的是这几种方法在实际中常常会混用，举一个实际一点的例子，如果要对一个很大的数组排序，单台机器的内存都放不下这个数组了，那该怎么办？\n首先将数组分成 k 份，然后分配给 k 台机器分别进行排序，排序完毕后我们有了 k 个 sorted\nlist，然后将 k 个 sorted\nlist 两两合并，当合并后的数据越来越大时，单台机器内存不足时，可以采取外排序，将两个 sorted\nlist 存储在硬盘中，每次取出前 n 个进行合并。\n也许在实际中有更好的方法，但是上面的例子中实现的并行化就是利用到了分治法和数据分解法，实际中还可以根据机器的配置情况分配不同的任务负载。\n","categories":["python","并行编程"],"tags":["python","操作系统"]},{"title":"详解 Python 程序与服务器连接的 WSGI 接口","url":"/2015/11/20/%E8%AF%A6%E8%A7%A3Python%E7%A8%8B%E5%BA%8F%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5%E7%9A%84WSGI%E6%8E%A5%E5%8F%A3/","content":"文章为转载，原文见这里，侵删\n这篇文章主要介绍了 Python 程序与服务器连接的 WSGI 接口，是 Python 网络编程学习当中的重要内容，需要的朋友可以参考下\n了解了 HTTP 协议和 HTML 文档，我们其实就明白了一个 Web 应用的本质就是：\n1. 浏览器发送一个 HTTP 请求；\n2. 服务器收到请求，生成一个 HTML 文档；\n3. 服务器把 HTML 文档作为 HTTP 响应的 Body 发送给浏览器；\n4. 浏览器收到 HTTP 响应，从 HTTP Body 取出 HTML 文档并显示。\n\n　\n所以，最简单的 Web 应用就是先把 HTML 用文件保存好，用一个现成的 HTTP 服务器软件，接收用户请求，从文件中读取 HTML，返回。Apache、Nginx、Lighttpd 等这些常见的静态服务器就是干这件事情的。\n如果要动态生成 HTML，就需要把上述步骤自己来实现。不过，接受 HTTP 请求、解析 HTTP 请求、发送 HTTP 响应都是苦力活，如果我们自己来写这些底层代码，还没开始写动态 HTML 呢，就得花个把月去读 HTTP 规范。正确的做法是底层代码由专门的服务器软件实现，我们用 Python 专注于生成 HTML 文档。因为我们不希望接触到 TCP 连接、HTTP 原始请求和响应格式，所以，需要一个统一的接口，让我们专心用 Python 编写 Web 业务。\n这个接口就是 WSGI：Web Server Gateway\nInterface。WSGI 接口定义非常简单，它只要求 Web 开发者实现一个函数，就可以响应 HTTP 请求。我们来看一个最简单的 Web 版本的 “Hello,\nweb!”：\ndef application(environ, start_response):      start_response('200 OK', [('Content-Type', 'text/html')])      return '&lt;h1&gt;Hello, web!&lt;/h1&gt;'  \n上面的 application () 函数就是符合 WSGI 标准的一个 HTTP 处理函数，它接收两个参数：\n\nenviron：一个包含所有 HTTP 请求信息的 dict 对象；\n\nstart_response：一个发送 HTTP 响应的函数。\n\n在 application () 函数中，调用 start_response('200 OK', [('Content-Type', 'text/html')]) 就发送了 HTTP 响应的 Header，注意 Header 只能发送一次，也就是只能调用一次 start_response () 函数。start_response () 函数接收两个参数，一个是 HTTP 响应码，一个是一组 list 表示的 HTTP\nHeader，每个 Header 用一个包含两个 str 的 tuple 表示。\n通常情况下，都应该把 Content-Type 头发送给浏览器。其他很多常用的 HTTP\nHeader 也应该发送。然后，函数的返回值 Hello,\nweb! 将作为 HTTP 响应的 Body 发送给浏览器。\n有了 WSGI，我们关心的就是如何从 environ 这个 dict 对象拿到 HTTP 请求信息，然后构造 HTML，通过 start_response () 发送 Header，最后返回 Body。整个 application () 函数本身没有涉及到任何解析 HTTP 的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。\n不过，等等，这个 application () 函数怎么调用？如果我们自己调用，两个参数 environ 和 start_response 我们没法提供，返回的 str 也没法发给浏览器。\n所以 application () 函数必须由 WSGI 服务器来调用。有很多符合 WSGI 规范的服务器，我们可以挑选一个来用。但是现在，我们只想尽快测试一下我们编写的 application () 函数真的可以把 HTML 输出到浏览器，所以，要赶紧找一个最简单的 WSGI 服务器，把我们的 Web 应用程序跑起来。\n好消息是 Python 内置了一个 WSGI 服务器，这个模块叫 wsgiref，它是用纯 Python 编写的 WSGI 服务器的参考实现。所谓 “参考实现” 是指该实现完全符合 WSGI 标准，但是不考虑任何运行效率，仅供开发和测试使用。\n我们先编写 hello.py，实现 Web 应用程序的 WSGI 处理函数：\n#hello.py  def application(environ, start_response):      start_response('200 OK', [('Content-Type', 'text/html')])      return '&lt;h1&gt;Hello, web!&lt;/h1&gt;'\n然后，再编写一个 server.py，负责启动 WSGI 服务器，加载 application () 函数：\n# server.py  # 从wsgiref模块导入:  from wsgiref.simple_server import make_server  # 导入我们自己编写的application函数:  from hello import application# 创建一个服务器，IP地址为空，端口是8000，处理函数是application:  httpd = make_server('', 8000, application)  print \"Serving HTTP on port 8000...\"   # 开始监听HTTP请求:  httpd.serve_forever()  \n确保以上两个文件在同一个目录下，然后在命令行输入 python server.py 来启动 WSGI 服务\n\n\n启动 server\n\n启动成功后，打开浏览器，输入\nhttp://localhost:8000/，就可以看到结果了\n\n\n浏览器观察\n\n再看看刚刚打开的 cmd 窗口，会输出请求的如下所示\n\n\n请求情况\n\n可以看到总共有两个请求源，其中 LC-PC 是通过电脑浏览器输入 localhost 访问，而 192.168.1.109 是通过手机浏览器访问的（电脑和手机在同一局域网下）；输出的内容包括请求的 ip，时间和 http 请求头。\n无论多么复杂的 Web 应用程序，入口都是一个 WSGI 处理函数。HTTP 请求的所有输入信息都可以通过 environ 获得，HTTP 响应的输出都可以通过 start_response () 加上函数返回值作为 Body。\n复杂的 Web 应用程序，光靠一个 WSGI 函数来处理还是太底层了，我们需要在 WSGI 之上再抽象出 Web 框架，进一步简化 Web 开发。\n\n","categories":["python"],"tags":["转载","python","web"]},{"title":"超参搜索中的 Bayesian Optimization 方法","url":"/2019/07/10/%E8%B6%85%E5%8F%82%E6%90%9C%E7%B4%A2%E4%B8%AD%E7%9A%84%20Bayesian%20Optimization%20%E6%96%B9%E6%B3%95/","content":"机器学习中存在着众多的超参数，如 model 中的超参，optimizer\n中的超参，loss function\n中的各种超参等，这些超参需要使用者根据经验设定，并根据训练结果进行调整，因为这些超参的最优值跟不同任务、不同数据集相关，\n没有一个非常通用的经验值。\n这一步骤往往繁琐耗时，为了简化这一过程，有了 Hyperparameter\noptimization\n的研究，其目的是自动搜索最优的超参。超参搜索最常见的方法是 grid\nsearch，random search，当然也有更高级的方法如基于启发式方法的 heuristic\nsearch、基于统计学的 bayesian optimization 等，本文主要介绍超参搜索中的\nBayesian Optimization 方法，这是超参搜索比较常见的做法，Google\n也将这部分作为一个 service 提供在 Google\nCloud 上。本文主要介绍 Bayesian Optimization 中的 GPR (Gaussian\nProcess Regression) + GP-BUCB (Gaussian Process Regression-Batch Upper\nConfidence Bound) 方法。\n\n更准确地说， Bayesian\nOptimization\n应该是一个算法框架，其目的是为了推断那些不能显示给出函数形式的 black-box\nfunction，而在超参搜索中，这个函数指的是将超参数 \\(x\\) 映射至最终的收益 \\(y\\) 的那个函数，这里的收益不一定是\nloss，还可以是 loss 与评估指标的融合，如在 ctr 预估中可以将 y\n定义为：\\(y = \\alpha_1 \\* auc - \\alpha_2 \\*\nlogloss\\)\n常见的 Bayesian Optimization 方法由两部分组成，高斯过程回归 (Gaussian\nProcess Regression, GPR) 和 EE (Exploration Exploitation) trade-off,\n下面分别介绍这两部分的具体过程\n高斯过程回归 (GPR)\n首先，从名字上看，高斯过程回归是一种回归方法，而回归就是给一堆已知的\nx 和 y，然后当拿出一个新 x\n的时候，能够预测出对应的新 y，但是不同于常见的回归方法最终预测出一个值，高斯过程回归最终预测的是新的\ny 的分布\n那高斯体现在哪里？顾名思义，Bayesian Optimization\n是一种贝叶斯方法，而贝叶斯方法往往少不了先验，而在这里高斯其实是作为一种先验，即我们认为所有的\ny 服从一个零均值的多维的联合高斯分布（注意这里每个 y\n自己也服从一个一维高斯分布），相比于一维高斯分布，多维高斯分布将原来的均值变为了均值向量，方差变为了协方差矩阵，（可参考多维高斯分布是如何由一维发展而来的？了解如何从一维高斯分布推广至多维高斯分布)\n至于过程 (process), 则是指随机过程 (stochastic\nprocess),\n这里指的是多个服从高斯分布的随机变量组成了一个随机过程，但是这里点对于我们理解\nGPR 不重要。\n推导 GPR 可从 Weight-space 角度 或 Function-space\n角度进行，具体可参考 Gaussian\nProcesses for Machine Learning 中 p7-p35 部分，这里为了简单起见只从\nFunction-space\n进行描述，并且会尽可能通过直观的方式描述这一点，某些符号与上面的 pdf\n可能不一致\n在训练集中，假设我们有 3 个 x, 记为 \\(x_1、x_2、 x_3\\), 以及这 3 个点对应的 y, 记为\n\\(f_1、f_2、f_3\\)，由于假设所有的 y\n服从一个均值为 0 的多维联合高斯分布，因此可写成如下形式 (下面图片摘自这篇文章）\n\n\n多维高斯分布\n\n现在问题关键是模型的协方差矩阵 K\n从哪儿来，为了解答这个问题，进行了另一个重要假设：\n如果两个 x\n比较相似（即离得比较近），那么对应的 y 值的相关性也就较高，即加入\nx3 和 x2 离得比较近，则认为 f3 和 f2 的 correlation 要比\nf3 和 f1 的 correlation 高。换言之，协方差矩阵是 X\n的函数而不是 y 的函数\n那么怎么衡量两个 x 比较相似？这个选择很多了，下面是一些简单的例子，在\nGPR 中常用的是 RBF kernel（就是 SVM 中的那个 kernel，理论上，SVM\n中的任意 kernel 在这里都适用）简单来说，RBF 隐式地对 x\n进行了升维并计算内积，提升了表达能力。\n\n协方差\n\n\\[\\begin{align} k(x_1, x_2) = E[(x_1 -\nE(x_1))*(x_2 - E(x_2)] \\end{align}\\]\n\n相关系数\n\n\\[\\begin{align} k(x_1, x_2) = \\frac{E[(x_1\n- E(x_1))*(x_2 - E(x_2)]}{\\sigma_{x_1}\\sigma_{x_2}}\n\\end{align}\\]\n\nRBF kernel\n\n\\[\\begin{align} k(x_1, x_2) =\n\\exp(-\\frac{||x_1 - x_2||_2^2}{2\\sigma^2}) \\end{align}\\]\n则当有新的 \\(x_\\*\\) 要预测新的 \\(f^\\*\\) 时，由于假设所有的 y (即 f)\n服从一个均值为 0 的多维联合高斯分布，因此可写成如下形式，\n而由于协方差矩阵是 \\(x\\)\n的函数，因此，下图中的 \\(K\\) 和 \\(K_\\*\\) 都可计算出来\n\n\n多维高斯分布 1\n\n其中 \\(K_\\*\\) 的计算方式如下，\\(k\\) 就是上面提到的衡量两个 \\(x\\) 有多近的函数，在 GPR 中也被称为\nconvariance function\n\\[\\begin{align}  K_\\* =  [k(x_\\*, x_1),\nk(x_\\*, x_2), k(x_\\*, x\\*_\\*3)]^T \\end{align}\\]\n有了所有 \\(f\\)\n的联合分布，根据多维高斯分布的性质，可计算出 \\(f^\\*\\) 的分布如下，即\n\n\ninference\n\n从上面的计算过程可知，GPR 的效果非常依赖于衡量两个 \\(x\\) 的相似度的函数 \\(k\\), 而 RBF kernel 中存在着超参 \\(\\sigma\\), 因此 GPR\n还会根据得到的样本进行 MLE，进而更新 RBF kernel\n中的超参数，其中要最大化的是样本的 log marginal likelihood,\n其定义如下\n\\[\\begin{align} \\log p(y|x,\\theta) =\n-\\frac{1}{2}\\log|K| - \\frac{1}{2}\n(y-\\mu)^TK^{-1}(y-\\mu)-\\frac{n}{2}\\log(2\\pi)\\end{align}\\]\n因此，得到已有的样本后，将上面式子中的协方差矩阵 \\(K\\) 中的超参数 \\(\\sigma\\)\n视为未知变量，便可通过梯度下降等方法更新超参数\nsklearn 中提供的 GPR 的实验，具体的使用方法及实现可参考 GaussianProcessRegressor\n通过上述的推导可知，对于每个新 \\(x_\\*\\), GPR 会预估出这个 \\(x_\\*\\) 对应的 \\(y_\\*\\)\n的分布 (分布的具体形式为高斯分布)，而分布中均值表示 \\(y_\\*\\)\n的预估值，方差则在一定程度上表示对这个预估值的置信程度（方差越大，预估值越不置信）\n因此，GPR 的预估值天然就具备了 EE (Exploration&amp;Exploitation)\n的属性，而这也是 Bayesian Optimization\n第二步需要解决的问题，解决的方法有很多选择，其中可以贪婪地选择均值最大的，也可以综合考虑均值和方差，这个选择的策略在\nBayesian Optimization 中也被称为 acquisition\nfunction；本文主要讲述其中的一种策略：UCB（Upper Confidence Bound)。\nEE tradeoff\n假设现在有了 10000 组候选超参，通过 GPR 分别预测出 10000\n组对应的均值和方差，那么该选择哪一组超参？这个问题可以很自然地通过 MAB(Multi-armed\nbandit) 建模，而选择期望收益最高的策略在 Bayesian Optimization 中\n也被称为 acquisition function，acquisition function\n有多种选择，其中最常用的便是 UCB\nGaussian Process\nOptimization in the Bandit Setting: No Regret and Experimental\nDesign 中最早提出了一种应用在 GPR 中的 UCB 方法，也称为\nGP-UCB，其方法的流程如下图所示\n\n\nGP-UCB\n\n上面的算法流程中每次迭代都是选择出一个 \\(x_t\\) ，通过训练或实验获得其真实的 \\(y_t\\), 并根据真实的结果更新 RBF kernel\n中的超参数；而选择 \\(x\\)\n根据的是其对应的均值和方差的加权求和的值，其中方差权重的值 \\(\\beta_t\\) 的选择跟 regret bound（MAB\n中的概念） 有关，上面的 paper\n中证明了这一点，具体证明过程可参考原文，这里只给出最终的结论\n\n\nGP-UCB-beta\n\n即 \\(\\beta =\n2\\log(|D|t^2\\pi^2/6\\delta)\\), 其中 \\(|D|\\) 为当前样本的个数，\\(\\delta\\) 需要根据置信程度选择，\\(\\delta\\) 越小，表示选择越为保守，更偏向于\nExploitation\nGP-UCB 方法每次只能选择一个 \\(x\\)\n进行试验，因此 Parallelizing\nExploration–Exploitation Tradeoffs with Gaussian Process Bandit\nOptimization 中对 GP-UCB 进行了简单的改进，每次能够选择多个 \\(x\\) 并行进行试验，其算法流程如下\n\n\nGP-BUCB\n\n上面的 fb 函数定义为 \\(fb[t] = \\lfloor\n(t-1)/B\\rfloor B\\), 则 \\(fb[0] =\n...fb[B] = 0, fb[B+1] = ... fb[2B] = B\\)，即每次选择 B\n组超参进行同时搜索，然后才根据反馈更新各组超参对应的 y 的均值\n，这种方法与在 GP-UCB 中直接选择 B 个最大的区别是这种方法在这 B\n次迭代中，每次都会根据选择了的 \\(x\\) 和\n\\(y\\) 更新 RBF kernel\n中的超参数，进而在下一次能够更新协方差矩阵\n新广告或者说冷启动问题中也常常涉及到 EE\n的问题，这里对两者的异同进行简单的比较\n其中最主要的差别是在新广告的中每个 arm (一个新广告)\n的收益是不能通过一次试验确定的，而在超参搜索中每个 arm (一组超参)\n的收益通过一次试验就能确定了\n在新广告的 EE 问题中，正是由于 arm\n的收益是不确定的，所以才需要多次的试验结果来确定每个 arm 的期望收益\n频率学派认为只要每个 arm 能够进行足够多次的实验，那每个 arm\n的收益便可通过后验数据统计得到，但是正因为无法进行足够多次的实验，才会在不够置信的历史收益基础上增加一个\nbound 作为 arm 的收益，而这就是 UCB\n的主要思想，如果考虑每次请求上下文那就是 Lin-UCB\n贝叶斯学派则认为每个 arm 的收益服从着特定的分布，首先会给每个 arm\n一个收益的先验分布，然后通过试验获得似然并更新先验分布获得后验，然后后验作为下一次的先验；通过迭代这个过程让每个\narm 的分布更加接近真实分布，而这就是 Thomson Sampling\n的思想，如果不考虑上下文，便是 Beta 先验 + Bernoulli\n似然；如果考虑了上下文，便是 Gaussian 先验 + Gaussian 似然 n\n关于这一类型的 EE 问题可参考 EE\n问题概述\n而在超参搜索的 EE\n问题中，问题就没那么复杂了，只要超参选定了，那么只要用这组超参进行模型的训练或\nAB 实验，其收益是确定的 (在其他实验条件基本确定下)\n小结\n本文主要介绍了超参搜索中的 Bayesian Optimization 中常用的 GPR+GP-UCB\n组合。GPR 是一种贝叶斯回归方法，能够回归出 y\n的分布而不是一个具体的值，其使用也不限于 Bayesian Optimization 中；而\nGP-UCB 与冷启动新广告中的 UCB 等 bandit\n算法也不完全相同，主要原因是在超参搜索中的一组超参 (arm) 的收益\n通过一次试验便能够确定了，而一个新广告（arm) 的收益在一次试验中并无法完全确定，其方法的区别可参考\nEE tradeoff 部分。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"通过 Flask, Docker, Jenkins 和 Kubernets 部署机器学习模型","url":"/2019/04/19/%E9%80%9A%E8%BF%87%20Flask,%20Docker,%20Jenkins%20%E5%92%8C%20Kubernets%20%E9%83%A8%E7%BD%B2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/","content":"本文主要介绍部署机器学习模型的一种自动化方式，如题所示，通过 Flask，Docker, Jenkins 和 Kubernets 实现。基本原理就是通过 Flask\n提供 RESTful\nAPI 接收客户端的 predict 请求，然后将这个服务打包成一个 docker image\n便于部署和迁移，当代码或模型更新时通过 Jenkins 触发自动构建新的 docker\nimage，而通过 kubernets\n管理容器则让整个服务具备伸缩性和可靠性。本文主要参考了 Deploy\na machine learning model in 10 minutes with Flask, Docker, and\nJenkins，并在其基础上进行了完善和拓展，如通过一个简单的 shell script\n实现 jenkins 的触发功能，并添加了 kubernets\n部分的介绍等。本文的对应的所有代码可从 DeployMachineLearningModel\n获取。\n\n下文基本可以依样画葫芦走一遍，为了避免不必要的麻烦，尽量不要在\nwindows 下配置，虽然上述这些工具也提供了 windows\n的版本，但是使用起来总是出现各种问题；也不要在 win10 的 wsl 中配置，因为\ndocker 涉及到了 linux 底层的 cgroup，在 wsl 中并不能直接安装\ndocker。本文的实验时最开始为了方便在上面提到的两个环境中进行了实验，结果是折腾了好久，最后通过在\nvirtual box 中的 ubuntu 16.04 进行以下的实验。\n下图摘自文章前面提到的 Deploy a machine learning model in 10 minutes\nwith Flask, Docker, and\nJenkins，从中可以看到清晰看到整个部署和访问的流程\n\n\ndeploy model\n\nFlask 提供 RESTful api\nFlask 的作用主要是提供 RESTful api 供客户端进行 predict，像\nGoogle、Microsoft、Face++ 这些公司提供的 AI\n服务（即人脸识别，表情识别等），基本都是通过 RESTful api\n提供的，其基本原理是客户端将通过 POST\n请求将需要预测的样本发送到服务器，然后服务器提取样本进行预测并返回结果；且通常还需要附带\nid\n判别身份，从而进行相应的扣费，这里为了简单起见不会去考虑这些问题。\n通过 Flask 能够非常简单地在搭建一个 HTTP Server\n并在指定端口监听，如果接收到 POST\n请求便调用模型进行预测并返回，因此首先需要训练模型并将训练好的模型 load\n进内存，为了简单起见，这里的任务是 sklearn 内置的 iris 分类。\n训练并保存模型\n训练并持久化模型的代码如下所示，对应 train_model.py\n文件\n# coding: utf-8import picklefrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn import tree# simple demo for traing and saving modeliris=datasets.load_iris()x=iris.datay=iris.target#labels for iris datasetlabels ={  0: \"setosa\",  1: \"versicolor\",  2: \"virginica\"}x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25)classifier=tree.DecisionTreeClassifier()classifier.fit(x_train,y_train)predictions=classifier.predict(x_test)#export the modelmodel_name = 'model.pkl'print(\"finished training and dump the model as {0}\".format(model_name))pickle.dump(classifier, open(model_name,'wb'))\n加载模型并提供调用 api\n通过 Flask 能够快速启动一个 http server\n并在不同的访问路径设置不同的处理函数，详细语法可参考官网教程。\n本文的例子很简单，如下代码所示（对应源文件\nserver.py)，首先把模型 load 进内存，然后设置了访问路径为\n/api 时调用模型进行\npredict，为了简单起见这里没做输入数据的检查和异常处理；最后\napp.run 启动了一个 server 并默认监听在 5000 端口。\n# coding: utf-8import picklefrom flask import Flask, request, jsonifyapp = Flask(__name__)# Load the modelmodel = pickle.load(open('model.pkl', 'rb'))labels = {  0: \"versicolor\",     1: \"setosa\",  2: \"virginica\"}@app.route('/api', methods=['POST'])def predict():    # Get the data from the POST request.    data = request.get_json(force = True)    predict = model.predict(data['feature'])    return jsonify(predict[0].tolist())if __name__ == '__main__':    app.run(debug = True, host = '0.0.0.0')\n利用以上两个文件，通过命令\npython train_model.py &amp;&amp; python server.py\n便可训练出一个模型并通过 http server 提供访问 api。\n客户端要进行预测时可通过如下代码（见源文件 client.py),\n这里的 192.168.31.78 是我的实验环境里面启动 httpserver\n的机器 ip（client.py 里面使用的是 8000 端口，因为利用了\ndocker 进行了端口映射，后文会对这一点进行讲解）\n# coding: utf-8import requests# Change the value of experience that you want to testurl = 'http://192.168.31.78:5000/api'feature = [[5.8, 4.0, 1.2, 0.2]]labels ={  0: \"setosa\",  1: \"versicolor\",  2: \"virginica\"}r = requests.post(url,json={'feature': feature})print(labels[r.json()])\n在同一局域网的机器运行上面的代码便能输出 setosa\n这个预测结果\nDocker 打包和运行程序\nDocker 的安装参考 Get\nDocker CE for Ubuntu， 这里不再赘述\n打包\n利用 Docker 可以将上述部署的环境打包成一个\nimage，便于部署、迁移和弹性扩展（配合 Kubernets\n使用），因此下文主要描述如何通过 Dockerfile 构建 image，关于 Dockerfile\n的详细语法可参考 文档，这里只列出本文用到的一些语法。\n类似 shell 脚本，Dockerfile 里面是一系列的指令，作用是让 Docker 通过\nDockerfile 和 docker build 命令自动构建出目标 image。\n在执行 docker build 命令时通过 -t 指定生成的 image 的\ntag，能够保存生成的 image，如\ndocker build -t shykes/myapp .，最后的 . 表示\nDockerfile 的目录，即这条命令是在 Dockerfile 所在目录下执行\nDockerfile 的基本原理是首先通过 FROM 命令获取一个基本的\nimage，然后在这个 image\n基础上通过各种命令配置好我们运行程序需要的环境，接着把我们的源文件复制到\nimage 里，进行构建和运行。\nDockerfile 中值得注意事项如下，为了保持原意这里不进行翻译\n\neach instruction is run independently, so RUN cd /tmp\nwill not have any effect on the next instructions\nbasic syntax is INSTRUCTION arguments， the\ninstruction is not case-sensitive. However,\nconvention is for them to be UPPERCASE to distinguish\nthem from arguments more easily.\nA Dockerfile must start with a FROM\ninstruction. The FROM instruction specifies the Base Image from\nwhich you are building\nFROM can appear multiple times within a single\nDockerfile to create multiple images or use one build stage as a\ndependency for another\nDocker treats lines that begin with # as a comment\nRUN &lt;command&gt; (the command is run in a shell,\nwhich by default is /bin/sh -c on Linux or\ncmd /S /C on Windows\nThere can only be one CMD instruction in a\nDockerfile. If you list more than one CMD then only the last\nCMD will take effect.\nRUN v.s CMD. RUN\nactually runs a command and commits the result; CMD does\nnot execute anything at build time, but specifies the intended command\nfor the image.\nThe WORKDIR instruction sets the working directory for\nany RUN, CMD, ENTRYPOINT, COPY and ADD\ninstructions that follow it in the Dockerfile. If the\nWORKDIR doesn’t exist, it will be created even if it’s not\nused in any subsequent Dockerfile instruction\nCOPY &lt;src&gt;... &lt;dest&gt;; The\nCOPY instruction copies new files or directories from\n&lt;src&gt; and adds them to the filesystem of the\ncontainer at the path &lt;dest&gt;;The\n&lt;dest&gt; is an absolute path, or a path relative to\nWORKDIR, If &lt;dest&gt; doesn’t exist, it is\ncreated along with all missing directories in its path.\nADD &lt;src&gt; &lt;dest&gt;; The ADD\ninstruction copies new files, directories or remote file\nURLs from &lt;src&gt; and adds them to the\nfilesystem of the image at the path &lt;dest&gt;\nCOPY v.s ADD. COPY only lets\nyou copy in a local file or directory from your host\n(the machine building the Docker image) into the Docker image itself.\nADD lets you do that too, but it also supports 2 other\nsources. First, with ADD you can use a remote\nURL instead of a local file / directory. Secondly, you\ncan extract a tar file** from the source directly into the\ndestination.\nEnvironment variables (declared with the ENV statement)\ncan also be used in certain instructions as variables to be interpreted\nby the Dockerfile; Environment variables are notated in the Dockerfile\neither with $variable_name or\n${variable_name}\n\n因此，构建上述的环境的 Dockerfile 如下所示，参考链接中的 Dockerfile\n中有两个 FROM 语句，分别表示 ubuntu 环境和 python 环境，且需要安装 pip\n等工具，这里直接通过 nitincypher/docker-ubuntu-python-pip\n提供这些功能\n# train and run the model with RESTful apiFROM nitincypher/docker-ubuntu-python-pipCOPY ./requirements.txt /app/requirements.txtWORKDIR /appRUN pip install -r requirements.txtCOPY . /appCMD python /app/train_model.py &amp;&amp; python /app/server.py\n实验的项目路径为 /opt/src/DeployMachineLearningModel,\n则构建 image 的命令为 docker build -t deploy_ml_model .,\n其过程如下所示，可以看到\n1) 构建前系统的 docker images\n情况，由于之前已经运行过这条命令，因此依赖的\nnitincypher/docker-ubuntu-python-pip 也已经 pull\n到本地了。如果是第一次运行，则下载\nnitincypher/docker-ubuntu-python-pip 需要一定的时间 2)\nDockerfile 中每条命令都是运行时的一个 step，在构建时不会运行\nCMD 的命令，而是通过 docker run 时才执行\n\n\nbuild\n\n构建完成后可以看到系统中的多了 deploy_ml_model 这个\nimage\n\n\nafter build\n\n运行\n接着需要运行这个 image，运行的 container 内部 Flask 在监听 5000\n端口，因此需要通过端口映射为外部机器可见的端口，通过命令\ndocker run -p 8000:5000 deploy_ml_model 可通过运行 docker\n的机器的 8000 端口访问 container 内部提供的 api，如下所示\n\n\nrun container\n\n将上面的客户端的代码的端口改成 8000 便是 client.py\n源文件了，运行 client.py 结果如下所示，\n\n\nresponse\n\n此时的 server 接收到一个 POST 请求，输出的日志如下\n\n\nserver log\n\n如果需要停止运行的 container，通过 docker stop 并指定\ncontainer 的 id 即可， container id\n并不需要全输入，只需要输入能系统能区分不同 container\n的程度即可。该过程如下所示\n\n\nstop container\n\nJenkins 或自定义脚本触发自动构建\n上面的构建流程中，只要每次代码或模型有更新便需要重新手动执行\ndocker build 和 docker run, 而通过 jenkins\n或自定义的脚本便能让这个流程自动化，这个过程需要结合 Github\n实现，即当代码仓库有更新时，便自动构建新的 image。\n其基本原理是 Github 在 repository 发生变化时，会向指定的 url 发送一个\nPOST 请求告知 repository 有更新，只要我们监听这个 url 并在收到这个 POST\n请求时进行更新即可，这个机制在 Github 中被称为 WebHooks。Github\n提供的 WebHooks 中涵盖了多种更新情况，不同的更新对应于不同的\nevent，可以在 Github 中自定义需要触发的事件，默认触发的是 PUSH\n事件（如 commit、PR 等）。\nJenkins 自动构建\nJenkins 在 Ubuntu 下的安装参考 Installing\nJenkins，这里不再赘述\nJenkins 是一个功能齐全的自动化构建工具，类似 Docker 通过 Dockerfile\n定义 image 的构建过程，jenkins 也能通过 Jenkinsfile\n定义工程的构建过程。\n但是本文只用到其接收到 Github 发送的 POST\n请求并触发其重新构建的功能，其配置流程如下，首先新建一个自由风格的项目，并配置其为\nGithub 项目，管理源码的方式为 git，如下所示\n\n\nconfigure\n\n然后配置触发方式和构建的命令如下图所示\n\n\nconfigure\n\n配置并保存后便可直接 “立即构建” 进行项目的构建，jenkins\n会自动下载仓库并进行构建，通过控制台输出可以看到构建过程，该过程如下所示\n\n\nbuild\n\n点击控制台输出后显示的日志\n\n\nlog\n\n上面提到了触发 jenkins 自动构建的原理，即当代码仓库有更新时，github\n会发送 POST 请求给 jenkins，然后 jenkins 会进行自动构建，这种情况下\njenkins 首先需要有一个能够接受 github 的 POST 请求的 url，但是 jenkins\n当前是部署在局域网内部的，这时便需要借助 ngrok 这个工具来生成一个 github 能够访问的\nurl 了\nngrok 的作用就是为局域网内部的机器生成一个 public\nurl，从而使得内部的服务能够被其他机器访问，其基本原理就是 ngrok\n在这个访问过程中提供了中转。ngrok 的下载和安装都很简单，可参考上面上面的\nngrok 的官网，这里不再赘述。\n由于 jenkins 在本地的端口是 8080，因此通过 ngrok 为 jenkins 生成\npublic url 如下所示，可以看到生成了 http 和 https\n两个类型的地址；最下面显示的是最近的请求情况，可以看到 github\n发送了 3 个更新的 POST 请求\n\n\nngrok\n\n得到 public url 后，需要将其配置到 github 项目的 webhook 中，打开\ngithub 项目的地址，点击 setting 进行设置，设置如下所示，Payload URL\n为通过 ngrok 得到的 public url 加上 /github-webhook/\n路径，注意不能省略最后的斜杆，否则会出现 403 No valid\ncrumb was included in the request 的错误\n\n\nwebhook\n\n点击 update webhook 后（第一次是 save）后，github 便会向 payload url\n发送一个 POST 请求，就是在上上一张图最下方显示的 POST 请求。\n这样当 github 的仓库有更新时就会自动触发 jenkins\n进行自动构建，但是由于前一个构建任务会一直运行 http server\n接受，因此会出现如下图的 already in progress 的问题，新的 build\n会被挂起，直到前一个 build 被终止（通过 docker stop)\n关掉服务\n\n\ntrigger build\n\n针对这个问题，这个 issue Pushing new\ncommit to existing PR does not stop previous build 给出了通过配置\njenkins\n的解决方法，但是我在我配置的环境中找不到这个设置选项，试了几遍后却依然找不到这个配置选项，所以就有了下面的自定义脚本进行自动构建。\n而针对这个问题，令一种解决方法是在构建命令时只写\ndocker build, 每次都只是生成最新的 image；而\ndocker run\n留给人工去启动，但是这样可能就显得不那么自动化了。\n自定义脚本进行自动构建\n细想一下上面的触发构建过程，本地需要做的是 jenkins 接受 github\n发过来的 POST 请求然后启动 docker build 和\ndocker run, 然后由于已经有 container\n在跑了，因此无法决定启动新的构建过程。\n那其实我们也可以自己建立一个 http server 接受 github 的 POST\n请求，在接受到请求后通过 docker stop 停掉当前正在运行的\ncontainer 并开始新的构建过程，而借助前文描述的\nFlask，我们可以很容易建立一个接受 POST 请求的 http\nserver，代码如下所示 (见源文件 hook_server.py)\n# -*- coding: utf-8 -*-from flask import Flask, jsonifyimport subprocessapp = Flask(__name__)@app.route('/github_webhook', methods=['POST'])def rebuild():    print('new commits to github repository')    ## subprocess.run can just deal with the first change    ## since it stuck in it, use popen instead    # subprocess.run(['sh', 'build_and_run.sh'])    subprocess.Popen(['sh', 'build_and_run.sh'])    return jsonify('got it')if __name__ == '__main__':    app.run(debug=True, host='0.0.0.0', port=8081)\n为了保持一致性，这里的路径也选择为 /github_webhook,\n为了简单起见，处理的函数只是接受请求，没有对 POST\n请求做进一步的解析，接收到命令后通过 subprocess\n新创建一个进程执行重新构建并运行 docker image 的脚本\nbuild_and_run.sh, 注意这里要使用\nsubprocess.Popen 而不是\nsubprocess.run, 因为 subprocess.run\n要等命令执行返回才能继续往下执行，而我们启动的服务也是一个 http\nserver。如果使用 subprocess.run\n只能在第一次的更新时触发自动构建，之后会一直保持在新创建的进程中而无法处理\ngithub 发过来的新的请求，因此要使用 subprocess.Popen\n避免这个问题，两者更详细的区别可参考 What\nis the difference between subprocess.popen and subprocess.run\n上面执行的脚本 build_and_run.sh 的具体内容如下，首先通过\ngit pull 更新代码，这里项目的代码的本地路径为\n\"/opt/src/DeployMachineLearningModel/\"，然后判断当前是否有正在运行的\ncontainer，如果有则先 stop，然后再执行构建过程，在构建和运行之间通过\ndocker image rm（等价于 docker rmi）删除\ndocker 的 &lt;none&gt;:&lt;none&gt; images, 这些 images\n也被称为 dangling images, 是被覆盖的原来的\nimage，会占用额外的磁盘空间，详细信息可参考 What\nare Docker &lt;none&gt;:&lt;none&gt; images?。\n#!/bin/bash# update codeproject_dir=\"/opt/src/DeployMachineLearningModel/\"cd $project_dir &amp;&amp; git pull# build and run with new coderunning_container=$(docker ps | grep deploy_ml_model | awk  -F ' ' '{print $1}')if [ -n \"$running_container\" ]; then    echo \"container id not empty, stop it firstly\"    docker stop $running_containerelse    echo \"empty container id\"fidocker build -t deploy_ml_model .docker image rm -f $(docker images -f \"dangling=true\" -q)docker run -p 8000:5000 deploy_ml_model\n同样需要通过 ngrok 映射本地的 http server 到一个 public url 并将\npublic url 添加到 github 项目的 webhook 中，如下图所示\n\n\nngrok script\n\n\n\nscript github webhook\n\n通过 python hook_server.py 运行脚本监听指定的 repository\n是否有新的 commit，如果有，则触发运行 build_and_run.sh\n脚本，其过程如下所示\n\n\nscript trigger run\n\nKubernets\n通过上面的三个步骤，已经基本能够形成一个自动化的部署方案了，个人的自娱自乐基本也够了，但是上面还只是单点的服务，缺乏高可用性和伸缩性。\n针对这一点，Docker 会经常与 Kubernets 配合使用，Kubernets\n是专门为容器化应用的自动部署、拓展和管理的一个分布式系统。Kubernets\n的前身是 Google 内部的系统 Brog，而 google 也参与了 Kubernets\n的设计，Kubernets + 容器的部署方式应该会是未来的发展趋势，这里主要根据\nLearn\nKubernetes Basics 总结 Kubernets\n的一些经典的使用方式。包括应用的部署，拓展，滚动更新等。\n由于实验环境需要多台机器，虽然 Minikube\n能够在单机上实现 Kubernets sigle-node 集群，但是根据 Install\nMinikube ，virtual box 中的虚拟机似乎不支持 VT-x or AMD-v\nvirtualization，因此，这里直接使用 Learn\nKubernetes Basics 提供的 shell 环境。\n基本架构\nKubernets cluster\n是经典主从架构（master-nodes)，主（master）负责管理集群，如调度、拓展、更新等，从（nodes) 则负责计算或提供服务，每个\nnode 通过 Kubelet 这个 agent 与 master\n通信，除此之外，node 中还要有容器运行环境如 Docker 或\nrkt。基本架构如下图所示\n\n\nbasic structure\n\nKubernets 提供的命令行程序 Kubectl（注意与 node 的\nKubelet 区分）能够获取与集群通信，获取集群信息，部署应用等，如下图是通过\nkubectl 获取通过 Minikube 启动的 Kubernets 集群的一些信息\n\nkubectl cluster-info：提供 web 界面查看应用的具体信息\n kubectl nodes：显示所有的 nodes 的信息\n\n\n\n basic info\n\n部署 (deployment)\n部署应用到 Kubernets 集群时，需要构建好要运行的 docker image\n的路径，部署使用的也是命令行程序 kubectl，命令是\nkubectl run NAME --image=image_url, NAME\n是指定的应用的名称，--image 则是指定的 image 的 url，通过\nkubectl get deployments\n可以看到当前部署的应用，如下图所示\n\n\ndeploy with run\n\n在 Kubernets cluser\n中启动了应用后，外部网络是无法直接访问这个应用的，这点跟 Docker\n有点相似，需要做映射，但是为了调试的便利性，kubectl 提供了\nkubectl proxy\n这个命令，相当于把 Cluster 内部的地址映射到本地机器，启动之后可通过本机访问\nKubernets cluser 内部 的应用。如下图所示是访问上面启动的应用\n\n\nkubectl proxy\n\nPods\n上面通过 kubectl 进行部署后，Kubernets 会在 node 中创建了 Pod\n来容纳 container，一个 node 中可能有多个 pod，Kubernetes 的 master\n会根据 node 的资源情况在不同 node 中分配 pod；pod 是 container\n和 其所包含的资源的机器，其定义如下，\n\nA Pod is a Kubernetes abstraction that represents a group of one or\nmore application containers (such as Docker or rkt),\nand some shared resources for those containers. Those\nresources include:\n\n\nShared storage, as Volumes\nNetworking, as a unique cluster IP address\nInformation about how to run each container, such as the container\nimage version or specific ports to use\n\nPod 相当于应用的 “逻辑主机”，而 a group of containers\n值得是一个应用中有若干个联系紧密的 container 协作，这些 containers\n具有相同的 IP。\n\n\npod\n\n除了 kubectl run, kubectl 常用的命令一下这些\n\nkubectl get：列出当前系统的资源（pods、nodes 等），后面跟着\n kubectl describe：列出资源的详细信息\n\n如下是通过这两条命令获取前面部署的应用的 pod 信息\n\n\npod info\n\n下面的命令则是查看 pod 的日志信息在 pod 中的 container\n执行命令，通过命令\nexport POD_NAME=$(kubectl get pods -o go-template --template '{ {range .items} }{ {.metadata.name}}{ {\"\\n\"}}{ {end}}')\n能够获取当前的 pod name\n\nkubectl logs $POD_NAME：打印 pod 中的 container\n的日志信息\nkubectl exec $POD_NAME: 在 pod 中的 container\n执行命令\n\n下面首先通过命令获取了 pod 的名称，然后通过 pod\n的名称查看其日志并执行命令，执行效果如下所示\n\n\nlog exec\n\nService\nService 可以说是比 Pod 更高一级的概念，假设部署某个应用时指定其\nreplicas 的数量是 3，那么就会有 3 个相互独立的 pods，每个 pod 都有自己的\nip，，而 service 就是这些 pods 的集合。Service 管理着这些 pod\n的失败重启等，从而向上提供 Pod 的抽象；service 的概念如下图所示\n\n\nservice\n\n关于 service 的定义如下\n\nA Service in Kubernetes is an abstraction which defines a logical set\nof Pods and a policy by which to\naccess them\n\n除了 pods，service 中还有一项是 policy，指的是让 cluster 内部的 pod\n供外界进行访问的方式，service 可设置的访问方式有下面四种\n\n\nClusterIP (default) - Exposes the Service on an\ninternal IP in the cluster. This type makes the Service only reachable\nfrom within the cluster.\nNodePort - Exposes the Service on the same port of\neach selected Node in the cluster using NAT. Makes a Service accessible\nfrom outside the cluster using\n&lt;NodeIP&gt;:&lt;NodePort&gt;. Superset of\nClusterIP.\nLoadBalancer - Creates an external load balancer in\nthe current cloud (if supported) and assigns a fixed, external IP to the\nService. Superset of NodePort.\nExternalName - Exposes the Service using an\narbitrary name (specified by externalName in the spec) by returning a\nCNAME record with the name. No proxy is used. This type requires v1.7 or\nhigher of kube-dns.\n\n\n通过 kubectl expose 能够让集群内部的 service\n供外界访问，如下指定的访问方式是 NodePort, kubernets\n默认会启动一个 keubernets 服务，就是第一条\nkubectl get services 所显示的内容，而经过\nkubectl expose 的服务也会出现在其中，内部端口 8080\n被映射为了外部的 32066 端口，通过外部 ip（命令中的 minikube ip） 和 32066\n端口便能访问内部的服务。\n\n\nexpose\n\nService 通过 Labels\n和 Selectors 来区分同一个 service 中的不同 pod，label 就是一系列的\nkey-value 对，label\n可结合具体的应用场景进行使用，如区分开发、测试和生产环境的\npod；区分同一个 pod 的不同版本等。\n部署时每个 pod 会被自动分配一个 label；通过\nkubectl describe deployment 查看其对应的 label，也可以在\nkubectl get 查看 pod 或 services 的信息时通过\n-l 参数指定具体的 pod 或 service，如下图所示\n\n\nsee label\n\n通过 kubectl label 可更改 pod 的 label，如下图所示\n\n\nchange label\n\n可以根据 label 删除 service，此时虽然外部无法访问 pod，但是集群内部的\npod 仍然在运行，如下图所示\n\n\ndelete service\n\n伸缩性 (scaling)\n伸缩性就是改变运行同一个 image 的 pods 的数量，如下图所示\n\n\npod\n\n可以通过 kubectl scale 命令指定 replica 的数量，也可以自动伸缩，如下图所示是将原来只有一个\npod 的 deployment 拓展到 4 个 pod， 从\nkubectl get deployments 可以看到当前 deployment 可用的 pod\n的数量\n\n\nscale pod\n\n而有了多个 pod, service 就要决定如何分配访问这些 pods\n的流量，上面提到的 service 设置的访问方式 LoadBalancer\n就是在这里使用 (需要注意的是 NodePort 和 LoadBalancer\n是可以共存)，通过下面访问多次的结果，可以看到每次访问的 pod\n都不一样，从而实现了负载均衡\n\n\nload balance\n\n滚动更新 (rolling updates)\n有了多个 pods，在更新 images 时便可以进行 rolling\nupdate，即不是一次性地 stop 所有 pods 然后同时进行更新，而是先停掉部分的\npods，然后进行更新，并根据这个方法更新所有的 pods。如下图所示\n\n\nrolling update\n\n这样的好处是在更新时不会让服务停止，如下图所示是更新前 pod\n的一些信息，可以看到此时 image 的版本均为 v1\n\n\nbefore update\n\n下面通过 kubectl set 更新上图所示的 deployment，使用了\nv2 版本的 image，在 kubectl set 后，可以看到原来的 pod 处于\nterminating 的状态，且多了四个新的 pod（可通过 AGE 区分），随着 update\n完成，只有新的 pods 在运行，image 版本均变为了 v2，通过\nkubectl rollout status 可以查看更新的情况。\n\n\nafter update\n\n除此之外，\nKubernets 中的每次更新都有版本记录，可进行回滚，如下图更新了一个不存在的\nimage，从 kubectl get pods 可以看到新的 pod 的状态是\nErrImagePull，通过 kubectl rollout undo\n即可进行版本的回滚，最后所有 pods 的状态均恢复正常，image 版本均为\nv2，如果再进行一次 kubectl rollout undo，那么 image\n版本就变为 v1 了。\n\n\nrollback\n\n总结\n本文主要介绍了部署机器学习模型的一种方式，通过 Flask，Docker，Jenkins\n和 Kubernets 共同构建。Flask 负责加载模型并提供 RESTful api，Docker\n负责把程序及其依赖的环境打包成镜像，Jenkins\n则可以在代码仓库有更新时触发自动构建，生成最新的\nimage，本文也通过自定义脚本的方式来实现了这一简单功能，但是 Jenkins\n是一个功能非常丰富的工具，在项目更大更复杂时，采用 Jenkins\n会更加方便。\n通过 Flask，Docker 和 Jenkins\n可以实现基本的自动化部署，但是此时的服务是单点的，不具备容灾性和伸缩性，通过\nKubernets 则可以较好地解决这个问题，只需要提供打包好的镜像，Kubernets\n便能够提供伸缩性服务，滚动更新，回滚等操作。\n","categories":["工具使用"],"tags":["机器学习","工具使用"]},{"title":"通过 sklearn 进行大规模机器学习","url":"/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","content":"sklearn 是 python\n中一个非常著名的机器学习库，但是一般都是在单机上使用而不支持分布式计算，因此往往跟大规模的机器学习扯不上关系。这里通过\nsklearn\n进行的大规模机器学习指的也不是分布式机器学习，而是指当数据量比内存要大时怎么通过\nsklearn 进行机器学习，更准确来说是 out-of-core learning，\n这里涉及到的一个核心思想是将数据转化为流式输入，然后通过 SGD\n更新模型的参数，当然其中还涉及到一些其他的细节和 trick，下面会详细描述。\n\nout-of-core learning\n上面的问题也称为 out-of-core learning，\n指的是机器的内存无法容纳训练的数据集，\n但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling\n与 mini-batch learning。\n第一种方法是 sampling（采样），采样\n可以针对样本数目或样本特征，能够减少样本的数量或者 feature 的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致\nvariance inflation\n问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用\nbias-variance 来解释就是出现了 high\nvariance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。\n解决采样带来的 high-variance\n问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有\nbagging。\n第二种方法是 mini-batch learning，这种方法不同于 sampling，\n利用了全部的数据，\n只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD\n恰好就能够提供这种模式的训练，因此 SGD\n是这种模式训练的核心。下面也主要针对这种方法进行讲述。\n通过 SGD\n进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息，如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。\n流式读取数据\n文件读取\n这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python\n读取文件的 readline 方法实现\nwith open(source_file, 'rb') as f:    line = f.readline()    while line:        # data processing        # training        line = f.readline()\n而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过\ncsv\n模块进行读取，下面以这个数据文件为例说明：https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\nSEP = ',' with open(source_file, 'rb') as f:    iterator = csv.reader(f, delimiter = SEP)    for n, row in enumerate(iterator):        if n == 0:            header = row        else:            # data processing            # training            pass    print ('Total rows: %i' % (n+1))    print ('Header: %s' % ', '.join(header))    print ('Sample values: %s' % ', '.join(row))\n输出为\nTotal rows: 17380Header: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cntSample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49\n在上面的例子中，每个样本是就是一个 row（list 类型），样本的 feature\n只能通过 row[index] 方式获取，假如要通过\nheader 中的名称获取， 可以修改上面获取\niterator 的代码，用\ncsv.DictReader(f, delimiter = SEP) 来获取\niterator，此时得到的 row 会是一个\ndictionary，key 为 header 中的列名，value 为对应的值；\n对应的代码为\nwith open(source_file, 'rb') as R:    iterator = csv.DictReader(R, delimiter=SEP)    for n, row in enumerate(iterator):        # data processing        # training        pass    print ('Total rows: %i' % (n+1))    print ('Sample values: %s' % row)\n输出为\nTotal rows: 17379Sample values: {'mnth': '12', 'cnt': '49', 'holiday': '0', 'instant': '17379', 'temp': '0.26', 'dteday': '2012-12-31', 'hr': '23', 'season': '1', 'registered': '37', 'windspeed': '0.1343', 'atemp': '0.2727', 'workingday': '1', 'weathersit': '1', 'weekday': '1', 'hum': '0.65', 'yr': '1', 'casual': '12'}\n除了通过 csv 模块进行读取，还可以通过\npandas 模块进行读取，pandas 模块可以说是处理 csv\n文件的神器，csv 每次只能读取一条数据，而\npandas 可以指定每次读取的数据的数目，如下所示\nimport pandas as pdCHUNK_SIZE = 1000with open(source_file, 'rb') as R:    iterator = pd.read_csv(R, chunksize=CHUNK_SIZE)     for n, data_chunk in enumerate(iterator):        print ('Size of uploaded chunk: %i instances, %i features' % (data_chunk.shape))        # data processing        # training        pass    print ('Sample values: \\n%s' % str(data_chunk.iloc[0]))\n对应的输出为 Size of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 379 instances, 17 featuresSample values: instant            17001dteday        2012-12-16season                 4yr                     1mnth                  12hr                     3holiday                0weekday                0workingday             0weathersit             2temp                0.34atemp             0.3333hum                 0.87windspeed          0.194casual                 1registered            37cnt                   38Name: 17000, dtype: object\n数据库读取\n上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不经能够有效进行增删查改等操作，而且通过\nDatabase\nNormalization 能够在不丢失信息的基础上减少数据冗余性。\n假设上面的数据已经存储在 SQLite\n数据库中，则流式读取的方法如下\nimport sqlite3import pandas as pdDB_NAME = 'bikesharing.sqlite'CHUNK_SIZE = 2500conn = sqlite3.connect(DB_NAME)conn.text_factory = str  # allows utf-8 data to be stored     sql = \"SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)\"DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)for j,data_chunk in enumerate(DB_stream):    print ('Chunk %i -' % (j+1)),    print ('Size of uploaded chunk: %i istances, %i features' % (data_chunk.shape))    # data processing    # training    pass\n输出为\nChunk 1 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 2 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 3 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 4 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 5 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 6 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 7 - Size of uploaded chunk: 2379 istances, 18 features\n样本的读取顺序\n上面简单提到了通过 SGD\n训练模型的时候，需要注意样本的顺序必须要是随机打乱的。\n假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像\nSGD 这种 online learning\n的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。\n这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行\nshuffle ，然后再通过 SGD 来进行训练。\n假如内存能够容纳这些数据， 那么所有的数据可以在内存中进行一次\nshuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行\nshuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle\n后再拼接， 下面是这两种 shuffle 方法的实现代码。\n在内存中进行 shuffle 之前可以通过 zlib\n对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下\nimport zlibfrom random import shuffledef ram_shuffle(filename_in, filename_out, header=True):    with open(filename_in, 'rb') as f:        zlines = [zlib.compress(line, 9) for line in f]        if header:            first_row = zlines.pop(0)    shuffle(zlines)    with open(filename_out, 'wb') as f:        if header:            f.write(zlib.decompress(first_row))        for zline in zlines:            f.write(zlib.decompress(zline))\n基于磁盘的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行\nshuffle， 为了能够实现整个数据集更彻底的 shuffle\n，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下\nfrom random import shuffleimport pandas as pdimport numpy as npimport osdef disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=','):    for i in range(iterations):        with open(filename_in, 'rb') as R:            iterator = pd.read_csv(R, chunksize=CHUNK_SIZE)             for n, df in enumerate(iterator):                if n==0 and header:                    header_cols =SEP.join(df.columns)+'\\n'                df.iloc[np.random.permutation(len(df))].to_csv(str(n)+'_chunk.csv', index=False, header=False, sep=SEP)        ordering = list(range(0,n+1))        shuffle(ordering)        with open(filename_out, 'wb') as W:            if header:                W.write(header_cols)            for f in ordering:                with open(str(f)+'_chunk.csv', 'r') as R:                    for line in R:                        W.write(line)                os.remove(str(f)+'_chunk.csv')        filename_in = filename_out        CHUNK_SIZE = int(CHUNK_SIZE / 2)\nsklearn 中的 SGD\n通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD\n进行训练，在 sklearn 中，\nsklearn.linear_model.SGDClassifier 和\nsklearn.linear_model.SGDRegressor 均是通过 SGD\n实现，只是一个用于分类，一个用于回归。下面以\nsklearn.linear_model.SGDClassifier\n为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。\n需要注意的参数有：\n\nloss : 表示具体的分类器，可选的值为\nhinge、log、modified_huber、squared_hinge、perceptron；如\nhinge 表示 SVM 分类器，log 表示 logistics regression 等\npenalty：正则项，用于防止过拟合 (默认为 L2 正则项)\nlearning_rate:\n表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档\n\n需要注意的方法主要就是 partial_fit(X, y, classes),\nX 和 y 是每次流式输入的数据，而\nclasses 则是具体的分类数目，若 classes\n数目大于 2，则会根据 one-vs-rest 规则训练多个分类器。\n需要注意的是 partial_fit\n只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的\nSGDClassfier 的一个简单例子。\nfrom sklearn import linear_modelimport pandas as pdCHUNK_SIZE = 1000n_iter = 10  # number of iteration over the whole datasetn_class = 7model = linear_model.SGDClassifier(loss = 'hinge', penalty ='l1',)for _ in range(n_iter):     with open(source_file, 'rb') as R:        iterator = pd.read_csv(R, chunksize=CHUNK_SIZE)         for n, data_chunk in enumerate(iterator):            model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class)))\n流式数据中的特征工程\nfeature scaling\n对于 SGD 算法，特征的 scaling\n会影响其优化过程，也就是只有将特征标准化（均值为 0，方差为 1）或归一化（处于\n[0,1]\n内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历\n2\n次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。\n由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为 (\n\\(\\mu\\) 为均值）\n\\[\\begin{align} \\sigma^2 = \\frac{1}{n}\n\\sum_x(x-\\mu)^2 \\end{align}\\]\n只有知道均值才能求解， 这意味着只有遍历一次求得 \\(\\mu\\) 后才能求 \\(\\sigma^2\\),\n这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得 \\(\\mu\\) 和 \\(\\sigma^2\\) 能够同时求出\n假如当前有 \\(n\\) 个样本，当前的均值\n\\(\\mu'\\) 可以简单求出，而当前的方差\n\\(\\sigma'^2\\) 可通以下公式求解\n\\[\\begin{align} \\sigma'^2 =\n\\frac{1}{n} \\sum_x(x^2 - 2x\\mu' + \\mu'^2) = \\frac{1}{n}\n\\sum_x(x^2) - \\frac{1}{n}(2n\\mu'^2 - n\\mu'^2) = \\frac{1}{n}\n\\sum_x(x^2) - \\mu'^2 \\end{align}\\]\n通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行\nshuffle 前后两者在均值和方差上的区别。\n# calculate the running mean,standard deviation, and range reporting the final resultimport os, csvraw_source = 'bikesharing/hour.csv' # unshuffleshuffle_source = 'bikesharing/shuffled_hour.csv'def running_statistic(source):    SEP=','    running_mean = list()    running_std = list()    with open(local_path+'/'+source, 'rb') as R:        iterator = csv.DictReader(R, delimiter=SEP)        x = 0.0        x_squared = 0.0        for n, row in enumerate(iterator):            temp = float(row['temp'])            if n == 0:                max_x, min_x = temp, temp            else:                max_x, min_x = max(temp, max_x),min(temp, min_x)            x += temp            x_squared += temp**2            running_mean.append(x / (n+1))            running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5)            # DATA PROCESSING placeholder            # MACHINE LEARNING placeholder            pass        print ('Total rows: %i' % (n+1))        print ('Feature \\'temp\\': mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f' \\               % (running_mean[-1], max_x, min_x, running_std[-1]))        return running_mean, running_stdprint '===========raw data file==========='raw_running_mean, raw_running_std = running_statistic(raw_source)print '===========shuffle data file==========='shuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source)\n输出如下 ===========raw data file===========Total rows: 17379Feature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193===========shuffle data file===========Total rows: 17379Feature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193\n两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的\nrunning_mean 和 running_std 进行可视化\n# plot how such stats changed as data was streamed from disk# get an idea about how many instances are required before getting a stable mean and standard deviation estimateimport matplotlib.pyplot as plt%matplotlib inlinefor mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)):    plt.plot(mean,'r-', label='mean')    plt.plot(std,'b-', label='standard deviation')    plt.ylim(0.0,0.6)    plt.xlabel('Number of training examples')    plt.ylabel('Value')     plt.legend(loc='lower right', numpoints= 1)    plt.show()# The difference in the two charts reminds us of the importance of randomizing the order of the observations. \n得到的结果如下\n原始的文件 \nshuffle 后的文件 \n可以看到，经过 shuffle\n后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD\n算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。\nhasing trick\n对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行\none-hot 编码需要知道这个 feature\n所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到\ncategorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的\nhashing trick 对 categorical feature 进行 one-hot\n编码，这种方法对只能遍历一遍的数据有效。\nhahsing trick 利用了 hash\n函数，通过 hash 后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对\ncategorical feature\n的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对 categorical\nfeature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用\nsklearn 中的 HashingVectorizer 进行这种编码的一个例子\nfrom sklearn.feature_extraction.text import HashingVectorizerh = HashingVectorizer(n_features=1000, binary=True, norm=None)sparse_vector = h.transform(['A simple toy example will make clear how it works.'])print(sparse_vector)\n输出如下 (0, 61)\t1.0(0, 271)\t1.0(0, 287)\t1.0(0, 452)\t1.0(0, 462)\t1.0(0, 539)\t1.0(0, 605)\t1.0(0, 726)\t1.0(0, 918)\t1.0\n这里定义的槽列的长度为 1000，即假设字典中的单词数目为 1000，\n然后将文本映射到这个槽列中，1 表示有这个单词，0 表示没有。\n总结\n本文主要介绍了如何进行 out-of-core\nlearning，主要思想就是将数据以流式方式读入，然后通过 SGD\n算法进行更新，在读入数据之前，首先需要对数据进行 shuffle\n操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"通过 python 发送邮件提醒网站的新评论","url":"/2016/01/17/%E9%80%9A%E8%BF%87%20python%20%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E6%8F%90%E9%86%92%E7%BD%91%E7%AB%99%E7%9A%84%E6%96%B0%E8%AF%84%E8%AE%BA/","content":"这篇文章是当时在新浪云上搭建博客的时候写的，后来因为新浪云收费了，把网站迁移到了 github 上。这里还是把文章贴出来，做个记录。\n最近在写本站的评论提醒功能的时候，需要通过 python\n发送邮件提醒具体哪些文章有了新评论，采用邮件的方式便于在特定时间处理所有的评论，比如说在第二天早上 7 点检查网站昨天是否有新的评论，假如有就会发送邮件显示那些有新评论的文章。\n\n实现思路如下：先检查数据库中是否有新的评论，假如有新的评论就发送邮件，否则不发送。\n如何检查新评论？在存储评论的表中有一个字段表示添加该评论的时间（datetime 类型），利用这个字段能够在今天判断昨天是否有新的评论，现假设评论表为 comment, 表示评论添加时间的字段为 created_date, 采用的 SQL 语句如下：\nselect * from comment  where date(created_date) =( select date_sub(curdate(),interval 1 day)  \n假如查询的语句不为空，那么说明有了新的评论，可以将关于评论的具体内容以邮件形式发送给管理员。下面是 python 发送邮件的实现方法之一：\nPS：虽然新浪云提供了用于发送邮件的 Mail 服务，当时对于普通用户每天好像会限定发送的次数，而且最严重的问题是邮件的延迟非常严重，建议还是使用 smtplib 模块。\n#encoding:utf-8  import smtplib #用于发送邮件  import string  #用于将要发送的内容格式化成邮件标准格式  from email.mime.text import MIMETextdef send_email(content): #content为发送邮件的正文内容，这里只涉及到文本      HOST=\"smtp.XXX.com\"#发送邮件smtp服务器      FROM=\"XXX@XXX.com\" #发送邮件的邮箱      PASSWORD=\"\"        #发送邮件的邮箱的密码      TO=\"XXX@XXX.com\"   #接收邮件的邮箱      SUBJECT=\"New Comments on your Website\" #邮件主题    msg=MIMEText(content,'plain','utf-8')  #邮件内容及编码方式      msg['Subject']=SUBJECT      msg['From']=FROM      msg['To']=TO    try:          s=smtplib.SMTP()          s.connect(HOST)          s.login(FROM,PASSWORD)          s.sendmail(FROM,[TO],msg.as_string())          s.quit()          return True      except EXception,e:          return False  \n下面提到的几点有助于更好地理解上面的代码\n\n通过免费提供的邮箱（如 163 邮箱，新浪邮箱）发送邮件的过程是先将邮件发送到对应邮箱的 smtp 服务器，然后由 smtp 服务器将邮件帮你发送到你要发送的邮箱。一般来说，邮箱对应的服务器都跟其名称有关，如 163 邮箱对应的是 smtp 服务器是 smtp.163.com, 新浪邮箱对应的 smtp 服务器是 smtp.sina.com；其他的邮箱相应可以从设置中找到。\nsmtplib 和 MIMEText 是 python 自带的库，不用另外安装，比较方便，其中 smtplib 负责发送邮件，MIMEText 则负责将要发送的文本内容统一成邮件的标准模式，加入要加入一些附件，需要 MIMEMultipart 这个模块。\n\n需要注意的是发送邮件的邮箱需要开启 smtp 服务 , 一般能够在邮箱的设置中开启。因为有些有些邮箱是不开启这个功能的，比如说新浪邮箱。\n发送邮件的效果图如下所示：\n\n能够发送邮件后，要考虑的问题就是如何自动化执行了，总不能每天手动执行一遍脚本吧，在 Linux 下可通过 crontab 来设置计划任务，同样新浪云也提供了类似的 cron 的服务，这样便可在每天的早上检查昨天是否有新评论在决定是否要发送邮件了。\n","categories":["python"],"tags":["python"]},{"title":"通过 Keras 实现 LRCN 模型","url":"/2018/04/18/%E9%80%9A%E8%BF%87%20Keras%20%E5%AE%9E%E7%8E%B0%20LRCN%20%E6%A8%A1%E5%9E%8B/","content":"本文主要介绍了如何通过 Keras 实现 LRCN 模型，模型出自论文 Long-term Recurrent Convolutional\nNetworks for Visual Recognition and\nDescription，最近需要用这个模型做个实验，在网上搜到的实现代码不多，因此这里记录一下，以供参考。\n\n这里的 LRCN 模型的结构如下图所示， 输入是 image sequence，然后通过\nCNN 提取每帧图像的特征，作为 LSTM 的输入，LSTM 可以为每帧预测一个\nlabel，也可在只在最后预测一个 label 作为整个 sequence 的\nlabel。这种想法非常自然，也是 video/image sequence 中的一个 base\nmodel。\n\n\nCNN_LSTM\n\n下面首先讲述如何在 Keras\n中构建这个模型，然后讲述数据加载的两种模式：分别对应于不定长的输入序列和固定长度的输入序列。\n构建模型\n在具体的实现中，对于训练数据集不大的情况下， CNN\n部分一般可采用预训练的模型，然后选择是否对其进行\nFineTunning，这里我采用的是在 ImageNet 上预训练的 VGG16，并且对 VGG16\n最上面的 5 层进行 FineTunning， 其他层的参数不变。\n另外，对于输入的每帧图像，通过 CNN 抽取出的 feature map 的\n大小为 (7,7,512)，而 LSTM 的输入的 size 是\n(batch_size, timesteps, input_dim)，因此需要将 (7,7,512)\n转为一个一维的 vector，这里我采用最简单的 Flatten()\n方法。实际上，在这里可以采用更加灵活的转换，如这篇论文 Diversified Visual Attention\nNetworks for Fine-Grained Object Classification 就提出了一种\nattention 机制处理这些 feature map。\n(7,7,512) 直接 Flatten 后的大小为 25088，直接输入 LSTM\n的话比较大，因此这里还加了一个 2048 的全连接层，这样输入 LSTM 的\ninput_dim 的大小就是 2048.\nLRCN 模型中的关键点在于为每个 LSTM 的 step 前连上 CNN 网络部分，在\nKeras 中可通过 TimeDistributed\n层来实现，同时如果需要长度不固定的输入序列时，对应的 sequence\nlength 的参数要设为 None，在下面的代码中 input_shape 设为了\n(None, 224, 224, 3), None 便是输入序列长度不固定，而 （224,\n224, 3）则是预训练 VGG 固定的输入大小。\n构建模型的 keras 代码如下，这里为了加快训练速度，将 LSTM 替换成了\nGRU\nfrom keras.applications.vgg16 import VGG16from keras.models import Sequential, Modelfrom keras.layers import Input, TimeDistributed, Flatten, GRU, Dense, Dropoutfrom keras import optimizersdef build_model():    pretrained_cnn = VGG16(weights='imagenet', include_top=False)    # pretrained_cnn.trainable = False    for layer in pretrained_cnn.layers[:-5]:        layer.trainable = False    # input shape required by pretrained_cnn    input = Input(shape = (224, 224, 3))     x = pretrained_cnn(input)    x = Flatten()(x)    x = Dense(2048)(x)    x = Dropout(0.5)(x)    pretrained_cnn = Model(inputs = input, output = x)    input_shape = (None, 224, 224, 3) # (seq_len, width, height, channel)    model = Sequential()    model.add(TimeDistributed(pretrained_cnn, input_shape=input_shape))    model.add(GRU(1024, kernel_initializer='orthogonal', bias_initializer='ones', dropout=0.5, recurrent_dropout=0.5))    model.add(Dense(categories, activation = 'softmax'))    model.compile(loss='categorical_crossentropy',                optimizer = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=1., clipvalue=0.5),                metrics=['accuracy'])    return model\n上面 LSTM 的参数初始化参考了知乎上这个答案：你在训练 RNN 的时候有哪些特殊的 trick？，主要就是\ninitializer 的方式选择、drop-out 和 graddient\nclipping 的设置。在我的实验中也证实了 graddient clipping\n的设置会直接影响到最终的进度，而且影响比较大。\n数据加载\n由于 RNN 本身的结构特点，使得其可接受变长的输入，而且往往原始的 img\nsequence\n数据也会符合这一特点，因此这里就有两种数据加载的方式，第一种是对原始数据不做处理，\n每个样本的长度不一定相同；第二种是从各个样本中抽取出固定的长度。\n但是这两种加载方式的输入的数据的 shape 都遵循着下面的模式\n(batch_size, sequence_length, width, height, channel)\n对于第一种加载方式，根据 这个\nissue，有两种处理方法\n\nZero-padding\nBatches of size 1\n\n这里采用的是第二种处理方法，也就是将 batch_size 设置为 1，\n即每次只用一个样本更新模型。因为第二种处理方法要事先设定一个固定长度 (可以是最长序列长度或其他方式获取的长度)，而且 padding 会让原来较短的序列变得更长，消耗的内存会有所增加。\n而对于第二种加载方式，给定固定的长度，需要尽可能 “均匀” 地从原始序列中抽出固定长度的序列，即帧与帧之间的间隔尽可能相等。但是这个可能会取决于具体的数据集和任务，在我的数据集上这样做是比较合理的。\n两种加载方式的比较如下：\n1）存储数据的方式不一样，固定长度的加载方式由于每个样本的 shape\n一样，因此可以直接 concatenate 成一个大的 ndarray，然后在训练时的\nbatch_size\n可设置成任意值；但是对于变长的加载方式，只能每次取一个样本，然后要通过\nnp.expand_dims(imgs, axis=0) 的方式为样本添加\nbatch_size 这个维度（前一种方式不用，因为 concatenate\n后会自动生成这个维度），然后训练模型同时将 batch_size 和\nepoch 设为 1。 2）训练速度和效果有差别。首先是 batch_size\n的不同使得训练速度上固定长度的方式比变长方式要快，这个比较好理解。\n其次，由于 batch_size 也是一个影响 RNN\n性能的重要参数，因此也会影响收敛性和效果。在我的实验中，batch_size\n设置大于 1 时效果更好。\n两种加载方式实现代码如下，加载的是一个样本的数据，\nimg_dir 目录中包含了一个样本的所有 image\nsequence，且根据文件名排序后的序列是根据时间序列的。 from collections import dequefrom keras.preprocessing import imagedef load_sample(img_dir, categories = 7, fixed_seq_len = None):    label = int(img_dir.split('/')[-2].split('_')[0]) - 1 # extract label from name of sample    img_names = sorted(os.listdir(img_dir))    imgs = []    if fixed_seq_len: # extract certain length of sequence        block_len = round(len(img_names)/fixed_seq_len)        idx = len(img_names) - 1        tmp = deque()        for _ in range(fixed_seq_len):            tmp.appendleft(img_names[idx])            idx = max(idx-block_len, 0)        img_names = tmp    for img_name in img_names:        img_path = img_dir + img_name        img = image.load_img(img_path, target_size=(224, 224))        x = image.img_to_array(img)        imgs.append(x)    imgs = np.array(imgs)    label = np_utils.to_categorical(label, num_classes=categories)    if not fixed_seq_len: # add dimension for batch_size        #（seq_len, width, height, channel）-&gt; (batch_size, seq_len, width, height, channel)        imgs = np.expand_dims(imgs, axis=0)         label = label.reshape(-1, categories)    return imgs, label\n最后，在设计网络结构的时候，可通过逐层测试输出的大小来判断每一层是够达到了预期输出的效果，在\nkeras 中直接通过 model.predict(input) 即可获得当前 model\n最后一层的输出。\n另外，Keras 虽然能够比较快速地通过其提供的各层 layer\n搭建出模型，但是如果要对模型进行更细致的设计的时候， Keras\n就不是那么好做了，这时候就要上 tensorflow/pytorch/mxnet\n这一类更加灵活的框架了。\n","categories":["机器学习"],"tags":["机器学习","深度学习"]},{"title":"通过 word2vec 与 CNN/RNN 对动作序列建模","url":"/2018/01/15/%E9%80%9A%E8%BF%87%20word2vec%20%E4%B8%8E%20CNN-RNN%20%E5%AF%B9%E5%8A%A8%E4%BD%9C%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/","content":"本文主要讲述如何通过 word2vec 和 CNN/RNN\n对动作序列建模，在最近的一个比赛中验证了这个思路，的确有一定效果，在二分类的准确率上能达到 0.87. 本文主要介绍这个方法的具体步骤，并以比赛和代码为例进行说明。\n\n这里提到的比赛是目前正在进行的精品旅行服务成单预测,\n该比赛就是要根据用户的个人信息，行为信息和订单信息来预测用户的下一个订单是否是精品服务。本文提到的方法是仅利用用户的行为信息，主要的思路是：将每个动作通过\nword2vec 转化为 embedding 表示，然后将动作序列转化为 embedding\n序列并作为 CNN/RNN 的输入。 下面依次介绍通过 word2vec 获得动作\nembedding，将 embedding\n作为 CNN 的输入和将 embedding 作为 RNN 的输入这三部分内容。\nword2vec 获取动作 embedding\nword2vec\n是一个很著名的无监督算法了，这个算法最初在 NLP 领域提出，可以通过词语间的关系构建词向量，进而通过词向量可获取词语的语义信息，如词语意思相近度等。而将\nword2vec 应用到动作序列中，主要是受到了知乎上这个答案的启发。因为\nword2vec\n能够挖掘序列中各个元素之间的关系信息，这里如果将每个动作看成是一个单词，然后通过\nword2vec 得出每个动作的 embedding 表示，那么这些 embedding\n之间会存在一定的关联程度，再将动作序列转为 embedding 序列，作为 CNN 或\nRNN 的输入便可挖掘整个序列的信息。\n这里训练动作 embedding 的方法跟训练 word embedding\n的方法一致，将每个户的每个动作看做一个单词、动作序列看做一篇文章即可。训练时采用的是\ngensim, 训练的代码很简单，embedding 的维度设为 300,\nfilter_texts 中每一行是一各用户的行为序列，行为之间用空格隔开。\nfrom gensim.models import word2vecvector_length = 300model = word2vec.Word2Vec(filter_texts, size = vector_length, window=2, workers=4)\n由于动作类型只有 9 种（1~9），也就是共有 9 个不同的单词，因此可将这 9\n个动作的 embedding 存在一个 np.ndarray 中，然后作为后面\nCNN/RNN 前的 embedding layer 的初始化权重。注意这里还添加了一个动作 0\n，原因是 CNN\n的输入要求长度一致，因此对于长度达不到要求长度的序列，需要在前面补\n0（补其他的不是已知的动作也可以）。代码如下\nimport numpy as npembedding_matrix = np.zeros((10, vector_length))for i in range(1, 10):    embedding_matrix[i] = model.wv[str(i)]\nCNN 对动作序列建模\nCNN 采用的模型是经典的 TextCNN, 模型结构如下图所示\n\n\nTextCNN\n\n这里通过 Keras 实现，具体代码如下\n首先需要处理序列，使得所有序列长度一致，这里选择的长度是\n50，具体代码如下，代码中的 x_original 是一个\nlist[list[int]]\n类型，表示所有用户的所有动作序列，对于长度比 max_len\n长的，从后往前截取 50 个最近时间的动作，而短的则在前面补 0.\nfrom keras.preprocessing import sequencemax_len = 50x_train = sequence.pad_sequences(x_original, maxlen=max_len)y_train = np.array(y_original)print(x_train.shape, y_train.shape)\n然后通过前面得到的 embedding_matrix 初始化 embedding\n层\nfrom keras.models import Sequential, Modelfrom keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, BatchNormalization, Activationfrom keras.layers.merge import Concatenatefrom keras import optimizersembedding_layer = Embedding(input_dim=embedding_matrix.shape[0],                            output_dim = embedding_dim,                            weights=[embedding_matrix],                            input_length=max_len,                            trainable=True)\n然后建立模型并训练，这里用了四种不同步长的卷积核，分别是\n2、3、5、8，比起原始的 TextCNN,\n用了两层的卷积层 (在这个任务上经过测试比一层的要好),\n后面的全连接层也拓展到了三层，具体代码如下\nNUM_EPOCHS = 100BATCH_SIZE = 64DROP_PORB = (0.5, 0.8)NUM_FILTERS = (64, 32)FILTER_SIZES = (2, 3, 5, 8)HIDDEN_DIMS = 1024FEATURE_DIMS = 256ACTIVE_FUNC = 'relu'sequence_input = Input(shape=(max_len, ), dtype='int32')embedded_seq = embedding_layer(sequence_input)# Convolutional blockconv_blocks = []for size in FILTER_SIZES:    conv = Convolution1D(filters=NUM_FILTERS[0],                         kernel_size=size,                         padding=\"valid\",                         activation=ACTIVE_FUNC,                         strides=1)(embedded_seq)    conv = Convolution1D(filters=NUM_FILTERS[1],                         kernel_size=2,                         padding=\"valid\",                         activation=ACTIVE_FUNC,                         strides=1)(conv)    conv = Flatten()(conv)    conv_blocks.append(conv)model_tmp = Concatenate()(conv_blocks) if len(conv_blocks) &gt; 1 else conv_blocks[0]model_tmp = Dropout(DROP_PORB[1])(model_tmp)model_tmp = Dense(HIDDEN_DIMS, activation=ACTIVE_FUNC)(model_tmp)model_tmp = Dropout(DROP_PORB[0])(model_tmp)model_tmp = Dense(FEATURE_DIMS, activation=ACTIVE_FUNC)(model_tmp)model_tmp = Dropout(DROP_PORB[0])(model_tmp)model_output = Dense(1, activation=\"sigmoid\")(model_tmp)model = Model(sequence_input, model_output)opti = optimizers.SGD(lr = 0.01, momentum=0.8, decay=0.0001)model.compile(loss='binary_crossentropy',              optimizer = opti,              metrics=['binary_accuracy'])model.fit(x_tra, y_tra, batch_size = BATCH_SIZE, validation_data = (x_val, y_val))\n由于最后要求的是 auc 指标，但是 Keras 中并没有提供，而 accuracy 与\nauc 还是存在一定差距的，因此可以在每个 epoch 后通过 sklearn\n计算 auc，具体代码如下\nfrom sklearn import metricsfor i in range(NUM_EPOCHS):    model.fit(x_tra, y_tra, batch_size = BATCH_SIZE, validation_data = (x_val, y_val))    y_pred = model.predict(x_val)    val_auc = metrics.roc_auc_score(y_val, y_pred)    print('val_auc:{0:5f}'.format(val_auc))\n这种方法最终的准确率约为 0.86，auc 约为 0.84\nRNN 对动作序列建模\n通过 RNN 进行建模与 CNN 类似，不同的是 RNN\n可接受不同长度的输入，但是根据这里的说明，对于输入也需要\npadding 的操作，只是 RNN 会将其自动忽略。\n因此，数据的预处理和构建 embedding 层的代码与 CNN\n中基本一致，这里只给出建立模型的代码，模型比较简单，首先是将输入通过\nembedding 层的映射后，作为以 LSMT/GRU 为基础单元构建的 RNN 的输入，\n最后通过 sigmoid 进行分类，具体代码如下\n# RNNs are tricky. Choice of batch size is important,# choice of loss and optimizer is critical, etc.model = Sequential()model.add(embedding_layer)model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2)))# model.add(LSTM(256))# model.add(Bidirectional(GRU(256)))model.add(Dense(1, activation='sigmoid'))opti = optimizers.SGD(lr = 0.01, momentum=0.8, decay=0.0001)# try using different optimizers and different optimizer configsmodel.compile(loss='binary_crossentropy',              optimizer='adam',              metrics=['accuracy'])\n通过 RNN 得出的最终效果比 CNN 要好一点，准确率约为 0.87，auc\n约为 0.85。但是训练起来非常慢，且参数非常的\ntricky，需要精调，这里我没有细调参数，模型也没有搞得很复杂，应该还有提升空间。\n综上，本文提供了一种对动态序列建模的思路：将动作序列通过\nword2vec，得到每个动作的 embedding 表示，然后将动作序列转化为 embedding\n序列并作为 CNN/RNN\n的输入。希望能够起到抛砖引玉的作用，如果您有更好的想法，欢迎交流。\n","categories":["机器学习"],"tags":["机器学习","深度学习","NLP"]},{"title":"通过 Selenium 和 PhantomJS 抓取带 JS 的网页","url":"/2016/08/10/%E9%80%9A%E8%BF%87Selenium%E5%92%8CPhantomJS%E6%8A%93%E5%8F%96%E5%B8%A6JS%E7%9A%84%E7%BD%91%E9%A1%B5/","content":"爬虫一般通过获取网页的源码，然后通过正则表达式或 html 解释器获取所需的信息，但是有的网页，不能直接通过\nlinux 下的 wget 命令、或者使用 Python\n中的 requests.get 这样的函数库来直接获取其真正展现给用户的信息，因为里面包含有 JavaScript 脚本，而该 JS 和页面数据的生成相关，需要通过 Firefox、Chrome 等浏览器渲染后才能得到想要看的结果。\n\n如亚马逊的商品列表\nhttps://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable\n通过上面提到的方法直接获取其网页源码是无法获取每个商品的标题信息的。\n一般来说，对于这种网页如下两种方案： 1.\n通过 Selenium 启动真正的浏览器（如：IE、Firefox）来打开该网页，然后调用 webdriver 获取想要的页面元素。\n2.\n通过浏览器渲染引擎解释网页，能够让其解析网页并执行网页中需要初始化 JS，然后将 JS、CSS 等执行后的 HTML 代码输出出来。\n启动真正的浏览器，可能带来两个问题：一个是需要的时间较长，另一个是 UI 自动化易受干扰、不够稳定。因此本文主要讲述通过第二种方法，也就是不启动浏览器进行获取这种网页上的信息。\n主要用到的库是 Selenium，通过 selenium 中的\nPhantomJS\n作为 webdriver 能够不启动浏览器来解释 js 文件并获取其解释后的源码。下面以上面的亚马逊商品列表页作为例子，通过 PhantomJS 来获取其页面上的商品标题。\n首先需要在官网下载 PhantomJS 的可执行文件。\n然后获取页面\nhttps://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable\n上的商品列表的 python 代码如下： # -*- coding: utf-8 -*-# @Author: LC# @Date:   2016-07-30 10:08:31# @Last modified by:   LC# @Last Modified time: 2016-08-22 17:08:46# @Email: liangchaowu5@gmail.comfrom selenium import webdriverfrom bs4 import BeautifulSouptarget_url = r'https://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable'driver = webdriver.PhantomJS(executable_path = r'H:/PythonModule/phantomjs/phantomjs-2.1.1-windows/bin/phantomjs.exe')driver.get(target_url)text = driver.page_source # 获取解释后的网页源码soup = BeautifulSoup(text, 'lxml')titles = soup.find_all('h2')for title in titles:    print title.get('data-attribute', '')driver.quit()\n上面的代码首先通过指定本地的 PhantomJS 的可执行文件可解释目标 url，获取其网页 html 代码，然后通过 BeautifulSoup 提取网页代码中的商品标题\n\n参考：http://smilejay.com/2013/12/try-phantomjs-with-selenium/\n","categories":["python","爬虫"],"tags":["python","爬虫"]},{"title":"遗传算法简介","url":"/2017/04/10/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/","content":"最近由于课程需要去调研了一下遗传算法，本文是调研结果的总结，主要介绍遗传算法的背景、基本流程、理论基础、变种及其应用。\n\n连续优化与组合优化\n连续优化\n连续优化 (continuous\noptimization)\n在某些教材中也被称为函数优化，其特点是变量的取值是连续的，或者说是变量的范围是实数域 (绝大多数情况)。对于一个可行域，其中包含着无数个可行解，这类问题一般利用目标函数可导的性质，通过如梯度，海塞矩阵等进行求解（如常见的梯度下降法，牛顿法等）。更详细的关于连续优化的方法可参考这篇文章\n组合优化\n相对于连续优化的另外一种优化是组合优化，组合优化的变量是离散的，也就是变量的空间只有有限多个的可行解。比如说经典的背包问题就是一个组合优化问题\n\n\n背包问题\n\n既然只有有限多个解，那么最直接想到的求解方法就是通过穷举这些解 (exhaustive\nsearch) 找到最优解，然而分析可知，穷举所需要的时间复杂度一般是指数级别的，比如说对于背包问题，有\n\\(n\\) 个物品的时候，共有 \\(n^2\\) 种可能性，时间复杂度是 \\(O(2^n)\\)。这样的问题已经是一个 NP-hard\n问题了，这里先简单介绍一下 P 问题，NP 问题和 NP-hard 问题的区别。\n一般来说，P 问题指的是能够在多项式时间复杂度内找到解的问题，而 NP\n问题指的是无法在多项式时间复杂度内找到答案的问题，但是能够在多项式时间复杂度内验证一个解是否正确的问题（如大素数的判断问题）；而\nNP-hard 问题则是指那种不仅无法在多项式时间复杂度内找到答案，\n甚至无法在多项式时间复杂度内验证一个解是否正确的问题（如背包问题，如果要找到最优，就必须对所有的可能进行比较，这样的时间复杂度就是指数级别了）。\n实际上，大多数的组合优化问题都是 NP-hard\n问题，虽然存在着某些特殊问题可以用多项式时间的方法来解决，如通过维特比算法求解\nHMM\n中最大概率的隐含状态转移路径。但是更多的情况下组合优化问题仍然是一个\nNP-hard 问题。\n于是为了解决这类得到最优解就需要耗时非常长的问题，人们退而求其次，去找一个接近最优解但是耗时在可接受范围内的解。进而诞生了一类方法:\n元启发式方法 (Metaheuristic)。\n元启发式方法 (Metaheuristic)\n元启发式方法 (Metaheuristic)\n是一类方法， 维基上对的定义如下 &gt;a metaheuristic is a higher-level\nprocedure or heuristic designed to find, generate, or select a heuristic\n(partial search algorithm) that may provide a sufficiently good\nsolution to an optimization problem\n元启发式方法的定义中已经说得很清楚了，这一类方法不确定能够提供最优解，但是会提供一个足够好的解，当然这个足够好好到什么程度也是与具体算法相关的，需要通过数学证明。但是这类方法能够在可接受时间内返回这个解。\n我们经常听到的一些算法都属于这种元启发式算法，包括：\n\n遗传算法 (Genetic Algorithms，GA)\n 模拟退火 (Simulated Annealing，SA)\n 蚁群算法 (Ant Colony Optimization，ACO)\n 粒子群算法 (Particle Swarm Optimization，PSO) ......\n\n这一类算法都是从自然现象中得到启发的，如遗传算法就是从进化论中得到启发的，认为 “物竞天择，适者生存”，一代代进化的基因是逐渐往好的方向发展的，对应到优化问题中就是逐渐收敛到比较好的解；模拟退火则是从金属加热后再冷却，金属中的分子最终有可能会到达一个内能比原来更低的位置，表示为一个更优解；蚁群算法和粒子群算法则是通过一个群体去搜索最优解，如对于非凸的优化，往往具有多个局部最优解，通过一个群体去搜索能够扩大搜索范围，从而以更大的概率收敛于全局最优解。\n关于元启发式方法在组合优化中的更详细的应用可这篇文献： Metaheuristics in\nCombinatorial Optimization: Overview and Conceptual Comparison\n遗传算法\n从前面可知，遗传算法是元启发式方法的一种，维基上对其的定义如下:\n\nGenetic algorithms are commonly used to generate high-quality\nsolutions to optimization and search problems by relying on bio-inspired\noperators such as mutation, crossover and selection。\n\n从维基的描述可知，遗传算法是收生物进化论的影响，因此遗传算法中的很多概念跟生物进化中的很多概念类似，而遗传算法主要的三个操作就是选择 (selection)，交叉 (crossover) 和编译 (mutataion).\n下面先介绍遗传算法中的基本概念：\n基本概念\n\n个体（individual） : 一个候选解\n种群（generation） : 一组候选解，代表某一代的所有个体\n染色体（chromosomes） :\n候选解的染色体表示，常见的用 01 字符串表示\n适应度（fitness） :\n个体的适应度，一般与优化问题中的目标函数值相关\n选择（selection）: 根据个体适应性从一个 generation 中选出\n适应性好的进入下一代\n交叉（crossover） : 类似于生物中两个个体进行杂交，两个 chromosomes\n进行相应的交叉\n变异（mutation）: chromosomes\n的某一位进行突变，如从 0 变到 1 或从 1 变到 0\n\n在执行遗传算法前，有两个东西必须先定义好\n\n可行解的表示方法，也就是常说的编码方式，上面说到了常用的有 01\n编码，除此之外，根据问题的不同，还存在其他的一些编码方式\n适应度函数，也就是计算上面适应度的函数，这个函数一般 yong 就是用目标函数，如要最大化某个函数，则将这个函数作为目标函数即可，反之最小化的时候加个负号即可。\n\n遗传算法是一个迭代算法，在进行初始化后按照 “选择，交叉，变异”\n的顺序进行迭代，当满足设定的结束条件 (例如达到一定的代数)，便将得到的解作为问题的解。其流程如下所示\n\n\n算法流程\n\n算法流程\n初始化 (Initialization)\n初始化主要是确定编码方式以及初始种群的数目，初始种群的数目由人为设定然后随机产生，设定的数目如果太小，可能会一开始就限制了搜索域较小，而如果设定的数目太大，计算量会比较大。\n编码方式则根据问题的约束条件和希望得到的解的精度确定，后面会给出一个详细的例子说明这个问题。\n选择 (Selection)\n选择模拟了生物进化中的 “适者生存”，根据当代的个体的适应度按概率选出下一代，适应度越高，则被选择的概率也越大。\n之所以不直接将适应度最大的若干个个体作为下一代，是因为这样可能会导致算法过早陷入局部最优解。在遗传算法里面这种现象称为 \"早熟（premature）\"。\n假如当代的个体总数是 \\(M\\)，而个体\n\\(a_i\\) 的适应度是 \\(f_i\\)，则个体 \\(a_i\\) 被选择到下一代的概率是 \\[\\begin{align} p_i = \\frac{f_i}{\\sum_{j=1}^{M}\nf_j} \\end{align}\\]\n交叉 (crossover)\n交叉跟生物上的杂交是一样的，只是生物中是双螺旋结构，而遗传算法中只有一条链。原始的遗传算法只会选择一个点进行交叉，如下如所示\n\n\nSingle-point crossover\n\n而假如对遗传算法进行改进，也可以在多个点进行交叉的操作\n\n\nTwo-point crossover\n\n交叉（crossover）的目的是从目前的所有解中组合出更优的解，但是尝试获得更优解的同时，也可能丢掉目前得到的最优解。而这也是传统的遗传算法无法收敛到全局最优的一个原因（详见后面的理论证明，对其进行了简单改进后可以收敛到全局最优）。\n交叉是按照一定的概率进行的，这个概率也需要人为设定，假如设得过大边很容易将当前的最优解破坏掉，并且容易陷入早熟 (premature)，而设得过小时则很难从当前的可行解中组合出更优的解。\n变异 (mutation)\n变异（mutation）跟生物中的变异也一样，随机改变染色体中的某一位，如下所示就是一个变异的示例\n\n\nmutation.jpg\n\n变异也是按照一定的概率进行的，这个概率也需要人为设定，而且这个概率一般会设置得比较小，如果设得较大时，就变成了类似随机搜索（random\nsearch）的方法；但是如果设得太小的时候，就会造成生物中的基因漂变 (genetic\ndrift) 的现象，从而导致收敛到一个局部最优。\n变异的主要作用是为了防止算法收敛到一个局部最优解。假如将找到最优解比作在多座山峰中找到最高的那一座，那么交叉就类似同一座山峰下的多个基因合力爬上山顶，而变异就类似于从一座山峰调到另外一座山峰，目的就是为了防止当前的山峰是一个局部最优解而将算法其作为一个全局最优解。\n终止 (termination)\n算法的终止条件由人为设定，一般是限制迭代的最大次数，或能够使用的最大的计算资源，或者是两次的解小于某个设定的阈值，下面是从维基百科中摘录的终止条件\n\n\nA solution is found that satisfies minimum criteria\n\n\n\nFixed number of generations reached\nAllocated budget (computation time/money) reached\nThe highest ranking solution's fitness is reaching or has reached a\nplateau such that successive iterations no longer produce better\nresults\nManual inspection\nCombinations of the above\n\n例子\n下面给出两个例子：分别是遗传算法解决背包问题和一个连续优化问题。\n遗传算法解决背包问题\n背包问题是一个很经典的组合优化问题，通过遗传算法可以求解，但是目前求解这类问题的最优方法是动态规划，是一个伪多项式时间复杂度问题。这里采用背包问题为例子是因为这个例子的编码方式有较好的解释。\n假设现在背包的容量为 \\(W\\), 共有 15\n个物品，每个物品的大小为 \\(w_i\\),\n价值为 \\(v_i\\),\n则可以得到下面的最优化问题\n\\[\\begin{align}max \\sum_{i=1}^{15}\nx_iv_i\\\\\ns.t. \\sum_{i}w_ix_i \\le W\\\\\nx_i \\in \\lbrace 0,1 \\rbrace\\end{align}\\]\n\\(x_i \\in \\lbrace 0,1 \\rbrace\\)\n表示是否将第 \\(i\\)\n件物品放到背包中。\n则通过遗传算法解决这个问题的流程如下\n初始化\n由于有 15 个物品，而每个物品只有两个状态，因此用一个长度为 15 的 01 字符串表示一个解，0 表示不拿该物品，1 表示那该物品。随机产生 10 个候选解如下所示，这里要注意每个候选解必须要满足约束条件\n\n\ninitialize\n\n由于没有给出每个物品具体的价值，这里用 \\(V_i\\) 表示第 \\(i\\) 个候选解通过公式 \\(\\sum_{j=1}^{15}x_jv_j\\) 求得的总价值。\n选择\n由于每个个体被选择到下一代的概率与其适应度成正比，因此第 \\(i\\) 个候选解被选择到下一代的概率为 \\[p_i = \\frac{V_i}{\\sum_{j=1}^{15}V_j}\\]\n如下为模拟的过程，注意一个个体可能会被选择多次\n\n\nselection\n\n交叉\n对选择出来的个体重新进行编号，然后进行交叉的操作，按照设定的交叉概率选择个体进行交叉的操作，交叉的位置也是随机产生的，如下所示是设定交叉概率为\n0.5 后的交叉结果\n\n\ncrossover\n\n变异\n变异也是按照变异概率选择个体来进行的，被选中的个体变异的位数可以是一位也可以是多位，如下图所示是设定变异概率为 0.2 的编译过程\n\n\nmutation\n\n终止\n设定一个终止条件，如最大的迭代次数，当满足这个条件时边终止算法，将当前得到的最优解作为问题的最优解；没满足时便按照\n\"选择 -&gt; 交叉 -&gt; 编译\" 进行迭代。\n利用遗传算法求解背包问题更详细的信息可参考这篇文献：A\ngenetic algorithm for the multidimensional knapsack problem\n遗传算法解决连续优化问题\n该例子来源于知乎，\n优化的问题如下所示\n\\[\\begin{align} \\max f(x) = x + 10sin(5x)\n+ 7cos(4x)， x \\in [0,9] \\end{align}\\]\n这个函数的图像如下所示\n\n\n连续优化例子\n\n显然这已经不是一个凸函数了，通过遗传算法解决这个问题的步骤跟背包问题的一样，最主要也是最重要的区别在于编码方式的不同。\n由于现在可行解是实数域了，仍旧采用 01 编码，则要解决的问题就是如何用 01 串表示一个实数。假设现在希望解精确到小数点的后四位，\n假如设定求解的精度为小数点后 4 位，可以将 x 的解空间划分为 \\((9-0)×10^4=90000\\) 个等分。\n由于 \\(2^{16}&lt;90000&lt;2^{17}\\)，则需要 17 位二进制数来表示这些解。换句话说，一个解的编码就是一个 17 位的二进制串。\n一开始，这些二进制串是随机生成的。一个这样的二进制串代表一条染色体串，这里染色体串的长度为 17。对于任何一条这样的染色体 chromosome，如何将它复原 (解码) 到\n[0,9] 这个区间中的数值呢？对于本问题，我们可以采用以下公式来解码：\nx = 0 + decimal(chromosome)×(9-0)/(2^17-1)\ndecimal( ): 将二进制数转化为十进制数\n更一般化的解码公式：\nf(x), x∈[lower_bound, upper_bound] x = lower_bound + decimal(chromosome)×(upper_bound-lower_bound)/(2^chromosome_size-1)\nlower_bound: 函数定义域的下限 upper_bound:\n函数定义域的上限 chromosome_size:\n染色体的长度通过上述公式，我们就可以成功地将二进制染色体串解码成 [0,9] 区间中的十进制实数解。\n后面的选择，交叉，变异的过程与上面的背包问题一样，这里不在给出，在经过\n100\n次迭代后，最终选出的最优个体为 00011111011111011，其适应度，也就是目标函数的值为 24.8554，对应的\n\\(x\\)\n为 7.8569, 从图像来看，已经是在最优解的位置了。\n理论基础\n上面介绍的内容仅仅是遗传算法的基本流程，但是经过这些仿生的操作能确保最终收敛吗？如果收敛的话能够收敛到全局最优还是局部最优？收敛到全局最优的概率是多少？\n在遗传算法刚提出来的时候为不少问题找到了一个较好的解，对于很多问题都能找到一个比较好的解，但是缺乏数学理论基础的保证，往往会让人产生上面这些问题。\n很多时候，实践的发展往往是超前于理论的，于是在遗传算法提出 5\n年后，它的提出者便发表了模式定理 (Schema\nTheory) 来说明遗传算法中发生 “优胜劣汰” 的原因。而后又有人通过马氏链证明了算法的收敛性，结果是原始的遗传算法并不能收敛到全局最优，但是经过简单改进的遗传算法能够收敛到全局最优。\n模式定理 (Schema Theory)\n基本概念\n问题引入： 求解方程 \\(F(x) = x^2\\)\n在 [0,31]\n上的最大值，使用固定长度二进制编码对种群中的个体进行编码，在计算适应度值时会发现一个规律，当个体编码的最左边为 1 时，适应度普遍较大，可以记为\n1***1，同理 0****\n的个体适应度偏低。由此可以引入以下一些基本概念：\n模式 (Schema)：编码的字符串中具有类似特征的子集。\n例如上述五位二进制字符串中，模式 *111*\n可代表 4 个个体。个体和模式的一个区别就是，个体是由 {0，1} 组成的编码串，模式是由 {0，1，*} 组成，*\n为通配符。\n模式阶 (Schema\nOrder)：表示模式中已有明确含义的字符个数，记为 o (s)，s 代表模式。例如 o (111)=3;\n阶数越低，说明模式的概括性越强，所代表的编码串个体数也越多。其中阶数为零的模式概括性最强。\n模式定义长度 (Schema Defining\nLength)：指第一个和最后一个具有含义的字符之间的距离，其可表示该模式在今后遗传操作中被破坏的可能性，越短则越小，长度为 0 最难被破坏。\n下面就是模式定理的具体定义：\n&gt; 适应度高于群体平均适应度的，长度较短，低阶的模式在遗传算法的迭代过程中将按指数规律增长。\n下面将推导经过选择，交叉和变异后如何得到上面的模式定理\n选择\n假设当前是第 \\(t\\) 代， 这一代中模式\n\\(s\\) 的个体数记为 \\(m(s,t)\\)，整个种群的数目为 \\(M\\)，个体 \\(a_i\\) 的适应度为 \\(f_i\\)\n则个体 \\(a_i\\) 被选择的概率为 \\[p_i = \\frac{f_i}{\\sum_{j=1}^{M} f_j}\\]\n因此经过选择后， 模式 \\(s\\)\n在下一代的数目为\n\\[\\begin{align} m(s, t+1) = M\\frac{m(s,t)\n\\overline {f(s)}}{\\sum_{j=1}^{M} f_j} = m(s,t) \\frac{\\overline\n{f(s)}}{\\overline {f}} \\end{align}\\]\n其中 \\(\\overline {f(s)}\\) 表示模式\n\\(s\\) 中个体的平均适应度，\\(\\overline\nf\\) 表示整个种群的平均适应度，也就是 \\[\\overline f = \\frac{\\sum_{j=1}^{M}\nf_j}{M}\\]\n可知只有当 \\(\\overline {f(s)} &gt; \\overline\nf\\), 模式 \\(s\\)\n中的个体才能增长，将上面的公式做简单的变化，则有\n\\[\\begin{align} m(s, t+1) = m(s,t)\n\\frac{\\overline {f(s)}}{\\overline {f}} = m(s,t) \\frac{(1+c)\\overline f\n}{\\overline {f}} = m(s,t)(1+c)  \\end{align}\\]\n则从开始到第 \\(t+1\\) 代，模式 \\(s\\) 的个体的数目为: \\[\\begin{align} m(s, t+1) = m(s, t)(1+c)^t\n\\end{align}\\]\n可以看到当模式平均适应度高于种群是适应度时，模式中的个体会呈指数形式增长。\n交叉\n记模式定义长度 (schema defining length) 为 \\(\\delta(s)\\)，染色体的长度记为 \\(\\lambda\\)\n则模式被破坏，也就是在定义长度内进行交叉的概率为 \\(p_d = \\frac{\\delta(s)}{\\lambda - 1}\\)\n因此模式存活的概率为 \\(p_s = 1 - p_d = 1 -\n\\frac{\\delta(s)}{\\lambda - 1}\\)\n假设交叉概率为 \\(p_c\\)，则存活概率为\n\\(p_s = 1 - p_c \\frac{\\delta(s)}{\\lambda -\n1}\\)\n在经过选择，交叉后，模式 \\(s\\)\n在下一代数目为\n\\[\\begin{align} m(s, t+1) = m(s,t)\n\\frac{\\overline {f(s)}}{\\overline {f}}[1 - p_c \\frac{\\delta(s)}{\\lambda\n- 1}] \\end{align}\\]\n从这里可以看到，模式的定义长度 (schema defining length)\n越小，则其存活的概率越大\n变异\n记模式的阶 (Schema Order) 为 \\(o(s)\\)，变异的概率为 \\(p_m\\)\n则原来的基因存活，也就是那些确定的位置都不发生变异的概率为 \\[p_s = (1-p_m)^{o(s)} \\approx 1 -\np_mo(s)\\]\n上式最后做了泰勒展开，因此经过选择，交叉和变异后，模式 \\(s\\) 在下一代种群的数目为\n\\[\\begin{align} m(s, t+1) = m(s,t)\n\\frac{\\overline {f(s)}}{\\overline {f}}[1 - p_c \\frac{\\delta(s)}{\\lambda\n- 1} - p_mo(s)] \\end{align}\\]\n从上式也可以看到，模式的阶越低，模式约容易存活。\n因此经过上面的分析，可以得到模式定理的定义\n\n适应度高于群体平均适应度的，长度较短，低阶的模式在遗传算法的迭代过程中将按指数规律增长。\n\n该定理阐明了遗传算法中发生 “优胜劣汰” 的原因。在遗传过程中能存活的模式都是定义长度短、阶次低、平均适应度高于群体平均适应度的优良模式。遗传算法正是利用这些优良模式逐步进化到最优解。\n收敛性分析\n虽然上面的模式定理说明了遗传算法有 “优胜劣汰” 的趋势，但是对于其收敛性并没有给出一个证明，这篇文献\nConvergence\nAnalysis of Canonical Genetic Algorithms\n对遗传算法的收敛性进行了证明。\n证明利用了有限状态的时齐马氏链的遍历性和平稳分布的性质，因为遗传算法可以被描述成一个有限状态的时齐马氏链，假如种群大小为\n\\(M\\), 染色体的长度为 \\(\\lambda\\), 采用 01 编码方式，将所有个体的染色体连起来作为一个状态，则种群的所有状态数目为\n\\(2^{M\\lambda}\\)。同时选择概率，变异概率和交叉概率可作为状态转移概率。\n由于证明的定义和引理较多，因此这里不详细描述证明过程，只给出最后的结论。\n证明给出的主要结论有两个： &gt;1. 传统的遗传算法不能收敛到全局最优\n&gt;2.\n在传统的遗传算法基础上每次选择的时候保留前面所有迭代里面的最优个体，最终能收敛到全局最优。\n从证明最终给出的结论可知，传统的遗传算法并不能收敛到全局最优，证明过程指出的是收敛到全局最优的概率小于 1，原因就是选择、交叉和变异中都带有一定的随机性，这种随机性导致了最优解可能会被抛弃；而改进后的遗传算法每一代都会保留着前面所有代数的最优解，因此最终会以概率 1 收敛到全局最优。\n改进方案 (Variants)\n下面介绍的是对原始的遗传算法进行改进后得到的变种，改进主要有三大方向：编码方式 (Chromosome\nrepresentation), 精英选择 (Elitism) 和参数自适应（Adaptive）。\n编码方式\n我们前面采用的均是 01 编码方式，但是实际上根据不同问题可以选择不同的编码方式，如格雷码（Gray\nCode）、浮点数编码都是可选的编码方式。\n二进制编码的缺点是：对于一些连续函数的优化问题，由于其随机性使得其局部搜索能力较差，如对于一些高精度的问题（如上题），当解迫近于最优解后，由于其变异后表现型变化很大，不连续，所以会远离最优解，达不到稳定。\n而格雷码能有效地防止这类现象，当一个染色体变异后，它原来的表现现和现在的表现型是连续的。主要优点有：\n\n便于提高遗传算法的局部搜索能力\n交叉、变异等遗传操作便于实现\n符合最小字符集编码原则\n便于利用模式定理对算法进行理论分析\n\n这篇文献 An\nImproved Genetic Algorithm for Pipe Network Optimization\n就适用了格雷码作为其编码方式。\n而使用浮点数编码的原因往往是对连续函数优化时二进制编码精度不够。\n精英选择\n精英选择实际上就是在前面收敛性证明中改进遗传算法使得其以概率 1 收敛到全局最优的改进方案，在这篇文献中\nRemoving\nthe Genetics from the Standard Genetic Algorithm\n对其做了详细的描述。\n参数自适应\n前面我们说到了变异的概率和交叉的概率都是人为设定的固定值，但是实际上这个概率在不同的种群下应该会有其对应的最优值。因此这篇文献\nAdaptive\nprobabilities of crossover and mutation in genetic\nalgorithms，将两个概率设定为下面的两个公式\n\\[\\begin{align} P_c=\n\\begin{cases}\n\\frac{k_1(f_{max}-f')}{f_{max}-f_{avg}}&amp;&amp; {f'\n\\geq   f_{avg}}\\\\\\\nk_2  &amp;&amp; {f' &lt; f_{avg}}\n\\end{cases}\\]\n\\end{align}\\[ P_m=\n\\begin{cases}\n\\frac{k_3(f_{max}-f')}{f_{max}-f_{avg}}  &amp;&amp; {f' \\geq\nf_{avg}}\\\\\\\nk_4 &amp;&amp; {f' &lt; f_{avg}}\n\\end{cases}\n\\]\\[\\begin {align}\n\n上面的公式表明，交叉率和变异率随着个体的适应度在种群平均适应度和最大适应度之间进行线性调整。上面的公式中\n$f_{max}$ 表示种群的最大适应度，$f_{avg}$ 表示种群的平均适应度，$f'$\n表示参与交叉的两个个体中较大的适应度。\n\n\n由上面的公式可知，当适应度越接近最大适应度时．交叉率和变异率越小，好处是降低了最优解被破坏的概率，坏处是当前的最优适应度等于最大适应度时，交叉率和变异率为零。这使得 AGA 在演化初期并不理想。因为在进化初期的群体中，较优个体几乎处于一种不发生变化的状态，而此时的优良个体不一定是全局最优解，这容易使演化走向局部收敛的可能性增加。\n\n因此这篇文献 [自适应遗传算法交叉变异算子的改进][29] 对上面的公式进行了如下的改进\n\n\\end {align}\\]\\[ P_c=\n\\begin{cases}\np_{cmin}-\\frac{（p_{cmax}-p_{cmin}）(f'-f_{avg})}{f_{max}-f_{avg}}&amp;&amp;\n{f' \\geq   f_{avg}}\\\\\\\np_{cmax}  &amp;&amp; {f' &lt; f_{avg}}\n\\end{cases}\\]\n\\[\\begin{align} P_m=\n\\begin{cases}\np_{mmin}-\\frac{（p_{mmax}-p_{mmin}）(f'-f_{avg})}{f_{max}-f_{avg}}&amp;&amp;\n{f' \\geq   f_{avg}}\\\\\\\np_{mmax}  &amp;&amp; {f' &lt; f_{avg}}\n\\end{cases}\n\\end{align}\\]\n\\(p_{cmin}\\), \\(p_{cmax}\\)\n分别表示交叉率取值的下限和上限；而 \\(p_{mmin}\\), \\(p_{mmax}\\)\n分别表示变异率取值的下限和上限，这样就避免了前面的概率出现 0 的情况。\n参考文献\n这些文献在上面已经在不同部分给出其相应的链接了，这里再次统一列出\nBlum C, Roli A. Metaheuristics in combinatorial optimization:\nOverview and conceptual comparison[J]. ACM Computing Surveys (CSUR),\n2003, 35(3): 268-308.\nChu P C, Beasley J E. A genetic algorithm for the multidimensional\nknapsack problem[J]. Journal of heuristics, 1998, 4(1): 63-86.\nGoldberg, David .Genetic Algorithms in Search, Optimization and\nMachine Learning[M].MA: Addison-Wesley Professional, 1989:ISBN\n978-0201157673\nRudolph G. Convergence analysis of canonical genetic algorithms[J].\nIEEE transactions on neural networks, 1994, 5(1): 96-101\nSrinivas M, Patnaik L M. Adaptive probabilities of crossover and\nmutation in genetic algorithms[J]. IEEE Transactions on Systems, Man,\nand Cybernetics, 1994, 24(4): 656-667.\nDandy G C, Simpson A R, Murphy L J. An improved genetic algorithm for\npipe network optimization[J]. Water resources research, 1996, 32(2):\n449-458.\nBaluja S, Caruana R. Removing the genetics from the standard genetic\nalgorithm[C].Machine Learning: Proceedings of the Twelfth International\nConference. 1995: 38-46.\nSrinivas M, Patnaik L M. Adaptive probabilities of crossover and\nmutation in genetic algorithms[J]. IEEE Transactions on Systems, Man,\nand Cybernetics, 1994, 24(4): 656-667.\n邝航宇，金晶，苏勇。自适应遗传算法交叉变异算子的改进 [J].\n计算机工程与应用，2006, 42 (12): 93-96.\n","categories":["数学"],"tags":["数学"]},{"title":"通配符与正则表达式","url":"/2016/01/01/%E9%80%9A%E9%85%8D%E7%AC%A6%E4%B8%8E%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","content":"通配符与正则表达式均可以匹配符合某些格式的字符串，但是通配符一般用于 Linux 的 shell 下，而正则表达式适用范围则更广，不仅 Linux 的 shell 下支持（grep、awk、sed 等工具），而且很多程序语言也支持（Java，Python 等）。\n\n通配符\n\n通配符 ? 匹配文件名中的单个字符，而通配符\n* 匹配零个或多个字符。像 data?.dat\n这样的模式将查找下列文件：\n\ndata1.dat  data2.dat  datax.dat  dataN.dat  \n使用 * 字符代替 ?\n字符扩大了找到的文件的数量。data*.dat\n匹配下列所有文件：\ndata.dat  data1.dat  data2.dat  data12.dat  datax.dat  dataXYZ.dat  \n正则表达式 (regular\nexpression)\n特殊字符\n特殊字符就是有特殊含义的字符，下面的表格将列出一些常用的特殊字符，如果要匹配这些特殊字符可以在其之前加上反斜杠 \\\n\n\n\n\n\n\n\n特殊字符\n含义\n\n\n\n\n.\n 匹配除了 \\n 以外的任何单个字符，与通配符中的 ? 作用相同\n\n\n？\n匹配前面的子表达式零次或一次，如.? 表示没有字符或一个任意字符，he(hee)? 表示 he 或 hehee\n\n\n+\n 匹配前面的子表达式一次或多次，如.+ 表示一个或一个以上的任意字符\n\n\n *\n 匹配前面的子表达式零次或多次，如.* 表示一个或一个以上的任意字符\n\n\n [ ]\n 匹配中括号内任意一个字符，如 [Pp]ython 表示 Python 或 python,[a-z] 匹配任何一个小写字母，[a-zA-Z0-9] 匹配任何一个字母或数字\n\n\n ( )\n 标记一个字表达式的开始和结束\n\n\n { }\n 花括号里面放的是限定符表达式，用来匹配前面指定的表达式重复的次数，如？等价于 {0,1}，+ 等价于 {1,}\n\n\n^\n 在方括号 [] 内使用表示不包含方括号内任意元素，其余情况均表示匹配输入字符串的起始位置\n\n\n $\n 匹配输入字符串的末尾\n\n\n表示字符和空格间的位置（也叫作字边界）\n\n\n\n\n任何其他非字边界的位置\n\n\n\n匹配包括下划线的任何单词字符。等价于 '[A-Za-z0-9_\n]'\n\n\n\n 匹配任何非单词字符。等价于 '[^A-Za-z0-9_\n]\n\n\n 匹配一个数字字符。等价于 [0-9]\n\n\n\n\n 匹配一个非数字字符。等价于 [^0-9]\n\n\n\n贪婪匹配和非贪婪匹配\n对于字符串 &lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;\n\n如果匹配的正则表达式为 &lt;(.*)&gt;，则为贪婪匹配，匹配出来的内容为 H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1，正则表达式没有括号时 (即为 &lt;.*&gt;)，匹配的内容为 &lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;\n\n如果匹配的正则表达式为 &lt;(.*?)&gt;，则为非贪婪匹配，匹配出来的内容为 H1 和 /H1\n\n这样应该可以看出非贪婪匹配（又称最小匹配）和贪婪匹配的区别了，贪婪匹配就是匹配符合正则表达式的尽可能多的内容，而非贪婪匹配则是匹配符合正则表达式的第一个子串（如果有多个符合）。\n一些例子\n\n\n\n正则表达式\n匹配内容\n\n\n\n\n [^\\/^]\n 除了 (\\)(/)(^) 之外的所有字符\n\n\n [^\\\"\\']\n 除了双引号 (\") 和单引号 (') 之外的所有字符\n\n\n.{2}\n 所有的两个字符\n\n\n\n两个制表符\n\n\n ^a{2,}$\n 以两个或两个以上的 a 开头的字符串\n\n\n ^a{2,4}$\naa,aaa 或 aaaa\n\n\n1{1,}$\n 所有的正数\n\n\n ^\\-{0,1}[0-9]{1,}$\n 所有的整数\n\n\n ^\\-?[0-9]*\\.?[0-9]+$\n 所有小数\n\n\n\n关于正则表达式的一些基本概念就是上面这些，当然正则表达式的功能比上面所说的还要强大得多。只是无需记住所有的特性，在需要用到的时候 google 一下即可。\n\n\n\n0-9↩︎\n\n\n","categories":["杂"],"tags":["Linux"]},{"title":"隐马尔可夫模型的三大问题及求解方法","url":"/2017/07/14/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98%E5%8F%8A%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95/","content":"本文主要介绍隐马尔可夫模型以及该模型中的三大问题的解决方法。\n\n隐马尔可夫模型的是处理序列问题的统计学模型，描述的过程为：由隐马尔科夫链随机生成不可观测的状态随机序列，然后各个状态分别生成一个观测，从而产生观测随机序列。\n在这个过程中，不可观测的序列称为状态序列 (state sequence),\n由此产生的序列称为观测序列 (observation sequence).\n该过程可通过下图描述\n\n\n隐马尔可夫模型\n\n上图中， \\(X_1,X_2,...X_T\\)\n是隐含序列，而 \\(O_1, O_2,..O_T\\)\n是观察序列\n隐马尔可夫模型由三个概率确定\n\n初始概率分布，即初始的隐含状态的概率分布，记为\n\\(\\pi\\)\n 状态转移概率分布，即隐含状态间的转移概率分布，记为\n\\(A\\)\n 观测概率分布，即由隐含状态生成观测状态的概率分布，\n记为 \\(B\\)\n\n以上的三个概率分布可以说就是隐马尔可夫模型的参数，而根据这三个概率，能够确定一个隐马尔可夫模型\n\\(\\lambda = (A, B, \\pi)\\)\n而隐马尔科夫链的三个基本问题为\n\n概率计算问题。即给定模型 \\(\\lambda = (A, B, \\pi)\\) 和观测序列 \\(O\\)，计算在模型 \\(\\lambda\\) 下观测序列出现的最大概率 \\(P(O|\\lambda)\\)\n 学习问题。即给定观测序列 \\(O\\)，估计模型的参数 \\(\\lambda\\),\n使得在该参数下观测序列出现的概率最大，即 \\(P(O|\\lambda)\\) 最大\n解码问题。给定模型 \\(\\lambda = (A, B, \\pi)\\) 和观测序列 \\(O\\)，计算最有可能产生这个观测序列的隐含序列\n\\(X\\), 即使得概率 \\(P(X|O, \\lambda)\\) 最大的隐含序列 \\(X\\)\n\n概率计算问题\n概率计算问题理论上可通过穷举法来解决，即穷举所有可能的隐含状态序列，然后计算所有可能的隐含序列生成观测序列的概率，假设观测序列的长度为\n\\(n\\),\n且每个观测状态对应的可能的隐含状态长度为 \\(m\\), 则这种方法的时间复杂度就是 \\(O(m^n)\\),\n这样的时间复杂度显然是无法接受的，因此在实际中往往不采用这种方法，而是采用前向算法和后向算法，\n前向算法和后向算法都是通过动态规划来减少计算的时间复杂度，两者的不同点就是计算的方向不同。\n前向算法\n前向算法需要先定义前向概率\n\n前向概率定义为到时刻 \\(t\\)\n为止观测序列为 \\(o_1,o_2,o_3...o_t\\)，且时刻 \\(t\\) 的隐含状态为所有的隐含状态中的第 \\(i\\) 个（记为 \\(q_i\\)），则前向概率可记为 \\[\\alpha_t(i) = P(o_1,o_2,o_3...o_t,x_t =\nq_i|\\lambda)\\]\n\n定义了前向概率，便可递归地求解前向概率和观测序列的概率 \\(P(o_1,o_2,o_3...o_t|\\lambda)\\)\n初始的状态为\n\\[\\begin{align} \\alpha_1(i) =\n\\pi_ib_i(o_1)~~i=1,..m \\end{align}\\]\n上式中的 \\(m\\) 表示隐含状态的数目，\n\\(\\pi_i\\) 表示各个隐含状态的初始概率，\n\\(b_i(o_1)\\) 表示第 \\(i\\) 个隐含状态生成观测状态 \\(o_1\\) 的概率\n则递推的公式为\n\\[\\begin{align} \\alpha_{t+1}(i) =\n(\\sum_{j=1}^N \\alpha_t(j) a_{ji} )b_i(o_{t+1})~~i=1,..m\n\\end{align}\\]\n上式中的 \\(a_{ji}\\) 表示从隐含状态\n\\(j\\) 转移到隐含状态 \\(i\\)\n的状态转移概率。通过下图可较直观地看到前向递推公式的过程\n\n\n前向算法\n\n最终计算得到的概率为（其中 \\(T\\)\n为观测序列的长度）\n\\[\\begin{align} P(O|\\lambda) =\n\\sum_{i=1}^{m}\\alpha_T(i) \\end{align}\\]\n后向算法\n类似前向算法，后向算法也可用于求解这个问题，只是方向是从后到前的。同样的，需要先定义后向概率\n\n后向概率指时刻 \\(t\\)\n的隐含状态为所有隐含状态中的第 \\(i\\)\n个 (记为 \\(q_i\\)), 且时刻 \\(t+1\\) 到 \\(T\\) 的观测序列为 \\(o_{t+1}, o_{t+2},....o_T\\) 的概率，记为 \\[\\beta_t(i) = P(o_{t+1}, o_{t+2},....o_T, x_t =\nq_i|\\lambda)\\]\n\n初始状态定义为\n\\[\\begin{align}  \\beta_T(i) =\n1~~i=1,2,...m  \\end{align}\\]\n这里概率为 1 的原因是概率为 1 的原因是本来还需要看看时刻 \\(T\\) 后面有什么东西，但因为最后一个时刻\n\\(T\\)\n后面已经没有时刻，即不需要再观测某个东西，所以随便给个什么状态都可以\n递推公式为\n\\[\\begin{align} \\beta_t(i) =\n\\sum_{j=1}^ma_{ij}b_j(o_{t+1})\\beta_{t+1}(j)~~i=1,2,...m\n\\end{align}\\]\n上面的式子中的符号与前向算法中的一致，其过程可通过下图更直观理解\n\n\n后向算法\n\n最终计算得到的概率为\n\\[\\begin{align} P(O|\\lambda) =\n\\sum_{i=1}^m \\pi_ib_i(o_1)\\beta_1(i) \\end{align}\\]\n分析可知，前向算法和后向算法的时间复杂度均是 \\(O(m^2T)\\), \\(m\\) 为隐含状态的个数，\\(T\\) 为序列长度\n学习问题\n学习问题要根据观测序列来推导模型参数，这一类的问题对应到概率论中的极大似估计问题。但是这里是有隐含变量的极大似然估计，因此直接无法通过直接求导进行求解，而要通过\nEM 算法 来求解这一类问题。\nEM\n算法是一类算法，用于解决有隐含变量的概率模型参数的极大似然估计，具体到隐马尔可夫模型中的具体算法是\nBaum-Welch\n算法。\n注：这里采用 EM\n算法的前提是问题仅给出了观测序列，假如同时给出了观测序列和隐含序列，可直接通过最大似然估计求解。\n问题的描述如下：给定的训练数据只包含 S 个长度为 T 的观测序列 \\(O = \\lbrace O_1, O_2, O_3...O_T \\rbrace\\)\n而没有对应的状态序列 \\(X\\)，目标是学习隐马尔可夫模型 \\(\\lambda = (A,B,\\pi)\\)\n的参数，则隐马尔可夫模型此时变为了一个含有隐含变量的概率模型，表示为 \\[P(O|\\lambda) = \\sum_I\nP(O|I,\\lambda)P(I|\\lambda)\\]\n这里只给出 Baum-Welch 算法的流程，而省去其推导过程\n1. 初始化模型参数：选取 \\(a_{ij}^{(0)},\nb_j^{(0)}, \\pi_i^{(0)}\\), 得到模型 \\(\\pi^{(0)} = (A^{(0)}, B^{(0)}, \\pi^{(0)})\\)\n2.E 步，求解两个中间变量 \\(\\gamma_t(i),\n\\xi_t(i,j)\\), 两者的含义如下 \\(\\gamma_t(i)\\)：给定模型 \\(\\lambda\\) 和观测序列 \\(O\\)，在时刻 \\(t\\) 的隐含状态为 \\(q_i\\) 的概率，即 \\(\\gamma_t(i) = P(x_t = q_i | O, \\lambda)\\)\n\\(\\xi_t(i,j)\\)：给定模型 \\(\\lambda\\) 和观测序列 \\(O\\)，在时刻 \\(t\\) 的隐含状态为 \\(q_i\\) , 时刻 \\(t+1\\) 的隐含状态为 \\(q_j\\) 的概率，即 \\(\\xi_t(i,j) = P(x_t = q_i, x_{t+1} = q_j | O,\n\\lambda)\\)\n结合前面的前向概率和后向概率的定义，计算这两个中间变量的公式如下 (\\(m\\) 表示隐含状态的总个数)\n\\[\\begin{align} \\gamma_t(i) =\n\\frac{\\alpha_t(i) \\beta_t(i)}{\\sum_{j=1}^m \\alpha_t(j) \\beta_t(j)}\n\\end{align}\\]\n\\[\\begin{align} \\xi_t(i,j) =\n\\frac{\\alpha_t(i) a_{ij} b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum_{p=1}^m\n\\sum_{q=1}^m \\alpha_t(p) a_{pq} b_q(o_{t+1})\\beta_{t+1}(q)}\n\\end{align}\\]\n3.M 步，同时 E\n步求解出的两个中间变量来求解模型的参数，求解公式如下\n\\[\\begin{align} a_{ij} =\n\\frac{\\sum_{t=1}^T \\xi_t(i,j)}{\\sum_{t=1}^T\n\\gamma_t(i)}  \\end{align}\\]\n\\[\\begin{align} b_j(k) =\n\\frac{\\sum_{t=1}^T \\gamma_t(j)I(o_t = v_k)}{\\sum_{t=1}^T \\gamma_t(j)}\n\\end{align}\\]\n\\[\\begin{align} \\pi_i = \\gamma_1(i)\n\\end{align}\\]\n上式中的 \\(I(o_t = v_k)\\) 表示时刻\n\\(t\\) 的观察状态为 \\(v_k\\) 时，\\(I(o_t\n= v_k)\\) 才为 1，否则为 0\n迭代进行 E 步骤和 M 步，将最终收敛的结果作为模型的参数\n解码问题\n解码问题理论上也可以通过穷举法来解决，就是穷举所有可能的隐含序列并计算在这个隐含序列下观测序列的概率，并选择概率最大的那个隐含序列，但是穷举所有可能的隐含序列的时间复杂度也是指数级别的，跟第一个问题一样，在实际中往往也是不常用的。\n实际中，解码问题通过动态规划来降低时间复杂度，并且已经有了成熟的解决方法，就是著名的维特比算法，具体的算法流程可参考这篇文章：维特比算法。\n","categories":["机器学习"],"tags":["机器学习","NLP"]}]