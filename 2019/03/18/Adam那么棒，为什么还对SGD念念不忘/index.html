<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "JmI6QmP3fMkLet6B",ck: "JmI6QmP3fMkLet6B"})</script>

  <link rel="apple-touch-icon" sizes="180x180" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/imgs/favicon.ico">
  <link rel="mask-icon" href="/imgs/favicon.ico" color="#222">
  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wulc.me","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="好久没更新了，最近在忙着写毕业论文，刚好写到与优化相关部分，想起了之前在知乎上收藏过的一篇很好的文章，重新看一遍还是觉得获益良多，特意转载。原文链接见这里，侵删。">
<meta property="og:type" content="article">
<meta property="og:title" content="Adam那么棒，为什么还对SGD念念不忘">
<meta property="og:url" content="https://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="好久没更新了，最近在忙着写毕业论文，刚好写到与优化相关部分，想起了之前在知乎上收藏过的一篇很好的文章，重新看一遍还是觉得获益良多，特意转载。原文链接见这里，侵删。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wulc.me/imgs/optimizer_visulization.jpg">
<meta property="og:image" content="https://wulc.me/imgs/adam_to_sgd_direction.jpg">
<meta property="article:published_time" content="2019-03-18T08:08:44.000Z">
<meta property="article:modified_time" content="2023-04-30T05:12:49.135Z">
<meta property="article:author" content="良超">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="转载">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wulc.me/imgs/optimizer_visulization.jpg">


<link rel="canonical" href="https://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/","path":"2019/03/18/Adam那么棒，为什么还对SGD念念不忘/","title":"Adam那么棒，为什么还对SGD念念不忘"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Adam那么棒，为什么还对SGD念念不忘 | 吴良超的学习笔记</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="吴良超的学习笔记" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">吴良超的学习笔记</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E6%A1%86%E6%9E%B6%E5%9B%9E%E9%A1%BE%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">一个框架回顾优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd"><span class="nav-number">1.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd-with-momentum"><span class="nav-number">1.2.</span> <span class="nav-text">SGD with Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd-with-nesterov-acceleration"><span class="nav-number">1.3.</span> <span class="nav-text">SGD with Nesterov
Acceleration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adagrad"><span class="nav-number">1.4.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adadelta-rmsprop"><span class="nav-number">1.5.</span> <span class="nav-text">AdaDelta &#x2F; RMSProp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam"><span class="nav-number">1.6.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nadam"><span class="nav-number">1.7.</span> <span class="nav-text">Nadam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam%E7%9A%84%E4%B8%A4%E5%AE%97%E7%BD%AA"><span class="nav-number">2.</span> <span class="nav-text">Adam的两宗罪</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adam%E7%BD%AA%E7%8A%B6%E4%B8%80%E5%8F%AF%E8%83%BD%E4%B8%8D%E6%94%B6%E6%95%9B"><span class="nav-number">2.1.</span> <span class="nav-text">Adam罪状一：可能不收敛</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam%E7%BD%AA%E7%8A%B6%E4%BA%8C%E5%8F%AF%E8%83%BD%E9%94%99%E8%BF%87%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98%E8%A7%A3"><span class="nav-number">2.2.</span> <span class="nav-text">Adam罪状二：可能错过全局最优解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%B0%E5%BA%95%E8%AF%A5%E7%94%A8adam%E8%BF%98%E6%98%AFsgd"><span class="nav-number">2.3.</span> <span class="nav-text">到底该用Adam还是SGD？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E9%80%89%E6%8B%A9%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%AD%96%E7%95%A5"><span class="nav-number">3.</span> <span class="nav-text">优化算法的选择与使用策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B7%AE%E5%BC%82"><span class="nav-number">3.1.</span> <span class="nav-text">不同优化算法的核心差异</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adamsgd-%E7%BB%84%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-number">3.2.</span> <span class="nav-text">Adam+SGD 组合策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E5%B8%B8%E7%94%A8tricks"><span class="nav-number">3.3.</span> <span class="nav-text">优化算法的常用tricks</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="良超"
      src="/files/profile.jpg">
  <p class="site-author-name" itemprop="name">良超</p>
  <div class="site-description" itemprop="description">盈亏同源，享受生活的随机性</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">249</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/WuLC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuLC" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/wuliangchao/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;wuliangchao&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangchaowu5@gmail.com" title="E-Mail → mailto:liangchaowu5@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/files/profile.jpg">
      <meta itemprop="name" content="良超">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吴良超的学习笔记">
      <meta itemprop="description" content="盈亏同源，享受生活的随机性">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Adam那么棒，为什么还对SGD念念不忘 | 吴良超的学习笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Adam那么棒，为什么还对SGD念念不忘
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-18 16:08:44" itemprop="dateCreated datePublished" datetime="2019-03-18T16:08:44+08:00">2019-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>好久没更新了，最近在忙着写毕业论文，刚好写到与优化相关部分，想起了之前在知乎上收藏过的一篇很好的文章，重新看一遍还是觉得获益良多，特意转载。原文链接见<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32230623">这里</a>，侵删。</p>
<span id="more"></span>
<p>机器学习界有一群炼丹师，他们每天的日常是：</p>
<p>拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。</p>
<p>不过，当过厨子的都知道，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。</p>
<p>机器学习也是一样，模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。(注：笔者有过亲身经历，曾经使用
Adam 一开局就陷入局部最优，检查了好多遍代码，最后近乎绝望地换了 SGD
，然后效果蹭蹭蹭上升)</p>
<p>说到优化算法，入门级必从 SGD
学起，老司机则会告诉你更好的还有AdaGrad/AdaDelta，或者直接无脑用
Adam。可是看看学术界的最新 paper，却发现一众大神还在用着入门级的
SGD，最多加个 Momentum 或者 Nesterov ，还经常会黑一下Adam。比如 UC
Berkeley的一篇论文就在Conclusion中写道(注：这篇论文是<a
target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning">The
Marginal Value of Adaptive Gradient Methods in Machine Learning</a>)</p>
<blockquote>
<p>Despite the fact that our experimental evidence demonstrates that
adaptive methods are not advantageous for machine learning, the Adam
algorithm remains incredibly popular. We are not sure exactly as to why
……</p>
</blockquote>
<p>无奈与酸楚之情溢于言表。</p>
<p>这是为什么呢？难道平平淡淡才是真？</p>
<h2 id="一个框架回顾优化算法">一个框架回顾优化算法</h2>
<p>首先我们来回顾一下各类优化算法。</p>
<p>深度学习优化算法经历了 <strong>SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad
-&gt; AdaDelta -&gt; Adam -&gt; Nadam</strong>
这样的发展历程。Google一下就可以看到很多的教程文章，详细告诉你这些算法是如何一步一步演变而来的。在这里，我们换一个思路，用一个框架来梳理所有的优化算法，做一个更加高屋建瓴的对比。</p>
<p>首先定义：待优化参数： <span class="math inline">\(w\)</span>
，目标函数： <span class="math inline">\(f(w)\)</span> ，初始学习率
<span class="math inline">\(\alpha\)</span></p>
<p>而后，开始进行迭代优化。在每个 epoch <span
class="math inline">\(t\)</span>, 执行如下操作：</p>
<ol type="1">
<li>计算目标函数关于当前参数的梯度： <span
class="math inline">\(g\_t=\nabla f(w\_t)\)</span></li>
<li>根据历史梯度计算一阶动量和二阶动量：<span class="math inline">\(m\_t
= \phi(g\_1, g\_2, \cdots, g\_t)\)</span>; <span
class="math inline">\(V\_t = \psi(g\_1, g\_2, \cdots,
g\_t)\)</span></li>
<li>计算当前时刻的下降梯度： <span class="math inline">\(\eta\_t =
\alpha \cdot m\_t / \sqrt{V\_t}\)</span></li>
<li>根据下降梯度进行更新： <span class="math inline">\(w\_{t+1} = w\_t -
\eta\_t\)</span></li>
</ol>
<p>掌握了这个框架，你可以轻轻松松设计自己的优化算法。</p>
<p>我们拿着这个框架，来照一照各种玄乎其玄的优化算法的真身。<strong>步骤3、4对于各个算法都是一致的，主要的差别就体现在1和2上。</strong></p>
<h3 id="sgd">SGD</h3>
<p>先来看SGD, SGD没有动量的概念，也就是说：</p>
<p><span class="math inline">\(m\_t = g\_t\)</span> <span
class="math inline">\(V\_t = I^2\)</span></p>
<p>代入步骤3，可以看到下降梯度就是最简单的</p>
<p><span class="math inline">\(\eta\_t = \alpha \cdot g\_t\)</span></p>
<p><strong>SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</strong></p>
<h3 id="sgd-with-momentum">SGD with Momentum</h3>
<p>为了抑制 SGD 的震荡，SGDM
认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM
全称是SGD with
momentum，<strong>在SGD基础上引入了一阶动量</strong>：</p>
<p><span class="math inline">\(m\_t = \beta\_1 \cdot m\_{t-1} +
(1-\beta\_1)\cdot g\_t\)</span></p>
<p>一阶动量是各个时刻梯度方向的<strong>指数移动平均值</strong>，约等于最近
<span class="math inline">\(1/(1-\beta\_1)\)</span>
个时刻的梯度向量和的平均值。</p>
<p>也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。
<strong><span class="math inline">\(\beta\_1\)</span>
的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。</strong></p>
<h3 id="sgd-with-nesterov-acceleration">SGD with Nesterov
Acceleration</h3>
<p>SGD
还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。</p>
<p>NAG 全称 Nesterov Accelerated Gradient，是在 SGD、SGD-M
的基础上的进一步改进，改进点在于步骤1。我们知道在时刻 t
的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，<strong>NAG在步骤
1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向</strong>：</p>
<p><span class="math inline">\(g\_t=\nabla f(w\_t-\alpha \cdot m\_{t-1}
/ \sqrt{V\_{t-1}})\)</span></p>
<p>然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。</p>
<h3 id="adagrad">AdaGrad</h3>
<p>此前我们都没有用到二阶动量。<strong>二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来</strong>。SGD
及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。<strong>对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</strong></p>
<p>怎么样去<strong>度量历史更新频率</strong>呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：</p>
<p><span class="math inline">\(V\_t = \sum\_{\tau=1}^{t}
g\_\tau^2\)</span></p>
<p>我们再回顾一下步骤3中的下降梯度：</p>
<p><span class="math inline">\(\eta\_t = \alpha \cdot m\_t /
\sqrt{V\_t}\)</span></p>
<p>可以看出，此时实质上的学习率由 <span
class="math inline">\(\alpha\)</span> 变成了 <span
class="math inline">\(\alpha / \sqrt{V\_t}\)</span> 。
一般为了避免分母为0，会在分母上加一个小的平滑项。因此 <span
class="math inline">\(\sqrt{V\_t}\)</span>
是恒大于0的，而且<strong>参数更新越频繁，二阶动量越大，学习率就越小</strong>。</p>
<p>这一方法在<strong>稀疏数据场景下表现非常好</strong>。但也存在一些问题：因为
<span class="math inline">\(\sqrt{V\_t}\)</span>
是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。</p>
<h3 id="adadelta-rmsprop">AdaDelta / RMSProp</h3>
<p>由于 AdaGrad
单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：<strong>不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是
AdaDelta 名称中 Delta 的来历</strong>。</p>
<p>修改的思路很简单。前面我们讲到，<strong>指数移动平均值大约就是过去一段时间的平均值</strong>，因此我们用这一方法来计算二阶累积动量：</p>
<p><span class="math inline">\(V\_t = \beta\_2 * V\_{t-1} + (1-\beta\_2)
g\_t^2\)</span></p>
<p>这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p>
<h3 id="adam">Adam</h3>
<p>谈到这里，Adam 和 Nadam
的出现就很自然而然了——它们是前述方法的集大成者。我们看到，SGD-M 在 SGD
基础上增加了一阶动量，AdaGrad 和 AdaDelta 在 SGD
基础上增加了二阶动量。<strong>把一阶动量和二阶动量都用起来，就是
Adam了——Adaptive + Momentum</strong>。</p>
<p>SGD的一阶动量：</p>
<p><span class="math inline">\(m\_t = \beta\_1 \cdot m\_{t-1} +
(1-\beta\_1)\cdot g\_t\)</span></p>
<p>加上AdaDelta的二阶动量：</p>
<p><span class="math inline">\(V\_t = \beta\_2 * V\_{t-1} + (1-\beta\_2)
g\_t^2\)</span></p>
<p>优化算法里最常见的两个超参数 <span
class="math inline">\(\beta\_1\)</span>, <span
class="math inline">\(\beta\_2\)</span>
就都在这里了，前者控制一阶动量，后者控制二阶动量。</p>
<h3 id="nadam">Nadam</h3>
<p>最后是Nadam。我们说Adam是集大成者，但它居然遗漏了Nesterov，这还能忍？必须给它加上，按照NAG的步骤1：</p>
<p><span class="math inline">\(g\_t=\nabla f(w\_t-\alpha \cdot m\_{t-1}
/ \sqrt{V\_t})\)</span></p>
<p>这就是Nesterov + Adam = Nadam了。</p>
<p>说到这里，大概可以理解为什么j经常有人说 Adam / Nadam
目前最主流、最好用的优化算法了。新手上路，先拿来一试，收敛速度嗖嗖滴，效果也是杠杠滴。</p>
<p>那为什么Adam还老招人黑，被学术界一顿鄙夷？难道只是为了发paper灌水吗？</p>
<h2 id="adam的两宗罪">Adam的两宗罪</h2>
<p>从理论上看，一代更比一代完善，Adam/Nadam
已经登峰造极了，为什么大家还是不忘初心SGD呢？</p>
<p>举个栗子。很多年以前，摄影离普罗大众非常遥远。十年前，傻瓜相机开始风靡，游客几乎人手一个。智能手机出现以后，摄影更是走进千家万户，手机随手一拍，前后两千万，照亮你的美（咦，这是什么乱七八糟的）。但是专业摄影师还是喜欢用单反，孜孜不倦地调光圈、快门、ISO、白平衡……一堆自拍党从不
care
的名词。技术的进步，使得傻瓜式操作就可以得到不错的效果，但是在特定的场景下，要拍出最好的效果，依然需要深入地理解光线、理解结构、理解器材。</p>
<p>优化算法大抵也如此。在上一篇中，我们用同一个框架让各类算法对号入座。可以看出，大家都是殊途同归，只是相当于在SGD基础上增加了各类学习率的主动控制。如果不想做精细的调优，那么Adam显然最便于直接拿来上手。</p>
<p>但这样的傻瓜式操作并不一定能够适应所有的场合。如果能够深入了解数据，研究员们可以更加自如地控制优化迭代的各类参数，实现更好的效果也并不奇怪。毕竟，精调的参数还比不过傻瓜式的Adam，无疑是在挑战顶级研究员们的炼丹经验！</p>
<h3 id="adam罪状一可能不收敛">Adam罪状一：可能不收敛</h3>
<p>这篇是正在深度学习领域顶级会议之一 ICLR 2018 匿名审稿中的 <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of
Adam and
Beyond</a>，探讨了Adam算法的收敛性，通过反例证明了Adam在某些情况下可能会不收敛。(注:
这篇论文已经成了 2018 ICLR 最佳论文)</p>
<p>回忆一下上文提到的各大优化算法的学习率：</p>
<p><span class="math inline">\(\eta\_t = \alpha /
\sqrt{V\_t}\)</span></p>
<p>其中，SGD
没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad
的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。</p>
<p>但 AdaDelta 和 Adam
则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得
<span class="math inline">\(V\_t\)</span>
可能会时大时小，不是单调变化。这就<strong>可能在训练后期引起学习率的震荡，导致模型无法收敛。</strong></p>
<p>这篇文章也给出了一个修正的方法。由于Adam中的学习率主要是由二阶动量控制的，为了保证算法的收敛，可以<strong>对二阶动量的变化进行控制，避免上下波动。</strong></p>
<p><span class="math inline">\(V\_t = max(\beta\_2 * V\_{t-1} +
(1-\beta\_2) g\_t^2, V\_{t-1})\)</span></p>
<p>通过这样修改，就保证了 <span class="math inline">\(||V\_t|| \geq
||V\_{t-1}||\)</span> ，从而使得<strong>学习率单调递减</strong>。</p>
<h3
id="adam罪状二可能错过全局最优解">Adam罪状二：可能错过全局最优解</h3>
<p>深度神经网络往往包含大量的参数，在这样一个维度极高的空间内，非凸的目标函数往往起起伏伏，拥有无数个高地和洼地。有的是高峰，通过引入动量可能很容易越过；但有些是高原，可能探索很多次都出不来，于是停止了训练。</p>
<p>近期Arxiv上的两篇文章谈到这个问题。</p>
<p>第一篇就是前文提到的吐槽 Adam 最狠的 The Marginal Value of Adaptive
Gradient Methods in Machine Learning
。文中说到，同样的一个优化问题，不同的优化算法可能会找到不同的答案，但自适应学习率的算法往往找到非常差的答案。他们通过一个特定的数据例子说明，<strong>自适应学习率算法可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。</strong></p>
<p>另外一篇是 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07628">Improving
Generalization Performance by Switching from Adam to
SGD</a>，进行了实验验证。他们 CIFAR-10 数据集上进行测试，<strong>Adam
的收敛速度比 SGD
要快，但最终收敛的结果并没有SGD好。他们进一步实验发现，主要是后期 Adam
的学习率太低，影响了有效的收敛。他们试着对Adam的学习率的下界进行控制，发现效果好了很多。</strong></p>
<p>于是他们提出了一个用来改进Adam的方法：<strong>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。</strong>这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。这篇文章把这一切换过程傻瓜化，给出了切换SGD的时机选择方法，以及学习率的计算方法，效果看起来也不错。</p>
<h3 id="到底该用adam还是sgd">到底该用Adam还是SGD？</h3>
<p>所以，谈到现在，到底Adam好还是SGD好？这可能是很难一句话说清楚的事情。去看学术会议中的各种paper，用SGD的很多，Adam的也不少，还有很多偏爱AdaGrad或者AdaDelta。可能研究员把每个算法都试了一遍，哪个出来的效果好就用哪个了。</p>
<p><strong>而从这几篇怒怼 Adam 的 paper
来看，多数都构造了一些比较极端的例子来演示了 Adam
失效的可能性。这些例子一般过于极端，实际情况中可能未必会这样，但这提醒了我们，理解数据对于设计算法的必要性。</strong>优化算法的演变历史，都是基于对数据的某种假设而进行的优化，那么某种算法是否有效，就要看你的数据是否符合该算法的胃口了。</p>
<p><strong>算法固然美好，数据才是根本。</strong></p>
<p>另一方面，Adam之流虽然说已经简化了调参，但是并没有一劳永逸地解决问题，默认参数虽然好，但也不是放之四海而皆准。因此，<strong>在充分理解数据的基础上，依然需要根据数据特性、算法特性进行充分的调参实验，找到自己炼丹的最优解。而这个时候，不论是Adam，还是SGD，于你都不重要了。</strong></p>
<p>少年，好好炼丹吧。</p>
<h2 id="优化算法的选择与使用策略">优化算法的选择与使用策略</h2>
<p>在前面两节中，我们用一个框架梳理了各大优化算法，并且指出了以Adam为代表的自适应学习率优化算法可能存在的问题。那么，在实践中我们应该如何选择呢？本文介绍Adam+SGD的组合策略，以及一些比较有用的tricks.</p>
<h3 id="不同优化算法的核心差异">不同优化算法的核心差异</h3>
<p>下降方向从第一篇的框架中我们看到，不同优化算法最核心的区别，就是第三步所执行的下降方向</p>
<p><span class="math inline">\(\eta\_t = (\alpha/ \sqrt{V\_t} ) \cdot
m\_t\)</span></p>
<p>这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。<strong>SGD
算法的下降方向就是该位置的梯度方向的反方向，带一阶动量的SGD的下降方向则是该位置的一阶动量方向。自适应学习率类优化算法为每个参数设定了不同的学习率，在不同维度上设定不同步长，因此其下降方向是缩放过（scaled）的一阶动量方向。</strong></p>
<p><strong>由于下降方向的不同，可能导致不同算法到达完全不同的局部最优点。</strong>
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.04010">An empirical analysis of the
optimization of deep network loss surfaces</a>
这篇论文中做了一个有趣的实验，他们把目标函数值和相应的参数形成的超平面映射到一个三维空间，这样我们可以直观地看到各个算法是如何寻找超平面上的最低点的。</p>
<figure>
<img data-src="https://wulc.me/imgs/optimizer_visulization.jpg"
alt="optimizer_compare" />
<figcaption aria-hidden="true">optimizer_compare</figcaption>
</figure>
<p>上图是论文的实验结果，横纵坐标表示降维后的特征空间，区域颜色则表示目标函数值的变化，红色是高原，蓝色是洼地。他们做的是配对儿实验，让两个算法从同一个初始化位置开始出发，然后对比优化的结果。可以看到，几乎任何两个算法都走到了不同的洼地，他们中间往往隔了一个很高的高原。这就说明，<strong>不同算法在高原的时候，选择了不同的下降方向。</strong></p>
<h3 id="adamsgd-组合策略">Adam+SGD 组合策略</h3>
<p>正是在每一个十字路口的选择，决定了你的归宿。如果上天能够给我一个再来一次的机会，我会对那个女孩子说：SGD！</p>
<p>不同优化算法的优劣依然是未有定论的争议话题。据我在paper和各类社区看到的反馈，主流的观点认为：<strong>Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。</strong></p>
<p>那么我们就会想到，可不可以把这两者结合起来，先用Adam快速下降，再用SGD调优，一举两得？思路简单，但里面有两个技术问题：</p>
<ol type="1">
<li><strong>什么时候切换优化算法？</strong>——如果切换太晚，Adam可能已经跑到自己的盆地里去了，SGD再怎么好也跑不出来了。</li>
<li><strong>切换算法以后用什么样的学习率？</strong>——Adam用的是自适应学习率，依赖的是二阶动量的累积，SGD接着训练的话，用什么样的学习率？</li>
</ol>
<p>上一篇中提到的论文 Improving Generalization Performance by Switching
from Adam to SGD 提出了解决这两个问题的思路</p>
<p>首先来看第二个问题，切换之后用什么样的学习率。Adam的下降方向是</p>
<p><span class="math inline">\(\eta\_t^{Adam} = (\alpha/ \sqrt{V\_t} )
\cdot m\_t\)</span></p>
<p>而 SGD 的下降方向是</p>
<p><span class="math inline">\(\eta\_t^{SGD} = \alpha^{SGD} \cdot
g\_t\)</span></p>
<p><span class="math inline">\(\eta\_t^{SGD}\)</span> 必定可以分解为
<span class="math inline">\(\eta\_t^{Adam}\)</span>
所在方向及其正交方向上的两个方向之和，如下图所示，这里p为Adam下降方向，g为梯度方向，r为SGD的学习率。</p>
<figure>
<img data-src="https://wulc.me/imgs/adam_to_sgd_direction.jpg"
alt="adam_to_sgd_direction" />
<figcaption aria-hidden="true">adam_to_sgd_direction</figcaption>
</figure>
<p>那么其在 <span class="math inline">\(\eta\_t^{Adam}\)</span>
方向上的投影就意味着 SGD 在 Adam 算法决定的下降方向上前进的距离，而在
<span class="math inline">\(\eta\_t^{Adam}\)</span> 的正交方向上的投影是
SGD 在自己选择的修正方向上前进的距离。</p>
<p>如果SGD要走完Adam未走完的路，那就首先要接过Adam的大旗——沿着 <span
class="math inline">\(\eta\_t^{Adam}\)</span>
方向走一步，而后在沿着其正交方向走相应的一步。</p>
<p>这样我们就知道该如何确定SGD的步长（学习率）了——<strong>SGD在Adam下降方向上的正交投影，应该正好等于Adam的下降方向（含步长）</strong>。也即：</p>
<p><span class="math inline">\(proj\_{\eta\_t^{SGD}} =
\eta\_t^{Adam}\)</span></p>
<p>解这个方程，我们就可以得到接续进行SGD的学习率：</p>
<p><span class="math inline">\(\alpha\_t^{SGD} =
((\eta\_t^{Adam})^T\eta\_t^{Adam})/((\eta\_t^{Adam})^Tg\_t)\)</span></p>
<p>为了减少噪声影响，作者使用移动平均值来修正对学习率的估计：</p>
<p><span class="math inline">\(\lambda\_t^{SGD} = \beta\_2 \cdot
\lambda\_{t-1}^{SGD}+(1-\beta\_2) \cdot \alpha\_t^{SGD}\)</span></p>
<p><span
class="math inline">\(\tilde{\lambda}\_t^{SGD}=\lambda\_t^{SGD}/(1-\beta\_2^t)\)</span></p>
<p>这里直接复用了Adam的 <span class="math inline">\(\beta\_2\)</span>
参数。</p>
<p>然后来看第一个问题，<strong>何时进行算法的切换</strong>。</p>
<p>作者的回答也很简单，那就是<strong>当 SGD
的相应学习率的移动平均值基本不变的时候</strong>，即：</p>
<p><span class="math inline">\(|\tilde{\lambda}\_t^{SGD} -
\alpha\_t^{SGD}|&lt;\epsilon\)</span>.
每次迭代玩都计算一下SGD接班人的相应学习率，如果发现基本稳定了，那就 SGD
以 <span class="math inline">\(\tilde{\lambda}\_t^{SGD}\)</span>
为学习率接班前进。</p>
<h3 id="优化算法的常用tricks">优化算法的常用tricks</h3>
<p>最后，分享一些在优化算法的选择和使用方面的一些tricks。</p>
<ol type="1">
<li><p><strong>首先，各大算法孰优孰劣并无定论</strong>。如果是刚入门，<strong>优先考虑
SGD+Nesterov Momentum 或者 Adam</strong>.（<a
target="_blank" rel="noopener" href="http://cs231n.github.io/neural-networks-3/">Standford 231n</a> :
The two recommended updates to use are either SGD+Nesterov Momentum or
Adam）</p></li>
<li><p><strong>选择你熟悉的算法</strong>——这样你可以更加熟练地利用你的经验进行调参</p></li>
<li><p><strong>充分了解你的数据</strong>——如果数据是非常稀疏的，那么优先考虑自适应学习率的算法。</p></li>
<li><p><strong>根据你的需求来选择</strong>——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。</p></li>
<li><p><strong>先用小数据集进行实验</strong>。论文 <a
target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf">Stochastic
Gradient Descent Tricks</a>
指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。</p></li>
<li><p><strong>考虑不同算法的组合</strong>。先用Adam进行快速下降，而后再换到
SGD 进行充分的调优。切换策略可以参考本文介绍的方法。</p></li>
<li><p><strong>数据集一定要充分的打散（shuffle）</strong>。这样在使用自适应学习率算法的时候，可以<strong>避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题</strong>。</p></li>
<li><p>训练过程中<strong>持续监控训练数据和验证数据</strong>上的目标函数值以及精度或者
AUC
等指标的变化情况。<strong>对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合</strong>。</p></li>
<li><p><strong>制定一个合适的学习率衰减策略</strong>。可以使用定期衰减策略，比如每过多少个
epoch 就衰减一次；或者利用精度或者 AUC
等性能指标来监控，<strong>当测试集上的指标不变或者下跌时，就降低学习率。</strong></p></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 数学</a>
              <a href="/tags/%E8%BD%AC%E8%BD%BD/" rel="tag"><i class="fa fa-tag"></i> 转载</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/02/18/Effective%20Go%20%E6%91%98%E8%AE%B0/" rel="prev" title="Effective Go 摘记">
                  <i class="fa fa-chevron-left"></i> Effective Go 摘记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/03/25/Leetcode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(496,%20975,%20503)-next%20greater-smaller%20element/" rel="next" title="Leetcode 解题报告(496, 975, 503)-next greater/smaller element">
                  Leetcode 解题报告(496, 975, 503)-next greater/smaller element <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-pen"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">良超</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
