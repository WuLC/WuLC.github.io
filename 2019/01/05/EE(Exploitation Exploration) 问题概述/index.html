<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "JmI6QmP3fMkLet6B",ck: "JmI6QmP3fMkLet6B"})</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/imgs/favicon.ico">
  <link rel="mask-icon" href="/imgs/favicon.ico" color="#222">
  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wulc.me","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="EE(Exploitation &amp; Exploration) 问题在计算广告&#x2F;推荐系统中非常常见，甚至在更广义的范围上，任意决策问题都会牵涉到 EE 问题。简单来说，这个问题就是要解决的是在决策时到底是根据已有经验选择最优的策略(Exploitation)，还是去探索一些新的策略来提升未来的收益(Exploration)。本文主要介绍解决这个问题的三种比较常见的方法：随机方法，UCB 方法">
<meta property="og:type" content="article">
<meta property="og:title" content="EE 问题概述">
<meta property="og:url" content="https://wulc.me/2019/01/05/EE(Exploitation%20Exploration)%20%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="EE(Exploitation &amp; Exploration) 问题在计算广告&#x2F;推荐系统中非常常见，甚至在更广义的范围上，任意决策问题都会牵涉到 EE 问题。简单来说，这个问题就是要解决的是在决策时到底是根据已有经验选择最优的策略(Exploitation)，还是去探索一些新的策略来提升未来的收益(Exploration)。本文主要介绍解决这个问题的三种比较常见的方法：随机方法，UCB 方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wulc.me/imgs/image_1cvujraje37f8ne6ej1l25o9e9.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1d06kc0hg38a172e1hs85rg1q8416.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1d0bpj8ghom61ki0e8q1no31urc9.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1d0c0s8c31qaqi6v12g51g1iprj2m.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1d0c9tmfu1jf9187016761golc433.png">
<meta property="og:image" content="https://wulc.me/imgs/image_1d0cb0gnb11ifqtmc971pj3p3t3t.png">
<meta property="article:published_time" content="2019-01-05T08:41:06.000Z">
<meta property="article:modified_time" content="2023-04-30T05:12:49.114Z">
<meta property="article:author" content="良超">
<meta property="article:tag" content="计算广告">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wulc.me/imgs/image_1cvujraje37f8ne6ej1l25o9e9.png">

<link rel="canonical" href="https://wulc.me/2019/01/05/EE(Exploitation%20Exploration)%20%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>EE 问题概述 | 吴良超的学习笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">吴良超的学习笔记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wulc.me/2019/01/05/EE(Exploitation%20Exploration)%20%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/files/profile.jpg">
      <meta itemprop="name" content="良超">
      <meta itemprop="description" content="盈亏同源，享受生活的随机性">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吴良超的学习笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          EE 问题概述
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-01-05 16:41:06" itemprop="dateCreated datePublished" datetime="2019-01-05T16:41:06+08:00">2019-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-30 13:12:49" itemprop="dateModified" datetime="2023-04-30T13:12:49+08:00">2023-04-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">计算广告</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>EE(Exploitation &amp; Exploration)
问题在计算广告/推荐系统中非常常见，甚至在更广义的范围上，任意决策问题都会牵涉到
EE
问题。简单来说，这个问题就是要解决的是<strong>在决策时到底是根据已有经验选择最优的策略(Exploitation)，还是去探索一些新的策略来提升未来的收益(Exploration)</strong>。本文主要介绍解决这个问题的三种比较常见的方法：随机方法，UCB
方法，Thompson sampling 方法，侧重于方法的具体流程和基本思想。</p>
<span id="more"></span>
<h2 id="mab-建模">MAB 建模</h2>
<p>EE 问题一般会通过 MAB(Multi-Armed Bandit) 进行建模, 如下所示，所有
arm 就是每次决策中可作出的选择，拉下某个 arm 表示作出了相应的选择。</p>
<figure>
<img src="https://wulc.me/imgs/image_1cvujraje37f8ne6ej1l25o9e9.png"
alt="MAB" />
<figcaption aria-hidden="true">MAB</figcaption>
</figure>
<p>MAB 符号化表述如下</p>
<ol type="1">
<li>MAB 可表示为一个二元组 &lt;<span class="math inline">\(A,
R\)</span>&gt;</li>
<li><span class="math inline">\(A\)</span> 表示为一系列可能的动作, <span
class="math inline">\(R(r|a)\)</span>
则表示给定动作下的奖赏的分布，</li>
<li>每一时刻根据给定策略从 <span class="math inline">\(A\)</span>
选择动作 <span class="math inline">\(a\_t\)</span>, 同时环境根据分布
<span class="math inline">\(R(r|a)\)</span> 生成奖赏 <span
class="math inline">\(r\_t\)</span></li>
<li>目标是最大化奖赏之和 <span class="math inline">\(\sum\_{t=1}^T
r\_t\)</span></li>
</ol>
<p>上面的第 3 步的策略就是下面要介绍的 EE
问题的解决方法，除去随机方法，<strong>UCB 方法和 Thompson sampling
方法的思想均是通过定义每个 arm 的收益的期望，然后选择收益期望最大的
arm</strong>。UCB 是<strong>频率学派</strong>的思想，认为每个 arm
的收益期望是固定的，通过试验记录得到其历史收益状况，然后加上一个 bound
构成了收益期望；Thompson sampling
则是<strong>贝叶斯学派</strong>的思想，认为 arm
的收益期望服从一个特定的概率分布，通过试验记录更新分布的参数，然后从每个
arm 的分布中产生收益期望。</p>
<p>而<strong>根据一个 arm
的历史试验记录判断其优劣又有两种方法，因而也衍生了两类 bandit
问题：Bernoulli Bandit 和 Contextual Bandit</strong>。在 Bernoulli
Bandit 中，认为每个 arm
的优劣（即当前试验是否产生收益）是服从伯努利分布的，而分布的参数可以通过历史收益状况求解；而在
Contextual Bandit中，没有直接定义出一个概率分布来描述每个 arm 的优劣,
而是假设了 arm 的优劣和描述 arm 的特征组成的向量 <span
class="math inline">\(x\)</span> 存在一个线性关系：<span
class="math inline">\(x^T \theta\)</span>，参数 <span
class="math inline">\(\theta\)</span> 可通过历史样本求解和更新。</p>
<p>UCB 方法和 Thompson sampling 方法均可解决这两类问题，UCB 解决
Bernoulli Bandit 的方法有 UCB1，UCB2 等，解决 Contextual Bandit 的方法有
LinUCB 等；而 Thomson Sampling 解决 Bernoulli Bandit 时采用了 Bernoulli
分布和 Beta 分布，解决 Contextual Bandit
时采用了两个正态分布。后面会详细介绍这些方法。</p>
<h2 id="随机方法">随机方法</h2>
<h3 id="epsilon-greedy"><span
class="math inline">\(\epsilon\)</span>-greedy</h3>
<p><span class="math inline">\(\epsilon\)</span>-greedy
是一种最简单的随机方法，原理很简单：每次决策时，以 1 - <span
class="math inline">\(\epsilon\)</span> 的概率选择最优的策略，以 <span
class="math inline">\(\epsilon\)</span> 的概率随机选择任意一个策略;
并且在每次做出决策获取到真实的 reward
后更新每个决策的收益情况（用于选择最优策略）。伪代码实现可参考 <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32335683">Multi-Armed Bandit:
epsilon-greedy</a></p>
<p><span class="math inline">\(\epsilon\)</span>-greedy
存在着以下几个比较显著的问题</p>
<ol type="1">
<li><span class="math inline">\(\epsilon\)</span>
是个超参数，设置过大会导致决策随机性过大，设置过小则会导致探索性不足</li>
<li><span class="math inline">\(\epsilon\)</span>-greedy
策略运行一段时间后，对各个 arm
的收益情况有所了解，但没有利用这些信息，仍然不做任何区分地随机
exploration（会选择到明显较差的item）</li>
<li><span class="math inline">\(\epsilon\)</span>-greedy
策略运行一段时间后，但仍然花费固定的精力去
exploration，浪费了本应该更多进行 exploitation 机会</li>
</ol>
<p>针对第 2 个问题，可以在 <span
class="math inline">\(\epsilon\)</span>-greedy
策略运行一段时间后，选择出收益最高的前 <span
class="math inline">\(n\)</span> 个 arm，然后 exploration 时从这 <span
class="math inline">\(n\)</span> 个 arm 中随机选择。</p>
<p>针对第 3 个问题，可以设置进行 exploration 的概率 <span
class="math inline">\(\epsilon\)</span>
随着策略进行的次数而逐渐下降，比如说可以取如下的对数形式, <span
class="math inline">\(m\)</span> 表示目前进行了 <span
class="math inline">\(m\)</span> 次的决策</p>
<p><span class="math display">\[\epsilon = \frac{1}{1 +
\log(m+1)}\]</span></p>
<h3 id="softmax">Softmax</h3>
<p>通过 Softmax 进行的 Exploration 也称为 Boltzmann
Exploration，这个方法通过一个温度参数来控制 exploration 和 exploitation
的比例，假设各个 arm 的历史收益为 <span
class="math inline">\(\mu\_0\)</span>, <span
class="math inline">\(\mu\_1\)</span>, ......, <span
class="math inline">\(\mu\_n\)</span>, 温度参数记为 <span
class="math inline">\(T\)</span>，则选择某个 arm 时参考的指标为</p>
<p><span class="math display">\[p\_i =
\frac{e^{\mu\_i/T}}{\sum\_{j=0}^{n} e^{\mu\_j/T}}(i=0, 1,....,
n)\]</span></p>
<p>当温度参数 <span class="math inline">\(T=1\)</span>,
上面的方法就是纯粹的 exploitation；而当 <span class="math inline">\(T
\to \infty\)</span> 时，上面的方法就是纯粹的 exploration，因此，可以控制
<span class="math inline">\(T\)</span> 的范围来控制 exploration 和
exploitation 的比例。某些文献也会将 <span
class="math inline">\(1/T\)</span> 称为学习率。一个很直观的想法就是让
<span class="math inline">\(T\)</span>
随着策略运行次数的增加而下降，这样便可让策略从偏向 exploration 转为偏向
exploitation。</p>
<p>但是，这篇 paper <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.10257.pdf">Boltzmann Exploration Done
Right</a> 证明了单调的学习率（即<span
class="math inline">\(1/T\)</span>）会导致收敛到局部最优，并提出了一种针对不同的
arm 采用不同的学习率的方法，但是形式已经不是上面的 softmax
形式了。文章涉及的证明和公式符号较多，这里不再展开阐述，感兴趣读者可自行参考。</p>
<h2 id="ucb-方法">UCB 方法</h2>
<p>假如能够对每个 arm
都进行足够多次的试验，根据大数定律，次数越多，这些试验结果统计得到的收益便会约接近各个
arm 真实的收益。然而在实际中，只能对各个 arm
进行有限次的试验，因此这会导致根据统计得到的收益跟真实的收益存在一个误差，<strong>UCB
的核心就在于如果预估这个误差(也就是 UCB 中的 B(bound))，然后将 arm
统计的收益加上其通过 UCB 方法计算出来的 bound
进行排序，选择最高的那个。</strong></p>
<h3 id="ucb1">UCB1</h3>
<p>UCB1 方法的理论基础是 <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding's
inequality</a>，该不等式的定义如下</p>
<blockquote>
<p>假设 <span class="math inline">\(X\_1, X\_2...X\_n\)</span>
是同一个分布产生的 <span class="math inline">\(n\)</span>
个独立变量，其均值为 <span class="math inline">\(\overline{X} =
\frac{1}{n}\sum\_{i=1}^n X\_i\)</span>, 则如下公式成立 <span
class="math display">\[p(|E[X] - \overline{X}| \le \delta) \ge 1 -
2e^{-2n\delta^2}\]</span></p>
</blockquote>
<p>更直观地说，该不等式表明了 <strong><span
class="math inline">\(n\)</span>
个独立同分布的变量的均值与该变量的真实期望的误差小于某个预设的阈值 <span
class="math inline">\(u\)</span> 会以概率 <span class="math inline">\(1
- e^{-2nu^2}\)</span> 恒成立</strong>。</p>
<p>回到我们的问题，可以将 <span class="math inline">\(X\_1,
X\_2...X\_n\)</span> 看做某个 arm 在 <span
class="math inline">\(n\)</span>
次试验中获得的收益，则通过上面的式子可以设定一个 <span
class="math inline">\(\delta\)</span> 使得公式成立, 然后用<span
class="math inline">\(\overline{X} + \delta\)</span> 来近似真实的收益
<span class="math inline">\(E(X)\)</span>；理论上也可用 <span
class="math inline">\(\overline{X} - \delta\)</span>，但是 UCB
方法会用上界，这也是 UCB 中 U(upper) 的含义。<strong>那么现在的问题便是
<span class="math inline">\(\delta\)</span> 该选多大了？</strong></p>
<p>UCB1 方法中将 <span class="math inline">\(\delta\)</span>
设为如下公式，公式中的 <span class="math inline">\(N\)</span>
表示目前所有 arm 试验的总次数，<span class="math inline">\(n\)</span>
表示某个 arm 的实验次数</p>
<p><span class="math display">\[ \delta =
\sqrt{\frac{2\ln{N}}{n}}\]</span></p>
<p>直观地看上面定义的 <span class="math inline">\(\delta\)</span>,
分子的 <span class="math inline">\(N\)</span> 对所有的 arm
是相同的，分母的 <span class="math inline">\(n\)</span> 则表示某个 arm
目前为止试验的次数，如果这个值越小，那么 <span
class="math inline">\(\delta\)</span> 便越大，相当于
exploration；而当各个 arm 的 <span class="math inline">\(n\)</span>
相同时，实际上就是在比较各个 arm 的历史收益情况了。</p>
<p>UCB1 方法的流程如下，该图摘自 <a
target="_blank" rel="noopener" href="https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/">Optimism
in the Face of Uncertainty: the UCB1 Algorithm</a></p>
<figure>
<img src="https://wulc.me/imgs/image_1d06kc0hg38a172e1hs85rg1q8416.png"
alt="UCB1" />
<figcaption aria-hidden="true">UCB1</figcaption>
</figure>
<p>可以看到 UCB1 的 bound 完全是由 Hoeffding's inequality
推导出来的，而除了 Hoeffding's inequality，其他的一些 inequality
也能够推导出相应的 bound，<a
target="_blank" rel="noopener" href="http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf">Reinforcement
Learning: Exploration vs Exploitation</a> 中就提到了一些其他的
inequality</p>
<ul>
<li>Bernstein’s inequality</li>
<li>Empirical Bernstein’s inequality</li>
<li>Chernoff inequality</li>
<li>Azuma’s inequality</li>
<li>.......</li>
</ul>
<h3 id="ucb2">UCB2</h3>
<p>从名字上基本就可以猜出 UCB2 是 UCB1 的改进，改进的地方是降低了 UCB1
的 regret 的上界，regret
指的是每次能获得的最大的收益与实际获得的收益的差距，这部分涉及到较多的数学证明，这里略去这部分，详细可参考
<a
target="_blank" rel="noopener" href="https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf">Finite-time
Analysis of the Multiarmed Bandit Problem</a>。UCB2
算法的流程如下，图片同样摘自 <a
target="_blank" rel="noopener" href="https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf">Finite-time
Analysis of the Multiarmed Bandit Problem</a></p>
<figure>
<img src="https://wulc.me/imgs/image_1d0bpj8ghom61ki0e8q1no31urc9.png"
alt="UCB2" />
<figcaption aria-hidden="true">UCB2</figcaption>
</figure>
<p>从流程图上可知，UCB2 与 UCB1 相似，也是为每个 arm 计算一个
bound，然后根据 arm 的历史收益和 bound 选出 arm ，只是对这个 arm
不止试验一次，而是试验 <span class="math inline">\(\tau(r\_j+1) -
\tau(r\_j)\)</span> 次。上面的 <span class="math inline">\(a\_{n,
r\_j}\)</span> 和 <span class="math inline">\(\tau(r)\)</span>
定义如下，由于 <span class="math inline">\(\tau(r)\)</span>
要为整数，因此取了上界</p>
<p><span class="math display">\[a\_{n,r} =
\sqrt{\frac{(1+\alpha)\ln(ne/\tau(r))}{2\tau(r)}}\]</span></p>
<p><span class="math display">\[\tau(r) = \lceil
(1+\alpha)^r\rceil\]</span></p>
<p>上面式子中的 <span class="math inline">\(\alpha\)</span>
是个超参数，根据上面给出的论文中的实验结果(如下图所示)，这个值不能取得太大，论文建议值是
0.0001</p>
<figure>
<img src="https://wulc.me/imgs/image_1d0c0s8c31qaqi6v12g51g1iprj2m.png"
alt="alpha" />
<figcaption aria-hidden="true">alpha</figcaption>
</figure>
<h3 id="linucb">LinUCB</h3>
<p>上面的 UCB1 和 UCB2 算法都是解决 Bernoulli Bandit
问题的，也就是假设每个 arm
的优劣是服从伯努利分布，而根据历史记录计算出的 <span
class="math inline">\(\overline
{x}\_j\)</span>（获得收益的试验次数和总试验次数的比值）其实就是伯努利分布的参数。</p>
<p>这样基于统计的方法很简单，但是问题也比较显著，因为 arm
的收益会跟多个因素有关（比如说某个 arm
在早上选择时没有收益，但是晚上就有了），利用这些信息可以预估得更准确；而基于统计的方法则忽略了这一点。</p>
<p>区别于 Bernoulli Bandit，这类利用了上下文信息的问题就是上面提到的
Contextual Bandit 问题，而 LinUCB 就是要解决这个问题的。 LinUCB
中没有直接定义出一个概率分布来描述每个 arm 的历史收益状况, 而是假设了
arm 的优劣和描述 arm 的特征的向量 <span class="math inline">\(x\)</span>
存在一个线性关系： <span class="math inline">\(x^T \theta\)</span></p>
<p>实际上这是一个经典的 Linear Regression(收益不在局限于 0 和 1)
问题，<span class="math inline">\(x\)</span> 是 arm
的特征组成的向量(需要根据具体的问题选择特征), <span
class="math inline">\(\theta\)</span>
则是模型的参数，每一次的试验就是一条样本，label 为具体的收益。</p>
<p>通过历史样本可以求解出 <span class="math inline">\(\theta\)</span>,
则在每次选择选择 arm 时，LinUCB 会用 <span class="math inline">\(x^T
\theta\)</span> 来替换掉 UCB1或UCB2 中的 <span
class="math inline">\(\overline {x}\_j\)</span>，但是这还不是 LinUCB
的全部，作为 UCB 类方法，LinUCB 中还是有个 bound
的，因为毕竟从历史记录只能对 arm
进行有限次的试验，预估出来的收益情况与真实的还是存在差距的。</p>
<p>UCB1 中推到出 bound 的 Hoeffding's inequality 不能直接应用到 LinUCB
中，而关于 linear regression 的 bound 最早是在这篇论文 <a
target="_blank" rel="noopener" href="https://arxiv.org/ftp/arxiv/papers/1205/1205.2606.pdf">Exploring
compact reinforcement-learning representations with linear
regression</a> 中提出的，这里不详细展开具体的证明过程了。提出 LinUCB
的论文 <a target="_blank" rel="noopener" href="http://rob.schapire.net/papers/www10.pdf">A
Contextual-Bandit Approach to Personalized News Article
Recommendation</a> 直接采用了这一结论，其表达形式，如下所示</p>
<blockquote>
<p><span class="math display">\[p(|x\_j^T\theta\_j - E(r|x\_j)| \le
\alpha \sqrt{x\_j^T(D\_j^TD\_j+I)^{-1}x\_j}) \ge 1- \delta\]</span></p>
</blockquote>
<p>即对于某个 arm <span class="math inline">\(j\)</span>, 计算出来的
<span class="math inline">\(x\_j^T\theta\_j\)</span>
与实际的期望相差小于 $ $ 的概率要大于 <span class="math inline">\(1-
\delta\)</span>, 其中 <span class="math inline">\(D\_j\)</span> 是 arm
<span class="math inline">\(j\)</span>
前面每次被观察到的特征组成的矩阵，比如说 arm <span
class="math inline">\(j\)</span> 前面被观察了 <span
class="math inline">\(m\)</span> 次，且特征组合成的向量的维度为 <span
class="math inline">\(d\)</span>, 则 <span
class="math inline">\(D\_j\)</span> 的大小为 <span
class="math inline">\(m\)</span> X <span
class="math inline">\(d\)</span>, <span class="math inline">\(I\)</span>
为单位向量，而 <span class="math inline">\(\alpha = 1 +
\sqrt{\ln(2/\delta)/2}\)</span> ,因此<strong>，只要根据概率选定 <span
class="math inline">\(\delta\)</span>，则各个arm 的 bound 便可通过 <span
class="math inline">\(\alpha
\sqrt{x\_j^T(D\_j^TD\_j+I)^{-1}x\_j}\)</span> 求出</strong>。</p>
<p>因此 LinUCB 算法流程如下，算法同时包含了选择 arm 的方式和更新 linear
regress 模型。</p>
<figure>
<img src="https://wulc.me/imgs/image_1d0c9tmfu1jf9187016761golc433.png"
alt="LinUCB1" />
<figcaption aria-hidden="true">LinUCB1</figcaption>
</figure>
<p>上面的每个 arm 的 linear model
的参数都是独立的，论文针对这对点设计了这些 model
共享的一些参数，即将原来某个 arm <span class="math inline">\(j\)</span>
计算出来的 <span class="math inline">\(x\_j^T\theta\_j\)</span> 换成了
<span class="math inline">\(x\_j^T\theta\_j + z\_j^T \beta\)</span>,
<span class="math inline">\(\beta\)</span> 是各个模型共享的的参数，<span
class="math inline">\(z\_j^T\)</span>
则是这些参数对应的特征。对应这种情况，也有了论文的第二种算法</p>
<figure>
<img src="https://wulc.me/imgs/image_1d0cb0gnb11ifqtmc971pj3p3t3t.png"
alt="LinUCB2" />
<figcaption aria-hidden="true">LinUCB2</figcaption>
</figure>
<h2 id="thompson-sampling-方法">Thompson sampling 方法</h2>
<p>前面的 UCB 方法采用的都是频率学派的思想，即认为评判某个 arm
的优劣的指标是个定值，如果有无限次的试验，便可准确地计算出这个值，但是由于现实中只能进行有限次的试验，因此预估出来的值是有偏差的，需要通过另外计算一个
bound 来衡量这个误差。</p>
<p>而下面要介绍的 Thompson sampling
方法采用的则是贝叶斯学派的思想，即认为评判某个 arm
的优劣的指标不再是个定值，而是服从着某种假定的分布(先验)，通过观察到的历史记录去更新这个分布的参数(似然)，得到了新的分布参数(后验),
然后不断重复这个过程。当需要进行比较时，从分布中随机产生一个样本即可。</p>
<h3 id="bernoulli-bandit">Bernoulli Bandit</h3>
<p>前面提到，Bernoulli Bandit 假设某个 arm
的优劣服从伯努利分布，即每次是否获得收益的服从参数为 <span
class="math inline">\(\theta\)</span> 的伯努利分布</p>
<p>$p(reward | ) Bernoulli() $</p>
<p>UCB 方法中的 UCB1 和 UCB2 都是通过简单的历史统计得到 <span
class="math inline">\(\overline {x}\_j\)</span> 来表示 <span
class="math inline">\(\theta\)</span> 的，但是贝叶斯学派则认为 <span
class="math inline">\(\theta\)</span>
服从着一个特定的分布，根据贝叶斯公式有</p>
<p><span class="math display">\[p(\theta|reward) =
\frac{p(reward|\theta) p{(\theta)}}{p(reward)} \propto p(reward|\theta)
p{(\theta)} ＝Bernoulli(\theta) p(\theta)\]</span></p>
<p><span class="math inline">\(p(\theta|reward)\)</span>
表示根据观察到的实验收益情况更新的后验概率，且由于似然 <span
class="math inline">\(p(reward|\theta)\)</span>
为伯努利分布，为了保持共轭便于计算；先验分布 <span
class="math inline">\(p(\theta)\)</span> 选择为了 Beta 分布，即 <span
class="math inline">\(Beta(\alpha, \beta)\)</span>，而两个分布相乘 <span
class="math inline">\(Bernoulli(\theta)*Beta(\alpha, \beta)\)</span>
会得到一个新的 Beta 分布, 简单来说，就是</p>
<ul>
<li>当 <span class="math inline">\(Bernoulli(\theta)\)</span>
的结果为1，则会得到 <span class="math inline">\(Beta(\alpha + 1,
\beta)\)</span></li>
<li>当 <span class="math inline">\(Bernoulli(\theta)\)</span>
的结果为0，则会得到 <span class="math inline">\(Beta(\alpha, \beta +
1)\)</span></li>
</ul>
<p>因此，Bernoulli Bandit 中的 Thompson Sampling 步骤如下, 图片摘自 <a
target="_blank" rel="noopener" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">A Tutorial on
Thompson Sampling</a></p>
<figure>
<img
src="https://wulc.me/imgs/image_1d0e3h5bo11oc15e5191q1n4d1hhi4a.png"
alt="Thompson Sampling for Bernoulli Bandit" />
<figcaption aria-hidden="true">Thompson Sampling for Bernoulli
Bandit</figcaption>
</figure>
<h3 id="contextual-bandit">Contextual Bandit</h3>
<p>在 Bernoulli Bandit 中，我们假设 arm
是否获得收益是服从伯努利分布的，即 <span class="math inline">\(p(reward
| \theta) \sim Bernoulli(\theta)\)</span></p>
<p>而在 Contextual Bandit
中，我们假设获得的收益和特征向量存在一个线性关系, 即$reward = x^T
$，因此无法像前面一样直接通过似然 <span class="math inline">\(p(reward |
\theta) \sim Bernoulli(\theta)\)</span> 来更新 <span
class="math inline">\(\theta\)</span></p>
<p>但是根据前面解决 Bernoulli Bandit 的思路，<strong>只要定义 <span
class="math inline">\(p(reward|\theta)\)</span> 和 <span
class="math inline">\(p(\theta)\)</span>
为合适的共轭分布，那么就可以使用Thompson Sampling来解决Contextual
Bandit</strong>, 因为根据贝叶斯公式有如下的公式</p>
<p><span class="math display">\[p(\theta|reward) \propto
p(reward|\theta) p{(\theta)}\]</span></p>
<p>理论上，只要 <span class="math inline">\(p(reward|\theta)\)</span> 和
<span class="math inline">\(p(\theta)\)</span>
为共轭即可，但是考虑到假设的分布的合理性，参考这篇论文 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1209.3352.pdf">Thompson Sampling for
Contextual Bandits with Linear
Payoffs</a>，分别对这两个分布都采用正态分布的形式,
论文给出了较多的数学证明，这里略去证明，直接给出最终结论</p>
<p>对于 <span class="math inline">\(p(reward|\theta)\)</span> ，由于估计
reward 为 <span class="math inline">\(x^T\theta\)</span>
，因此假设真实的 reward 服从以 <span
class="math inline">\(x^T\theta\)</span> 为中心、 <span
class="math inline">\(v^2\)</span> 为标准差的正态分布，即 <span
class="math inline">\(p(reward|\theta) \sim \mathcal{N}(x^T\theta,
v^2)\)</span>, <span class="math inline">\(v\)</span>
的具体含义后面会给出</p>
<p>为了运算上的便利性， <span class="math inline">\(p(\theta)\)</span>
一般会选择与 <span class="math inline">\(p(reward|\theta)\)</span>
共轭的形式; 由于 <span class="math inline">\(p(reward|\theta)\)</span>
是正态分布，那么 <span class="math inline">\(p(\theta)\)</span>
也应该是正态分布（正态分布的共轭还是正态分布）</p>
<p>为了便于给出后验概率的形式，论文首先定义了如下的等式</p>
<p><span class="math display">\[B(t) = I\_d + X^TX\]</span></p>
<p><span class="math display">\[\mu(t) = B(t)^{-1}(\sum\_{\tau =
1}^{t-1} x\_{\tau}^Tr\_{\tau})\]</span></p>
<p>上面的 <span class="math inline">\(t\)</span> 表示某个 arm 第 <span
class="math inline">\(t\)</span> 次的试验，<span
class="math inline">\(d\)</span> 表示特征的维度，<span
class="math inline">\(X\)</span> 的含义与 LinUCB 中介绍的一致，即是这个
arm 前 <span class="math inline">\(t-1\)</span>
次的特征组成的矩阵，在这个例子中其维度大小为 (<span
class="math inline">\(t-1\)</span>) X <span
class="math inline">\(d\)</span>, <span
class="math inline">\(r\_{\tau}\)</span> 则表示前面 <span
class="math inline">\(t-1\)</span> 次试验中第 <span
class="math inline">\(\tau\)</span> 次获得的 reward。</p>
<p>则 <span class="math inline">\(p(\theta)\)</span> 在 <span
class="math inline">\(t\)</span> 时刻服从正态分布 <span
class="math inline">\(\mathcal{N}(\mu(t), v^2B(t)^{-1})\)</span>, 而
<span class="math inline">\(p(\theta)\)</span> 与 <span
class="math inline">\(p(reward|\theta)\)</span> 相乘计算出来的后验概率
<span class="math inline">\(p(\theta|reward) \sim \mathcal{N}(\mu(t+1),
v^2B(t+1)^{-1})\)</span></p>
<p>前面提到的 <span class="math inline">\(v\)</span> 的定义为 <span
class="math inline">\(v =
R\sqrt{9d\ln(\frac{t}{\delta})}\)</span>，该式子中的 <span
class="math inline">\(t\)</span> 和 <span
class="math inline">\(d\)</span> 的含义与上面的相同，而 <span
class="math inline">\(R\)</span> 和 <span
class="math inline">\(\delta\)</span>
是需要自定义的两个常数，均是用在两个不等式中约束 regret bound的，其中
<span class="math inline">\(R \ge 0\)</span> ,<span
class="math inline">\(0 \le \delta \lt 1\)</span>,
具体的等式可参考原始论文，这里不再展开。</p>
<p>当 <span class="math inline">\(t = 1\)</span>
也就是在最开始一次试验都没有的时候， <span
class="math inline">\(p(\theta)\)</span> 会被初始化为 <span
class="math inline">\(\mathcal{N}(0, v^2I\_d)\)</span>,</p>
<p>而每次选择 arm 的策略跟上面的 Bernoulli Bandit 类似: 每个 arm 的
<span class="math inline">\(p(reward|\theta)\)</span> 分布分别产生一个
<span class="math inline">\(\theta\)</span>, 然后选择 <span
class="math inline">\(x^t\theta\)</span> 最大的那个 arm 进行试验。</p>
<h2 id="小结">小结</h2>
<p>本文主要介绍了几种针对 EE 问题的策略，除了最简单的随机方法，UCB 和
Thompson Sampling 是这个问题两种比较典型的方法，两个方法的思想均是为每个
arm 计算出一个收益值，然后根据这个值进行 ranking 然后选择值最大的
arm。</p>
<p>UCB 采用的是频率学派的思想，计算的收益值是历史收益加上一个
bound，可以认为历史收益是 Exploitation，而 bound 则是 Exploration；而
Thompson Sampling 方法中每个 arm 的收益值会从一个分布中产生，没有明显的
Exploitation 和 Exploration，但也可以认为从分布中较大概率产生的值是
Exploitation ，而较小概率产生的值是
Exploration。另外，文中重点是介绍这些方法的具体做法，更多关于这些方法的理论基础和数学推导可参考下面这些文献。</p>
<hr />
<p>参考：</p>
<ul>
<li><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32311522">开栏：智能决策系列</a></li>
<li><a
target="_blank" rel="noopener" href="https://x-algo.cn/index.php/2016/12/15/ee-problem-and-bandit-algorithm-for-recommender-systems/#">推荐系统的EE问题及Bandit算法</a></li>
<li><a
target="_blank" rel="noopener" href="http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf">Reinforcement
Learning: Exploration vs Exploitation</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.cs.cornell.edu/courses/cs6840/2017sp/lecnotes/6840sp17R_Kleinberg.pdf">Multi-Armed
Bandits and the Gittins Index</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.10257.pdf">Boltzmann Exploration
Done Right</a></li>
<li><a
target="_blank" rel="noopener" href="https://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/">Optimism
in the Face of Uncertainty: the UCB1 Algorithm</a></li>
<li><a
target="_blank" rel="noopener" href="https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf">Finite-time
Analysis of the Multiarmed Bandit Problem</a></li>
<li><a
target="_blank" rel="noopener" href="https://arxiv.org/ftp/arxiv/papers/1205/1205.2606.pdf">Exploring
compact reinforcement-learning representations with linear
regression</a></li>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf">A
Tutorial on Thompson Sampling</a></li>
<li><a target="_blank" rel="noopener" href="http://rob.schapire.net/papers/www10.pdf">A Contextual-Ba
ndit Approach to Personalized News Article Recommendation</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/" rel="tag"># 计算广告</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/12/28/LeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(739,901,907)-%E7%BA%BF%E6%80%A7%E6%97%B6%E9%97%B4%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E5%90%84%E4%B8%AA%E5%85%83%E7%B4%A0%E4%BD%9C%E4%B8%BA%E6%9C%80%E5%80%BC%E7%9A%84%E6%9C%80%E5%A4%A7%E8%8C%83%E5%9B%B4/" rel="prev" title="LeetCode 解题报告(739,901,907)-线性时间寻找数组中各个元素作为最值的最大范围">
      <i class="fa fa-chevron-left"></i> LeetCode 解题报告(739,901,907)-线性时间寻找数组中各个元素作为最值的最大范围
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/01/14/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(1)-MapReduce/" rel="next" title="分布式系统笔记(1)-MapReduce">
      分布式系统笔记(1)-MapReduce <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#mab-%E5%BB%BA%E6%A8%A1"><span class="nav-number">1.</span> <span class="nav-text">MAB 建模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">随机方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#epsilon-greedy"><span class="nav-number">2.1.</span> <span class="nav-text">\(\epsilon\)-greedy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax"><span class="nav-number">2.2.</span> <span class="nav-text">Softmax</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ucb-%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">UCB 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ucb1"><span class="nav-number">3.1.</span> <span class="nav-text">UCB1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ucb2"><span class="nav-number">3.2.</span> <span class="nav-text">UCB2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linucb"><span class="nav-number">3.3.</span> <span class="nav-text">LinUCB</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#thompson-sampling-%E6%96%B9%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">Thompson sampling 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bernoulli-bandit"><span class="nav-number">4.1.</span> <span class="nav-text">Bernoulli Bandit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#contextual-bandit"><span class="nav-number">4.2.</span> <span class="nav-text">Contextual Bandit</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="良超"
      src="/files/profile.jpg">
  <p class="site-author-name" itemprop="name">良超</p>
  <div class="site-description" itemprop="description">盈亏同源，享受生活的随机性</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">248</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/WuLC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuLC" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/wuliangchao/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;wuliangchao&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangchaowu5@gmail.com" title="E-Mail → mailto:liangchaowu5@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">良超</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
