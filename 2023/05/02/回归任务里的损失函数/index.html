<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

<script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
<script>LA.init({id: "JmI6QmP3fMkLet6B",ck: "JmI6QmP3fMkLet6B"})</script>

  <link rel="apple-touch-icon" sizes="180x180" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/imgs/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/imgs/favicon.ico">
  <link rel="mask-icon" href="/imgs/favicon.ico" color="#222">
  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wulc.me","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="在搜广推相关业务中，除了 ctr、cvr 这类常规的二分类任务，还存在着预估 stay_duration、LTV、ECPM、GMV 等一系列回归任务 ctr、cvr 这类二分类任务常用的损失函数是交叉熵损失，基本假设是事件服从伯努利分布，最终学习的输出是正样本的比例，而回归任务中存在着非常多种的损失函数可选，如 mse、mae、huber loss、log-normal、weighted logi">
<meta property="og:type" content="article">
<meta property="og:title" content="回归任务里的损失函数">
<meta property="og:url" content="https://wulc.me/2023/05/02/%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="在搜广推相关业务中，除了 ctr、cvr 这类常规的二分类任务，还存在着预估 stay_duration、LTV、ECPM、GMV 等一系列回归任务 ctr、cvr 这类二分类任务常用的损失函数是交叉熵损失，基本假设是事件服从伯努利分布，最终学习的输出是正样本的比例，而回归任务中存在着非常多种的损失函数可选，如 mse、mae、huber loss、log-normal、weighted logi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wulc.me/imgs/mse_derive1.jpg">
<meta property="og:image" content="https://wulc.me/imgs/mse_derive2.jpg">
<meta property="og:image" content="https://wulc.me/imgs/HuberLoss.jpg">
<meta property="og:image" content="https://wulc.me/imgs/LTV_Distribution.jpg">
<meta property="og:image" content="https://wulc.me/imgs/ZILN_model.jpg">
<meta property="og:image" content="https://wulc.me/imgs/lognormalVSmse.jpg">
<meta property="og:image" content="https://wulc.me/imgs/WeightedLogisticRegression.jpg">
<meta property="og:image" content="https://wulc.me/imgs/OrdinalRegressionLoss.jpg">
<meta property="article:published_time" content="2023-05-02T13:37:39.000Z">
<meta property="article:modified_time" content="2023-07-16T15:28:28.218Z">
<meta property="article:author" content="良超">
<meta property="article:tag" content="计算广告">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wulc.me/imgs/mse_derive1.jpg">


<link rel="canonical" href="https://wulc.me/2023/05/02/%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://wulc.me/2023/05/02/%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","path":"2023/05/02/回归任务里的损失函数/","title":"回归任务里的损失函数"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>回归任务里的损失函数 | 吴良超的学习笔记</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">吴良超的学习笔记</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#msemaehuber"><span class="nav-number">1.</span> <span class="nav-text">MSE&#x2F;MAE&#x2F;Huber</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#zilnlog-normal"><span class="nav-number">2.</span> <span class="nav-text">ZILN(Log-normal)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#weighted-logistics-regression"><span class="nav-number">3.</span> <span class="nav-text">Weighted Logistics
Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bucketing-with-softmax"><span class="nav-number">4.</span> <span class="nav-text">Bucketing With Softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ordinal-regression"><span class="nav-number">5.</span> <span class="nav-text">Ordinal Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">小结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="良超"
      src="/files/profile.jpg">
  <p class="site-author-name" itemprop="name">良超</p>
  <div class="site-description" itemprop="description">盈亏同源，享受生活的随机性</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">253</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/WuLC" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WuLC" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liangchaowu5@gmail.com" title="E-Mail → mailto:liangchaowu5@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wulc.me/2023/05/02/%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E9%87%8C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/files/profile.jpg">
      <meta itemprop="name" content="良超">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="吴良超的学习笔记">
      <meta itemprop="description" content="盈亏同源，享受生活的随机性">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="回归任务里的损失函数 | 吴良超的学习笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          回归任务里的损失函数
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-05-02 21:37:39" itemprop="dateCreated datePublished" datetime="2023-05-02T21:37:39+08:00">2023-05-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-tags"></i>
      </span>
      <span class="post-meta-item-text">标签</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">计算广告</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>在搜广推相关业务中，除了 ctr、cvr 这类常规的二分类任务，还存在着预估
stay_duration、LTV、ECPM、GMV 等一系列回归任务</p>
<p>ctr、cvr
这类二分类任务常用的损失函数是交叉熵损失，基本假设是事件服从伯努利分布，最终学习的输出是正样本的比例，而回归任务中存在着非常多种的损失函数可选，如
mse、mae、huber loss、log-normal、weighted logistics regression、softmax
等</p>
<p>每种损失函数都有其假设和适用范围，如果真实 label
分布与假设差异较大，容易导致结果不佳，因此，本文会重点关注这些常见 loss
的推导过程以及假设</p>
<span id="more"></span>
<h2 id="msemaehuber">MSE/MAE/Huber</h2>
<p>这三个 loss 是回归任务中最直观也是最常见的
loss，前两者都假设了误差服从某种特定分布，然后通过 <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE</a>
最终的 loss 形式，huber 则是前两种 loss 的混合版本</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mean_squared_error">MSE</a>
其假设是<strong>预估值和真实值的误差服从标准高斯分布</strong>，然后写出误差的似然(likelihood)函数，并通过
MLE 推导出来最终的 loss 的形式，其推导过程如下图所示（来自 <a
target="_blank" rel="noopener" href="https://cs229.stanford.edu/">CS229</a> 的教材）</p>
<p>根据误差服从均值为 0 的高斯分布可以写出如下的似然函数</p>
<p><img data-src="https://wulc.me/imgs/mse_derive1.jpg" height="50%" width="50%" alt="mse 推导"></p>
<p>然后根据 IID 的假设和 MLE，可以写出最终的损失函数</p>
<p><img data-src="https://wulc.me/imgs/mse_derive2.jpg" height="50%" width="50%" alt="mse 推导"></p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mean_absolute_error">MAE</a>
的推导跟 MSE 很类似，只是假设误差服从<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83#:~:text=%E5%9C%A8%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E7%BB%9F%E8%AE%A1,%E5%88%86%E5%B8%83%28Double%20exponential%20distribution%29%E3%80%82">拉普拉斯分布</a>，把拉普拉斯的概率密度函数替换成上面推导过程中的高斯分布，便能得到了最终
MAE 的损失函数形式</p>
<p>从 loss 函数形式，可以直观看到，对于第 <span
class="math inline">\(i\)</span> 个样本</p>
<ul>
<li><strong>MSE 反向传播的梯度大小为 <span
class="math inline">\(-(y^{(i)} -
\theta^{T}x^{(i)})x^{(i)}\)</span></strong></li>
<li><strong>MAE 反向传播的梯度为 <span class="math inline">\(-
\frac{y^{(i)} - \theta^{T}x^{(i)}}{|y^{(i)} -
\theta^{T}x^{(i)}|}x^{(i)}\)</span></strong> (其实就是 ±<span
class="math inline">\(x^{(i)}\)</span>, 绝对值的求导可参考 <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/491935204">绝对值的导数</a>)</li>
</ul>
<p>从回传的梯度可知，<strong>MSE
容易收到异常值的影响</strong>，比如说有个异常样本的 label
非常大，则算梯度时 <span class="math inline">\(y^{(i)} -
\theta^{T}x^{(i)}\)</span> 的值也会非常大，容易把样本带偏，而 MAE
受到这一点影响会更小</p>
<p>所以 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Huber_loss">Huber
Loss</a> 也是考虑到了这点，对于 label 较小的样本采用 MSE，label
比较大的样本，则采用 MAE, 整个损失的形式如下图所示</p>
<p><img data-src="https://wulc.me/imgs/HuberLoss.jpg" height="50%" width="50%" alt="Huber Loss"></p>
<h2 id="zilnlog-normal">ZILN(Log-normal)</h2>
<p>这是 google 一篇预估 LTV 的 paper 中提出来的 loss，<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.07753">A Deep Probabilistic Model for
Customer Lifetime Value Prediction</a></p>
<p>现实任务的数据往往是长尾切稀疏的，拿 paper 里的 LTV
任务为例，会有非常多的 0 值，也存在极端高的值，如下图所示是paper
里展示的 LTV 的分布（这里的 LTV 含义是首次购买后产生的价值，0
值代表很多客户只购买了一次）</p>
<p><img data-src="https://wulc.me/imgs/LTV_Distribution.jpg" height="50%" width="50%" alt="LTV 分布"></p>
<p>paper 提到直接用 MSE 会有如下问题，其实也是上面提到了 MSE
存在的问题</p>
<blockquote>
<p>MSE loss does not accommodate the <strong>significant fraction of
zero value LTV</strong> from one-time purchasers and can be sensitive to
extremely large LTV’s from top spenders</p>
</blockquote>
<p>所以 paper 提出了 ZILN(Zero-Inflated LogNormal) 的这个 loss，loss
形式如下图所示（<strong>注意这里的 <span
class="math inline">\(x\)</span> 是 label</strong>)</p>
<p><span class="math display">\[L_{ZILN}(x;p,\mu,\sigma)=
-\mathbf{1}_{x=0} \log(1-p) - \mathbf{1}_{x&gt;0}(\log p -
L_{Lognormal}(x;\mu,\sigma))\]</span></p>
<p>这里的 loss 其实有 2 项，对应着 paper 里建模 LTV 分成了 2
个任务，一个是预估购买概率（上面公式里的 <span
class="math inline">\(p\)</span>），另一个是预估购买金额,
，第一个任务是常规的分类的 cross entropy
损失；这种思想其实也比较常见，即引入中间信号建模，这样往往能让总体的效果更好，比如说把
ctr、cvr 分开建模，把 send2click 拆成 send2show 和 show2click
各自建模等</p>
<p>这里重点讲第二个任务，即预估金额的损失 <span
class="math inline">\(L_{Lognormal}(x;\mu,\sigma)\)</span>， 这个 loss
的形式如下图所示，其实就是对 <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Log-normal_distribution#Alternative_parameterizations">log-normal
分布</a>的概率密度函数取了 log 变换得到的，其隐含的<strong>假设是LTV服从
log-normal 分布</strong>，同时将 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 参数化，即用一个 DNN
来预估（注意这里的 <span class="math inline">\(x\)</span> 仍然是
label)</p>
<p><span class="math display">\[L_{Lognormal}(x;\mu,\sigma)=\log (x
\sigma \sqrt{2\pi})+\frac{(\log x - \mu)^2}{2 \sigma^2}\]</span></p>
<p>最终的模型会输出 3 个预估值 <span class="math inline">\(p\)</span>,
<span class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>，如下图所示</p>
<p><img data-src="https://wulc.me/imgs/ZILN_model.jpg" height="50%" width="50%" alt="ZILN 的预估值"></p>
<p>log-normal 形式的推导可以参考前面的 wiki 链接或者<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/490650039">对数正态分布（Log-Normal
Distribution）</a>，简单来看，当一个变量 <span
class="math inline">\(X\)</span> 服从 log-normal 分布时，其对数 <span
class="math inline">\(\ln X\)</span> 服从正态分布，反之亦然</p>
<p>相比于 MSE，log-normal 在预估值异常大时，loss
也不会非常大，一定程度上缓解了前面提到的 MSE 的问题</p>
<p><img data-src="https://wulc.me/imgs/lognormalVSmse.jpg" height="50%" width="50%" alt="Log-normal v.s MSE"></p>
<h2 id="weighted-logistics-regression">Weighted Logistics
Regression</h2>
<p>这个 loss 在 Google 这篇 2016 的神文里被应用来预估用户的观看时长，<a
target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf">Deep
Neural Networks for YouTube
Recommendations</a>；在实际业界，也有非常多的基于这个思想的 variant
和落地，相当于把回归任务转到了分类任务</p>
<p>从名字大概能猜到 Loss 的形式了，在一个分类的 loss 上做
reweight，具体的做法是使用 cross entropy
这个损失函数，然后对于<strong>正样本（在建模观看时长中是点击的样本），使用具体的观看时长
<span class="math inline">\(T\)</span> 来做
reweihgt</strong>，负样本不变</p>
<p>做了 reweight
后，相当于改变了原来的样本中的正负样本的比例（这里把观看的作为正样本，不观看/点击
作为负样本），我们知道 cross entropy 最终预估的概率值是正样本的比例，而
<strong>reweight 后，对于 label 为<span class="math inline">\(T\)</span>
的样本，相当于在分类任务中变为了 <span class="math inline">\(T\)</span>
个正样本</strong></p>
<p>则总体样本的正样本的比例变成了 <span
class="math inline">\(\frac{\sum_{i=1}^k T_{i}}{N + \sum_{i=1}^k T_{i}
-k}\)</span>, 其中 <span class="math inline">\(N\)</span>
是总样本数，<span class="math inline">\(T_i\)</span>
则是每个正样本的观看时长, <span class="math inline">\(k\)</span>
是正样本的数量（这里的推导过程跟 paper
里推导过程不太一样，但原理和结果是一样的，会更直观一点）</p>
<p>而根据 Logistics Regression 的预估值为正样本的比例可知</p>
<p><span
class="math display">\[\frac{1}{1+e^{-logit}}=\frac{\sum_{i=1}^k
T_{i}}{N + \sum_{i=1}^k T_{i} -k}\]</span></p>
<p>由于正样本的数量往往非常少，因此 paper 把 <span
class="math inline">\(k\)</span> 省略掉，令 <span
class="math inline">\(P=\sum_{i=1}^k T_{i}\)</span>,
则上面的式子变为了</p>
<p><span class="math display">\[\frac{1}{1+e^{-logit}}=\frac{P}{N +
P}\]</span></p>
<p>左边分子分母同时乘上 <span class="math inline">\(e^{logit}\)</span>,
右边分子分母同时除以 <span class="math inline">\(N\)</span>,
最终可以推导出 <span class="math inline">\(\frac{P}{N} =
e^{logit}\)</span>, 由于这里的 <span class="math inline">\(P\)</span>
是取了 sum 的，平均到每个样本上，最终 <strong>serving
时预估的时长的值就是 <span
class="math inline">\(e^{logit}\)</span></strong></p>
<p>则在训练时使用的损失函数还是 cross entropy，同时对正样本做
reweight，serving 时得到 logit 后，取 <span
class="math inline">\(e^{logit}\)</span> 作为最终的预估值，paper
里这张图很好地展示了这个过程</p>
<p><img data-src="https://wulc.me/imgs/WeightedLogisticRegression.jpg" height="50%" width="50%" alt="Log-normal v.s MSE"></p>
<p>上面的推导过程中忽略了原来样本中正样本的数量 <span
class="math inline">\(k\)</span>，从严格意义来讲，这会导致预估值有偏，所以最简单的方法是<strong>对于
label 为<span class="math inline">\(T\)</span> 的样本，把它当做 <span
class="math inline">\(T\)</span> 个正样本和 1
个负样本</strong>，相比于原来的做法，为原来每个正样本多增加了一个负样本</p>
<p>则原来的公式会变成如下所示，等式右边的分母里，第一个 <span
class="math inline">\(k\)</span> 表示原来的正样本的数量，第二个 <span
class="math inline">\(k\)</span> 则表示新增的负样本的数量，这样就不用有
“正样本的数量往往非常少可省略”的假设了</p>
<p><span
class="math display">\[\frac{1}{1+e^{-logit}}=\frac{\sum_{i=1}^k
T_{i}}{N + \sum_{i=1}^k T_{i} - k + k}\]</span></p>
<p>回到这个 loss 的假设，我们知道 cross entropy 的假设是 label
服从参数为 <span class="math inline">\(p\)</span> 的伯努利分布（这里的 p
为成功的概率），然后通过 MLE 可求得 cross entropy 的形式</p>
<p>而在 cross entropy 基础上做了如上的 reweight
后，相当于是<strong>假设了 label 服从了参数为 <span
class="math inline">\(p\)</span> 的<a
target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%B9%BE%E4%BD%95%E5%88%86%E4%BD%88">几何分布</a></strong>（不是非常严谨，因为这里的
<span class="math inline">\(p\)</span> 是失败的概率），reweight
时使用的值（即 label <span
class="math inline">\(T\)</span>），就是几何分布的 pdf 取 log
后提到前面的值，表示连续失败 <span class="math inline">\(T\)</span>
次（这里跟原来的几何分布的物理含义刚好相反）</p>
<h2 id="bucketing-with-softmax">Bucketing With Softmax</h2>
<p>在回归中，还有另一种套路就是分桶+softmax，其思路也很直观，就是对
label 的值域进行分桶，然后根据每个样本的 label
把样本分到某个桶里，训练时转为一个多分类问题，通过 softmax
损失函数进行训练</p>
<p>serving 时利用 softmax
预估的概率分布，对每个桶做加权求和,如下式所示，<span
class="math inline">\(n\)</span> 表示有 <span
class="math inline">\(n\)</span> 个桶，<span
class="math inline">\(p_i\)</span> 表示每个桶的概率，<span
class="math inline">\(v_i\)</span>
则表示每个桶的值（往往会取桶的中值）</p>
<p><span class="math display">\[pred = \sum_{i=1}^{n} p_i
v_i\]</span></p>
<p>这个方法的关键在于如何分桶，包括分桶的数量和每个桶的大小，这两个变量对最终的效果影响会比较大，分桶过多，容易导致每个桶的样本过于稀疏，而分桶过少，预估值有没有区分性</p>
<p>一般的做法是人工根据 label
的后验分布进行划分，且由于实际中的数据往往会比较长尾（即 label
较小的样本会比较多），所以会在 label
较小的时分桶数量较多，桶的间隔也会越小，目的还是让每个分桶的样本尽量均衡；但桶的数量和大小，是需要不断调整的超参</p>
<p>除了直接使用桶的均值作为最终的加权求和项，还可以在每个桶都套上一个前面提到的损失如
mse 等，不过这样就相当于是分区间的 multi-task 建模了，或者说是 ensemble
中的 stacking 方法了</p>
<p>在这类方法中，另一个常见的操作就是 ** label
smoothing**，这个方法的出发点是对于原来的方法丢掉了 label
之间的大小关系，比如说把 label=50 的样本分到[0,10] 的桶和 [51, 100]
的桶的损失是没有区分性的，因此自然的一个想法是让原来 one-hot 的 label
变得更加 smooth</p>
<p>具体的思想是在训练时对 label
做变换，让其变为类似高斯分布或拉普拉斯分布的形式，比如说原来的 label 是
[0, 0, 0, 0, 1, 0, 0, 0], 对其进行 smooth 后会变为 [0, 0, 0.01, 0.03,
0.9, 0.03, 0.02, 0.01], 而这里变换的方法也相当于一个超参了</p>
<p>总的来说，这个方法涉及到比较多的先验知识，包括如何分桶，如何选择
label smoothing 的函数；但是避免了对 label
的先验假设，理论上适用于任意的回归任务，但是需要定期
review，防止总体数据有变，先验假设失效</p>
<h2 id="ordinal-regression">Ordinal Regression</h2>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ordinal_regression">Ordinal
regression</a>
是一种适用在关注不同值的序，但不怎么关注具体的值的场景，套用 wiki
的话是这么说的</p>
<blockquote>
<p>variable whose value exists on an arbitrary scale where <strong>only
the relative ordering between different values is significant</strong>.
It can be considered an intermediate problem between regression and
classification</p>
</blockquote>
<p>常见的任务比如说评级，例如针对图片、视频的色情程度做出评级，不直接采用分类的原因，跟上面提到的
bucketing 方法里的 label smoothing 一样，分到不同的桶里没有区分性，</p>
<p>具体的 loss 形式也是基于 MLE 来推导，但是不同于前面的算式基于 PDF (<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Probability_density_function">Probability
Density Function</a> ) 做 MLE 的推导，这里采用的 CDF(<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">Cumulative
Distribution Function</a>)，这里用了基于 CDF 的
MLE，推导过程如下图所示，从推导过程可知，这里的推导的假设也是<strong>“误差
<span class="math inline">\(\epsilon\)</span>
服从标准正态分布”</strong>，跟 MSE 一样</p>
<p><img data-src="https://wulc.me/imgs/OrdinalRegressionLoss.jpg" height="70%" width="70%" alt="Ordinal Regression 推导"></p>
<p>用来区分样本处于不同区间的 <span
class="math inline">\(\theta\)</span> 也是需要通过训练得到的参数，最终的
serving 输出也是依赖 <span class="math inline">\(\theta\)</span>
和具体的预估值， 这篇文章提供了一个实现参考：<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/482153702">处理分级问题的利器Ordinal
Regression</a></p>
<p>值得注意的是，对于评级任务，label
是比较明确的，但是对于一般的回归任务，还是存在着划分区间，或者说给每个
label 评级的过程，这个跟上面的 softmax 类方法一样，也是需要拍的超参</p>
<h2 id="小结">小结</h2>
<p>本文主要描述了回归任务中常见的一些损失函数，每种损失函数背后的假设不相同，其适用范围也不一样</p>
<p>MSE、MAE、Huber Loss
是回归中比较常用和直观的回归损失，其假设是预估值和真实 label
服从正态分布或拉普拉斯分布</p>
<p>Log-normal 损失函数则直接假设 label 服从 log-normal 分布，通过 MLE
去求解 log-normal 的概率密度函数中均值 <span
class="math inline">\(\mu\)</span> 和方差 <span
class="math inline">\(\sigma\)</span> 这两个参数（二分类中的 cross
entropy
的求解原理也是这样，只是分布改成了伯努利分布，参数改成了事件发生的概率
<span class="math inline">\(p\)</span>）</p>
<p>除了直接预估，也有通过转为分类来间接预估最终的值的，主要有 2
类方法（1）Weighted Logistics regression （2）Bucketing With Softmax</p>
<p>Weighted Logistics regression 通过把一个 label 为 <span
class="math inline">\(T\)</span> 的样本当做 <span
class="math inline">\(T\)</span>
个正样本（和一个负样本），在统计意义上推导保证了预估的无偏，在实际中应用也较为广泛，其背后的假设是
label 服从几何分布</p>
<p>Bucketing With Softmax 则是通过对 label 进行先验分桶，然后通过
softmax 预估 label
落在每个桶的概率分布，最后对每个桶进行概率加权求和得到最终的预估值，这种方法的好处是对分布没有任何假设，理论上适用所有的分布，但效果非常依赖分桶的数量和桶大小，其中也涉及
label smoothing、stacking 等改进技巧</p>
<p>Ordinal Regression 则是一种关心序但不关心绝对值误差的回归
loss，其假设跟 MSE 一样，但是使用了 CDF 而不是 PDF
来推导最终的损失，并通过训练得到分割不同级别的阈值，一般在评级任务中较为常用；但是在更一般的回归任务中，还是会依赖人工划分区间给予不同样本不同的评级</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A/" rel="tag"><i class="fa fa-tag"></i> 计算广告</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/19/A%20Survey%20of%20Multi-Domain/" rel="prev" title="A Survey of Multi-Domain">
                  <i class="fa fa-chevron-left"></i> A Survey of Multi-Domain
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/05/20/Practical%20Wisdom%20from%20%22The%20Almanack%20of%20Naval%20Ravikant%22:Wealth/" rel="next" title="Practical Wisdom from "The Almanack of Naval Ravikant":Wealth">
                  Practical Wisdom from "The Almanack of Naval Ravikant":Wealth <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-pen"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">良超</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.4/jquery.min.js" integrity="sha256-oP6HI9z1XaZNBrJURtCoUT5SUnxFr8s3BzRl+cbzUq8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
